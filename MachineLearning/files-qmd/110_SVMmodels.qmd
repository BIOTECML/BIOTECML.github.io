# M√°quinas de Vector Soporte (SVM) {#sec-110}

Las M√°quinas de Vector Soporte (*Support Vector Machines*, SVMs) es un algoritmo de clasificaci√≥n y regresi√≥n desarrollado en la d√©cada de los 90. Aunque inicialmente se desarroll√≥ como un m√©todo de clasificaci√≥n binaria, su aplicaci√≥n se ha extendido a problemas de clasificaci√≥n m√∫ltiple y regresi√≥n. SVMs ha resultado ser uno de los mejores clasificadores para un amplio abanico de situaciones, por lo que se considera uno de los referentes dentro del √°mbito del aprendizaje autom√°tico.

Las M√°quinas de Vector Soporte se fundamentan en los clasificadores marginales maximales que se obtienen a partir del concepto matem√°tico de hiperplano. Por ese motivo, para comprender el funcionamiento de los SVM se requieren conocimientos m√°s profundos de √°lgebra lineal y optimizaci√≥n de los utilizados hasta ahora. En este tema no estamos interesados en los aspectos formales m√°s matem√°ticos y por ello se recomienda el libro [Support Vector Machines Succinctly](https://www.syncfusion.com/succinctly-free-ebooks/support-vector-machines-succinctly) by Alexandre Kowalczyk para indagar m√°s.

Para entender mejor el funcionamiento de este algoritmo utilizaremos como ejemplos principales aquellos dedicados a tareas de clasificaci√≥n.

## Clasificadores de vector soporte {#sec-110.1}

En un espacio p-dimensional, un hiperplano se define como un subespacio plano y af√≠n de dimensiones $ùëù‚àí1$ . El t√©rmino af√≠n significa que el subespacio no tiene por qu√© pasar por el origen. En un espacio de dos dimensiones, el hiperplano es un subespacio de una dimensi√≥n, es decir, una recta. En un espacio tridimensional, un hiperplano es un subespacio de dos dimensiones, un plano convencional. Para dimensiones $ùëù>3$ no es intuitivo visualizar un hiperplano, pero el concepto de subespacio con $ùëù‚àí1$ dimensiones se mantiene.

Para mostrar el uso de los hiperplanos tomamos un ejemplo muy sencillo de un problema de clasificaci√≥n con dos clases en dos dimensiones cuyos puntos vienen dados por:

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/svm001.png){fig-align="center" width="400" height="300"}

### Casos separables linealmente {#sec-110.1.1}

Si la distribuci√≥n de las observaciones es tal que se pueden separar linealmente de forma perfecta en las dos clases, entonces, la definici√≥n matem√°tica de un hiperplano es bastante simple. En el caso de dos dimensiones, el hiperplano se describe acorde a la ecuaci√≥n de una recta:

$$\beta_0+\beta_1ùë•_1+\beta_2ùë•_2=0$$

Dados los par√°metros $\beta_0$ , $\beta_1$ y $\beta_2$, todos los pares de valores $ùê±=(ùë•_1,ùë•_2)$ para los que se cumple la igualdad son puntos del hiperplano. Esta ecuaci√≥n puede generalizarse para p-dimensiones:

$$\beta_0+\beta_1ùë•_1+\beta_2ùë•_2 +...+\beta_px_p=0$$

y de igual manera, todos los puntos definidos por el vector $(ùê±=ùë•_1,ùë•_2,...,ùë•_ùëù)$ que cumplen la ecuaci√≥n pertenecen al hiperplano.

Cuando $ùê±$ no satisface la ecuaci√≥n:

$$\beta_0+\beta_1ùë•_1+\beta_2ùë•_2 +...+\beta_px_p < 0$$

o bien

$$\beta_0+\beta_1ùë•_1+\beta_2ùë•_2 +...+\beta_px_p > 0$$

el punto $ùê±$ cae a un lado o al otro del hiperplano. As√≠ pues, se puede entender que un hiperplano divide un espacio p-dimensional en dos mitades. Para saber en qu√© lado del hiperplano se encuentra un determinado punto $ùê±$, solo hay que calcular el signo de la ecuaci√≥n.

La definici√≥n de hiperplano para casos perfectamente separables linealmente resulta en un n√∫mero infinito de posibles hiperplanos, lo que hace necesario un m√©todo que permita seleccionar uno de ellos como clasificador √≥ptimo. En este problema podemos considerar diferentes hiperplanos (en este caso rectas) que nos permiten clasificar la muestra de datos de forma adecuada como podemos ver en el gr√°fico siguiente:

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/svm002.png){fig-align="center" width="400" height="300"}

Tenemos tres rectas y posibles soluciones al problema planteado. La soluci√≥n a este problema consiste en seleccionar como clasificador √≥ptimo el hiperplano que se encuentra m√°s alejado de todas las observaciones de entrenamiento. A este se le conoce como *maximal margin hyperplane* o hiperplano √≥ptimo de separaci√≥n. Para identificarlo, se tiene que calcular la distancia perpendicular de cada observaci√≥n a un determinado hiperplano. La menor de estas distancias (conocida como margen) determina cu√°n alejado est√° el hiperplano de las observaciones de entrenamiento. As√≠ pues, el *maximal margin hyperplane* se define como el hiperplano que consigue un mayor margen, es decir, que la distancia m√≠nima entre el hiperplano y las observaciones es lo m√°s grande posible. Aunque esta idea suena razonable, no es posible aplicarla, ya que habr√≠a infinitos hiperplanos contra los que medir las distancias. En la imagen siguiente se muestra la soluci√≥n gr√°fica del algoritmo SVM para este problema:

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/svm003.png){fig-align="center" width="400" height="300"}

La imagen anterior muestra el *maximal margin hyperplane*, formado por el hiperplano (l√≠nea negra continua y su margen, las dos l√≠neas discontinuas). Las tres observaciones equidistantes respecto al *maximal margin hyperplane* que se encuentran a lo largo de las l√≠neas discontinuas se les conoce como vectores de soporte, ya que son vectores en un espacio p-dimensional y soportan (definen) el *maximal margin hyperplane*. Cualquier modificaci√≥n en estas observaciones (vectores soporte) conlleva cambios en el *maximal margin hyperplane*. Sin embargo, modificaciones en observaciones que no son vector soporte no tienen impacto alguno en el hiperplano.

### Casos no separables linealmente {#sec-110.1.2}

El *maximal margin hyperplane* descrito en el apartado anterior es una forma muy simple y natural de clasificaci√≥n siempre y cuando exista un hiperplano de separaci√≥n. En la gran mayor√≠a de casos reales, los datos no se pueden separar linealmente de forma perfecta, por lo que no existe un hiperplano de separaci√≥n y no puede obtenerse un *maximal margin hyperplane*. Para el siguiente ejemplo se emplea un set de datos publicado en el libro *Elements of Statistical Learning* que contiene observaciones simuladas con una funci√≥n no lineal en un espacio de dos dimensiones (2 predictores). El objetivo es entrenar un modelo SVM capaz de clasificar las observaciones.

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/svm004.png){fig-align="center" width="400" height="300"} Para solucionar estas situaciones, se puede extender el concepto de *maximal margin hyperplane* para obtener un hiperplano que "casi" separe las clases, pero permitiendo que se cometan unos pocos errores. A este tipo de hiperplano se le conoce como *Support Vector Classifier* o *Soft Margin*.

### Soft margin SVM {#sec-110.1.3}

El *Maximal Margin Classifier* descrito en la secci√≥n anterior tiene poca aplicaci√≥n pr√°ctica, ya que rara vez se encuentran casos en los que las clases sean perfecta y linealmente separables. De hecho, incluso cumpli√©ndose estas condiciones ideales, en las que exista un hiperplano capaz de separar perfectamente las observaciones en dos clases, esta aproximaci√≥n sigue presentando dos inconvenientes:

-   Dado que el hiperplano tiene que separar perfectamente las observaciones, es muy sensible a variaciones en los datos. Incluir una nueva observaci√≥n puede suponer cambios muy grandes en el hiperplano de separaci√≥n (poca robustez).

-   Que el *maximal margin hyperplane* se ajuste perfectamente a las observaciones de entrenamiento para separarlas todas correctamente suele conllevar problemas de *overfitting*.

Por estas razones, es preferible crear un clasificador basado en un hiperplano que, aunque no separe perfectamente las dos clases, sea m√°s robusto y tenga mayor capacidad predictiva al aplicarlo a nuevas observaciones (menos problemas de *overfitting*). Esto es exactamente lo que consiguen los clasificadores de vector soporte, tambi√©n conocidos como *soft margin classifiers* o *Support Vector Classifiers*. Para lograrlo, en lugar de buscar el margen de clasificaci√≥n m√°s ancho posible que consigue que las observaciones est√©n en el lado correcto del margen; se permite que ciertas observaciones est√©n en el lado incorrecto del margen o incluso del hiperplano.

La identificaci√≥n del hiperplano que clasifique correctamente la mayor√≠a de las observaciones a excepci√≥n de unas pocas, es un problema de optimizaci√≥n convexa. Si bien la demostraci√≥n matem√°tica queda fuera del objetivo de esta introducci√≥n, es importante mencionar que el proceso incluye un hiperpar√°metro llamado ùê∂. ùê∂ controla el n√∫mero y severidad de las violaciones del margen (y del hiperplano) que se toleran en el proceso de ajuste. Si ùê∂=‚àû , no se permite ninguna violaci√≥n del margen y por lo tanto, el resultado es equivalente al *Maximal Margin Classifier* (teniendo en cuenta que esta soluci√≥n solo es posible si las clases son perfectamente separables). Cuando m√°s se aproxima ùê∂ a cero, menos se penalizan los errores y m√°s observaciones pueden estar en el lado incorrecto del margen o incluso del hiperplano. ùê∂ es, a fin de cuentas, el hiperpar√°metro encargado de controlar el balance entre sesgo y varianza del modelo. En la pr√°ctica, su valor √≥ptimo se identifica mediante validaci√≥n cruzada.

El proceso de optimizaci√≥n tiene la peculiaridad de que solo las observaciones que se encuentran justo en el margen o que lo violan influyen sobre el hiperplano. A estas observaciones se les conoce como vectores soporte y son las que definen el clasificador obtenido. Esta es la raz√≥n por la que el par√°metro ùê∂ controla el balance entre sesgo y varianza. Cuando el valor de ùê∂ es peque√±o, el margen es m√°s ancho, y m√°s observaciones violan el margen, convirti√©ndose en vectores soporte. El hiperplano est√°, por lo tanto, sustentado por m√°s observaciones, lo que aumenta el sesgo pero reduce la varianza. Cuando mayor es el valor de ùê∂, menor el margen, menos observaciones son vectores soporte y el clasificador resultante tiene menor sesgo pero mayor varianza.

Otra propiedad importante que deriva de que el hiperplano dependa √∫nicamente de una peque√±a proporci√≥n de observaciones (vectores soporte), es su robustez frente a observaciones muy alejadas del hiperplano.

### L√≠mites de separaci√≥n no lineales {#sec-110.1.4}

El *Support Vector Classifier* descrito en el apartado anterior consigue buenos resultados cuando el l√≠mite de separaci√≥n entre clases es aproximadamente lineal. Si no lo es, su capacidad decae dr√°sticamente. Una estrategia para enfrentarse a escenarios en los que la separaci√≥n de los grupos es de tipo no lineal consiste en expandir las dimensiones del espacio original.

El hecho de que los grupos no sean linealmente separables en el espacio original no significa que no lo sean en un espacio de mayores dimensiones. Las im√°genes siguientes muestran dos grupos cuya separaci√≥n en dos dimensiones no es lineal, pero s√≠ lo es al a√±adir una tercera dimensi√≥n.

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/svm005.png){fig-align="center" width="400" height="400"}

El m√©todo de M√°quinas Vector Soporte (SVM) se puede considerar como una extensi√≥n del *Support Vector Classifier* obtenida al aumentar la dimensi√≥n de los datos. Los l√≠mites de separaci√≥n lineales generados en el espacio aumentado se convierten en l√≠mites de separaci√≥n no lineales al proyectarlos en el espacio original. Este algoritmo se estudiar√° con detalle en el cuaderno siguiente, ya que por el momento nos centramos en la aplicaci√≥n del *Support Vector Classifier* en problemas de clasificaci√≥n y regresi√≥n.

## M√°quinas de vector soporte {#sec-110.2}

Como hemos visto en la √∫ltima imagen es necesario expandir las dimensiones del problema en cuesti√≥n para poder obtener clasificadores basados en hiperplanos lineales. A continuaci√≥n, estudiamos los aspectos te√≥ricos correspondientes a las m√°quinas de vector de soporte en este tipo de situaciones.

La pregunta que nos queda por responder es ¬øc√≥mo aumentamos la dimensi√≥n del espacio y cual es la dimensi√≥n correcta que debemos utilizar? La dimensi√≥n de un conjunto de datos puede transformarse combinando o modificando cualquiera de sus dimensiones. Por ejemplo, se puede transformar un espacio de dos dimensiones en uno de tres aplicando la siguiente funci√≥n:

$$ùëì(ùë•_1,ùë•_2)=(ùë•_1^2,2\sqrt{x_1 x_2},ùë•_2^2)$$

Esta es solo una de las infinitas transformaciones posibles, ¬øc√≥mo saber cu√°l es la adecuada? Es aqu√≠ donde el concepto de kernel entra en juego. Un kernel (K) es una funci√≥n que devuelve el resultado del producto escalar entre dos vectores realizado en un nuevo espacio dimensional distinto al espacio original en el que se encuentran los vectores. Aunque no se ha entrado en detalle en las f√≥rmulas matem√°ticas empleadas para resolver el problema de optimizaci√≥n, esta contiene un producto escalar. Si se sustituye este producto escalar por un kernel, se obtienen directamente los vectores soporte (y el hiperplano) en la dimensi√≥n correspondiente al kernel. A esto se le suele conocer como *kernel trick* porque, con solo una ligera modificaci√≥n del problema original, se puede obtener el resultado para cualquier dimensi√≥n. A continuaci√≥n se muestran los m√°s utilizados. De ahora en adelante:

$$<x_i, x_j> = x_i^t x_j$$

representa el producto escalar entre $x_i$ y $x_j$.

### Kernel lineal {#sec-110.2.1}

El kernel viene definido por la expresi√≥n:

$$k(x_i, x_j) = x^t_i x_j,$$

que es simplemente el producto escalar del vector de caracter√≠sticas. Si se emplea un Kernel lineal, el clasificador que obtenemos es id√©ntico al que obten√≠amos en el cuaderno anterior sin el aumento de dimensiones.

### Kernel polin√≥mico {#sec-110.2.2}

El kernel viene definido por la expresi√≥n:

$$k(x_i, x_j) = (\gamma x^t_i x_j + \tau)^d.$$

Cuando se emplea $ùëë=1$ y $\tau = 0$, el resultado es el mismo que el de un kernel lineal. Si $ùëë>1$ , se generan l√≠mites de decisi√≥n no lineales, aumentando la no linealidad a medida que aumenta $ùëë$. No suele ser recomendable emplear valores de $ùëë$ mayores a 5 por problemas de sobreajuste. El valor de $\gamma$ controla el comportamiento del kernel.

### Kernel Gaussiano (RBF) {#sec-110.2.3}

El kernel viene definido por la expresi√≥n:

$$k(x_i, x_j) = exp(-\gamma||x-x^t||^2), \quad \gamma >0.$$

El valor de $\gamma$ controla el comportamiento del kernel, cuando es muy peque√±o, el modelo final es equivalente al obtenido con un kernel lineal. A medida que aumenta su valor, tambi√©n lo hace la flexibilidad del modelo.

### Kernel sigmoidal {#sec-110.2.4}

El kernel viene definido por la expresi√≥n:

$$k(x_i, x_j) = tanh(\gamma x^t_i x_j + \tau).$$

Los kernels descritos son solo unos pocos de los muchos que existen. Cada uno tiene una serie de hiperpar√°metros cuyo valor √≥ptimo puede encontrarse mediante validaci√≥n cruzada. No puede decirse que haya un kernel que supere al resto, depende en gran medida de la naturaleza del problema que se est√© tratando. Ahora bien, tal como indican los autores de A Practical Guide to Support Vector Classification, es muy recomendable probar el kernel RBF. Este kernel tiene dos ventajas: que solo tiene dos hiperpar√°metros que optimizar (ùõæy la penalizaci√≥n ùê∂ com√∫n a todos los SVM) y que su flexibilidad puede ir desde un clasificador lineal a uno muy complejo.

### SVM en problemas de regresi√≥n {#sec-110.2.5}

Las m√°quinas de vectores soporte (SVM) son bien conocidas en problemas de clasificaci√≥n. Sin embargo, el uso de SVM en regresi√≥n no est√° tan bien documentado. Este tipo de modelos se conoce como regresi√≥n de vectores de soporte (SVR).

En la mayor√≠a de los modelos de regresi√≥n lineal, el objetivo es minimizar la suma de errores al cuadrado. Tomemos como ejemplo los m√≠nimos cuadrados ordinarios (MCO). La funci√≥n objetivo para MCO con un predictor (caracter√≠stica) es la siguiente:

$$\underset{\beta}{min} \sum_{i=1}^n (y_i-\beta x_i)^2.$$

Lasso, Ridge y ElasticNet son extensiones de esta sencilla ecuaci√≥n, con un par√°metro de penalizaci√≥n adicional que pretende minimizar la complejidad y/o reducir el n√∫mero de caracter√≠sticas utilizadas en el modelo final. En cualquier caso, el objetivo -como ocurre con muchos modelos- es reducir el error del conjunto de pruebas.

Sin embargo, ¬øqu√© ocurre si s√≥lo nos preocupa reducir el error hasta cierto punto? ¬øY si no nos importa lo grandes que sean nuestros errores, siempre que est√©n dentro de un rango aceptable?

SVR nos da la flexibilidad de definir cu√°nto error es aceptable en nuestro modelo y encontrar una l√≠nea adecuada (o hiperplano en dimensiones superiores) para ajustarse a los datos.

En contraste con OLS, la funci√≥n objetivo de SVR se encarga de minimizar los coeficientes - m√°s espec√≠ficamente, la norma l2 del vector de coeficientes - no el error al cuadrado. El t√©rmino de error se maneja en las restricciones, donde se establece el error absoluto menor o igual a un margen especificado, llamado el error m√°ximo, $\epsilon$. Podemos ajustar epsilon para obtener la precisi√≥n deseada de nuestro modelo. Nuestra nueva funci√≥n objetivo y las restricciones son las siguientes:

$$\text{Funci√≥n: }\underset{\beta}{min} \quad \frac{1}{2} ||\mathbf{\beta}||^2 $$

$$\text{Restricci√≥n: } |y_i-\beta x_i| \leq \epsilon$$ ![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/svm006.png){fig-align="center" width="400" height="400"} Este algoritmo no est√° exento de problemas ya que aunque se resuelve la funci√≥n objetivo algunos de los puntos siguen quedando fuera de los m√°rgenes establecidos. Como tal, tenemos que tener en cuenta la posibilidad de errores que sean mayores que $\epsilon$. Esto se hace introduciendo variables de holgura.

El concepto de variables de holgura es sencillo: para cualquier valor que quede fuera de $\epsilon$, podemos denotar su desviaci√≥n del margen como $\xi$. Sabemos que estas desviaciones pueden existir, pero aun as√≠ nos gustar√≠a minimizarlas en la medida de lo posible. Por lo tanto, podemos a√±adir estas desviaciones a la funci√≥n objetivo:

$$\text{Funci√≥n: }\underset{\beta}{min} \quad \frac{1}{2} ||\mathbf{\beta}||^2 + C \sum_{i=1}^n |\xi|$$

$$\text{Restricci√≥n: } |y_i-\beta x_i| \leq \epsilon + |\xi|$$ ![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/svm007.png){fig-align="center" width="400" height="400"} Ahora tenemos un hiperpar√°metro adicional, C, que podemos ajustar. A medida que C aumenta, nuestra tolerancia para los puntos fuera de œµ tambi√©n aumenta. A medida que C se acerca a 0, la tolerancia se aproxima a 0 y la ecuaci√≥n colapsa en la simplificada (aunque a veces inviable).

Antes de comenzar con la implementaci√≥n de las SVM en `mlr3` vamos a cargar las librar√≠as necesarias:

```{r}
#| label: svm-001
#| message: false
#| results: false
#| warning: false

# Paquetes anteriores
library(tidyverse)
library(sjPlot)
library(knitr) # para formatos de tablas
library(skimr)
library(DataExplorer)
library(GGally)
library(gridExtra)
library(ggpubr)
library(cvms)
library(kknn)
theme_set(theme_sjplot2())

# Paquetes AA
library(mlr3verse)
library(mlr3tuning)
library(mlr3tuningspaces)
```

## M√°quinas de vector soporte en mlr3 {#sec-110.3}

Para implementar las m√°quinas de vector soporte en el paquete `mlr3` disponemos de dos funciones:

-   `regr.svm` para tareas de regresi√≥n.
-   `classif.svm` para tareas de clasificaci√≥n

que utilizan como base las funciones definidas en la librer√≠a `e1071`.

Podemos cargar los algoritmos con el c√≥digo siguiente:

```{r}
#| label: svm-002
#| message: false
#| warning: false

# Learner tarea de clasificaci√≥n
lsvm_classif = lrn("classif.svm")
# Learner tarea de regresi√≥n
lsvm_regr = lrn("regr.svm")
```

En este caso los hiperpar√°metros de ambos algoritmos no son los mismos aunque coinciden en la mayor√≠a. A continuaci√≥n se muestran todos ellos:

```{r}
#| label: svm-003
#| warning: false
#| message: false

# Hiperpar√°metros para SVM clasificaci√≥n
lsvm_classif$param_set$ids()
# Hiperpar√°metros para SVM regresi√≥n
lsvm_regr$param_set$ids()
```

Los par√°metros m√°s relevantes son:

-   `scale`: valor l√≥gico que indica si debemos estandarizar las variables.
-   `kernel`: que indica el kernel a utilizar (`linear`, `polynomial`, `radial` o rbf, y `sigmoid`). Por defecto se usa el `radial`.
-   `degree`: par√°metro $d$ del kernel polin√≥mico. Por defecto se utiliza el valor 3.
-   `gamma`: par√°metro $\gamma$ de todos los kernel salvo el lineal. Por defecto toma el valor $1/muestras$.
-   `coef0`: par√°metro $\tau$ de los kernel polinomial y sigmoidal. Por defecto toma el valor 0.
-   `cost`: par√°metro $C$ que representa el coste establecido por la violaci√≥n del contraste. Por defecto toma el valor 1.
-   `tolerance`: tolerancia para la finalizaci√≥n del algoritmo. Valor por defecto igual a 0.001.
-   `epsilon`: epsilon en la funci√≥n de p√©rdida. Por defecto toma el valor 0.1

## Bancos de datos {#sec-110.4}

Para ejemplificar el uso de estos algoritmos vamos a introducir los bancos de datos `stroke` y `penguins`. En este caso vamos a modificar el objetivo del banco de datos `penguins`, ya que cambiamos a una tarea de clasificaci√≥n donde estamos interesados en determinar el sexo del sujeto en funci√≥n del resto de predictoras. Vamos a cargar los datos y prepararlos para el an√°lisis definiendo las tareas correspondientes y el c√≥digo de preprocesado. Como estos algoritmos no permiten trabajar directamente con predictoras de tipo factor es necesario hacer una codificaci√≥n en el preprocesamiento.

### Stroke {#sec-110.4.1}

Seg√∫n la Organizaci√≥n Mundial de la Salud (OMS), el ictus es la segunda causa de muerte en el mundo, responsable de aproximadamente el 11% del total de fallecimientos. El banco de datos Stroke se utiliza para predecir si es probable que un paciente sufra un ictus en funci√≥n de los par√°metros de entrada como el sexo, la edad, diversas enfermedades y estatus de fumador. Cada fila de los datos proporciona informaci√≥n relevante sobre el paciente. El objetivo se encuentra en la variable `stroke` que puede tomar dos valores posibles. Hay valores perdidos en la variable `bmi`.

```{r}
#| label: svm-004
#| warning: false
#| message: false

# Leemos datos
stroke = read_rds("stroke.rds")
# creamos la tarea
tsk_stroke = as_task_classif(stroke, target = "stroke")
# informaci√≥n de la tarea
print(tsk_stroke)
```

Representamos la informaci√≥n contenida en la tarea

```{r}
#| label: svm-005
#| warning: false
#| message: false
#| fig-width: 10
#| fig-height: 14
#| fig-cap: "Representaci√≥n gr√°fica tarea stroke"

autoplot(tsk_stroke, type ="duo")
```

Generamos ahora el c√≥digo para el preprocesado de los datos. En este caso tenemos imputaci√≥n, estandarizaci√≥n y codificaci√≥n.

```{r}
#| label: svm-006
#| warning: false
#| message: false

pp_stroke = 
   po("scale", param_vals = list(center = TRUE, scale = TRUE)) %>>%
   po("imputemedian", affect_columns = selector_type("numeric")) %>>%
   po("encode", param_vals = list(method = "one-hot"))
```

Por √∫ltimo creamos la divisi√≥n de muestras:

```{r}
#| label: svm-007
#| warning: false
#| message: false

# Generamos variable de estrato
tsk_stroke$col_roles$stratum <- "stroke"
# Fijamos semilla para asegurar la reproducibilidad del modelo
set.seed(135)
# Creamos la partici√≥n
splits = mlr3::partition(tsk_stroke, ratio = 0.8)
# Muestras de entrenamiento y validaci√≥n
tsk_train_stroke = tsk_stroke$clone()$filter(splits$train)
tsk_test_stroke  = tsk_stroke$clone()$filter(splits$test)
```

### Penguins

El banco de datos ya ha sido descrito en detalle en temas anteriores, salvo por el hecho de que cambiamos a una tarea de clasificaci√≥n. En este caso tenemos valores perdidos en la respuesta y debemos eliminar dichas muestras.

```{r}
#| label: svm-008
#| warning: false
#| message: false

# Leemos datos
penguins = read_rds("penguins.rds")
# Seleccionamos observaciones missing
ids = which(is.na(penguins$sex) == TRUE)
data_penguins = penguins[-ids,]
# creamos la tarea
tsk_penguins = as_task_classif(data_penguins, target = "sex")
# informaci√≥n de la tarea
print(tsk_penguins)
```

Representamos la informaci√≥n contenida en la tarea

```{r}
#| label: svm-009
#| warning: false
#| message: false
#| fig-width: 10
#| fig-height: 14
#| fig-cap: "Representaci√≥n gr√°fica tarea penguins"

autoplot(tsk_penguins, type ="duo")
```

Generamos ahora el c√≥digo para el preprocesado de los datos. En este caso tenemos estandarizaci√≥n y codificaci√≥n.

```{r}
#| label: svm-010
#| warning: false
#| message: false

pp_penguins = 
   po("scale", param_vals = list(center = TRUE, scale = TRUE)) %>>%
   po("encode", param_vals = list(method = "one-hot"))
```

Por √∫ltimo creamos la divisi√≥n de muestras:

```{r}
#| label: svm-011
#| warning: false
#| message: false

# Generamos variable de estrato
tsk_penguins$col_roles$stratum <- "sex"
# Fijamos semilla para asegurar la reproducibilidad del modelo
set.seed(135)
# Creamos la partici√≥n
splits = mlr3::partition(tsk_penguins, ratio = 0.8)
# Muestras de entrenamiento y validaci√≥n
tsk_train_penguins = tsk_penguins$clone()$filter(splits$train)
tsk_test_penguins  = tsk_penguins$clone()$filter(splits$test)
```

## Nuestros primeros modelos {#sec-110.5}

En primer lugar consideramos modelos b√°sicos de SVM para los dos bancos de datos presentados. Trabajaremos con las opciones por defecto para poder comparar los resultados con el modelo optimizado que veremos posteriormente. Tambi√©n compararemos los resultados con otros modelos de clasificaci√≥n de los estudiados hasta ahora.

### Stroke {#sec-110.5.1}

En primer lugar generamos el graphlearner correspondiente a este banco de datos y entrenamos el algoritmo. Como ya estandarizamos los datos ponemos como false al par√°metro `scale`.

```{r}
#| label: svm-012
#| warning: false
#| message: false

# Definimos learner para predecir la probabilidad
lsvm_classif = lrn("classif.svm", scale = FALSE, predict_type = "prob")
# Graphlearner
gr = pp_stroke %>>% lsvm_classif
gr = GraphLearner$new(gr)
# Entrenamiento
gr$train(tsk_train_stroke)
```

Podemos estudiar el funcionamiento del algoritmo una vez obtenidas las predicciones tanto para la muestra de entrenamiento y validaci√≥n. En este caso consideramos diferentes m√©tricas para valorar la clasificaci√≥n obtenida.

```{r}
#| label: svm-013
#| warning: false
#| message: false

# Predicci√≥n de la muestra de entrenamiento y validaci√≥n
pred_train = gr$predict(tsk_train_stroke)
pred_test = gr$predict(tsk_test_stroke)
# scores de validaci√≥n
measures = msrs(c("classif.acc", "classif.bacc", "classif.bbrier", "classif.auc"))
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci√≥n
pred_test$score(measures)
```

Centr√°ndonos en la muestra de validaci√≥n, el porcentaje de clasificaci√≥n correcta parece indicar que el modelo propuesto es muy adecuado, sin embargo el porcentaje de clasificaci√≥n correcta ponderado nos indica que solo el 50% de los casos (ponderarlos por su peso en el banco de datos) ha sido clasificado correctamente. Esto ocurre en ocasiones cuando una de las categor√≠as de la respuesta est√° muy desequilibrada con respecto a la otra. De hecho, un efecto similar ocurre con le valor de AUC. Podemos estudiar con m√°s detalle este efecto mediante la matriz de confusi√≥n.

```{r}
#| label: svm-014
#| message: false
#| warning: false

# Cargamos la librer√≠a para representar la matriz de confusi√≥n
cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

En la tabla se aprecia claramente el efecto comentado anteriormente. Ninguna de las observaciones originales clasificadas con ictus es clasifica de forma correcta por el modelo (lo que provoca u porcentaje de clasificaci√≥n correcta ponderado del 50%). De hecho, podemos ver que le porcentaje de clasificaci√≥n correcta del 95.1% se corresponde √∫nicamente con las observaciones originales que no hab√≠an padecido ictus. Este porcentaje es as√≠ de alto por la diferencia de muestras en ambos grupos (972 para los que no han sufrido ictus, frente a los 50 que si lo han sufrido). Podemos concluir que el modelo no resulta efectivo ya que no es capaz de clasificar ninguna observaci√≥n del grupo de los que han sufrido ictus.

En los puntos siguientes vernos si podemos modificar el algoritmo para mejorar nuestros resultados. De momento vamos a compara los resultados con otros algoritmos de clasificaci√≥n como el clasificador Bayes y el algoritmo kNN con sus opciones por defecto. Para ello establecemos los modelos de aprendizaje y evaluamos el porcentaje de clasificaci√≥n correcta ponderada en ambas situaciones.

```{r}
#| label: svm-015
#| message: false
#| warning: false

# Learner na√Øve Bayes
lrn_nb = lrn("classif.naive_bayes", predict_type = "prob")
gr_nb = as_learner(pp_stroke %>>% lrn_nb)
# Learner kNN
lrn_knn = lrn("classif.kknn", scale = FALSE, predict_type = "prob")
gr_knn = as_learner(pp_stroke %>>% lrn_knn)
# Definimos modelo de remuestreo
remuestreo = rsmp("cv", folds = 10)
# Grid de comparaci√≥n de modelos
design = benchmark_grid(tsk_stroke, list(gr, gr_nb, gr_knn), remuestreo)
# Combinaci√≥n de soluciones
bmr = benchmark(design)
# Soluci√≥n agregada del criterio de validaci√≥n
bmr$aggregate(msr("classif.bacc"))
```

Aunque con le clasificador na√Øve bayes alcanzamos un 66% de clasificaci√≥n correcta, tambi√©n es cierto que con el kNN por defecto alcanzamos un valor similar que con SVM. Esto refuerza el hecho de que es necesario mejorar nuestro modelo, si es posible, ya que tenemos otro algoritmo con le que alcanzamos resultados mucho mejores.

### Penguins {#sec-110.5.2}

Analizamos ahora el banco de datos `penguins` comenzando por definir el graphlearner asociado.

```{r}
#| label: svm-016
#| warning: false
#| message: false

# Definimos learner para predecir la probabilidad
lsvm_classif = lrn("classif.svm", scale = FALSE, predict_type = "prob")
# Graphlearner
gr = pp_penguins %>>% lsvm_classif
gr = GraphLearner$new(gr)
# Entrenamiento
gr$train(tsk_train_penguins)
```

Analizamos ahora la validez el modelo:

```{r}
#| label: svm-017
#| warning: false
#| message: false

# Predicci√≥n de la muestra de entrenamiento y validaci√≥n
pred_train = gr$predict(tsk_train_penguins)
pred_test = gr$predict(tsk_test_penguins)
# scores de validaci√≥n
measures = msrs(c("classif.acc", "classif.bacc", "classif.bbrier", "classif.auc"))
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci√≥n
pred_test$score(measures)
```

En este caso tanto el porcentaje de clasificaci√≥n correcta como el ponderado son muy similares y superiores al 90% indicando que el modelo funciona relativamente bien para clasificar las posibles respuestas en funci√≥n de las predictoras consideradas. tanto el score de brier como e AUC proporcionan resultados satisfactorios. Veamos por √∫ltimo la matriz de confusi√≥n:

```{r}
#| label: svm-018
#| message: false
#| warning: false

# Cargamos la librer√≠a para representar la matriz de confusi√≥n
cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

En este caso tenemos un 7.5% de porcentaje de error de clasificaci√≥n par la muestra de validaci√≥n, pero no tenemos el efecto inadecuado del modelo como en el banco de datos `stroke`.

Como en el caso anterior vamos a comparar los resultados de este algoritmo con otros utilizados anteriormente.

```{r}
#| label: svm-019
#| message: false
#| warning: false

# Learner na√Øve Bayes
lrn_nb = lrn("classif.naive_bayes", predict_type = "prob")
gr_nb = as_learner(pp_penguins %>>% lrn_nb)
# Learner kNN
lrn_knn = lrn("classif.kknn", scale = FALSE, predict_type = "prob")
gr_knn = as_learner(pp_penguins %>>% lrn_knn)
# Definimos modelo de remuestreo
remuestreo = rsmp("cv", folds = 10)
# Grid de comparaci√≥n de modelos
design = benchmark_grid(tsk_penguins, list(gr, gr_nb, gr_knn), remuestreo)
# Combinaci√≥n de soluciones
bmr = benchmark(design)
# Soluci√≥n agregada del criterio de validaci√≥n
bmr$aggregate(msr("classif.bacc"))
```

En este caso el resultado obtenido con el algoritmo SVM es superior a los otros dos algoritmos. Con el kNN hay una diferencia de un 2%, pero con respecto a Na√Øve Bayes hay una mejora del 22% en el porcentaje de clasificaci√≥n correcta. El algoritmo funciona bien y tan solo nos queda afinarlo un poco para tratar de mejorar si es posible dicho porcentaje.

## Optimizando los modelos {#sec-110.6}

Para el proceso de optimizaci√≥n de los modelos vamos a considerar √∫nicamente los par√°metros `cost` y `gamma`, ya que fijaremos el kernel de tipo `radial`. Para favorecer la convergencia del algoritmo de optimizaci√≥n consideramos la transformaci√≥n de los par√°metros num√©ricos mediante la funci√≥n logaritmo, ya que de esta forma se reduce el espacio de b√∫squeda. Adem√°s indicamos `type = "C-classification"` para indicar al proceso de optimizaci√≥n que estamos en un problema de clasificaci√≥n.

### Stroke {#sec-110.6.1}

Recuperamos y adaptamos el c√≥digo que ya estudiamos en el tema [-@sec-60].

```{r}
#| label: svm-022
#| message: false
#| warning: false
#| results: hide

# Algoritmo de aprendizaje definiendo el espacio de b√∫squeda
lsvm_classif = lrn("classif.svm", scale = FALSE, predict_type = "prob",
                    kernel = "radial",
                    gamma = to_tune(1e-04, 10000, logscale = TRUE),
                    cost = to_tune(1e-04, 10000, logscale = TRUE),
                    type = "C-classification")
# Proceso de aprendizaje
gr =  pp_stroke %>>% lsvm_classif
gr = GraphLearner$new(gr)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci√≥n fijando el n√∫mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_stroke,
  learner = gr,
  resampling = rsmp("cv", folds = 5),
  measures = msr("classif.bacc"),
  term_evals = 15
)
```

Veamos los resultados obtenidos:

```{r}
#| label: svm-023
#| message: false
#| warning: false

# Veamos si converge el algoritmo
instance$is_terminated
# Resultados del proceso de optimizaci√≥n
instance$result_learner_param_vals[c("classif.svm.gamma", "classif.svm.cost")]
# Valor de la m√©trica para resultado √≥ptimo
instance$result_y
```

El proceso de optimizaci√≥n ha finalizado encontrando los valores √≥ptimos de `gamma` y `cost`, pero sin embargo el porcentaje de clasificaci√≥n ponderado sigue siendo del 50%. Esto muestra que este algoritmo no resulta v√°lido para resolver este problema de clasificaci√≥n.

### Penguins {#sec-110.6.2}

Procedemos ahora con el banco de datos penguins. Tomamos las mismas opciones de configuraci√≥n que en el problema anterior.

```{r}
#| label: svm-024
#| message: false
#| warning: false
#| results: hide

# Algoritmo de aprendizaje definiendo el espacio de b√∫squeda
lsvm_classif = lrn("classif.svm", scale = FALSE, predict_type = "prob",
                    kernel = "radial",
                    gamma = to_tune(1e-04, 10000, logscale = TRUE),
                    cost = to_tune(1e-04, 10000, logscale = TRUE),
                    type = "C-classification")
# Proceso de aprendizaje
gr =  pp_penguins %>>% lsvm_classif
gr = GraphLearner$new(gr)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci√≥n fijando el n√∫mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_penguins,
  learner = gr,
  resampling = rsmp("cv", folds = 10),
  measures = msr("classif.bacc"),
  term_evals = 20
)
```

Veamos los resultados obtenidos:

```{r}
#| label: svm-025
#| message: false
#| warning: false

# Veamos si converge el algoritmo
instance$is_terminated
# Resultados del proceso de optimizaci√≥n
instance$result_learner_param_vals[c("classif.svm.gamma", "classif.svm.cost")]
# Valor de la m√©trica para resultado √≥ptimo
instance$result_y
```

De nuevo el porcentaje de clasificaci√≥n ponderado para la configuraci√≥n √≥ptima de hiperpar√°metros es del 92% (similar a la configuraci√≥n por defecto), lo que demuestra que el algoritmo es bastante robusto frente a cambios en dichos valores y resulta adecuado para este problema.

## SVM para tareas de regresi√≥n {#sec-110.7}

Aunque el algoritmo SVM se puede utilizar para resolver tareas de regresi√≥n no es los habitual. Si se desea utilizar este algoritmo con b√∫squeda √≥ptima tan solo hay que modificar el par√°metro `type` con el valor `eps-regression`.

## Ejercicios {#sec-110.8}

1.  Ajustar un modelo de aprendizaje autom√°tico basado en un modelo SVM para el banco de datos `Mushroom`[-@sec-mushroom].
2.  Ajustar un modelo de aprendizaje autom√°tico basado en un modelo SVM para el banco de datos `Water potability`[-@sec-waterpot].
3.  Ajustar un modelo de aprendizaje autom√°tico basado en un modelo SVM para el banco de datos `Hepatitis`[-@sec-hepatitis].
4.  Ajustar un modelo de aprendizaje autom√°tico basado en un modelo SVM para el banco de datos `Abalone`[-@sec-abalone].
