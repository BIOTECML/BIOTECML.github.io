# Modelos Boosting {#sec-140}

El *boosting* es una t茅cnica de modelado de conjunto que intenta construir un *strong learner* a partir de un n煤mero de *weak learner* secuenciales, todos basados en el mismo algoritmo de predicci贸n o clasificaci贸n. El proceso de construcci贸n del *strong learner* comienza fijando un modelo inicial sobre los datos de entrenamiento y obteniendo los errores de dicho modelo (errores de predicci贸n o clasificaci贸n). A continuaci贸n, se construye un segundo modelo que intenta corregir los errores presentes en el primer modelo mediante la asignaci贸n de pesos a todos los datos de entrenamiento en funci贸n del error cometido en la primera etapa. Este procedimiento contin煤a y se a帽aden modelos hasta que se predice correctamente todo el conjunto de datos de entrenamiento o se a帽ade el m谩ximo n煤mero de modelos. Finalmente se combinan los resultados de los diferentes modelos secuenciales para obtener el modelo final. Los m茅todos de *boosting* m谩s empleados son ***AdaBoost***, ***Gradient Boosting***, ***XGBoost*** y ***LightGBM***. Casi todos ellos toman como *weak learner* basado en 谩rboles de decisi贸n, pero en teor铆a se pueden utilizar con otro tipo de algoritmos de aprendizaje autom谩tico. A continuaci贸n podemos ver una imagen del proceso secuencial de construcci贸n para un problema de clasificaci贸n:

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/boosting-1.png){fig-align="center" width="550" height="400"}

Antes de pasar a describir los diferentes modelos debemos entender el algoritmo del descenso del gradiente que nos permite construir modelos secuenciales de forma m谩s o menos autom谩tica. Las diferentes versiones de este algoritmo nos proporciona los diferentes modelos de boosting que estudiaremos.

## Descenso del gradiente {#sec-140.1}

El t茅rmino ***boosting*** proviene del principal algoritmo que se utiliza como base para el ajuste de los diferentes modelos secuenciales que conforman este tipo de soluci贸n. Dicho algoritmo es el conocido como **algoritmo de descenso del gradiente** (*gradient descent algorithm*) y es muy utilizado en los algoritmos de optimizaci贸n usados para minimizar funciones de coste o p茅rdida. Su aplicaci贸n se encuentra muy extendida en diferentes algoritmos de aprendizaje autom谩tico.

Dada una funci贸n de p茅rdida $f$ que depende del par谩metro $\theta$ , y fijada una tasa de aprendizaje $\lambda$, el algoritmo del descenso del gradiente se estructura de la forma siguiente:

1.  Fijar un valor inicial $\theta_0$ para el par谩metro de inter茅s.

2.  Obtener la derivada parcial de la funci贸n con respecto a $\theta$, y evaluarla en $\theta_0$ (gradiente descendiente):

$$\bigtriangledown f_{\theta_0} = \left[\frac{\partial f}{\partial \theta} \right ]_{\theta = \theta_0}$$

3.  Calcular un nuevo valor del par谩metro mediante el "descenso" del valor anterior a partir del gradiente obtenido y la tasa de aprendizaje:

$$\theta_1 = \theta_0 - \lambda *\bigtriangledown f_{\theta_0}$$

4.  Comprobar si el cambio en la actualizaci贸n del par谩metro es inferior a un fijado previamente (llamada criterio de parada), de forma que si es afirmativo el algoritmo se detiene, y en caso contrario se actualiza el gradiente y se pasa a un nuevo valor de $\theta$.

Este algoritmo se puede generalizar a situaciones m谩s generales con m煤ltiples par谩metros. Sin embargo, la mayor dificultad estriba en que no tenemos asegurado alcanzar el m铆nimo absoluto de la funci贸n $f$, ya que cuando el algoritmo encuentra un m铆nimo relativo resulta imposible salir de dicho punto.

Por su modo de implementaci贸n, el descenso del gradiente puede realizarse de tres modos diferentes para la funci贸n de p茅rdida establecida en cada uno de los algoritmos de aprendizaje autom谩tico:

-   **Descenso de gradiente por lotes**. Este es un tipo de descenso de gradiente que procesa todas las muestras de entrenamiento en cada iteraci贸n del descenso de gradiente. Si el n煤mero de muestras de entrenamiento es grande, el descenso de gradiente por lotes es computacionalmente muy caro.

-   **Descenso de gradiente estoc谩stico**. Este es un tipo de descenso de gradiente que procesa una muestra de entrenamiento por iteraci贸n. Por lo tanto, los par谩metros se actualizan incluso despu茅s de cada iteraci贸n. Por lo tanto, es bastante m谩s r谩pido que el descenso de gradiente por lotes. Pero de nuevo, cuando el n煤mero de muestras de entrenamiento es grande, incluso entonces se procesa s贸lo una muestra que puede ser una sobrecarga adicional para el sistema ya que el n煤mero de iteraciones ser谩 bastante grande.

-   **Descenso de gradiente en mini lotes**. Este es un tipo de descenso de gradiente que funciona m谩s r谩pido que el descenso de gradiente por lotes y el descenso de gradiente estoc谩stico. Aqu铆 se procesan $b$ muestras por iteraci贸n donde $b$ es inferior al tama帽o de la muestra de entrenamiento. As铆, aunque el n煤mero de muestras de entrenamiento sea grande, se procesa en lotes de tama帽o $b$ de una sola vez. Por lo tanto, funciona para los ejemplos de entrenamiento m谩s grandes y tambi茅n con un menor n煤mero de iteraciones.

## Algorirmos Boosting {#sec-140.2}

A continuaci贸n se muestran los algoritmos principales que hacen uso del descenso de gradiente para los modelos de conjunto. Casi todos ellos utilizan como modelo base los 谩rboles de decisi贸n. Como en el caso de los algoritmos de *bagging* mostraremos los aspectos te贸ricos de cada uno de ellos aplicados a un problema de clasificaci贸n, aunque se pueden generalizar a los problemas de regresi贸n de forma inmediata. Para finalizar con cada algoritmo se presenta la funci贸n en mlr3 que nos permite el ajuste de dicho modelo.

### AdaBoost {#sec-140.2.1}

Fue el primer algoritmo en hacer uso del *boosting* en los algoritmos de aprendizaje autom谩tico. Si estamos interesados en un problema de clasificaci贸n con dos grupos posibles necesitamos como punto de partida:

1.  Un *weak learner* que sea capaz de predecir la variable respuesta con un porcentaje de acierto ligeramente superior a lo esperado por azar.

2.  Codificar las dos clases de la variable respuesta como +1 y -1.

3.  Un peso inicial e igual para todas las observaciones que forman el *set* de entrenamiento.

Una vez que estos tres puntos se han establecido, se inicia un proceso iterativo. En la primera iteraci贸n, se ajusta el *weak learner* empleando los datos de entrenamiento y los pesos iniciales (todos iguales). Con el *weak learner* ajustado y almacenado, se predicen las observaciones de entrenamiento y se identifican aquellas bien y mal clasificadas. Con esta informaci贸n:

-   Se actualizan los pesos de las observaciones, disminuyendo el de las que est谩n bien clasificadas y aumentando el de las mal clasificadas.

-   Se asigna un peso total al *weak learner*, proporcional al total de aciertos. Cuantos m谩s aciertos consiga el *weak learner*, mayor su influencia en el conjunto del *ensemble*.

En la siguiente iteraci贸n, se llama de nuevo al *weak learner* y se vuelve a ajustar, esta vez, empleando los pesos actualizados en la iteraci贸n anterior. El nuevo *weak learner* se almacena, obteniendo as铆 un nuevo modelo para el conjunto. Este proceso se repite $M$ veces, generando un total de $M$ *weak learners*. Para clasificar nuevas observaciones, se obtiene la predicci贸n de cada uno de los *weak learners* que forman el conjunto y se agregan sus resultados, ponderando el peso de cada uno acorde al peso que se le ha asignado en el ajuste. El objetivo detr谩s de esta estrategia es que cada nuevo *weak learner* se centra en predecir correctamente las observaciones que los anteriores no han sido capaces. A continuaci贸n se muestra la estructura del algoritmo.

Consideramos $y$ la variable respuesta, $X$ el conjunto de variables predictoras, $N$ n煤mero de muestras de entrenamiento, $M$ n煤mero de iteraciones de aprendizaje, $G_m$ *weak learner* en la iteraci贸n $m$, $w_i$ peso de la observaci贸n $i$, y $\alpha_m$ el peso del *weak learner* $m$, de forma que el algoritmo viene dado por:

1.  Inicializamos los pesos de las observaciones

$$w_i = \frac{1}{N}, \quad i=1,...,N$$

2.  Para $m=1$ hasta $M$:

-   Ajustar el *weak learner* $G_m$ utilizando las muestras de entrenamiento y los pesos $w_i$, para obtener la predicci贸n $\hat{y}_i$ de cada $y_i$

-   Calcular el error del *weak learner* como:

$$err_m = \frac{\sum_{i=1}^N w_i I(y_i \neq \hat{y}_i)}{\sum_{i=1}^N w_i}$$

-   Calcular el peso asignado al *weak learner* $G_m$:

$$\alpha_m = log\left(\frac{1-err_m}{err_m}\right)$$

-   Actualizar los pesos de las observaciones:

$$w_i = w_i exp[\alpha_m I(y_i \neq \hat{y}_i)], \quad i=1,...,N$$

3.  Construcci贸n del *strong learner* agregando todos los *weak learner* obtenidos en el proceso iterativo ponder谩ndolos por su peso:

$$G(x) = sign\left[\sum_{m=1}^M \alpha_m G_m(X)\right]$$

### Gradient Boosting {#sec-140.2.2}

El *gradient boosting* o refuerzo del gradiente es uno de los algoritmos de aprendizaje autom谩tico m谩s populares. Es lo suficientemente potente como para encontrar cualquier relaci贸n no lineal entre el objetivo del modelo y las variables predictoras, y tiene una gran facilidad de uso ya que nos permite trabajar con valores perdidos, valores at铆picos y valores categ贸ricos de alta cardinalidad sin ning煤n tratamiento especial. De forma habitual este tipo de algoritmos toman como *weak learner* los 谩rboles de decisi贸n lo que provoca que muchas veces este algoritmo se conoce como ***gradient boosting tree***. Se basa en un proceso de boosting donde la actualizaci贸n en cada iteraci贸n se realiza mediante el algoritmo del descenso del gradiente.

De forma sencilla el *gradient boosting* ajusta un primer *weak learner* $f_1$ con el que se predice la variable respuesta $y$, obteni茅ndose los errores $yf_1(x)$. A continuaci贸n, se ajusta un nuevo modelo $_2$, que intenta predecir los residuos del modelo anterior, en otras palabras, trata de corregir los errores que ha hecho el modelo $_1$:

$$f_1(x) \sim y,\qquad f_2(x) \sim y-f_1(x)$$

En la siguiente iteraci贸n, se calculan los residuos de los dos modelos de forma conjunta $_1()_2()$, los errores cometidos por $_1$ y que $_2$ no ha sido capaz de corregir, y se ajusta un tercer modelo $_3$ para tratar de corregirlos:

$$f_3(x) \sim y-f_1(x)-f_2(x)$$

Este proceso se repite $M$ veces, de forma que cada nuevo modelo minimiza los errores del anterior, y construimos el strong learner como:

$$y \sim f_1(x) + f_2(x) + ... + f_M(x)$$

Dado que el objetivo de *Gradient Boosting* es ir minimizando los residuos iteraci贸n a iteraci贸n, es susceptible de sobreajuste. Una forma de evitar este problema es emplear la tasa de aprendizaje ($\lambda$) sobre cada *weak learner* en el proceso de *boosting*, de forma que el predictor final viene dado por:

$$F(x) = \lambda f_1(x) + \lambda f_2(x) + ... + \lambda f_M(x)$$ \### Gradient Boosting en mlr3 {#sec-140.2.4}

### XGBoost {#sec-140.2.3}

El algoritmo m谩s famoso que utiliza como base el *gradient boosting* es el *extreme gradient boosting* (*XGBosst*) que estudiamos a continuaci贸n. Todos los aspectos t茅cnicos de este algoritmo se pueden consultar [aqu铆](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).

Antes de comentar las diferencias existentes entre *gradient boosting* y *extreme gradient boosting* veamos cuales son su puntos en com煤n:

-   Algoritmos basados en 谩rboles: tanto *XGBoost* como *Gradient Boosting* utilizan 谩rboles de decisi贸n como estimadores base.

-   Objetivo de predicci贸n: los 谩rboles se construyen utilizando los residuos, no las etiquetas de clase reales. Por lo tanto, a pesar de que nos centramos en problemas de clasificaci贸n, los estimadores base de estos algoritmos son 谩rboles de regresi贸n y no 谩rboles de clasificaci贸n. Esto se debe a que los residuos son continuos y no discretos. Al mismo tiempo, sin embargo, algunas de las f贸rmulas que se presentan a continuaci贸n son 煤nicas para la clasificaci贸n, as铆 que no podemos asumir su aplicaci贸n exactamente igual a los problemas de regresi贸n.

-   Profundidad del 谩rbol: ambos algoritmos permiten controlar el tama帽o m谩ximo de los 谩rboles para minimizar el riesgo de sobreajuste de los datos.

-   M茅todos de conjunto: similares a *Random Forest* o *AdaBoost*, estos algoritmos construyen muchos 谩rboles en el proceso. Al final, la predicci贸n final se basa en todos los 谩rboles.

-   Tasa de aprendizaje: el valor de cada 谩rbol se escala por la tasa de aprendizaje. Esto permite que el algoritmo tenga una mejora m谩s gradual y constante en cada paso.

A continuaci贸n se muestra una imagen resumen del funcionamiento de *XGBoost*:

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/XGBoost.png){fig-align="center" width="550" height="400"} Las diferencias entre ambos algoritmos se basan en la construcci贸n de los diferentes 谩rboles de decisi贸n secuenciales.

*Gradient Boosting* utiliza un m茅todo est谩ndar para construir 谩rboles de regresi贸n, en el que se utiliza una m茅trica t铆pica como el MSE (error cuadr谩tico medio) u otra similar para determinar la mejor divisi贸n del 谩rbol. El algoritmo calcula el MSE para cada una de las posibles divisiones de nodos y luego elige la que tenga el menor MSE como la que se utilizar谩 en el 谩rbol.

Por el contrario, *XGBoost* utiliza su propio m茅todo de construcci贸n de 谩rboles en el que la puntuaci贸n de similitud y la ganancia determinan las mejores divisiones de nodos. La puntuaci贸n de similitud o *similarity score*(SS) se define como:

$$SS = \frac{\left(\sum_{i=1}^n r_i \right)^2}{\sum_{i=1}^n  \left[pp_i(1-pp_i)\right] + \lambda}$$

donde:

-   $r_i$ son los residuos o diferencia entre el valor observado actual y el valor predicho.

-   $pp_i$ es la probabilidad previa o probabilidad de un evento calculada en un paso anterior. Se supone que la probabilidad inicial es de 0.5 para cada observaci贸n, que se utiliza para construir el primer 谩rbol. Para cualquier 谩rbol posterior, la probabilidad anterior se recalcula bas谩ndose en la predicci贸n inicial y en las predicciones de todos los 谩rboles anteriores.

-   $\lambda$ es un par谩metro de regularizaci贸n. Su aumento reduce desproporcionadamente la influencia de las hojas peque帽as (las que tienen pocas observaciones) mientras que s贸lo tiene un impacto menor en las hojas m谩s grandes (las que tienen muchas observaciones).

Una vez que tenemos la puntuaci贸n de similitud para cada hoja, calculamos la ganancia (*gain*) utilizando la siguiente f贸rmula:

$$Gain = SS_{i} + SS_{d} - SS_{r},$$

donde $SS_i$, $SS_d$, y $SS_r$ son las puntuaciones de similitud de la divisi贸n de la rama izquierda, de la divisi贸n de la rama derecha, y el nodo ra铆z del que parten ambas ramas respectivamente. La divisi贸n del nodo con la mayor ganancia se elige como la mejor divisi贸n del 谩rbol.

La introducci贸n de $\lambda$ en el proceso de evaluaci贸n de SS es la principal diferencia con *gradient boosting* ya que en este 煤ltimo el SS se calcula sin a帽adir ese t茅rmino de regularizaci贸n.

Adem谩s de utilizar su propia manera de construir y podar 谩rboles, *XGBoost* tambi茅n tiene varias optimizaciones incorporadas para hacer el entrenamiento m谩s r谩pido cuando se trabaja con conjuntos de datos enormes. He aqu铆 algunas de las principales:

-   Algoritmo *Greedy* aproximado - utiliza cuantiles ponderados cuando busca la mejor divisi贸n de nodos en lugar de evaluar cada divisi贸n posible.

-   Aprendizaje paralelo: puede dividir los datos en conjuntos de datos m谩s peque帽os para ejecutar procesos en paralelo.

-   *Sparsity-Aware Split Finding* - cuando tiene algunos datos perdidos, calcula *Gain* poniendo las observaciones con valores perdidos en la hoja izquierda. A continuaci贸n, hace lo mismo coloc谩ndolas en la hoja de la derecha y elige el escenario que produce una mayor ganancia.

-   Acceso consciente del efectivo - *XGBoost* utiliza la memoria cach茅 de la CPU para almacenar los gradientes y as铆 poder calcular las puntuaciones de similitud m谩s r谩pidamente.

### LightGBM {#sec-140.2.4}

*LightGBM*, abreviatura de ***light gradient-boosting machine***, es un algoritmo que toma como base el *gradient boosting*, y que fue desarrollado originalmente por Microsoft. Utiliza como modelo de partida los 谩rboles de decisi贸n y se utiliza para la clasificaci贸n y otras tareas de aprendizaje autom谩tico cuando el conjunto de la muestra de entrenamiento es muy grande. Sus puntos fuertes son la mejora en el rendimiento y la escalabilidad. *LightGBM* ampl铆a el algoritmo de *gradient boosting* a帽adiendo un tipo de selecci贸n autom谩tica de predictoras centr谩ndose en la evoluci贸n del algoritmo hacia las ramas de 谩rbol de decisi贸n con mayores gradientes. Esto puede dar lugar a una gran aceleraci贸n del entrenamiento y a una mejora del rendimiento predictivo.

*LightGMB* posee muchas de las ventajas de *XGBoost* como la optimizaci贸n dispersa, el entrenamiento paralelo, las funciones de p茅rdida m煤ltiples, la regularizaci贸n, el *bagging* y la detenci贸n temprana. Una de las principales diferencias entre ambos algoritmos es la construcci贸n de los 谩rboles. *LightGBM* no construye un 谩rbol por niveles, fila por fila, como hacen la mayor铆a de las implementaciones, sino que lo hace por hojas. Adem谩s, *LightGBM* no utiliza el algoritmo de aprendizaje de 谩rbol de decisi贸n basado en la ordenaci贸n, que busca el mejor punto de divisi贸n en valores de caracter铆sticas ordenados, como hacen *XGBoost* u otras implementaciones. En su lugar, *LightGBM* implementa un algoritmo de aprendizaje de 谩rbol de decisi贸n basado en un histograma altamente optimizado, que ofrece grandes ventajas tanto en eficiencia como en consumo de memoria. El algoritmo *LightGBM* utiliza dos t茅cnicas novedosas llamadas *Gradient-Based One-Side Sampling* (GOSS) y *Exclusive Feature Bundling* (EFB) que permiten que el algoritmo se ejecute m谩s r谩pidamente manteniendo un alto nivel de precisi贸n.

El muestreo unilateral basado en el gradiente, o GOSS por sus siglas en ingl茅s, es una modificaci贸n del m茅todo *gradient boosting* que centra la atenci贸n en aquellas muestras de entrenamiento que dan lugar a un gradiente mayor, lo que a su vez acelera el aprendizaje y reduce la complejidad computacional del m茅todo.

La agrupaci贸n de rasgos exclusivos, o EFB por sus siglas en ingl茅s, es un m茅todo para agrupar rasgos dispersos (en su mayor铆a nulos) mutuamente excluyentes, como los niveles de variables categ贸ricas que han sido codificadas mediante *hot-encoding*. Como tal, es un tipo de selecci贸n autom谩tica de caracter铆sticas.

Juntos, estos dos modificaciones dentro del algoritmo de gradiente *boosting* pueden acelerar el tiempo de entrenamiento del algoritmo hasta 20 veces.

## Algortimos Boosting en mlr3 {#sec-140.3}

A continuaci贸n se muestran las funciones principales para la obtenci贸n de los algoritmos de boosting presentados.

### AdaBoost {#sec-140.3.1}

Por el momento este algoritmo solo se encuentra disponible para tareas de clasificaci贸n mediante la funci贸n `classif.AdaBoostM1` del paquete `mlr3extralearners`. Para su uso es necesario tener instalada la librer铆a `RWeka`. Los hiperpar谩metros m谩s relevantes para este modelo son:

-   `P`: Porcentaje de peso masa en el que basar el entrenamiento. Por defecto toma el valor `100`.
-   `Q`: Si se usa remuestreo para el boosting. Por defecto el valor es `False`
-   `S`: Semilla aleatoria. Por defecto toma el valor `1`.
-   `I`: N煤mero de iteraciones. Por defecto se establece el valor `10`.
-   `W`: Tipo de weak learner utiliza como modelo de base. Por defecto se usan 谩rboles de decisi贸n.

### Gradient Boosting {#sec-140.3.2}

Los algoritmos principales para gradient boosting en mlr3 son `classif.gbm` para las tareas de clasificaci贸n, y `regr.gbm` para tareas de regresi贸n, que se encuentran disponibles en el paquete `mlr3extralearners`. Para poder utilizarlos es necesario tener instalada la librer铆a `gbm`.

Los par谩metros m谩s relevantes para ambas funciones son:

-   `distribution`: cadena de caracteres que especifica el nombre de la distribuci贸n a utilizar o una lista con un nombre de componente que especifica la distribuci贸n y cualquier par谩metro adicional necesario. Para tareas de clasificaci贸n las opciones disponibles son `bernoulli` (target con respuestas 0-1), `adaboost` (utiliza la funci贸n de p茅rdida exponencial de Adaboost para variables 0-1), `huberized` (funci贸n de p茅rdida de huber para variables 0-1), `multinomial` (para respuestas tipo factor.). Por defecto se utiliza el valor `bernouilli`. Para tareas de regresi贸n las opciones disponibles son `gaussian` (donde se utiliza la funci贸n de p茅rdida cuadr谩tica), `laplace` (funci贸n de p茅rdida del valor absoluto), `poisson` (para respuestas que son conteos), y `tdist` (para usar la funci贸n de p茅rdida basada en la distribuci贸n t). la opci贸n por defecto es `gaussian`.
-   `n.tress`: n煤mero de 谩rboles de decisi贸n utilizados como weak learner. Por defecto es valor es `100`.
-   `interaction.depth`: N煤mero entero que especifica la profundidad m谩xima de cada 谩rbol. El valor por defecto es `1`.
-   `n.minobsinnode`: N煤mero m铆nimo de observaciones en los nodos terminales en los 谩rboles de decisi贸n. El valor por defecto es `10`.
-   `shrinkage`: Tasa de aprendizaje. Por defecto toma el valor `0.001`.
-   `bag.fraction`: fracci贸n de las observaciones del conjunto de entrenamiento seleccionadas aleatoriamente para proponer el siguiente 谩rbol de la expansi贸n. Valor por defecto igual a 0.5
-   `train.fraction`: Las primeras observaciones de `train.fraction*nrows(data)` se utilizan para ajustar el gbm y el resto se utiliza para calcular estimaciones fuera de muestra de la funci贸n de p茅rdida. El valor por defecto es `1`.
-   `cv.folds`: N煤mero de validaciones cruzadas consideradas. Por defecto se toma el valor `0`.
-   `n.cores`: N煤mero de procesadores utilizados. Por defecto se toma el valor `1`.

### XGBoost {#sec-140.3.3}

Los algoritmos principales para XGBoost (extrem gradient boosting) en mlr3 son `classif.xgboost` para las tareas de clasificaci贸n, y `regr.xgboost` para tareas de regresi贸n, que se encuentran disponibles en el paquete `mlr3extralearners`. Para poder utilizarlos es necesario tener instalada la librer铆a `xgboost`. La mayor dificultad con estas dos funciones es la gran cantidad de hiperpar谩metros disponibles para su ajuste. Los m谩s interesantes son:

-   `eta`: que controla la tasa de aprendizaje, y que por defecto es igual a `0.3`.
-   `gamma`: reducci贸n m铆nima de p茅rdida requerida para realizar una partici贸n adicional en un nodo de hoja del 谩rbol. Cuanto mayor, m谩s conservador ser谩 el algoritmo. El valor por defecto es 0.
-   `lambda`: par谩metro de regularizaci贸n. El valor por defecto es `1`.

### LightGBM {#sec-140.3.4}

Los algoritmos principales para LightGBM en mlr3 son `classif.lightgbm` para las tareas de clasificaci贸n, y `regr.lightgbm` para tareas de regresi贸n, que se encuentran disponibles en el paquete `mlr3extralearners`. Para poder utilizarlos es necesario tener instalada la librer铆a `lightgbm`. Este algoritmo tiene una cantidad inmensa de hiperpar谩metros que nos describiremos. Se puede consultar este [enlace](https://mlr3extralearners.mlr-org.com/reference/mlr_learners_classif.lightgbm.html). Tal vez uno de los m谩s relevantes es `early_stopping` que os permite indicar si debemos realizar parada temprana para evitar sobre ajuste.

::: {.callout-note appearance="simple" title="Clasificaci贸n binaria"}
**Dada la gran cantidad de hiperpar谩metros que involucran la mayor铆a de estos modelos en las aplicaciones que presentamos a continuaci贸n utilizaremos solo las opciones por defecto, sin b煤squeda del 贸ptimo.**
:::

## Aplicaciones {#sec-140.4}

En este apartado vamos a utilizar los bancos de datos del tema anterior para ejemplificar el uso de los algoritmos de boosting. Antes de presentar los bancos de datos de nuevo, cargamos todas las librer铆as necesarias as铆 como las necesarias para los diferentes algoritmos de boosting.

```{r}
#| label: boosting-001
#| message: false
#| results: false
#| warning: false

# Paquetes anteriores
library(tidyverse)
library(sjPlot)
library(knitr) # para formatos de tablas
library(skimr)
library(DataExplorer)
library(GGally)
library(gridExtra)
library(ggpubr)
library(cvms)
library(kknn)
library(rpart.plot)
theme_set(theme_sjplot2())

# Paquetes AA
library(mlr3verse)
library(mlr3tuning)
library(mlr3tuningspaces)
library(gbm)
library(RWeka)
library(xgboost)
library(lightgbm)
```

### Bancos de datos {#sec-140.4.1}

Para ejemplificar el uso de los modelos de bagging b谩sicos vamos a utilizar tres bancos de datos: `Stroke`, `Water Potability`, y `Housing in California` que se pueden consultar en el tema [-@sec-40]. De los tres con el 煤nico con el que no hemos trabajado hasta ahora es `Water Potability`. A continuaci贸n se muestra el c贸digo necesario para la carga de cada uno de esos bancos de datos, y la creaci贸n de la tarea correspondiente. Los dos primeros corresponden a problemas de clasificaci贸n mientras que el 煤ltimo se corresponde con un problema de regresi贸n.

#### Stroke

El c贸digo para este banco de datos aparece a continuaci贸n. Para poder ejecutar todos los modelos debemos convertir la respuesta en variable 1-0.

```{r}
#| label: boosting-002
#| warning: false
#| message: false

# Leemos datos
stroke = read_rds("stroke.rds")
# Eliminamos la variable id
stroke = stroke %>% dplyr::select(-id)
# creamos la tarea
tsk_stroke = as_task_classif(stroke, target = "stroke", positive ="Yes")
# Generamos variable de estrato
tsk_stroke$col_roles$stratum <- "stroke"
```

#### Water Potability

El c贸digo para crear la tares es:

```{r}
#| label: boosting-003
#| warning: false
#| message: false

# Leemos datos
waterpot = read_rds("waterpot.rds")
# creamos la tarea
tsk_water = as_task_classif(waterpot, target = "Potability", positive="1")
# Generamos variable de estrato
tsk_water$col_roles$stratum <- "Potability"
```

#### Housing in California

Cargamos los datos correspondientes:

```{r}
#| label: boosting-004
#| warning: false
#| message: false

# Carga de datos
housingCA = read_rds("housingCA.rds")
# Creaci贸n de task
tsk_housing = as_task_regr(housingCA, target = "median_house_value")
```

### Modelos {#sec-140.4.2}

Puesto que los modelos a considerar quedan todos englobados dentro de los algoritmos de boosting, vamos a dise帽ar un an谩lisis conjunto sobre todos ellos haciendo uso de las funciones `benchmark()` y `benchmark_grid()`. Para cada banco de datos estableceos los diferentes modelos de aprendizaje y seleccionamos el que mejor funciona en cada caso. Para ello hemos de tener en cuenta que no todos ellos se pueden utilizar. Por ejemplo xgboost no permite predictoras de tipo factor con lo que una forma de abordar esa situaci贸n es codificando los factores para tener solo variables num茅ricas. En nuestro caso vamos a considerar los algoritmos de `AdaBoost`, `XGBoost`, y `lightGBM` para problemas de clasificaci贸n, y `GBM`, `XGBoost`, y `lightGBM` para problemas de regresi贸n.

#### Stroke

Al tratarse de un problema de clasificaci贸n podemos considerar los cuatro algoritmos presentados. A continuaci贸n se detalla el c贸digo para poder implantarlos. En cada uno de ellos se consideran las tareas de preprocesamiento correspondientes. En este caso consideramos todas las tareas para poder comparar con los resultado que proporcionar铆an otro tipo de modelos. En este caso el modelo `classif.gbm` no se puede ajustar, aunque se muestra el c贸digo correspondiente.

```{r}
#| label: boosting-005
#| warning: false
#| message: false

# Preprocesamiento
pp_stroke =  
   po("scale", param_vals = list(center = TRUE, scale = TRUE)) %>>%
   po("imputemedian", affect_columns = selector_type("numeric")) %>>%
   po("encode", param_vals = list(method = "one-hot"))
 

# Modelo de aprendizaje AdaBoost
# ==============================
lrn1 = lrn("classif.AdaBoostM1")
stroke_adaboost = as_learner(pp_stroke %>>% lrn1)

# Modelo de aprendizaje XGBoost
# =======================================
lrn2 = lrn("classif.xgboost")
stroke_xgboost = as_learner(pp_stroke %>>% lrn2)

# Modelo de aprendizaje lightGBM
# =======================================
lrn3 = lrn("classif.lightgbm")
stroke_lightgbm = as_learner(pp_stroke %>>% lrn3)
```

Definimos el proceso de remuestreo necesario para la combinaci贸n de modelos y el proceso de combinaci贸n de todos los modelos definidos:

```{r}
#| label: boosting-006
#| message: false
#| warning: false
#| results: hide

set.seed(321)
# Remuestreo
remuestreo = rsmp("cv", folds = 10)
# Grid
design = benchmark_grid(tsk_stroke, 
                        list(stroke_adaboost, stroke_xgboost, stroke_lightgbm), 
                        remuestreo)
# Combinaci贸n de soluciones
bmr = benchmark(design)
```

Podemos ver ahora los resultados obtenidos con cada algoritmo:

```{r}
#| label: boosting-007
#| message: false
#| warning: false
#| results: hide

autoplot(bmr, measure = msr("classif.bacc"))
```

Tanto el modelo `xgboost` como `lightgbm` son os que proporcionan mejores resultados del porcentaje de clasificaci贸n ponderado. En cualquier caso los resultados no mejoran significativamente los resultados obtenidos hasta ahora para este banco de datos. Vamos a intentar optimizar el modelo `xgboost` utilizando el espacio de b煤squeda definido el la librer铆a `mlr3tuningspaces` pero integr谩ndolo manualmente. Para ello utilizamos los valores de b煤squeda definidos en https://mlr3tuningspaces.mlr-org.com/reference/mlr_tuning_spaces_default.html.

```{r}
#| label: boosting-008
#| message: false
#| warning: false
#| results: hide

boost_classif_stroke = lrn("classif.xgboost", 
                         eta = to_tune(1e-04, 1, logscale = TRUE),
                         nrounds = to_tune(1,5000),
                         max_depth = to_tune(1,20),
                         colsample_bytree = to_tune(0.1,1),
                         colsample_bylevel = to_tune(0.1,1),
                         lambda = to_tune(1e-03, 1000, logscale = TRUE),
                         alpha = to_tune(1e-03, 1000, logscale = TRUE),
                         subsample = to_tune(0.1, 1)  
                         )
gr_stroke =  pp_stroke %>>% boost_classif_stroke
gr_stroke = GraphLearner$new(gr_stroke)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci贸n fijando el n煤mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_stroke,
  learner = gr_stroke,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.bacc"),
  term_evals = 50
)
```

Podemos ver el resultado del proceso de optimizaci贸n con:

```{r}
#| label: boosting-009
#| message: false
#| warning: false

instance$result$classif.bacc
```

El porcentaje de clasificaci贸n correcta ha alcanzado el 53.85 que es el valor m谩s alto obtenido hasta ahora para este banco de datos. Podemos analizar con m谩s detalle este modelo:

```{r}
#| label: boosting-010
#| message: false
#| warning: false

# Modelo de aprendizaje
boost_classif_stroke = lrn("classif.xgboost", 
                         eta = instance$result_x_domain$classif.xgboost.eta,
                         nrounds = instance$result_x_domain$classif.xgboost.nrounds,
                         max_depth = instance$result_x_domain$classif.xgboost.max_depth,
                         colsample_bytree = instance$result_x_domain$classif.xgboost.colsample_bytree,
                         colsample_bylevel = instance$result_x_domain$classif.xgboost.colsample_bylevel,
                         lambda = instance$result_x_domain$classif.xgboost.lambda,
                         alpha = instance$result_x_domain$classif.xgboost.alpha,
                         subsample = instance$result_x_domain$classif.xgboost.subsample  
                         )
gr_stroke =  pp_stroke %>>% boost_classif_stroke
gr_stroke = GraphLearner$new(gr_stroke)

# Divisi贸n de muestras
set.seed(432)
splits = mlr3::partition(tsk_stroke, ratio = 0.8)
tsk_train_stroke = tsk_stroke$clone()$filter(splits$train)
tsk_test_stroke  = tsk_stroke$clone()$filter(splits$test)

# Entrenamiento del modelo
gr_stroke$train(tsk_train_stroke)
```

Obtenemos ahora las predicciones del modelo y la matriz de confusi贸n

```{r}
#| label: boosting-011
#| message: false
#| warning: false

# Predicci贸n de la muestra de entrenamiento y validaci贸n
pred_train = gr_stroke$predict(tsk_train_stroke)
pred_test = gr_stroke$predict(tsk_test_stroke)
# scores de validaci贸n
measures = msr("classif.bacc")
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci贸n
pred_test$score(measures)
# matriz de confusi贸n
cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

El porcentaje de clasificaci贸n sobre la muestra de validaci贸n es del 53.55%. El resultado de la matriz de confusi贸n es algo superior al de otros modelos propuestos anteriormente. Adem谩s, en este caso si clasificamos algunas muestras con `stroke` dado que originalmente proven铆an de ese grupo.

Para finalizar el estudio de este modelo es necesario estudiar la validez de la soluci贸n y valorar la curva de aprendizaje correspondiente:

```{r}
#| label: boosting-012
#| message: false
#| warning: false

# Fijamos semilla
set.seed(135)
# Definimos proceso de validaci贸n cruzada kfold con k=10
resamp = rsmp("cv", folds = 10)
# Remuestreo
rr = resample(tsk_stroke, gr_stroke, resamp, store_models=TRUE)
# An谩lisis de los valores obtenidos con los scores definidos anteriormente
skim(rr$score(measures))
```

El valor medio del porcentaje de clasificaci贸n correcta ponderada se sit煤a en el 53.9% con una desviaci贸n t铆pica del 3%. Aunque los resultados son bastante estables el rango de valores abarca modelos con peores porcentajes que los vistos anteriormente. Por 煤ltimo representamos la curva de aprendizaje.

```{r}
#| label: boosting-013
#| echo: false
#| message: false
#| warning: false

# Funci贸n que nos permite obtener los valores asociados a la curva de aprendizaje
learningcurve = function(task, learner, score, ptr, rpeats)
{
  # Par谩metros de la funci贸n
  # task: tarea
  # learner: algoritmo de aprendizaje
  # score: nombre del score a utilizar
  # ptr: vector con las proporciones de tama帽os de muestra de entrenamiento
  # rpeats: n煤mero de repeticiones para cada proporci贸n de tama帽o de muestra de entrenamiento
  
  # Definimos los scores para cada conjunto de muestra
  mtrain = msr(score, predict_sets = "train")
  mtest = msr(score, predict_sets = "test")
  # Configuramos el learner para que evalu茅 los scores en la muestra de validaci贸n y test
  learner$predict_sets = c("train", "test")
  # Incicializamos vector de scores agregados para la muestra de entrenamiento y validaci贸n
  sco_train = c()
  sco_test = c()
  for(i in 1:length(ptr))
  {
    # estructura de muestreo: 5 repeticiones con porcentaje muestra entrenamiento ptr[i]
    subsam = rsmp("subsampling", repeats = rpeats, ratio = ptr[i])
    # ejecuci贸n de remuestreo
    rr = resample(task, learner, subsam)
    sco_train[i] = rr$aggregate(mtrain)
    sco_test[i] = rr$aggregate(mtest)
  }
  # Matriz de resultados
  res = data.frame(ptr, sco_train, sco_test)
  resdf = res %>% pivot_longer(!ptr, names_to = "Sample", values_to = "MSR")
  return(resdf)
}
```

```{r}
#| label: boosting-014
#| message: false
#| warning: false

ptr = seq(0.1, 0.9, 0.1)
lcurve = learningcurve(tsk_stroke, gr_stroke, "classif.bacc", ptr = ptr, rpeats = 10)
datos = lcurve[lcurve$Sample=="sco_test",]
ggplot(datos, aes(ptr, MSR)) + 
    geom_line() +
    labs(x ="Proporci贸n tama帽o muestra entrenamiento", y = "% Clasificaci贸n correcta") +
    scale_x_continuous(breaks=ptr)
```

Podemos ver que el mejor valor para el porcentaje de clasificaci贸n correcta en la muestra de validaci贸n se alcanza para un tama帽o de muestra de entrenamiento del 70%.

#### Water Potability

Como en el caso anterior comenzamos definiendo todos los modelos. En este caso todas las predictoras son de tipo num茅rico.

```{r}
#| label: boosting-015
#| warning: false
#| message: false

# Preprocesado
pp_water = po("scale", param_vals = list(center = TRUE, scale = TRUE)) %>>%
  po("imputemedian", affect_columns = selector_type("numeric"))
# Modelo de aprendizaje AdaBoost
# ==============================
lrn1 = lrn("classif.AdaBoostM1")
water_adaboost = as_learner(pp_water %>>% lrn1)

# Modelo de aprendizaje XGBoost
# =======================================
lrn2 = lrn("classif.xgboost")
water_xgboost = as_learner(pp_water %>>% lrn2)

# Modelo de aprendizaje lightGBM
# =======================================
lrn3 = lrn("classif.lightgbm")
water_lightgbm = as_learner(pp_water %>>% lrn3)
```

Definimos el proceso de remuestreo necesario para la combinaci贸n de modelos y el proceso de combinaci贸n de todos los modelos definidos:

```{r}
#| label: boosting-016
#| message: false
#| warning: false
#| results: hide

set.seed(321)
# Remuestreo
remuestreo = rsmp("cv", folds = 10)
# Grid
design = benchmark_grid(tsk_water, 
                        list(water_adaboost, water_xgboost, water_lightgbm), 
                        remuestreo)
# Combinaci贸n de soluciones
bmr = benchmark(design)
```

Podemos ver ahora los resultados obtenidos con cada algoritmo:

```{r}
#| label: boosting-017
#| message: false
#| warning: false
#| results: hide

autoplot(bmr, measure = msr("classif.bacc"))
```

Para este conjunto de datos el algoritmo `lightgbm` es claramente mejor que los otros con un porcentaje medio de clasificaci贸n ponderada por encima del 60%. Este resultado es mucho mejor que los obtenidos hasta ahora para este conjunto de datos. Analizamos los resultados de este modelo introduciendo el proceso de optimizaci贸n para la tasa de aprendizaje:

```{r}
#| label: boosting-018
#| message: false
#| warning: false
#| results: hide

boost_classif_water = lrn("classif.lightgbm", 
                         learning_rate = to_tune(1e-04, 10, logscale = TRUE)
                         )
gr_water =  as_learner(pp_water %>>% boost_classif_water)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci贸n fijando el n煤mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_water,
  learner = gr_water,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.bacc"),
  term_evals = 50
)
```

Podemos ver el resultado del proceso de optimizaci贸n con:

```{r}
#| label: boosting-019
#| message: false
#| warning: false

instance$result$classif.bacc
```

El porcentaje de clasificaci贸n correcta ha alcanzado el 61.405 que es el valor m谩s alto obtenido hasta ahora para este banco de datos. Podemos analizar con m谩s detalle este modelo:

```{r}
#| label: boosting-020
#| message: false
#| warning: false

# Modelo de aprendizaje
boost_classif_water = lrn("classif.lightgbm", 
                         learning_rate = instance$result_x_domain$classif.lightgbm.learning_rate
                         )
gr_water =  as_learner(pp_stroke %>>% boost_classif_water)

# Divisi贸n de muestras
set.seed(432)
splits = mlr3::partition(tsk_water, ratio = 0.8)
tsk_train_water = tsk_water$clone()$filter(splits$train)
tsk_test_water  = tsk_water$clone()$filter(splits$test)

# Entrenamiento del modelo
gr_water$train(tsk_train_water)
```

Obtenemos ahora las predicciones del modelo y la matriz de confusi贸n

```{r}
#| label: boosting-021
#| message: false
#| warning: false

# Predicci贸n de la muestra de entrenamiento y validaci贸n
pred_train = gr_water$predict(tsk_train_water)
pred_test = gr_water$predict(tsk_test_water)
# scores de validaci贸n
measures = msr("classif.bacc")
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci贸n
pred_test$score(measures)
# matriz de confusi贸n
cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

El porcentaje de clasificaci贸n sobre la muestra de validaci贸n es del 61.35%. El resultado de la matriz de confusi贸n es superior al de otros modelos propuestos anteriormente. En este caso el modelo ya es capaz de detectar muestras como potables, lo que no ocurr铆a con otros modelos. De hecho, el 15.4% de las muestras que eran potables son clasificadas como tales con el modelo propuesto. Sin embargo, a煤n tenemos un 23.6% de muestras que originalmente eran potables y que nuestro modelo no es capaz de clasificar correctamente.

Estudiamos la validez de la soluci贸n y la curva de aprendizaje.

```{r}
#| label: boosting-022
#| message: false
#| warning: false

# Fijamos semilla
set.seed(135)
# Definimos proceso de validaci贸n cruzada kfold con k=10
resamp = rsmp("cv", folds = 10)
# Remuestreo
rr = resample(tsk_water, gr_water, resamp, store_models=TRUE)
# An谩lisis de los valores obtenidos con los scores definidos anteriormente
skim(rr$score(measures))
```

El valor medio del porcentaje de clasificaci贸n correcta ponderada se sit煤a en el 60.8% con una desviaci贸n t铆pica del 3%. En este caso el valor m铆nimo se sit煤a en el 55.8% mostrando un mejor comportamiento que el resto de modelos utilizados sobre este banco de datos. Por 煤ltimo representamos la curva de aprendizaje.

```{r}
#| label: boosting-023
#| message: false
#| warning: false

ptr = seq(0.1, 0.9, 0.1)
lcurve = learningcurve(tsk_water, gr_water, "classif.bacc", ptr = ptr, rpeats = 10)
datos = lcurve[lcurve$Sample=="sco_test",]
ggplot(datos, aes(ptr, MSR)) + 
    geom_line() +
    labs(x ="Proporci贸n tama帽o muestra entrenamiento", y = "% Clasificaci贸n correcta") +
    scale_x_continuous(breaks=ptr)
```

En este caso el porcentaje de clasificaci贸n correcta no deja de aumentar conforme aumenta la muestra de tama帽o de entrenamiento. Esto puede significar un problema de sobreajuste ya que cuanto m谩s aumentamos mejor resultado tenemos. Como en el banco de datos anterior podr铆amos utilizar un porcentaje del 70% o 80%.

#### Housing in California

Veamos ahora el an谩lisis del modelo de regresi贸n. En este caso consideramos los modelos `gbm`, `xgboost`, y `ligthgbm`. Comenzamos definiendo los modelos de aprendizaje.

```{r}
#| label: boosting-024
#| warning: false
#| message: false

# Preprocesado
pp_housing = 
   po("scale", param_vals = list(center = TRUE, scale = TRUE)) %>>%
   po("imputemedian", affect_columns = selector_type("numeric")) %>>%
   po("encode", param_vals = list(method = "one-hot"))

# Modelo de aprendizaje AdaBoost
# ==============================
lrn1 = lrn("regr.gbm")
housing_gbm = as_learner(pp_housing %>>% lrn1)

# Modelo de aprendizaje XGBoost
# =======================================
lrn2 = lrn("regr.xgboost")
housing_xgboost = as_learner(pp_housing %>>% lrn2)

# Modelo de aprendizaje lightGBM
# =======================================
lrn3 = lrn("regr.lightgbm")
housing_lightgbm = as_learner(pp_housing %>>% lrn3)
```

Definimos el proceso de remuestreo necesario para la combinaci贸n de modelos y el proceso de combinaci贸n de todos los modelos definidos:

```{r}
#| label: boosting-025
#| message: false
#| warning: false
#| results: hide

set.seed(321)
# Remuestreo
remuestreo = rsmp("cv", folds = 10)
# Grid
design = benchmark_grid(tsk_housing, 
                        list(housing_gbm, housing_xgboost, housing_lightgbm), 
                        remuestreo)
# Combinaci贸n de soluciones
bmr = benchmark(design)
```

Podemos ver ahora los resultados obtenidos con cada algoritmo, utilizando el score `smape`:

```{r}
#| label: boosting-026
#| message: false
#| warning: false
#| results: hide

autoplot(bmr, measure = msr("regr.smape"))
```

El modelo con un menor valor de `smape` es `lightgbm`, seguido muy cerca por `gbm`. De nuevo, analizamos este modelo buscando el 贸ptimo de la tasa de aprendizaje.

```{r}
#| label: boosting-027
#| message: false
#| warning: false
#| results: hide

boost_regr_housing = lrn("regr.lightgbm", 
                         learning_rate = to_tune(1e-04, 10, logscale = TRUE)
                         )
gr_housing =  as_learner(pp_housing %>>% boost_regr_housing)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci贸n fijando el n煤mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_housing,
  learner = gr_housing,
  resampling = rsmp("cv", folds = 3),
  measures = msr("regr.smape"),
  term_evals = 50
)
```

Podemos ver el resultado del proceso de optimizaci贸n con:

```{r}
#| label: boosting-028
#| message: false
#| warning: false

instance$result$regr.smape
```

El valor de `smape` obtenido es el m谩s bajo de todos los modelos propuestos hasta ahora, por lo que vamos a estudiar con algo m谩s de detalle este modelo:

```{r}
#| label: boosting-029
#| message: false
#| warning: false

# Modelo de aprendizaje
boost_regr_housing = lrn("regr.lightgbm", 
                         learning_rate = instance$result_x_domain$regr.lightgbm.learning_rate
                         )
gr_housing =  as_learner(pp_housing %>>% boost_regr_housing)

# Divisi贸n de muestras
set.seed(432)
splits = mlr3::partition(tsk_housing, ratio = 0.8)
tsk_train_housing = tsk_housing$clone()$filter(splits$train)
tsk_test_housing  = tsk_housing$clone()$filter(splits$test)

# Entrenamiento del modelo
gr_housing$train(tsk_train_housing)
```

Obtenemos ahora las predicciones del modelo y evaluamos los scores sobre la muestra de entrenamiento y validaci贸n.

```{r}
#| label: boosting-030
#| message: false
#| warning: false

# Predicci贸n de la muestra de entrenamiento y validaci贸n
pred_train = gr_housing$predict(tsk_train_housing)
pred_test = gr_housing$predict(tsk_test_housing)
# scores de validaci贸n
measures = msr("regr.smape")
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci贸n
pred_test$score(measures)
```

El resultado sigue siendo bastante bueno para la muestra de validaci贸n. Finalizamos con el an谩lisis de validaci贸n y la representaci贸n de la curva de aprendizaje.

Estudiamos la validez de la soluci贸n y la curva de aprendizaje.

```{r}
#| label: boosting-031
#| message: false
#| warning: false

# Fijamos semilla
set.seed(135)
# Definimos proceso de validaci贸n cruzada kfold con k=10
resamp = rsmp("cv", folds = 10)
# Remuestreo
rr = resample(tsk_housing, gr_housing, resamp, store_models=TRUE)
# An谩lisis de los valores obtenidos con los scores definidos anteriormente
skim(rr$score(measures))
```

El valor medio del `smape` se sit煤a en el 0.157 con una desviaci贸n t铆pica del 0.001. El rango de scores es muy estrecho indicando que la soluci贸n es muy estable. Por 煤ltimo representamos la curva de aprendizaje.

```{r}
#| label: boosting-032
#| message: false
#| warning: false

ptr = seq(0.1, 0.9, 0.1)
lcurve = learningcurve(tsk_housing, gr_housing, "regr.smape", ptr = ptr, rpeats = 10)
datos = lcurve[lcurve$Sample=="sco_test",]
ggplot(datos, aes(ptr, MSR)) + 
    geom_line() +
    labs(x ="Proporci贸n tama帽o muestra entrenamiento", y = "sMAPE") +
    scale_x_continuous(breaks=ptr)
```

Como ocurr铆a con el banco de datos anterior, el `smape` se reduce cuando incrementa el tama帽o de muestra de entrenamiento. Podemos tener alg煤n problema de sobreaprendizaje que deber铆amos controlar para evitar resultados sesgados. Un tama帽o del 70% parece una buena opci贸n.

## Ejercicios {#sec-140.5}

1.  Ajustar un modelo de aprendizaje autom谩tico basado en bosques aleatorios para el banco de datos `Mushroom`[-@sec-mushroom].
2.  Ajustar un modelo de aprendizaje autom谩tico basado en bosques aleatorios para el banco de datos `Hepatitis`[-@sec-hepatitis].
3.  Ajustar un modelo de aprendizaje autom谩tico basado en bosques aleatorios para el banco de datos `Abalone`[-@sec-abalone].
4.  Ajustar un modelo de aprendizaje autom谩tico basado en bosques aleatorios para el banco de datos `Us economic time series`[-@sec-usaets].
5.  Ajustar un modelo de aprendizaje autom谩tico basado en bosques aleatorios para el banco de datos `QSAR`[-@sec-qsar].
