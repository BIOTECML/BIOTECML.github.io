<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MachineLearning - 15&nbsp; Análisis discriminante (AD)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./160_PrinCompmodels.html" rel="next">
<link href="./04_NonSupervisedAA.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos coincidentes",
    "search-copy-link-title": "Copiar enlace para buscar",
    "search-hide-matches-text": "Ocultar coincidencias adicionales",
    "search-more-match-text": "más coincidencia en este documento",
    "search-more-matches-text": "más coincidencias en este documento",
    "search-clear-button-title": "Limpiar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Entregar"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Análisis discriminante (AD)</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MachineLearning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Prefacio</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./01_IntroCourse.html" class="sidebar-item-text sidebar-link">Parte 1. Introducción</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_introAD.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción al análisis de datos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20_introAA.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introducción al Aprendizaje Automático (AA)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./30_RandRstudio.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introducción a R y RStudio</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./40_DataBases.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bases de datos</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./02_FirstStepsAA.html" class="sidebar-item-text sidebar-link">Parte 2. Primeros pasos</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./50_AED.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introducción al análisis de datos</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./03_SupervisedAA.html" class="sidebar-item-text sidebar-link">Parte 3. Aprendizaje supervisado</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./60_RegressionModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Modelos de Regresión</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./70_LogisticModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Modelos de Regresión Logística</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./80_SurvivalModels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Modelos de supervivencia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./90_BayesianClassif.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Modelos de clasificación Naïve Bayes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100_kNNmodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Modelo de los k vecinos más cercanos (kNN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./110_SVMmodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Máquinas de Vector Soporte (SVM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./120_DTmodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Árboles de decisiónn (DT)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./130_Ensemblemodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Modelos de conjunto (Ensemble models)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./140_Boostingmodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Modelos Boosting</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./04_NonSupervisedAA.html" class="sidebar-item-text sidebar-link">Parte 4. Aprendizaje no supervisado</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./150_Discriminantmodels.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Análisis discriminante (AD)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./160_PrinCompmodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Componentes principales (CP)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./170_MDSmodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Métodos de escalado multidimensional (MDS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./180_Clustermodels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Análisis cluster</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">Referencias</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#sec-150.1" id="toc-sec-150.1" class="nav-link active" data-scroll-target="#sec-150.1"><span class="toc-section-number">15.1</span>  Planteamiento del problema</a></li>
  <li><a href="#sec-150.2" id="toc-sec-150.2" class="nav-link" data-scroll-target="#sec-150.2"><span class="toc-section-number">15.2</span>  AD lineal</a>
  <ul>
  <li><a href="#sec-150.2.1" id="toc-sec-150.2.1" class="nav-link" data-scroll-target="#sec-150.2.1"><span class="toc-section-number">15.2.1</span>  Fundamentos</a></li>
  <li><a href="#sec-150.2.2" id="toc-sec-150.2.2" class="nav-link" data-scroll-target="#sec-150.2.2"><span class="toc-section-number">15.2.2</span>  Estimación de parámetros</a></li>
  <li><a href="#sec-150.2.3" id="toc-sec-150.2.3" class="nav-link" data-scroll-target="#sec-150.2.3"><span class="toc-section-number">15.2.3</span>  Variables canócicas discriminantes</a></li>
  <li><a href="#sec-150.2.4" id="toc-sec-150.2.4" class="nav-link" data-scroll-target="#sec-150.2.4"><span class="toc-section-number">15.2.4</span>  Probabilidades de cada clase</a></li>
  <li><a href="#sec-150.2.5" id="toc-sec-150.2.5" class="nav-link" data-scroll-target="#sec-150.2.5"><span class="toc-section-number">15.2.5</span>  Precisión de la solución</a></li>
  </ul></li>
  <li><a href="#sec-150.3" id="toc-sec-150.3" class="nav-link" data-scroll-target="#sec-150.3"><span class="toc-section-number">15.3</span>  AD no lineal</a></li>
  <li><a href="#sec-150.4" id="toc-sec-150.4" class="nav-link" data-scroll-target="#sec-150.4"><span class="toc-section-number">15.4</span>  AD en R</a>
  <ul>
  <li><a href="#sec-150.4.1" id="toc-sec-150.4.1" class="nav-link" data-scroll-target="#sec-150.4.1"><span class="toc-section-number">15.4.1</span>  Bancos de datos</a>
  <ul class="collapse">
  <li><a href="#water-potability" id="toc-water-potability" class="nav-link" data-scroll-target="#water-potability"><span class="toc-section-number">15.4.1.1</span>  Water Potability</a></li>
  <li><a href="#wine-recognition" id="toc-wine-recognition" class="nav-link" data-scroll-target="#wine-recognition"><span class="toc-section-number">15.4.1.2</span>  Wine recognition</a></li>
  <li><a href="#abalone" id="toc-abalone" class="nav-link" data-scroll-target="#abalone"><span class="toc-section-number">15.4.1.3</span>  Abalone</a></li>
  </ul></li>
  <li><a href="#sec-150.4.2" id="toc-sec-150.4.2" class="nav-link" data-scroll-target="#sec-150.4.2"><span class="toc-section-number">15.4.2</span>  Modelos</a>
  <ul class="collapse">
  <li><a href="#water-potability-1" id="toc-water-potability-1" class="nav-link" data-scroll-target="#water-potability-1"><span class="toc-section-number">15.4.2.1</span>  Water potability</a></li>
  <li><a href="#wine-recognition-1" id="toc-wine-recognition-1" class="nav-link" data-scroll-target="#wine-recognition-1"><span class="toc-section-number">15.4.2.2</span>  Wine recognition</a></li>
  <li><a href="#abalone-1" id="toc-abalone-1" class="nav-link" data-scroll-target="#abalone-1"><span class="toc-section-number">15.4.2.3</span>  Abalone</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-150.5" id="toc-sec-150.5" class="nav-link" data-scroll-target="#sec-150.5"><span class="toc-section-number">15.5</span>  Ejercicios</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-150" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Análisis discriminante (AD)</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>El Análisis Discriminante es un método de clasificación supervisado de variables cualitativas en el que dos o más grupos son conocidos a priori y nuevas observaciones se clasifican en uno de ellos en función de sus características.</p>
<p>Supongamos que un conjunto de objetos se clasifica en una serie de grupos; el Análisis Discriminante equivale a un análisis de regresión donde la variable dependiente es categórica y tiene como categorías la etiqueta de cada uno de los grupos, y donde las variables independientes son continuas y determinan a qué grupos pertenecen los objetos. Se trata de encontrar relaciones lineales entre las variables continuas que mejor discriminen en los grupos dados a los objetos. Además, se trata de definir una regla de decisión que asigne un objeto nuevo, que no sabemos clasificar previamente, a uno de los grupos prefijados.</p>
<p>Haciendo uso del teorema de Bayes, el análisis discriminante estima la probabilidad de que una observación, dado un determinado valor de los predictores, pertenezca a cada una de las clases de la variable cualitativa, <span class="math inline">\(P(Y=k|X=x)\)</span>. Finalmente se asigna la observación a la clase <span class="math inline">\(k\)</span> para la que la probabilidad predicha es mayor.</p>
<p>El proceso de un análisis discriminante puede resumirse en 6 pasos:</p>
<ul>
<li>Disponer de un conjunto de datos de entrenamiento (<em>training data</em>) en el que se conoce a que grupo pertenece cada observación.</li>
<li>Calcular las probabilidades previas (<em>prior probabilities</em>): la proporción esperada de observaciones que pertenecen a cada grupo.</li>
<li>Determinar si la varianza o matriz de covarianzas de las predictoras es homogénea en todos los grupos.</li>
<li>Estimar los parámetros necesarios para las funciones de probabilidad condicional, verificando que se cumplen las condiciones para hacerlo.</li>
<li>Calcular el resultado de la función discriminante. El resultado de esta determina a qué grupo se asigna cada observación.</li>
<li>Utilizar validación cruzada para estimar las probabilidades de clasificaciones erróneas.</li>
</ul>
<section id="sec-150.1" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="sec-150.1"><span class="header-section-number">15.1</span> Planteamiento del problema</h2>
<p>Para mostrar el uso del algoritmo de Análisis Discriminante comenzamos con la situación más sencilla donde disponemos de una única predictora y dos grupos.</p>
<p>Consideramos una respuesta <span class="math inline">\(Y\)</span> de tipo categórico con <span class="math inline">\(K=2\)</span> grupos y una predictora <span class="math inline">\(X\)</span> de tipo numérico. Se denota entonces:</p>
<ul>
<li><p><span class="math inline">\(\pi_1\)</span> y <span class="math inline">\(\pi_2\)</span> como las probabilidades previas de que una observación aleatoria pertenezca a la clase <span class="math inline">\(k\)</span> de la respuesta <span class="math inline">\(Y\)</span>.</p></li>
<li><p><span class="math inline">\(f_k(x) \equiv P(X=x|Y=k), k=1,2\)</span> son las funciones de densidad de probabilidad condicional de <span class="math inline">\(X\)</span> para una observación que pertenece a la clase <span class="math inline">\(k\)</span>. Cuanto mayor sea <span class="math inline">\(f_k(X)\)</span> mayor la probabilidad de que una observación de la clase <span class="math inline">\(k\)</span> adquiera un valor de <span class="math inline">\(X≈x\)</span>.</p></li>
<li><p><span class="math inline">\(P(Y=k|X=x)\)</span> son las probabilidades a posteriori de que una observación pertenezca a la clase <span class="math inline">\(k\)</span> siendo <span class="math inline">\(x\)</span> el valor del predictor.</p></li>
</ul>
<p>Haciendo uso del teorema de Bayes tenemos que:</p>
<p><span class="math display">\[P(Y=k|X=x) =  \frac{\pi_kf_k(x)}{\sum_{i=1}^2 \pi_kf_k(X)}\]</span></p>
<p>La clasificación con menor error (clasificación de Bayes) se consigue asignando la observación a aquel grupo que maximice la probabilidad posterior. Dado que el denominador <span class="math inline">\(\sum_{i=1}^2 \pi_kf_k(X)\)</span> es igual para todas las clases, la norma de clasificación es equivalente a decir que se asignará cada observación a aquel grupo para el que <span class="math inline">\(\pi_kf_k(x)\)</span> sea mayor. Si las probabilidades a priori son iguales la regla de clasificación asigna a la categoría <span class="math inline">\(k\)</span> con mayor <span class="math inline">\(f_k(x)\)</span>.</p>
<p>Si tenemos en cuenta los errores de clasificación <span class="math inline">\(c(i|j)\)</span> de clasificar un objeto en el grupo <span class="math inline">\(i\)</span> cuando realmente pertenece al <span class="math inline">\(j\)</span>, la regla de clasificación para clasificar en el grupo 2 frente al 1 viene dada por:</p>
<p><span class="math display">\[\frac{f_2(x)\pi_2}{c(2|1)} &gt; \frac{f_1(x)\pi_1}{c(1|2)}.\]</span></p>
<p>De forma análoga podemos establecer la regla de clasificación en el grupo 1 frente al 2. Si los errores de clasificación y probabilidades iniciales son iguales la regla de clasificación se simplifica a la evaluación de la función de densidad dentro de cada grupo.</p>
<p>Generalizar la regla de decisión cuando disponemos de un conjunto <span class="math inline">\(X\)</span> de predictoras consiste simplemente en la evaluación de las probabilidades a priori y las densidades correspondientes.</p>
<p>Los dos algoritmos más conocidos del análisis discriminante son el <strong>análisis discriminante lineal (LDA)</strong> y el <strong>análisis discriminante cuadrático (QDA)</strong>. La principal diferencia entre ellos es que el LDA asume que la matriz de varianzas y covarianzas asociada con las predictoras en cada uno de los grupos son iguales, es decir, asumimos homogeneidad de varianzas entre los grupos, mientras que en QDA dichas matrices no tienen por qué ser iguales.</p>
</section>
<section id="sec-150.2" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="sec-150.2"><span class="header-section-number">15.2</span> AD lineal</h2>
<p>Los aspectos teóricos relacionados con el LDA se estructuran presentando en primer lugar los fundamentos del algoritmo, el proceso de estimación necesario para evaluar las funciones discriminantes y finalizamos con el proceso de obtención de las funciones discriminantes canónicas. Comenzamos con la situación más sencilla donde consideramos dos grupos pero generalizaremos al caso de <span class="math inline">\(k\)</span> grupos.</p>
<section id="sec-150.2.1" class="level3" data-number="15.2.1">
<h3 data-number="15.2.1" class="anchored" data-anchor-id="sec-150.2.1"><span class="header-section-number">15.2.1</span> Fundamentos</h3>
<p>Para que la clasificación basada en Bayes sea posible, se necesita conocer la probabilidad poblacional de que una observación cualquiera pertenezca a cada clase (<span class="math inline">\(\pi_k\)</span>) y la probabilidad poblacional de que una observación que pertenece a la clase <span class="math inline">\(k\)</span> adquiera el valor <span class="math inline">\(x\)</span> en el predictor <span class="math inline">\(f_k(x)\)</span>.</p>
<p>En el caso de la probabilidad a priori la estimación suele ser sencilla. La probabilidad de que una observación cualquiera pertenezca a la clase <span class="math inline">\(k\)</span> es igual al número de observaciones de esa clase entre el número total de observaciones <span class="math inline">\(\hat{\pi} = n_k/N\)</span>, donde <span class="math inline">\(n_k\)</span> es el número de observaciones en el grupo <span class="math inline">\(k\)</span> y <span class="math inline">\(N\)</span> es el número total de observaciones.</p>
<p>La estimación de <span class="math inline">\(f_k(X)\)</span> no es tan directa y por ahora asumimos que dichas funciones de densidad son distribuciones normales multivariantes con vector de medias (<span class="math inline">\(\mu_i\)</span>) distintas para cada grupo pero con la misma matriz de varianzas-covarianzas (<span class="math inline">\(\Sigma\)</span>), es decir:</p>
<p><span class="math display">\[f_i(X) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}} exp\left\{-\frac{1}{2}(x-\mu_i)^t \Sigma^{-1}(x-\mu_i)\right\}.\]</span></p>
<p>De esta forma, la regla de clasificación en el grupo 2 frente al grupo 1 (considerando logaritmos), asumiendo que los errores de clasificación son iguales, viene dada por:</p>
<p><span class="math display">\[log(\pi_2) + log(f_2(X)) &gt; log(\pi_1) + log(f_1(X))\]</span></p>
<p>que sustituyendo por las funciones de densidad correspondientes tenemos que:</p>
<p><span class="math display">\[log(\pi_2) - \frac{1}{2}(x-\mu_2)^t \Sigma^{-1}(x-\mu_2) &gt; log(\pi_1) + - \frac{1}{2}(x-\mu_1)^t \Sigma^{-1}(x-\mu_1)\]</span></p>
<p>que tras operar nos conduce a:</p>
<p><span class="math display">\[ (x-\mu_1)^t \Sigma^{-1}(x-\mu_1) &gt; (x-\mu_2)^t \Sigma^{-1}(x-\mu_2) - 2log(\pi_2/\pi_1).\]</span></p>
<p>Los términos <span class="math inline">\((x-\mu_1)^t \Sigma^{-1}(x-\mu_1)\)</span> y <span class="math inline">\((x-\mu_2)^t \Sigma^{-1}(x-\mu_2)\)</span> son las distancias de Mahalanobis del punto <span class="math inline">\(x\)</span> con respecto a las medias de cada uno de los grupos. Mediante cálculos sencillos la regla de clasificación anterior viene dada por:</p>
<p><span class="math display">\[w^{t}x = w^{t}\left(\frac{\mu_1 + \mu_2}{2}\right)-log(\pi_2/\pi_1)\]</span></p>
<p>donde <span class="math inline">\(w=\Sigma^{-1}(\mu_2-\mu_1)\)</span>. La ecuación anterior corresponde con la de un hiperplano de forma que el procedimiento de clasificación puede resumirse así:</p>
<ul>
<li>Dados los valores de <span class="math inline">\(\mu_1\)</span>, <span class="math inline">\(\mu_2\)</span> y <span class="math inline">\(\Sigma\)</span> obtenemos en primer lugar el vector <span class="math inline">\(w\)</span>.</li>
<li>Escribir la función discriminante como una combinación lineal de los valores de la variable con los pesos dados por el vector <span class="math inline">\(w\)</span>:</li>
</ul>
<p><span class="math display">\[g(x) = w^{t}x,\]</span></p>
<ul>
<li>Introducir en esta función los valores observados para el nuevo individuo a clasificar, <span class="math inline">\(x_0 = (x_{10},...,x_{k0})\)</span> y evaluar la regla de clasificación establecida.</li>
</ul>
<p>Si los costes y probabilidades a priori son iguales la regla de decisión se reduce entonces a clasificar en el grupo 2 si:</p>
<p><span class="math display">\[w^{t}x &gt; w^{t}\left(\frac{\mu_1+\mu_2}{2}\right)\]</span></p>
<p>Esta regla equivale a proyectar el punto x que queremos clasificar y las medias de ambas poblaciones sobre una recta, y después asignar el punto a aquella población de cuya media se encuentre más próxima en la proyección. La función <span class="math inline">\(w^{t}x\)</span> se denomina función discriminante.</p>
</section>
<section id="sec-150.2.2" class="level3" data-number="15.2.2">
<h3 data-number="15.2.2" class="anchored" data-anchor-id="sec-150.2.2"><span class="header-section-number">15.2.2</span> Estimación de parámetros</h3>
<p>En la práctica, a pesar de tener una certeza considerable de que <span class="math inline">\(X\)</span> se distribuye de forma normal dentro de cada clase, los valores <span class="math inline">\(\mu_1...,\mu_k\)</span>, <span class="math inline">\(\pi_1,...,\pi_k\)</span> y <span class="math inline">\(\Sigma\)</span> se desconocen, por lo que tienen que ser estimados a partir de las observaciones. En este caso abordamos directamente el caso de <span class="math inline">\(K\)</span> grupos o poblaciones.</p>
<p>La matriz general de predictoras <span class="math inline">\(X\)</span> de dimensiones q × n (q variables y n individuos), se puede particionar ahora en <span class="math inline">\(K\)</span> matrices correspondientes a las subpoblaciones. Se denomina <span class="math inline">\(x_{ijk}\)</span> a los elementos de estas submatrices, donde <span class="math inline">\(i\)</span> representa el individuo, <span class="math inline">\(j\)</span> la variable y <span class="math inline">\(k\)</span> el grupo o submatriz. Llamaremos <span class="math inline">\(n_k\)</span> al número de elementos en el grupo <span class="math inline">\(k\)</span> y el número total de observaciones se denomina <span class="math inline">\(n\)</span>.</p>
<p>Se denomina <span class="math inline">\(x^{t}_{ik}\)</span> al vector fila que contiene los q valores de las variables para el individuo <span class="math inline">\(i\)</span> en el grupo <span class="math inline">\(k\)</span>, es decir,</p>
<p><span class="math display">\[x^{t}_{ik} = (x_{i1k},...,x_{iqk})\]</span></p>
<p>El vector de medias dentro de cada clase o grupo es:</p>
<p><span class="math display">\[\bar{x}_k = \frac{1}{n_k}\sum_{i=1}^{n_k} x_{ik}\]</span></p>
<p>mientras que la matriz de varianzas-covarianzas para los sujetos del grupo <span class="math inline">\(k\)</span> viene dada por:</p>
<p><span class="math display">\[\hat{S}_k=\frac{1}{n_k-1} \sum_{i=1}^{n_k} (x_{ik}-\bar{x}_k)(x_{ik}-\bar{x}_k)^t.\]</span></p>
<p>Si los <span class="math inline">\(k\)</span> grupos o clases tienen la misma matriz de varianzas-covarianzas una estimación de dicha matriz viene dada por:</p>
<p><span class="math display">\[\hat{S}_{w} = \sum_{k=1}^K \frac{n_k-1}{n-k}\hat{S}_k.\]</span></p>
<p>Llamaremos <span class="math inline">\(W\)</span> a la matriz de sumas de cuadrados dentro de las clases que viene dada por:</p>
<p><span class="math display">\[W = (n-K)\hat{S}_w.\]</span></p>
<p>Podemos utilizar ahora las estimaciones consideradas para obtener las correspondientes funciones discriminantes.</p>
</section>
<section id="sec-150.2.3" class="level3" data-number="15.2.3">
<h3 data-number="15.2.3" class="anchored" data-anchor-id="sec-150.2.3"><span class="header-section-number">15.2.3</span> Variables canócicas discriminantes</h3>
<p>El enfoque anterior puede generalizarse para encontrar variables canónicas que tengan el máximo poder discriminante para clasificar nuevos elementos respecto a las poblaciones. El objetivo es, en lugar de trabajar con las variables originales <span class="math inline">\(X\)</span>, definir <span class="math inline">\(r\)</span> variables canónicas, <span class="math inline">\(z_i, i = 1,...,r\)</span> donde <span class="math inline">\(r = min(K − 1, q)\)</span>, que sean combinación lineal de las originales <span class="math inline">\(z_i = w_i^tx\)</span> de modo que:</p>
<ul>
<li><p>Las medias de las poblaciones <span class="math inline">\(\mu_k\)</span> se expresan en términos de las variables canónicas donde <span class="math inline">\(z_1,...,z_K\)</span> son vectores r × 1 cuyas coordenadas son las proyecciones de las medias sobre las <span class="math inline">\(r\)</span> variables canónicas;</p></li>
<li><p>Se hace lo mismo para el punto <span class="math inline">\(x_0\)</span> a clasificar donde se denota a <span class="math inline">\(z_0\)</span> como dicho vector;</p></li>
<li><p>Clasificamos el punto en aquella población de cuya media se encuentre más próxima, con la distancia euclídea, en el espacio de las variables canónicas <span class="math inline">\(z\)</span>; es decir, lo clasificaremos en la población <span class="math inline">\(i\)</span> si</p></li>
</ul>
<p><span class="math display">\[(z_0-z_i)'(z_0-z_i) = \underset{k}{min} (z_0-z_k)'(z_0-z_k).\]</span></p>
<p>Tan sólo nos resta establecer el procedimiento para la obtención de las variables canónicas. Para resolver este problema debemos buscar un vector <span class="math inline">\(w\)</span> tal que, cuando proyectamos los puntos sobre él, se obtiene la máxima variabilidad entre los grupos en relación a la variabilidad dentro de los grupos. La media de las observaciones del grupo <span class="math inline">\(k\)</span> en esta nueva variable será:</p>
<p><span class="math display">\[\bar{z}_k = w^t\bar{x}_k\]</span></p>
<p>y la media para todos los datos viene dada por:</p>
<p><span class="math display">\[\bar{z}_T=w^t\bar{x}_T.\]</span></p>
<p>Se desea encontrar el vector <span class="math inline">\(w\)</span> de manera que la separación entre las medias de los grupos sea máxima. Una medida de la distancia entre las medias <span class="math inline">\(z_1,..., z_K\)</span> es la suma de cuadrados entre las medias dada por</p>
<p><span class="math display">\[\sum_{k=1}^K n_k(\bar{z}_k-\bar{z}_T)^2 = w^tBw,\]</span></p>
<p>con <span class="math inline">\(B\)</span> la matriz de suma de cuadrados entre grupos. Para juzgar si este término es grande o pequeño, debemos compararlo con la variabilidad intrínseca de los datos o variabilidad intra clase dada por:</p>
<p><span class="math display">\[\sum_{j=1}^{n_k}\sum_{k=1}^K (z_{ik}-\bar{z}_k)^2 = w^tWw.\]</span></p>
<p>En definitiva, el criterio para encontrar la mejor dirección de proyección consiste en maximizar la separación relativa entre las medias, dada por:</p>
<p><span class="math display">\[\underset{w}{max} \frac{w^tBw}{w^tWw}.\]</span></p>
<p>La solución con este problema de maximización viene dada por los vectores propios asociados a los valores propios ordenados según su valor de mayor a menor de la matriz <span class="math inline">\(W^{-1}B.\)</span> Por tanto, la primera función discriminante se corresponde con la combinación lineal de predictoras donde los valores de <span class="math inline">\(w\)</span> son los valores del vector propio asociado con el mayor valor propio de <span class="math inline">\(W^{-1}B\)</span>. Los vectores propios de dicha matriz no serán, en general, ortogonales, y además el rango de dicha matriz será <span class="math inline">\(r\)</span>, es decir, el número máximo de funciones discriminantes que podemos construir. Tenemos entonces la funciones discriminantes dadas por la expresión:</p>
<p><span class="math display">\[z=U^tx\]</span></p>
<p>donde <span class="math inline">\(U^t\)</span> es la matriz de dimensiones <span class="math inline">\(r \times q\)</span> que contiene los vectores propios de <span class="math inline">\(W^{-1}B\)</span> por filas. Las variables canónicas así obtenidas resuelven el problema de clasificación, de forma que, para clasificar un nuevo individuo <span class="math inline">\(x_0\)</span> basta con calcular sus coordenadas <span class="math inline">\(z_0\)</span> con la expresión anterior y asignarle al grupo de cuya media transformada esté más próxima mediante la distancia euclídea.</p>
<p>Si trabajamos con variables estandarizadas los vectores propios obtenidos nos proporcionan además la relevancia de cada predictora en la función de discriminación. Así mismo podemos utilizar los valores propios obtenidos para valorar la capacidad discriminatoria del modelo o el número de funciones discriminantes necesarias para obtener una buena clasificación. Este procedimiento es similar al seguido con las componentes principales para la reducción de la dimensión.</p>
</section>
<section id="sec-150.2.4" class="level3" data-number="15.2.4">
<h3 data-number="15.2.4" class="anchored" data-anchor-id="sec-150.2.4"><span class="header-section-number">15.2.4</span> Probabilidades de cada clase</h3>
<p>Una alternativa a la solución anterior a partir de los estimadores anteriores viene dada por los logaritmos de las probabilidades de cada etiqueta <span class="math inline">\(k\)</span> que se pueden obtener a partir de la expresiones:</p>
<p><span class="math display">\[log(P(y=k|x) = -0.5(x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + log(\pi_k)) + Constante\]</span></p>
<p>o alternativamente</p>
<p><span class="math display">\[log(P(y=k|x) = \beta_k^{t}x+\beta_{k0} + Constante\]</span></p>
<p>con <span class="math inline">\(\beta_k = \Sigma^{-1}\mu_k\)</span> y <span class="math inline">\(\beta_{k0} = -0.5\mu^t\Sigma^{-1}\mu_k + log(\pi_k).\)</span></p>
<p>Si solo tenemos dos posibles etiquetas tan solo disponemos de una única ecuación y vector de pesos <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="sec-150.2.5" class="level3" data-number="15.2.5">
<h3 data-number="15.2.5" class="anchored" data-anchor-id="sec-150.2.5"><span class="header-section-number">15.2.5</span> Precisión de la solución</h3>
<p>La precisión o bondad de la solución obtenida se analiza mediante los procedimientos habituales de los problemas de clasificación entre los que el más relevante es el análisis de la matriz de confusión.</p>
</section>
</section>
<section id="sec-150.3" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="sec-150.3"><span class="header-section-number">15.3</span> AD no lineal</h2>
<p>El análisis discriminante no lineal se asocia casi siempre con El clasificador cuadrático o <em>Quadratic Discriminat Analysis</em> QDA se asemeja en gran medida al LDA, con la única diferencia de que el QDA considera que cada clase k tiene su propia matriz de covarianzas (<span class="math inline">\(\Sigma_k\)</span>) y, como consecuencia, la función para estimar el logaritmo de la probabilidad de cada clase viene dada por:</p>
<p><span class="math display">\[log(P(y=k|x) = -0.5log|\Sigma_k|-0.5(x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + log(\pi_k)\]</span></p>
<p>Estas funciones no son lineales por lo que se generan límites de decisión curvos y pueden aplicarse a situaciones en las que la separación entre grupos no es lineal. Además, dado que consideramos matrices de varianzas-covarianzas distintas no resulta necesario la verificación de la hipótesis de homogeneidad.</p>
</section>
<section id="sec-150.4" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="sec-150.4"><span class="header-section-number">15.4</span> AD en R</h2>
<p>Aunque en <code>mlr3</code> se encuentran disponibles los modelos de aprendizaje <code>classif.lda</code> y <code>classif.qda</code> para el análisis discriminante lineal y cuadrático, es este caso optamos por utilizar directamente las funciones de <code>R</code> disponibles en las librerías <code>MASS</code> (instalada por defecto) y <code>rda</code> (que es necesario instalar). En los puntos siguientes se mostrara como hacer uso de las funciones de esas librerías para llevar a cabo el análisis discriminante. Para las soluciones gráficas deberemos instalar además los paquetes <code>klaR</code> y <code>ggord</code>. Para instalar este segundo es necesario ejecutar el código siguiente:</p>
<p>En primer lugar cargamos todas las librerías necesarias:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Paquetes anteriores</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">library</span>(sjPlot)</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="fu">library</span>(knitr) <span class="co"># para formatos de tablas</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="fu">library</span>(skimr)</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="fu">library</span>(DataExplorer)</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="fu">library</span>(GGally)</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="fu">library</span>(ggpubr)</span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="fu">library</span>(cvms)</span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="fu">library</span>(kknn)</span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="fu">library</span>(rda)</span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="fu">library</span>(klaR)</span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="fu">library</span>(ggord)</span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="fu">theme_set</span>(<span class="fu">theme_sjplot2</span>())</span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="co"># Paquetes AA</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="fu">library</span>(mlr3verse)</span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="fu">library</span>(mlr3tuning)</span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="fu">library</span>(mlr3tuningspaces)</span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="fu">library</span>(gbm)</span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="fu">library</span>(RWeka)</span>
<span id="cb1-24"><a href="#cb1-24"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb1-25"><a href="#cb1-25"></a><span class="fu">library</span>(lightgbm)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="sec-150.4.1" class="level3" data-number="15.4.1">
<h3 data-number="15.4.1" class="anchored" data-anchor-id="sec-150.4.1"><span class="header-section-number">15.4.1</span> Bancos de datos</h3>
<p>Para ejemplificar el uso de los modelos de análisis dicriminante vamos a utilizar tres bancos de datos: <code>water potability</code> para la clasificación de dos grupos, <code>Wine recognotion</code>, y <code>Abalone</code> para la clasificación de tres grupos. A continuación presentamos los tres bancos de datos y el código para cargar las bases de datos correspondientes.</p>
<section id="water-potability" class="level4" data-number="15.4.1.1">
<h4 data-number="15.4.1.1" class="anchored" data-anchor-id="water-potability"><span class="header-section-number">15.4.1.1</span> Water Potability</h4>
<p>El código para cargar el banco de datos es:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Leemos datos</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>waterpot <span class="ot">=</span> <span class="fu">read_rds</span>(<span class="st">"waterpot.rds"</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co"># creamos la tarea</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>tsk_water <span class="ot">=</span> <span class="fu">as_task_classif</span>(waterpot, <span class="at">target =</span> <span class="st">"Potability"</span>, <span class="at">positive=</span><span class="st">"1"</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co"># Generamos variable de estrato</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>tsk_water<span class="sc">$</span>col_roles<span class="sc">$</span>stratum <span class="ot">&lt;-</span> <span class="st">"Potability"</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="wine-recognition" class="level4" data-number="15.4.1.2">
<h4 data-number="15.4.1.2" class="anchored" data-anchor-id="wine-recognition"><span class="header-section-number">15.4.1.2</span> Wine recognition</h4>
<p>En este banco de datos se recoge el resultado de un análisis químico de vinos cultivados en la misma región de Italia pero procedentes de tres cultivos distintos. El análisis determinó las cantidades de 13 características que se encuentran en cada una de las muestras de vinos. El objetivo que perseguimos es clasificar cada muestra en una de estas tres clases de vino (<code>Class label</code>) en función de sus características de tipo numérico. Este banco de datos no contiene valores perdidos.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Leemos datos</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>winerecognition <span class="ot">=</span> <span class="fu">read_rds</span>(<span class="st">"winerecognition.rds"</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="fu">names</span>(winerecognition) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"Class"</span>, <span class="st">"Alcohol"</span>, <span class="st">"Malic_acid"</span>, <span class="st">"Ash"</span>, <span class="st">"Alcalinity_of_ash"</span>, <span class="st">"Magnesium"</span>, <span class="st">"Total_phenols"</span>, <span class="st">"Flavanoids"</span>, <span class="st">"Nonflavanoid_phenols"</span>, <span class="st">"Proanthocyanins"</span>, <span class="st">"Color_intensity"</span>, <span class="st">"Hue"</span>, <span class="st">"OD280_OD315_of_diluted_wines"</span>, <span class="st">"Proline"</span>)</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co"># creamos la tarea</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>tsk_wine <span class="ot">=</span> <span class="fu">as_task_classif</span>(winerecognition, <span class="at">target =</span> <span class="st">"Class"</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co"># Generamos variable de estrato</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>tsk_wine<span class="sc">$</span>col_roles<span class="sc">$</span>stratum <span class="ot">&lt;-</span> <span class="st">"Class"</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="abalone" class="level4" data-number="15.4.1.3">
<h4 data-number="15.4.1.3" class="anchored" data-anchor-id="abalone"><span class="header-section-number">15.4.1.3</span> Abalone</h4>
<p>En este conjunto de datos se recoge información sobre los abulones, de la familia de los moluscos. Se está interesado en medir su desarrollo, que viene determinado principalmente por su desarrollo sexual. Concretamente se consideran tres estados de desarrollo asociados con el atributo <code>Sex</code>: <code>M``(machos),</code>F<code>(hembras), e</code>I` (infantil o sin desarrollo sexual). Para clasificar cada sujeto se utiliza un conjunto de características que son de tipo numérico.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Leemos los datos</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>abalone <span class="ot">=</span> <span class="fu">read_rds</span>(<span class="st">"abalone.rds"</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="fu">names</span>(abalone) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"Sex"</span>, <span class="st">"Length"</span>, <span class="st">"Diameter"</span>, <span class="st">"Height"</span>, <span class="st">"Whole_weight"</span>, <span class="st">"Shucked_weight"</span>, <span class="st">"Viscera_weight"</span>, <span class="st">"Shell_weight"</span>, <span class="st">"Rings"</span>)  </span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="co"># creamos la tarea</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>tsk_abalone <span class="ot">=</span> <span class="fu">as_task_classif</span>(abalone, <span class="at">target =</span> <span class="st">"Sex"</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="co"># Generamos variable de estrato</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>tsk_abalone<span class="sc">$</span>col_roles<span class="sc">$</span>stratum <span class="ot">&lt;-</span> <span class="st">"Sex"</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="sec-150.4.2" class="level3" data-number="15.4.2">
<h3 data-number="15.4.2" class="anchored" data-anchor-id="sec-150.4.2"><span class="header-section-number">15.4.2</span> Modelos</h3>
<p>Para cada banco de datos probamos diferentes modelos de análisis discriminante para ver como afectan las predictoras en la clasificación a través de la construcción de las funciones discriminantes.</p>
<section id="water-potability-1" class="level4" data-number="15.4.2.1">
<h4 data-number="15.4.2.1" class="anchored" data-anchor-id="water-potability-1"><span class="header-section-number">15.4.2.1</span> Water potability</h4>
<p>Comenzamos con el análisis del banco de datos de potabilidad del agua donde queremos discriminar entres dos grupos. En este caso solo podremos obtener una función discriminante como combinación de las predictoras consideradas. Comenzamos por dividir las muestras e imputar valores perdidos.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># División de muestras</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="fu">set.seed</span>(<span class="dv">432</span>)</span>
<span id="cb5-3"><a href="#cb5-3"></a>splits <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">partition</span>(tsk_water, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb5-4"><a href="#cb5-4"></a>tsk_train_water <span class="ot">=</span> tsk_water<span class="sc">$</span><span class="fu">clone</span>()<span class="sc">$</span><span class="fu">filter</span>(splits<span class="sc">$</span>train)</span>
<span id="cb5-5"><a href="#cb5-5"></a>tsk_test_water  <span class="ot">=</span> tsk_water<span class="sc">$</span><span class="fu">clone</span>()<span class="sc">$</span><span class="fu">filter</span>(splits<span class="sc">$</span>test)</span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="co"># preprocesado</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>pp_water <span class="ot">=</span> <span class="fu">po</span>(<span class="st">"scale"</span>, <span class="at">param_vals =</span> <span class="fu">list</span>(<span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>  <span class="fu">po</span>(<span class="st">"imputemedian"</span>, <span class="at">affect_columns =</span> <span class="fu">selector_type</span>(<span class="st">"numeric"</span>))</span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co"># Obtención de muestras</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>train <span class="ot">=</span> pp_water<span class="sc">$</span><span class="fu">train</span>(tsk_train_water)</span>
<span id="cb5-11"><a href="#cb5-11"></a>test <span class="ot">=</span> pp_water<span class="sc">$</span><span class="fu">train</span>(tsk_test_water)</span>
<span id="cb5-12"><a href="#cb5-12"></a>water_train <span class="ot">=</span> train[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">data</span>()</span>
<span id="cb5-13"><a href="#cb5-13"></a>water_test <span class="ot">=</span> test[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">data</span>()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comenzamos con el modelo discriminante lineal.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Modelo para la muestra de entrenamiento</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>ad.lineal <span class="ot">=</span> <span class="fu">lda</span>(Potability<span class="sc">~</span>., water_train)</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co"># Resultados del modelo</span></span>
<span id="cb6-4"><a href="#cb6-4"></a>ad.lineal</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Potability ~ ., data = water_train)

Prior probabilities of groups:
        1         0 
0.3900763 0.6099237 

Group means:
  Chloramines Conductivity      Hardness Organic_carbon      Solids
1  0.02445992  0.008382819 -0.0002709496    -0.04684461  0.04526142
0 -0.01564333 -0.005361227  0.0001732857     0.02995945 -0.02894691
     Turbidity     Sulfate Trihalomethanes          ph
1 -0.007620776 -0.03646853   -0.0004807166  0.01402887
0  0.004873863  0.01680100    0.0013832328 -0.01559109

Coefficients of linear discriminants:
                        LD1
Chloramines     -0.36974474
Conductivity    -0.10857240
Hardness         0.00524105
Organic_carbon   0.58084020
Solids          -0.55670481
Turbidity        0.12275871
Sulfate          0.43158436
Trihalomethanes  0.01286696
ph              -0.37522197</code></pre>
</div>
</div>
<p>Los resultados del modelo son:</p>
<ul>
<li>Probabilidades a priori de cada uno de los grupos (<code>ad.lineal$prior</code>).</li>
<li>Medias de las predictoras para cada uno de los grupos (<code>ad.lineal$means</code>).</li>
<li>Coeficientes de la función discriminante para cada predictora (<code>ad.lineal$scaling</code>). En este caso los coeficientes positivos favorecen el tipo potable, frente a los negativos que favorecen el tipo no potable. La ecuación de la función discriminante viene dada por (redondeando a dos decimales los coeficientes):</li>
</ul>
<p><span class="math display">\[LD1 = -0.11 Chloramines + 0.17 Conductivity + 0.01 Hardness + 0.64 Organic_carbon -0.74  Solids + 0.13Turbidity + 0.10Sulfate -0.10Trihalomethanes -0.18ph\]</span> Los coeficientes también nos permiten saber que predictoras son más relevantes en la función de discriminación. tan solo debemos ver que coeficientes son más grandes en valor absoluto. Para este modelo las predictoras <code>Organic_carbon</code> y <code>Solids</code> son las más relevantes en sentido positivo y negativo respectivamente.</p>
<p>Si se dispone de más de una función discriminante la solución nos proporciona la proporción de variabilidad explicada (discriminación) de cada función.</p>
<p>A continuación representamos las puntuaciones de la función discriminante tanto para la muestra de entrenamiento como de validación para valorar la capacidad de discriminación de dicha función. Recordemos que el análisis de reducción nos permite pasar del conjunto inicial de 9 predictoras a una única predictora dada por la función discriminante.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Muestra de entrenamiento</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>p <span class="ot">&lt;-</span> <span class="fu">predict</span>(ad.lineal, water_train)</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="fu">ldahist</span>(<span class="at">data =</span> p<span class="sc">$</span>x[,<span class="dv">1</span>], <span class="at">g =</span> water_train<span class="sc">$</span>Potability)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-007-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Muestra de validación</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>p <span class="ot">&lt;-</span> <span class="fu">predict</span>(ad.lineal, water_test)</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="fu">ldahist</span>(<span class="at">data =</span> p<span class="sc">$</span>x[,<span class="dv">1</span>], <span class="at">g =</span> water_test<span class="sc">$</span>Potability)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-007-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>En ambas muestras podemos apreciar que las puntuaciones discriminantes para ambos grupos son muy similares, lo que sin duda provoca que nuestra función de discriminación no resulta muy adecuada. Para verificar este hecho vamos a obtener la matriz de confusión correspondiente a este modelo y calcularemos el porcentaje de clasificación correcta ponderada.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># predicción validación</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>pred_test <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">truth =</span> water_test<span class="sc">$</span>Potability, <span class="at">response =</span> <span class="fu">predict</span>(ad.lineal, water_test)<span class="sc">$</span>class)</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="co"># matriz de confusión</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>cm <span class="ot">=</span> <span class="fu">confusion_matrix</span>(pred_test<span class="sc">$</span>truth, pred_test<span class="sc">$</span>response)</span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="fu">plot_confusion_matrix</span>(cm<span class="sc">$</span><span class="st">`</span><span class="at">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-008-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Como era de esperar los resultados de la matriz de confusión muestran el mal funcionamiento del análisis planteado. Podemos ver el porcentaje de clasificación correcta ponderada con:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>cm<span class="sc">$</span><span class="st">`</span><span class="at">Balanced Accuracy</span><span class="st">`</span> </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5097656</code></pre>
</div>
</div>
<p>Como era de esperar el resultado obtenido es muy malo.</p>
<p>Comenzamos ahora el análisis utilizando un modelo discriminante cuadrático.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Modelo para la muestra de entrenamiento</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>ad.quad <span class="ot">=</span> <span class="fu">qda</span>(Potability<span class="sc">~</span>., water_train)</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="co"># Resultados del modelo</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>ad.quad</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
qda(Potability ~ ., data = water_train)

Prior probabilities of groups:
        1         0 
0.3900763 0.6099237 

Group means:
  Chloramines Conductivity      Hardness Organic_carbon      Solids
1  0.02445992  0.008382819 -0.0002709496    -0.04684461  0.04526142
0 -0.01564333 -0.005361227  0.0001732857     0.02995945 -0.02894691
     Turbidity     Sulfate Trihalomethanes          ph
1 -0.007620776 -0.03646853   -0.0004807166  0.01402887
0  0.004873863  0.01680100    0.0013832328 -0.01559109</code></pre>
</div>
</div>
<p>La solución del análisis discriminante cuadrático solo nos proporciona las probabilidades a priori y las medias de cada predictora en cada nivel de la respuesta. En este caso no podemos analizar directamente las puntuaciones de la función discriminante, pero si podemos estudiar la matriz de confusión asociada al modelo.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># predicción validación</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>pred_test <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">truth =</span> water_test<span class="sc">$</span>Potability, <span class="at">response =</span> <span class="fu">predict</span>(ad.quad, water_test)<span class="sc">$</span>class)</span>
<span id="cb15-3"><a href="#cb15-3"></a><span class="co"># matriz de confusión</span></span>
<span id="cb15-4"><a href="#cb15-4"></a>cm <span class="ot">=</span> <span class="fu">confusion_matrix</span>(pred_test<span class="sc">$</span>truth, pred_test<span class="sc">$</span>response)</span>
<span id="cb15-5"><a href="#cb15-5"></a><span class="fu">plot_confusion_matrix</span>(cm<span class="sc">$</span><span class="st">`</span><span class="at">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-011-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Los resultados de la matriz de confusión muestran que este modelo tiene un comportamiento mejor que el lineal. Veamos el porcentaje de clasificación correcta ponderada.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>cm<span class="sc">$</span><span class="st">`</span><span class="at">Balanced Accuracy</span><span class="st">`</span> </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6476563</code></pre>
</div>
</div>
<p>El resultado obtenido es casi tan bueno como el del mejor modelo que habíamos obtenido hasta ahora para este conjunto de datos.</p>
</section>
<section id="wine-recognition-1" class="level4" data-number="15.4.2.2">
<h4 data-number="15.4.2.2" class="anchored" data-anchor-id="wine-recognition-1"><span class="header-section-number">15.4.2.2</span> Wine recognition</h4>
<p>En este caso disponemos de tres clases de vinos por lo que por defecto el análisis discriminante obtendrá dos funciones discriminantes en su solución. Comenzamos por el modelo lineal definiendo las muestras de entrenamiento y test.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># División de muestras</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="fu">set.seed</span>(<span class="dv">432</span>)</span>
<span id="cb18-3"><a href="#cb18-3"></a>splits <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">partition</span>(tsk_wine, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb18-4"><a href="#cb18-4"></a>tsk_train_wine <span class="ot">=</span> tsk_wine<span class="sc">$</span><span class="fu">clone</span>()<span class="sc">$</span><span class="fu">filter</span>(splits<span class="sc">$</span>train)</span>
<span id="cb18-5"><a href="#cb18-5"></a>tsk_test_wine  <span class="ot">=</span> tsk_wine<span class="sc">$</span><span class="fu">clone</span>()<span class="sc">$</span><span class="fu">filter</span>(splits<span class="sc">$</span>test)</span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="co"># preprocesado</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>pp_wine <span class="ot">=</span> <span class="fu">po</span>(<span class="st">"scale"</span>, <span class="at">param_vals =</span> <span class="fu">list</span>(<span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)) </span>
<span id="cb18-8"><a href="#cb18-8"></a><span class="co"># Obtención de muestras</span></span>
<span id="cb18-9"><a href="#cb18-9"></a>train <span class="ot">=</span> pp_wine<span class="sc">$</span><span class="fu">train</span>(<span class="fu">list</span>(tsk_train_wine))</span>
<span id="cb18-10"><a href="#cb18-10"></a>test <span class="ot">=</span> pp_wine<span class="sc">$</span><span class="fu">train</span>(<span class="fu">list</span>(tsk_test_wine))</span>
<span id="cb18-11"><a href="#cb18-11"></a>wine_train <span class="ot">=</span> train[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">data</span>()</span>
<span id="cb18-12"><a href="#cb18-12"></a>wine_test <span class="ot">=</span> test[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">data</span>()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comenzamos con el modelo discriminante lineal.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># Modelo para la muestra de entrenamiento</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>ad.lineal <span class="ot">=</span> <span class="fu">lda</span>(Class<span class="sc">~</span>., wine_train)</span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="co"># Resultados del modelo</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>ad.lineal</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Class ~ ., data = wine_train)

Prior probabilities of groups:
        1         2         3 
0.3309859 0.4014085 0.2676056 

Group means:
  Alcalinity_of_ash    Alcohol        Ash Color_intensity  Flavanoids
1        -0.7530190  0.9290783  0.2623347       0.2151580  0.97090136
2         0.2474275 -0.8949264 -0.3883909      -0.8735059  0.02533243
3         0.5602243  0.1932664  0.2581198       1.0441423 -1.23885033
         Hue   Magnesium Malic_acid Nonflavanoid_phenols
1  0.4831927  0.43590994 -0.3321547         -0.588689445
2  0.4193928 -0.33253923 -0.3337043         -0.009089938
3 -1.2267223 -0.04034292  0.9113793          0.741750800
  OD280_OD315_of_diluted_wines Proanthocyanins    Proline Total_phenols
1                    0.7560692       0.5466696  1.1669914    0.88203324
2                    0.2617065       0.0384392 -0.7046074   -0.06383159
3                   -1.3276979      -0.7338027 -0.3864730   -0.99518846

Coefficients of linear discriminants:
                                     LD1         LD2
Alcalinity_of_ash             0.57237474 -0.62067698
Alcohol                      -0.33382774  0.84308154
Ash                          -0.11751893  0.71721561
Color_intensity               1.05810079  0.62228850
Flavanoids                   -1.63396480 -0.25165780
Hue                          -0.19664072 -0.48817442
Magnesium                     0.01040250 -0.05100801
Malic_acid                    0.29608846  0.24885233
Nonflavanoid_phenols         -0.13248561 -0.23038223
OD280_OD315_of_diluted_wines -0.77246451  0.06190253
Proanthocyanins               0.01485653 -0.21661733
Proline                      -0.87734633  0.94935006
Total_phenols                 0.30663221 -0.23954012

Proportion of trace:
   LD1    LD2 
0.6883 0.3117 </code></pre>
</div>
</div>
<p>En este caso podemos ver que la primera función discriminante alcanza el 68.8% de variabilidad explicada, mientras que la segunda alcanza el 31.2%. Recordemos que las funciones discriminantes se construyen por orden, de forma que las primeras siempre tiene mayor variabilidad explicada que las siguientes. Podemos ver la relevancia de cada predictora en las funciones discriminantes obtenidas mediante un gráfico de los coeficientes obtenidos en cada una de ellas.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>df <span class="ot">=</span> <span class="fu">as.data.frame</span>(ad.lineal<span class="sc">$</span>scaling)</span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(LD1, LD2, <span class="at">label =</span> <span class="fu">rownames</span>(df))) <span class="sc">+</span> </span>
<span id="cb21-3"><a href="#cb21-3"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb21-4"><a href="#cb21-4"></a>  <span class="fu">geom_label</span>() <span class="sc">+</span></span>
<span id="cb21-5"><a href="#cb21-5"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-015-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>En este gráfico las variables situadas más lejos del origen de coordenadas son las más relevantes en el análisis. Las predictoras más relevantes sobre la primera función discriminante son aquellas con mayor valor sobre el eje x (positivo o negativo), mientras que sobre la segunda son aquellas con mayor valor sobre el eje y (positivo o negativo). las que se sitúan sobre la diagonal contribuyen por igual en ambas funciones discriminantes.</p>
<p>Para entender mejor los resultados del modelo realizamos otras representaciones. En primer lugar vemos las puntuaciones de las funciones discriminantes para cada grupo.</p>
<p>Nos vamos a centrar en los resultados sobre la muestra de entrenamiento:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># primera función discriminante</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>p <span class="ot">&lt;-</span> <span class="fu">predict</span>(ad.lineal, wine_train)</span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="fu">ldahist</span>(<span class="at">data =</span> p<span class="sc">$</span>x[,<span class="dv">1</span>], <span class="at">g =</span> wine_train<span class="sc">$</span>Class)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-016-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>En este caso las puntuaciones discriminantes si se separan para los tres grupos. El grupo 1 es el que tiene puntuaciones más bajas, mientras que el 3 es el que las tienen más altas. Veamos los resultados para la segunda función discriminante:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># segunda función discriminante</span></span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="fu">ldahist</span>(<span class="at">data =</span> p<span class="sc">$</span>x[,<span class="dv">2</span>], <span class="at">g =</span> wine_train<span class="sc">$</span>Class)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-017-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>En este caso se puede distinguir el grupo 2 respecto de los otros dos que para esta función resultan indistinguibles. La solución nos indica que:</p>
<ul>
<li>El grupo 1 viene caracterizado por puntuaciones bajas en LD1 y altas en LD2.</li>
<li>El grupo 2 viene caracterizado por puntuaciones medias en LD1 y bajas en LD2.</li>
<li>El grupo 3 viene caracterizado por puntuaciones altas en LD1 y altas en LD2.</li>
</ul>
<p>Para entender mejor la solución podemos representar la solución conjunta de las dos funciones discriminantes:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="fu">ggord</span>(ad.lineal, wine_train<span class="sc">$</span>Class, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">7</span>, <span class="dv">5</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">7</span>, <span class="dv">8</span>), <span class="at">txt =</span> <span class="dv">3</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-018-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Con el gráfico conjunto tratamos de representar la información de los grupos proporcionada por el análisis discriminante y la relevancia de las predictoras en cada una de ellas. Por ejemplo, podemos ver que el grupo 3 se caracteriza mayoritariamente por <code>color_intensity</code>. también podemos apreciar fácilmente si la solución proporcionada por el modelo es adecuada. Para relacionar más fácilmente el poder discriminatorio del modelo se suelen representar los mapas de clasificación. la forma habitual es obtener los mapas de clasificación dos a dos de todas las predictoras del modelo.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="fu">partimat</span>(Class<span class="sc">~</span>., <span class="at">data =</span> wine_train, <span class="at">method =</span> <span class="st">"lda"</span>, <span class="at">prec =</span> <span class="dv">200</span>, <span class="at">plot.matrix =</span> <span class="cn">TRUE</span>, <span class="at">image.colors =</span> <span class="fu">c</span>(<span class="st">"darkgoldenrod1"</span>, <span class="st">"snow2"</span>, <span class="st">"skyblue2"</span>))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-019-1.png" class="img-fluid" width="1344"></p>
</div>
</div>
<p>Dada la gran cantidad de predictoras se hace bastante difícil visualizar los resultados. Para mostrar mejor los resultados nos concentramos en las cuatro predictoras más relevantes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="fu">partimat</span>(Class<span class="sc">~</span>Color_intensity <span class="sc">+</span> Flavanoids <span class="sc">+</span> Proline <span class="sc">+</span> Alcalinity_of_ash, <span class="at">data =</span> wine_train, <span class="at">method =</span> <span class="st">"lda"</span>, <span class="at">prec =</span> <span class="dv">200</span>, <span class="at">plot.matrix =</span> <span class="cn">TRUE</span>, <span class="at">image.colors =</span> <span class="fu">c</span>(<span class="st">"darkgoldenrod1"</span>, <span class="st">"snow2"</span>, <span class="st">"skyblue2"</span>))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-020-1.png" class="img-fluid" width="1344"></p>
</div>
</div>
<p>En los mapas de color podemos ver las funciones discriminantes, las zonas de clasificación, y los puntos incorrectamente clasificados (valores en rojo). Para finalizar el análisis de este modelo estudiamos la tabla de clasificación y el porcentaje de clasificación correcta.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># predicción validación</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>pred_test <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">truth =</span> wine_test<span class="sc">$</span>Class, <span class="at">response =</span> <span class="fu">predict</span>(ad.lineal, wine_test)<span class="sc">$</span>class)</span>
<span id="cb27-3"><a href="#cb27-3"></a><span class="co"># matriz de confusión</span></span>
<span id="cb27-4"><a href="#cb27-4"></a>cm <span class="ot">=</span> <span class="fu">confusion_matrix</span>(pred_test<span class="sc">$</span>truth, pred_test<span class="sc">$</span>response)</span>
<span id="cb27-5"><a href="#cb27-5"></a><span class="fu">plot_confusion_matrix</span>(cm<span class="sc">$</span><span class="st">`</span><span class="at">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-021-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Se observa que el porcentaje de clasificación obtenido con el modelo discriminante lineal es del 100%. No es necesario obtener el porcentaje de clasificación correcta. Aunque el modelo lineal proporciona unos resultados muy buenos vamos a explorar la solución cuadrática.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># Modelo para la muestra de entrenamiento</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>ad.quad <span class="ot">=</span> <span class="fu">qda</span>(Class<span class="sc">~</span>., wine_train)</span>
<span id="cb28-3"><a href="#cb28-3"></a><span class="co"># Resultados del modelo</span></span>
<span id="cb28-4"><a href="#cb28-4"></a>ad.quad</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
qda(Class ~ ., data = wine_train)

Prior probabilities of groups:
        1         2         3 
0.3309859 0.4014085 0.2676056 

Group means:
  Alcalinity_of_ash    Alcohol        Ash Color_intensity  Flavanoids
1        -0.7530190  0.9290783  0.2623347       0.2151580  0.97090136
2         0.2474275 -0.8949264 -0.3883909      -0.8735059  0.02533243
3         0.5602243  0.1932664  0.2581198       1.0441423 -1.23885033
         Hue   Magnesium Malic_acid Nonflavanoid_phenols
1  0.4831927  0.43590994 -0.3321547         -0.588689445
2  0.4193928 -0.33253923 -0.3337043         -0.009089938
3 -1.2267223 -0.04034292  0.9113793          0.741750800
  OD280_OD315_of_diluted_wines Proanthocyanins    Proline Total_phenols
1                    0.7560692       0.5466696  1.1669914    0.88203324
2                    0.2617065       0.0384392 -0.7046074   -0.06383159
3                   -1.3276979      -0.7338027 -0.3864730   -0.99518846</code></pre>
</div>
</div>
<p>Veamos las zonas de predicción correspondientes a este modelo para las mismas cuatro predictoras consideradas antes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="fu">partimat</span>(Class<span class="sc">~</span>Color_intensity <span class="sc">+</span> Flavanoids <span class="sc">+</span> Proline <span class="sc">+</span> Alcalinity_of_ash, <span class="at">data =</span> wine_train, <span class="at">method =</span> <span class="st">"qda"</span>, <span class="at">prec =</span> <span class="dv">200</span>, <span class="at">plot.matrix =</span> <span class="cn">TRUE</span>, <span class="at">image.colors =</span> <span class="fu">c</span>(<span class="st">"darkgoldenrod1"</span>, <span class="st">"snow2"</span>, <span class="st">"skyblue2"</span>))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-023-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Podemos ver que las regiones de decisión ahora no se dividen únicamente mediante rectas (modelo lineal), sino mediante curvas. Veamos la capacidad de clasificación de este modelo.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># predicción validación</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>pred_test <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">truth =</span> wine_test<span class="sc">$</span>Class, <span class="at">response =</span> <span class="fu">predict</span>(ad.quad, wine_test)<span class="sc">$</span>class)</span>
<span id="cb31-3"><a href="#cb31-3"></a><span class="co"># matriz de confusión</span></span>
<span id="cb31-4"><a href="#cb31-4"></a>cm <span class="ot">=</span> <span class="fu">confusion_matrix</span>(pred_test<span class="sc">$</span>truth, pred_test<span class="sc">$</span>response)</span>
<span id="cb31-5"><a href="#cb31-5"></a><span class="fu">plot_confusion_matrix</span>(cm<span class="sc">$</span><span class="st">`</span><span class="at">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-024-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Como en el caso anterior la tablas de clasificación proporciona un porcentaje de acierto del 100%. Ambos modelos son igualmente buenos para este conjunto de datos.</p>
</section>
<section id="abalone-1" class="level4" data-number="15.4.2.3">
<h4 data-number="15.4.2.3" class="anchored" data-anchor-id="abalone-1"><span class="header-section-number">15.4.2.3</span> Abalone</h4>
<p>Para finalizar nos centraremos en el banco de datos <code>Abalone</code>. Comenzaremos de nuevo con el modelo lineal y luego compararemos los resultados con el cuadrático. Como siempre comenzamos con la preparación de los datos:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a><span class="co"># División de muestras</span></span>
<span id="cb32-2"><a href="#cb32-2"></a><span class="fu">set.seed</span>(<span class="dv">432</span>)</span>
<span id="cb32-3"><a href="#cb32-3"></a>splits <span class="ot">=</span> mlr3<span class="sc">::</span><span class="fu">partition</span>(tsk_abalone, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb32-4"><a href="#cb32-4"></a>tsk_train_abalone <span class="ot">=</span> tsk_abalone<span class="sc">$</span><span class="fu">clone</span>()<span class="sc">$</span><span class="fu">filter</span>(splits<span class="sc">$</span>train)</span>
<span id="cb32-5"><a href="#cb32-5"></a>tsk_test_abalone  <span class="ot">=</span> tsk_abalone<span class="sc">$</span><span class="fu">clone</span>()<span class="sc">$</span><span class="fu">filter</span>(splits<span class="sc">$</span>test)</span>
<span id="cb32-6"><a href="#cb32-6"></a><span class="co"># preprocesado</span></span>
<span id="cb32-7"><a href="#cb32-7"></a>pp_abalone <span class="ot">=</span> <span class="fu">po</span>(<span class="st">"scale"</span>, <span class="at">param_vals =</span> <span class="fu">list</span>(<span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)) </span>
<span id="cb32-8"><a href="#cb32-8"></a><span class="co"># Obtención de muestras</span></span>
<span id="cb32-9"><a href="#cb32-9"></a>train <span class="ot">=</span> pp_abalone<span class="sc">$</span><span class="fu">train</span>(<span class="fu">list</span>(tsk_train_abalone))</span>
<span id="cb32-10"><a href="#cb32-10"></a>test <span class="ot">=</span> pp_abalone<span class="sc">$</span><span class="fu">train</span>(<span class="fu">list</span>(tsk_test_abalone))</span>
<span id="cb32-11"><a href="#cb32-11"></a>abalone_train <span class="ot">=</span> train[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">data</span>()</span>
<span id="cb32-12"><a href="#cb32-12"></a>abalone_test <span class="ot">=</span> test[[<span class="dv">1</span>]]<span class="sc">$</span><span class="fu">data</span>()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comenzamos con el modelo discriminante lineal.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># Modelo para la muestra de entrenamiento</span></span>
<span id="cb33-2"><a href="#cb33-2"></a>ad.lineal <span class="ot">=</span> <span class="fu">lda</span>(Sex<span class="sc">~</span>., abalone_train)</span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="co"># Resultados del modelo</span></span>
<span id="cb33-4"><a href="#cb33-4"></a>ad.lineal</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Sex ~ ., data = abalone_train)

Prior probabilities of groups:
        F         I         M 
0.3129862 0.3213645 0.3656493 

Group means:
    Diameter     Height     Length      Rings Shell_weight Shucked_weight
F  0.4754051  0.4468167  0.4614434  0.3677620    0.4612112      0.3989033
I -0.8310516 -0.7533291 -0.8115554 -0.6371833   -0.8070032     -0.7660812
M  0.3234662  0.2796279  0.3182821  0.2452175    0.3144799      0.3318481
  Viscera_weight Whole_weight
F      0.4682521    0.4504576
I     -0.8204998   -0.8216986
M      0.3203151    0.3366004

Coefficients of linear discriminants:
                      LD1        LD2
Diameter       -1.1096845 -0.8741631
Height         -0.1910499 -0.4255549
Length          0.7124536  0.2202397
Rings          -0.3332679  0.4250325
Shell_weight    0.2163388 -1.2061618
Shucked_weight  0.1245385  2.0237261
Viscera_weight -0.6412314 -2.1059409
Whole_weight   -0.1802106  2.1471252

Proportion of trace:
  LD1   LD2 
0.985 0.015 </code></pre>
</div>
</div>
<p>La primera función discriminante es la causante del 98.5% de variabilidad explicada, lo que implica que la segunda función tiene poco peso en el proceso de discriminación, para el caso del modelo lineal. Las predictoras más relevantes en este caso son <code>Diameter</code> y <code>Viscera_weight</code> en la parte negativa de la función, y <code>Length</code> en la parte positiva. Veamos el gráfico de las variables:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a>df <span class="ot">=</span> <span class="fu">as.data.frame</span>(ad.lineal<span class="sc">$</span>scaling)</span>
<span id="cb35-2"><a href="#cb35-2"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(LD1, LD2, <span class="at">label =</span> <span class="fu">rownames</span>(df))) <span class="sc">+</span> </span>
<span id="cb35-3"><a href="#cb35-3"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb35-4"><a href="#cb35-4"></a>  <span class="fu">geom_label</span>() <span class="sc">+</span></span>
<span id="cb35-5"><a href="#cb35-5"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-027-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Representamos ahora las puntuaciones de cada función discriminante en cada uno de los grupos:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a><span class="co"># primera función discriminante</span></span>
<span id="cb36-2"><a href="#cb36-2"></a>p <span class="ot">&lt;-</span> <span class="fu">predict</span>(ad.lineal, abalone_train)</span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="fu">ldahist</span>(<span class="at">data =</span> p<span class="sc">$</span>x[,<span class="dv">1</span>], <span class="at">g =</span> abalone_train<span class="sc">$</span>Sex)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-028-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Con la primera función discriminante parece que separamos el grupo <code>I</code> de los grupos <code>F</code> y <code>M</code> que resultan indistinguibles. Veamos los resultados para la segunda función discriminante:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a><span class="co"># segunda función discriminante</span></span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="fu">ldahist</span>(<span class="at">data =</span> p<span class="sc">$</span>x[,<span class="dv">2</span>], <span class="at">g =</span> abalone_train<span class="sc">$</span>Sex)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-029-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Como era de esperar, dado el bajo valor discriminante de esta función, no somo capaces de distinguir los tres grupos con la segunda función discriminante. En lugar de hacer los mapas de cada clase pasamos directamente a estudiar la matriz de confusión.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># predicción validación</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>pred_test <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">truth =</span> abalone_test<span class="sc">$</span>Sex, <span class="at">response =</span> <span class="fu">predict</span>(ad.lineal, abalone_test)<span class="sc">$</span>class)</span>
<span id="cb38-3"><a href="#cb38-3"></a><span class="co"># matriz de confusión</span></span>
<span id="cb38-4"><a href="#cb38-4"></a>cm <span class="ot">=</span> <span class="fu">confusion_matrix</span>(pred_test<span class="sc">$</span>truth, pred_test<span class="sc">$</span>response)</span>
<span id="cb38-5"><a href="#cb38-5"></a><span class="fu">plot_confusion_matrix</span>(cm<span class="sc">$</span><span class="st">`</span><span class="at">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-030-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Los porcentajes se encuentran bastante repartidos, pero si podemos ver que el mayor error de clasificación (con un 15.4%) se alcanza la intentar identificar los sujetos de los grupos <code>M</code> y <code>F</code>. Veamos el porcentaje de clasificación correcta ponderado:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a>cm<span class="sc">$</span><span class="st">`</span><span class="at">Balanced Accuracy</span><span class="st">`</span> </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6460549</code></pre>
</div>
</div>
<p>El porcentaje solo alcanza el 64.6%, es decir, nos estamos equivocando en 1 de cada 3 sujetos a la hora de clasificarlo en su grupo de origen. Veamos que ocurre si utilizamos un modelo cuadrático:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1"></a><span class="co"># Modelo para la muestra de entrenamiento</span></span>
<span id="cb41-2"><a href="#cb41-2"></a>ad.quad <span class="ot">=</span> <span class="fu">qda</span>(Sex<span class="sc">~</span>., abalone_train)</span>
<span id="cb41-3"><a href="#cb41-3"></a><span class="co"># Resultados del modelo</span></span>
<span id="cb41-4"><a href="#cb41-4"></a>ad.quad</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
qda(Sex ~ ., data = abalone_train)

Prior probabilities of groups:
        F         I         M 
0.3129862 0.3213645 0.3656493 

Group means:
    Diameter     Height     Length      Rings Shell_weight Shucked_weight
F  0.4754051  0.4468167  0.4614434  0.3677620    0.4612112      0.3989033
I -0.8310516 -0.7533291 -0.8115554 -0.6371833   -0.8070032     -0.7660812
M  0.3234662  0.2796279  0.3182821  0.2452175    0.3144799      0.3318481
  Viscera_weight Whole_weight
F      0.4682521    0.4504576
I     -0.8204998   -0.8216986
M      0.3203151    0.3366004</code></pre>
</div>
</div>
<p>Analizamos la clasificación obtenida con este modelo:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a><span class="co"># predicción validación</span></span>
<span id="cb43-2"><a href="#cb43-2"></a>pred_test <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">truth =</span> abalone_test<span class="sc">$</span>Sex, <span class="at">response =</span> <span class="fu">predict</span>(ad.quad, abalone_test)<span class="sc">$</span>class)</span>
<span id="cb43-3"><a href="#cb43-3"></a><span class="co"># matriz de confusión</span></span>
<span id="cb43-4"><a href="#cb43-4"></a>cm <span class="ot">=</span> <span class="fu">confusion_matrix</span>(pred_test<span class="sc">$</span>truth, pred_test<span class="sc">$</span>response)</span>
<span id="cb43-5"><a href="#cb43-5"></a><span class="fu">plot_confusion_matrix</span>(cm<span class="sc">$</span><span class="st">`</span><span class="at">Confusion Matrix</span><span class="st">`</span>[[<span class="dv">1</span>]]) </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="150_Discriminantmodels_files/figure-html/discriminant-033-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>La matriz de confusión obtenida es bastante similar a la del caso lineal, así que no esperamos una mejora sustancial en el porcentaje de clasificación correcta.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a>cm<span class="sc">$</span><span class="st">`</span><span class="at">Balanced Accuracy</span><span class="st">`</span> </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6436839</code></pre>
</div>
</div>
<p>El valor del 64.3% es muy similar al del modelo lineal. Esta visto que con las predictoras utilizadas no podemos construir un modelo discriminante que nos permita distinguir el sexo de cada uno de los elementos de la muestra con una alta precisión.</p>
</section>
</section>
</section>
<section id="sec-150.5" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="sec-150.5"><span class="header-section-number">15.5</span> Ejercicios</h2>
<ol type="1">
<li>Ajustar un modelo de aprendizaje automático basado en análisis discriminante para el banco de datos <code>Iris</code><a href="40_DataBases.html#sec-iris"><span>4.3.3</span></a>.</li>
<li>Ajustar un modelo de aprendizaje automático basado en análisis discriminante para el banco de datos <code>Breast cancer</code><span class="quarto-unresolved-ref">?sec-breastcancer</span>.</li>
<li>Ajustar un modelo de aprendizaje automático basado en análisis discriminante para el banco de datos <code>Wine quality</code><a href="40_DataBases.html#sec-winequality"><span>4.3.8</span></a>.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "¡Copiado!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "¡Copiado!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04_NonSupervisedAA.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Parte 4. Aprendizaje no supervisado</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./160_PrinCompmodels.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Componentes principales (CP)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2023, IA4LEGOS. Universidad Miguel Hernández de Elche</div>   
  </div>
</footer>



</body></html>