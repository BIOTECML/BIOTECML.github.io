# rboles de decisi贸nn (DT) {#sec-120}

Los 谩rboles de decisi贸n son modelos predictivos, englobados dentro de los algoritmos de aprendizaje supervisado no param茅trico, basados en reglas binarias con las que se consiguen repartir las observaciones en funci贸n de sus caracter铆sticas y predecir as铆 el valor de la variable respuesta bien sea num茅rica o categ贸rica.

Muchos m茅todos predictivos generan modelos globales en los que una 煤nica ecuaci贸n se aplica a todo el espacio muestral. En situaciones pr谩cticas que implican m煤ltiples predictores, que interaccionan entre ellos de forma compleja y no lineal, es muy dif铆cil encontrar un 煤nico modelo global que sea capaz de reflejar la relaci贸n entre las variables. Con los modelos basados en 谩rboles de decisi贸n resulta m谩s sencillo manejar las interacciones y situaciones con muchos predictores. Es esta caracter铆stica la que les proporciona gran parte de su potencial.

A lo largo de este documento se explora la forma en que se construyen y predicen los 谩rboles de decisi贸n para problemas de clasificaci贸n, que adem谩s resultan elementos fundamentales de modelos predictivos m谩s complejos como *Random Forest* y *Gradient Boosting Machine*.

Como su propio nombre indica, los 谩rboles de decisi贸n se estructuran en forma de 谩rbol, en la que cada rama representa una decisi贸n sobre una de las variables predictoras proporcionando dos sub-ramas para cada una de las soluciones de la regla binaria asociada a dicha rama. A continuaci贸n se presentan algunas de las terminolog铆as m谩s importantes relacionadas con un rbol de Decisi贸n:

-   **Nodo ra铆z:** generalmente representa toda la muestra y es el nodo superior del 谩rbol de decisi贸n (ra铆z del 谩rbol).
-   **Separaci贸n:** proceso de divisi贸n de un nodo en dos o m谩s subnodos.
-   **Nodo de decisi贸n:** es un nodo o subnodo que divide los datos en otros subnodos.
-   **Nodo terminal:** los nodos que no se dividen se denominan nodos terminales; son las salidas finales del 谩rbol de decisi贸n.
-   **Poda:** lo contrario de la separaci贸n. Cuando se elimina un subn煤cleo de un nodo de decisi贸n, el proceso se denomina poda.
-   **Sub谩rbol:** una subsecci贸n de todo el 谩rbol se denomina rama o sub谩rbol.
-   **Nodo padre:** un nodo dividido en subnodos se denomina nodo padre.
-   **Nodo hijo:** cualquier subnodo de un nodo padre se llama nodo hijo.

A continuaci贸n se muestra gr谩ficamente la estructura de un 谩rbol de decisi贸n.

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/decisiontree.png){fig-align="center" width="550" height="400"}

En todo el documento iremos detallando la construcci贸n te贸rica de los 谩rboles de decisi贸n tanto para tareas de clasificaci贸n como regresi贸n.

## rboles de decisi贸n en tareas de clasificaci贸n {#sec-120.1}

Para construir un 谩rbol de clasificaci贸n, se emplea un m茅todo de divisi贸n binaria recursiva en cuyo proceso es necesario tener en cuenta que:

-   Todo el conjunto de datos se considera como parte del nodo ra铆z al comienzo del proceso de divisi贸n.

-   Dado que se producen divisiones de tipo binario a cada paso del algoritmo se prefieren los valores de caracter铆sticas categ贸ricas en el proceso de construcci贸n. Si los valores son continuos, deben discretizarse antes de construir el modelo.

-   Las muestras se distribuyen recursivamente en funci贸n de las reglas de decisi贸n establecidas con respecto a la predictora considerada en cada paso del algoritmo.

-   El proceso de divisi贸n se debe realizar mediante un enfoque estad铆stico estableciendo una funci贸n de p茅rdida o ganancia que es necesario optimizar.

Antes de presentar el algoritmo completo para la creaci贸n del 谩rbol de decisi贸n se presenta el enfoque estad铆stico para la construcci贸n de las reglas de decisi贸n 贸ptimas.

### Medidas de impureza {#sec-120.1.1}

A la hora de establecer los criterios estad铆sticos a tener en cuenta en la construcci贸n del 谩rbol de decisi贸n existen varias alternativas, todas ellas con el objetivo de encontrar nodos lo m谩s puros/homog茅neos posibles. Antes de presentar las medidas m谩s habituales introducimos el concepto de entrop铆a que est谩 directamente relacionado con la construcci贸n del 谩rbol.

La incertidumbre en nuestro conjunto de datos o la medida del desorden se llama entrop铆a. Su valor describe el grado de aleatoriedad de un nodo concreto, de forma que, cuanto mayor es la entrop铆a, mayor ser谩 la aleatoriedad en el conjunto de datos, y por tanto menos influencia tiene la predictora considerada en la divisi贸n del 谩rbol. La f贸rmula general de la entrop铆a en un conjunto de datos categ贸ricos con k clases viene dada por:

$$H = \sum_{i=1}^{k} -p_ilog(p_i)$$

donde $p_i$ es la proporci贸n de observaciones de la categor铆a $i$ en el conjunto de datos considerado.

Los m茅todos de medidas m谩s empleadas son:

-   **Ratio de error de clasificaci贸n.** Se define como la proporci贸n de observaciones que no pertenecen a la clase mayoritaria del nodo, definida como:

$$E_m = 1- \underset{k}{max} \text{ } \hat{p}_{mk}$$

> donde $\hat{p}_{mk}$ representa la proporci贸n de observaciones del nodo $$ que pertenecen a la clase $$. A pesar de la sencillez de esta medida, no es suficientemente sensible para crear buenos 谩rboles, por lo que, en la pr谩ctica, no suele emplearse.

-   **Ganancia de informaci贸n.** La ganancia de informaci贸n ayuda a determinar el orden en que las predictoras consideradas deben ser utilizadas para dividir un nodo o no. Es simplemente una medida de los cambios en la entrop铆a tras la segmentaci贸n de un conjunto de datos basado en una predictora espec铆fica. Calcula cu谩nta informaci贸n nos proporciona una caracter铆stica sobre una clase. En funci贸n del valor de la ganancia de informaci贸n, dividimos los nodos y construimos un 谩rbol de decisi贸n. El nodo/atributo con mayor ganancia de informaci贸n se divide primero en una estructura de 谩rbol, que siempre maximiza el valor de la ganancia de informaci贸n. La expresi贸n para dicha medida viene dada por:

$$D = - \sum_{k=1}^{K} \hat{p}_{mk} log(\hat{p}_{mk}).$$

> Los conocidos como algortimos C4.5 y C5.0 utilizan este criterio para la construcci贸n del 谩rbol.

-   **ndice de Gini.** El 铆ndice de Gini, tambi茅n conocido como impureza de Gini o coeficiente de Gini, mide la probabilidad de que un nuevo valor de una variable aleatoria se clasifique incorrectamente si se clasificara al azar utilizando la distribuci贸n de etiquetas de clase del conjunto de datos. T茅cnicamente cuantifica la varianza total en el conjunto de las $$ clases del nodo $$, es decir, mide la pureza del nodo mediante la expresi贸n:

$$G_m = \sum_{k=1}^{K} \hat{p}_{mk} (1-\hat{p}_{mk}).$$

> Cuando $\hat{p}_{mk}$ es cercano a 0 o a 1 (el nodo contiene mayoritariamente observaciones de una sola clase), el t茅rmino correspondiente es muy peque帽o. Como consecuencia, cuanto mayor sea la pureza del nodo, menor el valor del 铆ndice Gini.

> El algoritmo CART (*Classification and Regression Tree*) utiliza este criterio para la construcci贸n del 谩rbol.

-   **Ji-cuadrado.** Esta aproximaci贸n consiste en identificar si existe una diferencia significativa entre los nodos hijos y el nodo parental, es decir, si hay evidencias de que la divisi贸n consigue una mejora. Para ello, se aplica un test estad铆stico ji-cuadrado de bondad de ajuste empleando como distribuci贸n esperada $_0$ la frecuencia de cada clase en el nodo parental. Cuanto mayor el estad铆stico $^2$ , mayor es la evidencia estad铆stica de que existe una diferencia. Los 谩rboles generados con este criterio de divisi贸n reciben el nombre de CHAID (*Chi-square Automatic Interaction Detector*).

Independientemente de la medida empleada como criterio de selecci贸n de las divisiones, el proceso de construcci贸n del 谩rbol siempre es el mismo:

1.  Para cada posible divisi贸n se calcula el valor de la medida considerada en cada uno de los dos nodos resultantes.
2.  Se suman los dos valores, ponderando cada uno por la fracci贸n de observaciones que contiene cada nodo. Este paso es muy importante, ya que no es lo mismo dos nodos puros con 2 observaciones, que dos nodos puros con 100 observaciones. Si consideramos como $n_A$ y $n_B$ el n煤mero de observaciones en los nodos A y B resultantes de la divisi贸n con $n=n_A+n_B$, y por $p_A$ y $p_B$ las medidas de pureza calculadas para cada uno de ellos el criterio de divisi贸n se basa en:

$$\frac{n_a}{n}p_A + \frac{n_b}{n}p_B$$

3.  La divisi贸n con menor o mayor valor (dependiendo de la medida empleada) se selecciona como punto de corte 贸ptimo.

### El algortmo CART {#sec-120.1.2}

El algoritmo CART es uno de los m谩s extendidos en la construcci贸n de 谩rboles de decisi贸n. Este funciona dividiendo primero el conjunto de entrenamiento por caracter铆sticas $k$ y umbrales $t_k$. M谩s concretamente, de entre todos los pares $(k, t_k)$ se eligen los que producen los subconjuntos m谩s puros ponderados por su tama帽o.

La funci贸n de p茅rdida en la que se basa el funcionamiento del algoritmo viene dada por:

$$J(k, t_k)=\frac{n_a}{n}G_A + \frac{n_B}{n}G_B$$

donde $G_A$ y $G_B$ son respectivamente las medidas de impureza asociadas con cada uno de los nodos resultantes, que en este caso es el 铆ndice de Gini.

Una vez que el algoritmo CART divide con 茅xito los datos de entrenamiento iniciales en dos subconjuntos, hace lo mismo con ambos subconjuntos. El algoritmo se detiene cuando no puede encontrar una divisi贸n que reduzca la impureza.

Al algoritmo CART no le importa si su divisi贸n actual conduce a una hoja 贸ptima en la parte inferior. S贸lo le importa encontrar la mejor divisi贸n posible en la hoja actual. En este sentido, no necesariamente da lugar a una soluci贸n 贸ptima. Por desgracia, se sabe que encontrar el 谩rbol 贸ptimo es un problema NP-Completo con una complejidad de $O(exp(n))$.

### Tratamiento de sobreajuste {#sec-120.1.3}

El proceso de construcci贸n de 谩rboles tiende a reducir r谩pidamente el error de entrenamiento, es decir, el modelo se ajusta muy bien a las observaciones empleadas como entrenamiento. Como consecuencia, se genera un sobreajuste que reduce su capacidad predictiva al aplicarlo a nuevos datos. La raz贸n de este comportamiento radica en la facilidad con la que los 谩rboles se ramifican adquiriendo estructuras complejas. De hecho, si no se limitan las divisiones, todo 谩rbol termina ajust谩ndose perfectamente a las observaciones de entrenamiento creando un nodo terminal por observaci贸n. Las dos estrategias m谩s habituales para prevenir este problema es limitar el tama帽o del 谩rbol (parada temprana) y el proceso de podado (*pruning*).

**Parada temprana**

El tama帽o final que adquiere un 谩rbol puede controlarse mediante reglas de parada que detengan la divisi贸n de los nodos dependiendo de si se cumplen o no determinadas condiciones. El nombre de estas condiciones puede variar dependiendo del software o librer铆a empleada, pero suelen estar presentes en todos ellos.

-   *Observaciones m铆nimas para divisi贸n:* define el n煤mero m铆nimo de observaciones que debe tener un nodo para poder ser dividido. Cuanto mayor el valor, menos flexible es el modelo.

-   *Observaciones m铆nimas de nodo terminal:* define el n煤mero m铆nimo de observaciones que deben tener los nodos terminales. Su efecto es muy similar al de observaciones m铆nimas para divisi贸n.

-   *Profundidad m谩xima del 谩rbol:* define la profundidad m谩xima del 谩rbol, entendiendo por profundidad m谩xima el n煤mero de divisiones de la rama m谩s larga (en sentido descendente) del 谩rbol. Cuanto menor el valor, menos flexible es el modelo.

-   *N煤mero m谩ximo de nodos terminales:* define el n煤mero m谩ximo de nodos terminales que puede tener el 谩rbol. Una vez alcanzado el l铆mite, se detienen las divisiones. Su efecto es similar al de controlar la profundidad m谩xima del 谩rbol.

-   *Reducci贸n m铆nima de error:* define la reducci贸n m铆nima de error que tiene que conseguir una divisi贸n para que se lleve a cabo.

Todos estos par谩metros son lo que se conoce como hiperpar谩metros porque no se aprenden durante el entrenamiento del modelo. Su valor tiene que ser especificado por el usuario en base a su conocimiento del problema y mediante el uso de estrategias de validaci贸n.

**Podado del 谩rbol**

La estrategia de controlar el tama帽o del 谩rbol mediante reglas de parada tiene un inconveniente, el 谩rbol crece seleccionando la mejor divisi贸n en cada momento. Al evaluar las divisiones sin tener en cuenta las que vendr谩n despu茅s, nunca se elige la opci贸n que resulta en el mejor 谩rbol final, a no ser que tambi茅n sea la que genera en ese momento la mejor divisi贸n. A este tipo de estrategias se les conoce como *greedy*.

Una alternativa no *greedy* que consigue evitar el sobreajuste consiste en generar los 谩rboles m谩s grandes posibles, sin establecer condiciones de parada m谩s all谩 de las necesarias por las limitaciones computacionales, y despu茅s podarlos (*pruning*), mantener la estructura que consigue un test error bajo. La selecci贸n del sub-谩rbol 贸ptimo puede hacerse mediante validaci贸n cruzada, sin embargo, dado que los 谩rboles crecen lo m谩ximo posible (tienen muchos nodos terminales) no suele ser viable estimar el test error de todas las posibles sub-estructuras que se pueden generar. En su lugar, se recurre a la "poda de complejidad de costes" o "poda del eslab贸n m谩s d茅bil".

La poda de complejidad por costes es un m茅todo de penalizaci贸n de tipo "coste" mas "penalizaci贸n", similar al empleado en *Ridge Regression* o *Lasso*. En este caso, se busca el sub-谩rbol $$ que minimiza la ecuaci贸n:

$$\text{coste} + \alpha|T|$$

donde $|T|$ es el n煤mero de nodos terminales del 谩rbol. El t茅rmino de penalizaci贸n, eval煤a los modelos en funci贸n del n煤mero de nodos terminales (a mayor n煤mero, mayor penalizaci贸n). El grado de penalizaci贸n se determina mediante el par谩metro de ajuste $\alpha$. Cuando $\alpha=0$, la penalizaci贸n es nula y el 谩rbol resultante es equivalente al 谩rbol original. A medida que se incrementa dicho par谩metro, la penalizaci贸n es mayor y, como consecuencia, los 谩rboles resultantes son de menor tama帽o. El valor 贸ptimo de $\alpha$ puede identificarse mediante validaci贸n cruzada.

### Predicci贸n y evaluaci贸n del modelo {#sec-120.1.4}

Tras la creaci贸n de un 谩rbol, las observaciones de entrenamiento quedan agrupadas en los nodos terminales. Para predecir una nueva observaci贸n se recorre el 谩rbol en funci贸n del valor de sus predictores hasta llegar a uno de los nodos terminales. En el caso de clasificaci贸n, suele emplearse la moda de la variable respuesta de los elementos que aparecen en el nodo terminal como valor de predicci贸n. Lo habitual adem谩s es acompa帽ar dicho valor con el porcentaje de cada clase en el nodo terminal, lo que aporta informaci贸n sobre la confianza de la predicci贸n, en caso de que los porcentajes de las diferentes clases se encuentren muy pr贸ximos.

## rboles de decisi贸n en tareas de regresi贸n {#sec-120.2}

Los 谩rboles de regresi贸n son el subtipo de 谩rboles de predicci贸n que se aplica cuando la variable respuesta es continua. En t茅rminos generales, en el entrenamiento de un 谩rbol de regresi贸n, las observaciones se van distribuyendo por bifurcaciones (nodos) generando la estructura del 谩rbol hasta alcanzar un nodo terminal.

El proceso de entrenamiento de un 谩rbol de decisi贸n para problemas de regresi贸n es similar al del proceso de clasificaci贸n donde se produce una divisi贸n sucesiva del espacio de los predictores generando regiones no solapantes (nodos terminales) $_1$ , $_2$ , $_3$ , ..., $_$ . Aunque, desde el punto de vista te贸rico las regiones podr铆an tener cualquier forma, si se limitan a regiones rectangulares (de m煤ltiples dimensiones), se simplifica en gran medida el proceso de construcci贸n y se facilita la interpretaci贸n.

### Medidas de impureza {#sec-120.2.1}

En los 谩rboles de regresi贸n, el criterio empleado con m谩s frecuencia para identificar las divisiones es la suma de cuadrados residual (SCE). El objetivo es encontrar las $$ regiones $(_1 ,..., _)$ que minimizan la suma de cuadrados del error total:

$$SCE = \sum_{j=1}^J \sum_{i \in R_j} (_\hat{y}_{R_j})^2,$$

donde $\hat{y}_{R_j}$ es la media de la variable respuesta en la regi贸n $_$ . Una descripci贸n menos t茅cnica equivale a decir que se busca una distribuci贸n de regiones tal que, el sumatorio de las desviaciones al cuadrado entre las observaciones y la media de la regi贸n a la que pertenecen sea lo menor posible.

Desafortunadamente, no es posible considerar todas las posibles particiones del espacio de los predictores. Por esta raz贸n, se recurre a lo que se conoce como divisi贸n binaria recursiva. Esta soluci贸n sigue la misma idea que la selecci贸n de predictores *stepwise* (*backward* o *fordward*) en regresi贸n lineal m煤ltiple, no eval煤a todas las posibles regiones pero, alcanza un buen balance computaci贸n-resultado.

### Algoritmo CART para problemas de regresi贸n {#sec-120.2.2}

EL algoritmo CART para problemas de regresi贸n se basa en el m茅todo de divisi贸n binaria recursiva cuyo objetivo es encontrar, en cada iteraci贸n, el predictor $_$ y el punto de corte (umbral) $$ tal que, si se distribuyen las observaciones en las regiones ${|<}$ y ${|ヰ}$ , se consigue la mayor reducci贸n posible en la suma de cuadrados de los residuos (SCE). El algoritmo seguido es:

1.  El proceso se inicia en lo m谩s alto del 谩rbol, donde todas las observaciones pertenecen a la misma regi贸n.

2.  Se identifican todos los posibles puntos de corte $$ para cada uno de los predictores $(_1, _2 ,..., _)$. En el caso de predictores cualitativos, los posibles puntos de corte son cada uno de sus niveles. Para predictores continuos, se ordenan de menor a mayor sus valores, emple谩ndose el punto intermedio entre cada par de valores como punto de corte.

3.  Se calcula la SCR total que se consigue con cada posible divisi贸n identificada en el paso 2:

$$\sum_{i: x_i \in R_1(j,s)} (y_i-\hat{y}_{R_1})^2 +\sum_{i: x_i \in R_2(j,s)} (y_i-\hat{y}_{R_2})^2$$

> donde el primer t茅rmino es la SCE de la regi贸n 1 y el segundo t茅rmino es la SCE de la regi贸n 2, siendo cada una de las regiones el resultado de separar las observaciones acorde al predictor $$ y valor $$.

4.  Se selecciona el predictor $_$ y el punto de corte $s$ que resulta en la menor SCE total, es decir, que da lugar a las divisiones m谩s homog茅neas posibles. Si existen dos o m谩s divisiones que consiguen la misma mejora, la elecci贸n entre ellas es aleatoria.

5.  Se repiten de forma iterativa los pasos 1 a 4 para cada una de las regiones que se han creado en la iteraci贸n anterior hasta que se alcanza alguna norma de parada. Algunas de las m谩s empleadas son: alcanzar una profundidad m谩xima, que ninguna regi贸n contenga menos de n observaciones, que el 谩rbol tenga un m谩ximo de nodos terminales o que la incorporaci贸n del m谩s nodos no reduzca el error en al menos un % m铆nimo.

Para mejorar el funcionamiento de este algoritmo se suelen incorporar estrategias para evitar evaluar todos los posibles puntos de corte. Por ejemplo, para predictores continuos, primero se crea un histograma que agrupa los valores y luego se eval煤an los puntos de corte de cada regi贸n del histograma.

### Tratamiento del sobreajuste {#sec-120.2.3}

El tratamiento del sobreajuste utiliza los mismos procedimientos que en el caso de los 谩rboles de decisi贸n para clasificaci贸n. Leer el cuaderno anterior para conocer todos los detalles.

### Predicci贸n y evaluaci贸n del modelo {#sec-120.2.4}

Tras la creaci贸n de un 谩rbol, las observaciones de entrenamiento quedan agrupadas en los nodos terminales. Para predecir una nueva observaci贸n, se recorre el 谩rbol en funci贸n de los valores que tienen sus predictores hasta llegar a uno de los nodos terminales. En el caso de regresi贸n, el valor predicho suele ser la media de la variable respuesta de las observaciones de entrenamiento que est谩n en ese mismo nodo. Si bien la media es el valor m谩s empleado, se puede utilizar cualquier otro (mediana, cuantil...).

Sin embargo, la predicci贸n de un 谩rbol de decisi贸n para regresi贸n puede verse como una variante de vecinos cercanos en la que, solo las observaciones que forman parte del mismo nodo terminal que la observaci贸n predicha, tienen influencia. Siguiendo esta aproximaci贸n, la predicci贸n del 谩rbol se define como la media ponderada de todas las observaciones de entrenamiento, donde el peso de cada observaci贸n depende 煤nicamente de si forma parte o no del mismo nodo terminal.

Imaginemos que tenemos un 谩rbol con cuatro nodos terminales con observaciones y valores de la respuesta para la muestra de entrenamiento:

-   nodo 1: 1 (10), 3 (24), 7 (16)
-   nodo 2: 4 (8), 10 (14)
-   nodo 3: 2 (18), 3 (24), 5 (2), 9 (20)
-   nodo 4: 6 (9), 8 (10)

de forma que realizamos la predicci贸n de una nueva observaci贸n y esta cae en el nodo tres. La predicci贸n viene dada entonces por la media ponderada del n煤mero de observaciones:

$$\hat{\mu} = 0.25*18 + 0.25*24 + 0.25*2 + 0.25*20 = 16$$

## Ventajas y desventajas de los 谩rboles de decisi贸n {#sec-120.3}

Entre las ventajas y desventajas del uso de 谩rboles de decisi贸n podemos considerar:

**Ventajas**

-   Los 谩rboles son f谩ciles de interpretar a煤n cuando las relaciones entre predictores son complejas.

-   Los modelos basados en un solo 谩rbol (no es el caso de *random forest*, *boosting*) se pueden representar gr谩ficamente a煤n cuando el n煤mero de predictores es mayor de 3.

-   Los 谩rboles pueden, en teor铆a, manejar tanto predictores num茅ricos como categ贸ricos sin tener que crear variables *dummy* o *one-hot-encoding*. En la pr谩ctica, esto depende de la implementaci贸n del algoritmo que tenga cada librer铆a.

-   Al tratarse de m茅todos no param茅tricos, no es necesario que se cumpla ning煤n tipo de distribuci贸n espec铆fica.

-   Por lo general, requieren mucha menos limpieza y preprocesado de los datos en comparaci贸n con otros m茅todos de aprendizaje estad铆stico (por ejemplo, no requieren estandarizaci贸n).

-   No se ven muy influenciados por observaciones an贸malas.

-   Si para alguna observaci贸n el valor de un predictor no est谩 disponible, a pesar de no poder llegar a ning煤n nodo terminal, se puede conseguir una predicci贸n empleando todas las observaciones que pertenecen al 煤ltimo nodo alcanzado. La precisi贸n de la predicci贸n se ver谩 reducida pero al menos podr谩 obtenerse.

-   Son muy 煤tiles en la exploraci贸n de datos, ya que permiten identificar de forma r谩pida y eficiente las variables (predictores) m谩s importantes.

-   Son capaces de seleccionar predictores de forma autom谩tica.

**Desventajas**

-   La capacidad predictiva de los modelos basados en un 煤nico 谩rbol es bastante inferior a la conseguida con otros modelos. Esto es debido a su tendencia al sobreajuste y a la alta varianza. Sin embargo, existen t茅cnicas m谩s complejas que, haciendo uso de la combinaci贸n de m煤ltiples 谩rboles (*bagging*, *random forest*, *boosting*), consiguen mejorar en gran medida este problema.

-   Son sensibles a datos de entrenamiento desbalanceados (una de las clases domina sobre las dem谩s).

-   Cuando tratan con predictores continuos, pierden parte de su informaci贸n al categorizarlos en el momento de la divisi贸n de los nodos.

-   Los predictores continuos tienen mayor probabilidad de contener, solo por azar, alg煤n punto de corte 贸ptimo, por lo que suelen verse favorecidos en la creaci贸n de los 谩rboles.

-   No son capaces de extrapolar fuera del rango valores observados para los predictores en los datos de entrenamiento.

## rboles de decisi贸n en mlr3 {#sec-120.4}

Antes de comenzar con la implementaci贸n de los DT en `mlr3` vamos a cargar las librer铆as necesarias:

```{r}
#| label: dt-001
#| message: false
#| results: false
#| warning: false

# Paquetes anteriores
library(tidyverse)
library(sjPlot)
library(knitr) # para formatos de tablas
library(skimr)
library(DataExplorer)
library(GGally)
library(gridExtra)
library(ggpubr)
library(cvms)
library(kknn)
theme_set(theme_sjplot2())

# Paquetes AA
library(mlr3verse)
library(mlr3tuning)
library(mlr3tuningspaces)
```

Para implementar los 谩rboles de decisi贸n en el paquete `mlr3` disponemos de varias funciones tanto para las tareas de clasificaci贸n como de regresi贸n, pero nosotros nos centraremos en los algoritmos m谩s b谩sicos:

-   `classif.rpart` para la obtenci贸n de 谩rboles de decisi贸n en tareas de clasificaci贸n.
-   `regr.rpart` para la obtenci贸n de 谩rboles de decisi贸n en tareas de regresi贸n.

que utilizan como base las funciones definidas en la librer铆a `rpart`.

Podemos cargar los algoritmos con su hiperpar谩metros por defecto con el c贸digo siguiente:

```{r}
#| label: dt-002
#| message: false
#| warning: false

# Learner tarea de clasificaci贸n
ldt_classif = lrn("classif.rpart", keep_model = TRUE)
# Learner tarea de regresi贸n
ldt_regr = lrn("regr.rpart", keep_model = TRUE)
```

En este caso los hiperpar谩metros de ambos algoritmos son los mismos. A continuaci贸n se muestran todos ellos:

```{r}
#| label: dt-003
#| warning: false
#| message: false

# Hiperpar谩metros para DT clasificaci贸n
ldt_classif$param_set$ids()
```

Los par谩metros m谩s relevantes son:

-   `cp`: par谩metro de complejidad. Cualquier divisi贸n que no disminuya la falta general de ajuste en un factor de cp no es tenida en cuenta. Por ejemplo, con la divisi贸n de anova, esto significa que el R cuadrado general debe aumentar en cp en cada paso. El papel principal de este par谩metro es para ahorrar tiempo de c谩lculo eliminando divisiones que obviamente no valen la pena. B谩sicamente, el usuario informa al programa que cualquier divisi贸n que no mejora el ajuste por cp probablemente ser谩 eliminado mediante validaci贸n cruzada, y que por lo tanto el programa no necesita perseguirlo. La opci贸n por defecto es 0.01 y puede tomar cualquier valor en el rango $[0, 1]$.
-   `maxcompete`: el n煤mero de divisiones de competidores retenidas en la salida. Es 煤til saber no solo qu茅 divisi贸n se eligi贸, pero qu茅 variable qued贸 en segundo, tercer lugar, etc. La opci贸n por defecto es 4 con valores en el intervalo $[0, \infty]$.
-   `maxdepth`: Establece la profundidad m谩xima de cualquier nodo del 谩rbol final, con el nodo ra铆z contado como profundidad 0. Los valores superiores a 30 rpart dar谩n resultados sin sentido en m谩quinas de 32 bits. Por defecto se utiliza el valor 30 con un rango de valores posibles en el intervalo $[1, 30]$.
-   `maxsurrogate`: n煤mero de divisiones sustitutas retenidas en la salida. Si se establece en cero, el tiempo de c谩lculo se reducir谩, ya que aproximadamente la mitad del tiempo de c谩lculo (aparte del de configuraci贸n) se utiliza en la b煤squeda de divisiones sustitutas. Por defecto se utiliza el valor de 5 con un rango de valores posibles en el intervalo $[0, \infty]$.
-   `minbucket`: el n煤mero m铆nimo de observaciones en cualquier nodo terminal. Si solo se especifica uno de minbucket o minsplit, el c贸digo establece minsplit en minbucket\*3 o minbucket en minsplit/3, seg煤n corresponda. No tiene valor por defecto pero el rango de valores posibles en el intervalo $[1, \infty]$.
-   `minsplit`: el n煤mero m铆nimo de observaciones que deben existir en un nodo para que se intente una divisi贸n. El valor por defecto es 20 con una rango de valores posibles en el intervalo $[1, \infty]$.
-   `surrogatestyle`: controla la selecci贸n de la mejor sustituto. Si se establece en 0 (predeterminado), el programa usa el n煤mero total de clasificaciones correctas para una posible variable sustituta; si se establece en 1, usa el porcentaje correcto, calculado sobre los valores no faltantes del sustituto. La primera opci贸n penaliza m谩s severamente las covariables con una gran cantidad de valores faltantes. El valor por defecto es 0 con valores posibles en el rango $[0, 1]$.
-   `usesurrogate`: c贸mo utilizar sustitutos en el proceso de divisi贸n. 0 significa solo visualizaci贸n; una observaci贸n con un valor faltante para la regla de divisi贸n principal no se env铆a m谩s abajo en el 谩rbol. 1 significa utilizar sustitutos, en orden, para dividir a los sujetos a los que les falta la variable principal; si faltan todos los sustitutos, la observaci贸n no se divide. Para el valor 2, si faltan todos los sustitutos, env铆e la observaci贸n en la direcci贸n mayoritaria. Un valor de 0 corresponde a la acci贸n del 谩rbol, y 2 a las recomendaciones de Breiman y otros (1984). El valor por defecto es 2 con valores en el intervalo $[0, 2]$.
-   `xval`: n煤mero de validaciones cruzadas. El valor por defecto es 10 con valores en el rango $[0, \infty]$.

## Bancos de datos {#sec-120.5}

Para ejemplificar el uso de estos algoritmos vamos a utilizar los bancos de datos `stroke`, `penguins` para tareas de clasificaci贸n, y `electricity` para tareas de regresi贸n. Dado que los algoritmos DT permiten trabajar con factores, y predictores sin escalar no resulta necesario realizar dicha tarea de preprocesamiento. Lo que si resulta necesaria es imputar los valores perdidos como veremos en el punto siguiente.

### Stroke {#sec-120.5.1}

Cargamos los datos y definimos la tarea correspondiente

```{r}
#| label: dt-004
#| warning: false
#| message: false

# Leemos datos
stroke = read_rds("stroke.rds")
# Eliminamos la variable id
stroke = stroke %>% dplyr::select(-id)
# creamos la tarea
tsk_stroke = as_task_classif(stroke, target = "stroke")
```

Creamos ahora la divisi贸n de muestras:

```{r}
#| label: dt-005
#| warning: false
#| message: false

# Generamos variable de estrato
tsk_stroke$col_roles$stratum <- "stroke"
# Fijamos semilla para asegurar la reproducibilidad del modelo
set.seed(432)
# Creamos la partici贸n
splits = mlr3::partition(tsk_stroke, ratio = 0.8)
# Muestras de entrenamiento y validaci贸n
tsk_train_stroke = tsk_stroke$clone()$filter(splits$train)
tsk_test_stroke  = tsk_stroke$clone()$filter(splits$test)
```

### Penguins {#sec-120.5.2}

El banco de datos ya ha sido descrito en detalle en temas anteriores pero en este caso vamos a utilizar el que se encuentra disponible en la librer铆a `mlr3`. Definimos la tarea.

```{r}
#| label: dt-006
#| warning: false
#| message: false

# creamos la tarea
tsk_penguins = tsk("penguins")
```

Creamos ahora las muestras de entrenamiento y validaci贸n:

```{r}
#| label: dt-009
#| warning: false
#| message: false

# Generamos variable de estrato
tsk_penguins$col_roles$stratum <- "sex"
# Fijamos semilla para asegurar la reproducibilidad del modelo
set.seed(432)
# Creamos la partici贸n
splits = mlr3::partition(tsk_penguins, ratio = 0.8)
# Muestras de entrenamiento y validaci贸n
tsk_train_penguins = tsk_penguins$clone()$filter(splits$train)
tsk_test_penguins  = tsk_penguins$clone()$filter(splits$test)
```

### Electricity {#sec-120.5.3}

Cargamos los datos y generamos las muestras de entrenamiento y validaci贸n.

```{r}
#| label: dt-010
#| message: false
#| warning: false

# Carga de datos
electricity = read_rds("electricity.rds")
# Creaci贸n de task
tsk_electricity = as_task_regr(electricity, target = "PE")
```

Ahora la divisi贸n de muestras:

```{r}
#| label: dt-011
#| message: false
#| warning: false

# Fijamos semilla para asegurar la reproducibilidad del modelo
set.seed(432)
# Creamos la partici贸n
splits = mlr3::partition(tsk_electricity, ratio = 0.8)
# Muestras de entrenamiento y validaci贸n
tsk_train_electricity = tsk_electricity$clone()$filter(splits$train)
tsk_test_electricity  = tsk_electricity$clone()$filter(splits$test)
```

## Nuestros primeros modelos {#sec-120.6}

En primer lugar consideramos modelos b谩sicos de DT para los dos bancos de datos presentados. Trabajaremos con las opciones por defecto para poder comparar los resultados con el modelo optimizado que veremos posteriormente. Tambi茅n compararemos los resultados con otros modelos de clasificaci贸n de los estudiados hasta ahora.

### Stroke {#sec-120.6.1}

En primer lugar generamos el algoritmo DT para este banco de datos y entrenamos el modelo. Dado que los datos preprocesados se generan a trav茅s de un graphlearner no podemos representar directamente la soluci贸n del 谩rbol de decisi贸n. Despu茅s de ver la soluci贸n habitual veremos como completar los datos para tener que definir 煤nicamente un learner y poder representar la soluci贸n mediante la funci贸n `autoplot`.

```{r}
#| label: dt-012
#| warning: false
#| message: false

# Preprocesamiento
pp_stroke =  po("imputemedian", affect_columns = selector_type("numeric"))
# Modelo de aprendizaje combinando preprocesado y algoritmo
ldt_classif_stroke = as_learner(pp_stroke %>>% lrn("classif.rpart", keep_model = TRUE, predict_type = "prob"))
# Entrenamiento del modelo
ldt_classif_stroke$train(tsk_train_stroke)
```

Podemos ver el funcionamiento del algoritmo obteniendo el 谩rbol de clasificaci贸n proporcionado en la fase de entrenamiento.

```{r}
#| label: dt-013
#| warning: false
#| message: false

ldt_classif_stroke$model$classif.rpart$model
```

Como se puede ver el algoritmo no es capaz de realizar ninguna divisi贸n. Esto se puede deber a dos motivos: i) este algoritmo no funciona bien para este banco de datos, ii) hay que modificar los hiperpar谩metros del modelo porque resultan muy restrictivos. En el segundo caso podemos buscar una soluci贸n 贸ptima y ver que tipo de 谩rbol de decisi贸n resulta.

### Penguins {#sec-120.6.2}

Procedemos directamente con el modelo ya que los datos han sido preparados. Al cargar directamente el `learner` podemos utilizar la funci贸n `autoplot` para representar la soluci贸n.

```{r}
#| label: dt-016
#| warning: false
#| message: false

# Modelo de aprendizaje 
ldt_classif_penguins = lrn("classif.rpart", keep_model = TRUE)
# Entrenamiento del modelo
ldt_classif_penguins$train(tsk_train_penguins)
```

Vemos el 谩rbol obtenido:

```{r}
#| label: dt-017
#| warning: false
#| message: false

ldt_classif_penguins$model
```

Aunque seguramente resultar谩 m谩s f谩cil mediante un gr谩fico:

```{r}
#| label: dt-018
#| warning: false
#| message: false

autoplot(ldt_classif_penguins)
```

El modelo proporciona tres nodos terminales con 120, 50, y 104 casos respectivamente. Cada uno de los nodos terminales viene determinado principalmente por una especie en particular. El 谩rbol selecciona en primer lugar como predictor la variable `flipper_length` con valor de corte 207.5, y posteriormente los menores a ese valor se subdividen de acuerdo a `bill_length` con valor de corte 43.35. De esta forma podemos establecer que:

-   La especie `Adelie` se caracteriza principalmente por `flipper_length` menor a 207.5 y `bill_length` menor a 43.35.
-   La especie `Chinstrap` se caracteriza principalmente por `flipper_length` menor a 207.5 y `bill_length` mayo o igual a 43.35.
-   La especie `Gentoo` se caracteriza principalmente por `flipper_length` mayor o igual a 207.5.

Sin embargo, la clasificaci贸n no es perfecta porque podemos ver que los nodos terminales combinan resultados de diferentes especies. Podemos ver los porcentajes de cada especie en cada nodo terminal con:

```{r}
#| label: dt-019
#| warning: false
#| message: false

ldt_classif_penguins$model
```

donde podemos ver que el nodo terminal identificado como 4) tiene un 96.67% de muestras de Adelie, un 3.33% Chinstrap y un 0% de Gentoo. Los \* indican los nodos terminales del 谩rbol obtenido. Antes de valorar la clasificaci贸n obtenida vamos a estudiar la relevancia de cada predictora en la construcci贸n del 谩rbol. Para ello utilizamos el m茅todo `importance()`.

```{r}
#| label: dt-020
#| warning: false
#| message: false

ldt_classif_penguins$importance()
```

La tabla proporciona el orden de importancia de las variables en la construcci贸n del 谩rbol de decisi贸n. De las cinco predictoras disponibles la soluci贸n solo considera dos de ellas. Podemos identificar las predictoras seleccionadas con el c贸digo:

```{r}
#| label: dt-021
#| warning: false
#| message: false

ldt_classif_penguins$selected_features()
```

Ahora podemos estudiar la capacidad explicativa del modelo. Como no he os solicitado predecir la probabilidad no podemos obtener los scores asociados (`bbrier` y `auc`):

```{r}
#| label: dt-022
#| warning: false
#| message: false

# Predicci贸n de la muestra de entrenamiento y validaci贸n
pred_train = ldt_classif_penguins$predict(tsk_train_penguins)
pred_test = ldt_classif_penguins$predict(tsk_test_penguins)
# scores de validaci贸n
measures = msrs(c("classif.acc", "classif.bacc", "classif.bbrier", "classif.auc"))
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci贸n
pred_test$score(measures)
```

El algoritmo se comporta bastante bien ya que alcanzamos un porcentaje de clasificaci贸n correcta del 91.5% (similar al de modelos anteriores para estos datos). Podemos estudiar la matriz de confusi贸n:

```{r}
#| label: dt-023
#| message: false
#| warning: false

# Cargamos la librer铆a para representar la matriz de confusi贸n
cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

Podemos ver que los errores de clasificaci贸n en cada especie son a lo sumo de dos ejemplares, lo que indica que con tan solo esas dos predictoras somos capaces de construir un modelo con una gran capacidad de clasificaci贸n/predicci贸n.

### Electricity {#sec-120.6.3}

Para finalizar este apartado de modelos iniciales vamos a finalizar con el primer 谩rbol de decisi贸n para un modelo de regresi贸n. En este caso no tenemos valore perdidos por lo que podemos implementar directamente el algoritmo de aprendizaje.

```{r}
#| label: dt-024
#| warning: false
#| message: false

# Modelo de aprendizaje 
ldt_regr_electricity = lrn("regr.rpart", keep_model = TRUE)
# Entrenamiento del modelo
ldt_regr_electricity$train(tsk_train_electricity)
```

Veamos la soluci贸n gr谩fica del 谩rbol:

```{r}
#| label: dt-025
#| warning: false
#| message: false

autoplot(ldt_regr_electricity)
```

Para este conjunto de datos el 谩rbol de decisi贸n obtenido es bastante m谩s complejo con 7 nodos terminales. Lo que puede resultar m谩s curioso es que se utiliza la misma variable en diferentes niveles del 谩rbol. El algoritmo determina en funci贸n de las subdivisiones que va realizando si una variable ya utilizada debe utilizarse de nuevo con unos puntos de corte distintos aunque siempre consistentes con lo decidido en las ramas superiores). De hecho, en los diagramas de caja podemos observar que el nodo terminal con mayores valores de `PE` se corresponde con la regla de decisi贸n `AT` menor que 8.725, que es una combinaci贸n de los resultados de las divisiones anteriores. Por otro lado, el nodo terminal con menores valores de `PE` se corresponde con la regla de decisi贸n `AT` mayor o igual a 23.055 y `V` mayor o igual a 66. Podemos ver la relevancia de las predictoras con:

```{r}
#| label: dt-026
#| warning: false
#| message: false

ldt_regr_electricity$importance()
```

A partir de dichos valores podemos valorar la importancia relativa en t茅rminos de porcentaje mediante el c贸digo:

```{r}
#| label: dt-027
#| warning: false
#| message: false

importancia = ldt_regr_electricity$importance()
100*importancia/sum(importancia)
```

Podemos ver que `AT` tiene una importancia relativa del 44.40%. Adem谩s podemos conocer el valor medio estimado de la respuesta para cada nodo sin m谩s que ver el modelo ajustado.

```{r}
#| label: dt-028
#| warning: false
#| message: false

ldt_regr_electricity$model
```

En la 煤ltima columna aparecen los valores estimados de la respuesta `PE` dentro de cada nodo del 谩rbol. podemos estudiar ahora las precisiones de las estimaciones con el an谩lisis de las predicciones tanto para la muestra de entrenamiento como la de validaci贸n.

```{r}
#| label: dt-029
#| message: false
#| warning: false

# Predicci贸n de la muestra de entrenamiento
pred_train = ldt_regr_electricity$predict(tsk_train_electricity)
# Predicci贸n de la muestra de validaci贸n
pred_test = ldt_regr_electricity$predict(tsk_test_electricity)
# Scores de validaci贸n
measures = msrs(c("regr.rsq", "regr.mse", "regr.smape"))
# Valores de validaci贸n entrenamiento y validaci贸n
pred_train$score(measures)
pred_test$score(measures)
```

Los resultados son muy buenos, incluso mejorando los que vimos para este mismo conjunto de datos con otros algoritmos. La ventaja principal es que la selecci贸n de predictoras se hace autom谩ticamente con la construcci贸n del 谩rbol. Por 煤ltimo analizamos los gr谩ficos de predicci贸n (m谩s concretamente el de valores observados versus valores predichos) para comprender de forma m谩s precisa el proceso de predicci贸n en los modelos de 谩rboles. Para ello realizamos los gr谩ficos siguientes.

```{r}
#| label: dt-030
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 10
#| fig-cap: "Gr谩ficos del modelo para muestras de entrenamiento y validaci贸n. Task Electricity."

# Muestra de entrenamiento
p1 = autoplot(pred_train, type = "xy") + labs(title = "Observados vs predichos")

# Muestra de validaci贸n
p2 = autoplot(pred_test, type = "xy") + labs(title = "Observados vs predichos")

ggarrange(p1, p2, ncol = 2)
```

Como se puede ver tanto el gr谩fico de la muestra de entrenamiento como de validaci贸n la predicci贸n no representa una nube de untos como en modelos anteriores de regresi贸n, sino que 煤nicamente se predice un valor por cada nodo terminal presenten el modelo. En este caso hay 7 nodos terminales y por eso solo tenemos 7 valores predichos, o que provoca que aparezcan 7 columnas de predicci贸n y no una nube de puntos. Este efecto en la predicci贸n es debida a que no tenemos una ecuaci贸n que nos proporcione valores, sino 煤nicamente nodos donde todas las observaciones alojadas en 茅l se les asigna el mismo valor de predicci贸n, que en este caso es el valor medio de la respuesta para todas las observaciones en ese nodo.

En el punto siguiente tratamos de mejorar los modelos obtenidos en este bloque mediante un b煤squeda 贸ptima de los hiperpar谩metros del modelo.

## Optimizando los modelos {#sec-120.7}

Para el proceso de optimizaci贸n de los modelos vamos a considerar 煤nicamente los par谩metros `cp`, `minsplit` y `maxdepth`. Aunque este algoritmo permite configurar muchos hiperpar谩metros nos centramos en estos para buscar una mejora sobre los modelos b谩sicos aunque esta sea m铆nimo. No buscamos mejorar en exceso el modelo sino ver como funciona la selecci贸n de hiperpar谩metros en este modelo de aprendizaje.

### Stroke {#sec-120.7.1}

A continuaci贸n se muestra el c贸digo de optimizaci贸n para el banco de datos `Stroke`. Recordemos que con las opciones por defecto no resulta posible obtener ning煤n 谩rbol de decisi贸n. Para el par谩metro `cp` utilizaremos la escala logaritmo para la b煤squeda del 贸ptimo. Para la profundidad m谩xima del 谩rbol consideramos el intervalo $[3, 8]$ para evitar tener un 谩rbol demasiado peque帽o o demasiado grande. Por 煤ltimo, consideramos que el tama帽o m铆nimo de los nodos terminales debe estar en el intervalo $[5, 50]$. Consideramos una evaluaci贸n de 30 iteraciones debido al tama帽o de la base de datos.

```{r}
#| label: dt-031
#| message: false
#| warning: false
#| results: hide

ldt_classif_stroke = lrn("classif.rpart", keep_model = TRUE, predict_type = "prob",
                         cp = to_tune(1e-04, 1, logscale = TRUE),
                         maxdepth = to_tune(3, 8),
                         minsplit = to_tune(5, 50)
                         )
gr_stroke =  pp_stroke %>>% ldt_classif_stroke
gr_stroke = GraphLearner$new(gr_stroke)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci贸n fijando el n煤mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_stroke,
  learner = gr_stroke,
  resampling = rsmp("cv", folds = 5),
  measures = msr("classif.bacc"),
  term_evals = 30
)
```

Veamos los resultados obtenidos:

```{r}
#| label: dt-032
#| message: false
#| warning: false

# Veamos si converge el algoritmo
instance$is_terminated
# Resultados del proceso de optimizaci贸n
instance$result_learner_param_vals
# Valor de la m茅trica para resultado 贸ptimo
instance$result_y
```

El proceso de optimizaci贸n ha finalizado encontrando los valores 贸ptimos de `cp` igual a 0.0003562633, `minsplit` igual 10, y `maxdepth` igual a 6. El porcentaje de clasificaci贸n correcta ponderada es del 50.9% mejorando los resultados del modelo por defecto, pero no mucho los de otros modelos de clasificaci贸n vistos anteriormente. El modelo sigue siendo bastante pobre en t茅rminos predictivos. Utilizamos los valores obtenidos para establecer un nuevo 谩rbol de decisi贸n:

```{r}
#| label: dt-033
#| message: false
#| warning: false

# Nuevo modelo
ldt_classif_stroke = lrn("classif.rpart", keep_model = TRUE, predict_type = "prob",
                         cp = instance$result_x_domain$classif.rpart.cp,
                         maxdepth = instance$result_x_domain$classif.rpart.maxdepth,
                         minsplit = instance$result_x_domain$classif.rpart.minsplit
                         )
gr_stroke =  pp_stroke %>>% ldt_classif_stroke
gr_stroke = GraphLearner$new(gr_stroke)
# Entrenamiento del modelo
gr_stroke$train(tsk_train_stroke)
```

Para representar la soluci贸n utilizamos la librer铆a `rpart.plot`:

```{r}
#| label: dt-034
#| message: false
#| warning: false

modelo = gr_stroke$model$classif.rpart$model
library(rpart.plot)
rpart.plot(modelo, type = 5)
```

En el caso de 谩rboles de clasificaci贸n binaria la informaci贸n presentada en los nodos terminales es:

-   La categor铆a predicha para ese nodo
-   La probabilidad predicha de la clase de inter茅s (en este caso `Yes`).
-   El porcentaje de observaciones en el nodo.

Dado que los sujetos que sufren un ictus (249) es muy bajo en comparaci贸n con los que no (4861) es l贸gico que los porcentaje de observaciones en los nodos terminales identificados con `Yes` sean muy bajos. Podemos ver las reglas de clasificaci贸n:

```{r}
#| label: dt-035
#| message: false
#| warning: false

rpart.rules(modelo)
```

Las reglas viene ordenadas de menor a mayor porcentaje de respuesta de sujetos que ha sufrido un ictus dentro de cada nodo terminal. Los tres 煤ltimos contienen s贸lo sujetos con ictus y podemos ver sus indicadores de clasificaci贸n con detalle. Analizamos ahora la importancia de las predictoras en este modelo:

```{r}
#| label: dt-036
#| message: false
#| warning: false

importancia = gr_stroke$model$classif.rpart$model$variable.importance
100*importancia/sum(importancia)
```

En la tabla anterior observamos que `age` y `avg_glucose_level` son las predictoras m谩s importantes, seguidas de `smoking_status` y `heart_disease`. Estas variables marcan el perfil de los sujetos con mayor probabilidad de ictus, Podemos buscar en las reglas de clasificaci贸n para encontrar los pintos de corte de cada una de ellas. Para ver el comportamiento del modelo obtenemos la matriz de confusi贸n:

```{r}
#| label: dt-037
#| warning: false
#| message: false

# Predicci贸n de la muestra de entrenamiento y validacion
pred_train = gr_stroke$predict(tsk_train_stroke)
pred_test = gr_stroke$predict(tsk_test_stroke)
# scores de validaci贸n
measures = msrs(c("classif.acc", "classif.bacc", "classif.bbrier", "classif.auc"))
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci贸n
pred_test$score(measures)
# Matriz de confusi贸n
cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

Podemos ver que en este caso la matriz de confusi贸n si reparte observaciones entre todas las combinaciones, aunque el mayor error se comete de nuevo al clasificar casi todas observaciones originales con ictus como sanas.

El proceso de optimizaci贸n nos ha permitido construir un 谩rbol de decisi贸n pero su poder de clasificaci贸n real para distinguir individuos sanos de enfermos es muy bajo. Para analizar la estabilidad de la soluci贸n planteamos una an谩lisis de validaci贸n cruzada y el an谩lisis de la curva de aprendizaje.

```{r}
#| label: dt-038
#| message: false
#| warning: false

# Fijamos semilla
set.seed(135)
# Definimos proceso de validaci贸n cruzada kfold con k=10
resamp = rsmp("cv", folds = 10)
# Remuestreo
rr = resample(tsk_stroke, gr_stroke, resamp, store_models=TRUE)
```

```{r}
#| label: dt-039
#| message: false
#| warning: false

measure = msr("classif.bacc")
# Resumen Scores individuales
scores = rr$score(measure)
skim(scores)
```

El valor estimado del porcentaje de clasificaci贸n ponderado se sit煤a en el 51.59% con una desviaci贸n del 1.89%. Para finalizar analizamos la curva de aprendizaje:

```{r}
#| label: dt-040
#| echo: false
#| message: false
#| warning: false

# Funci贸n que nos permite obtener los valores asociados a la curva de aprendizaje
learningcurve = function(task, learner, score, ptr, rpeats)
{
  # Par谩metros de la funci贸n
  # task: tarea
  # learner: algoritmo de aprendizaje
  # score: nombre del score a utilizar
  # ptr: vector con las proporciones de tama帽os de muestra de entrenamiento
  # rpeats: n煤mero de repeticiones para cada proporci贸n de tama帽o de muestra de entrenamiento
  
  # Definimos los scores para cada conjunto de muestra
  mtrain = msr(score, predict_sets = "train")
  mtest = msr(score, predict_sets = "test")
  # Configuramos el learner para que evalue los scores en la muestra de validaci贸n y test
  learner$predict_sets = c("train", "test")
  # Incicializamos vector de scores agregados para la muestra de entrenamiento y validaci贸n
  sco_train = c()
  sco_test = c()
  for(i in 1:length(ptr))
  {
    # estrucura de muestreo: 5 repeticiones con porcentaje muestra entrenamiento ptr[i]
    subsam = rsmp("subsampling", repeats = rpeats, ratio = ptr[i])
    # ejecuci贸n de remuestreo
    rr = resample(task, learner, subsam)
    sco_train[i] = rr$aggregate(mtrain)
    sco_test[i] = rr$aggregate(mtest)
  }
  # Matriz de resultados
  res = data.frame(ptr, sco_train, sco_test)
  resdf = res %>% pivot_longer(!ptr, names_to = "Sample", values_to = "BACC")
  return(resdf)
}
```

```{r}
#| label: dt-041
#| message: false
#| warning: false
#| fig-cap: "Curva de aprendizaje"

ptr = seq(0.1, 0.9, 0.1)
lcurve = learningcurve(tsk_stroke, gr_stroke, "classif.bacc", ptr = ptr, rpeats = 10)
# Gr谩fico
ggplot(lcurve, aes(ptr, BACC, color = Sample)) + 
    geom_line() +
    labs(x ="Proporci贸n tama帽o muestra entrenamiento", y = "BACC",color = "Muestra") +
    scale_color_hue(labels = c("Validaci贸n", "Entrenamiento")) +
    scale_x_continuous(breaks=ptr)
```

El porcentaje de clasificaci贸n correcta ponderado para la muestra de validaci贸n se mantiene bastante estable cuando variamos el tama帽o de la muestra de entrenamiento.

### Penguins {#sec-120.7.2}

Planteamos ahora el proceso de optimizaci贸n del 谩rbol de decisi贸n para el banco de datos `Penguins`. Recordemos que el porcentaje de clasificaci贸n correcta ponderado es del 94.3%. Para este proceso consideramos una configuraci贸n de hiperpar谩metros similar al del ejemplo anterior.

```{r}
#| label: dt-042
#| message: false
#| warning: false
#| results: hide

ldt_classif_penguins = lrn("classif.rpart", keep_model = TRUE, predict_type = "prob",
                         cp = to_tune(1e-04, 1, logscale = TRUE),
                         maxdepth = to_tune(3, 8),
                         minsplit = to_tune(5, 50)
                         )

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci贸n fijando el n煤mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_penguins,
  learner = ldt_classif_penguins,
  resampling = rsmp("cv", folds = 5),
  measures = msr("classif.bacc"),
  term_evals = 30
)
```

Veamos los resultados obtenidos:

```{r}
#| label: dt-043
#| message: false
#| warning: false

# Veamos si converge el algoritmo
instance$is_terminated
# Resultados del proceso de optimizaci贸n
instance$result_learner_param_vals
# Valor de la m茅trica para resultado 贸ptimo
instance$result_y
```

A simple vista ya podemos ver que hemos mejorado un 2% (alcanzamos el 96.6%) nuestro porcentaje de clasificaci贸n. Utilizamos los valore obtenidos para generar el nuevo 谩rbol de decisi贸n.

```{r}
#| label: dt-044
#| message: false
#| warning: false

# Nuevo modelo
ldt_classif_penguins = lrn("classif.rpart", keep_model = TRUE, predict_type = "prob",
                         cp = instance$result_x_domain$cp,
                         maxdepth = instance$result_x_domain$maxdepth,
                         minsplit = instance$result_x_domain$minsplit
                         )
# Entrenamiento del modelo
ldt_classif_penguins$train(tsk_train_penguins)
```

Representamos la soluci贸n:

```{r}
#| label: dt-045
#| message: false
#| warning: false

modelo = ldt_classif_penguins$model
rpart.plot(modelo, type = 5)
```

En este caso la informaci贸n proporcionada en los nodos terminales es:

-   Clase mayoritaria del nodo terminal, es decir, predicci贸n proporciona por el modelo para esa rama del 谩rbol.
-   Los porcentajes de cada una de las clases en ese nodo terminal.
-   Porcentaje de observaciones en ese nodo respecto del total de muestras.

Podemos ver claramente cuales son los nodos m谩s relevantes mirando los porcentaje de observaciones y extraer las reglas de clasificaci贸n correspondientes (recordemos que las variables est谩n estandarizadas:

-   El nodo clasificado con `Adelie` que contiene el 41% de las muestras se caracterizan por `flipper_length` menor a 0.47 y `bill_length` menor a -0.31.
-   El nodo clasificado con `Chinstrap` que contiene el 18% de las muestras se caracterizan por `flipper_length` menor a 0.47 y `bill_length` mayor o igual a -0.13, e `island` igual a `Dream`.
-   El nodo clasificado con `Gentoo` que contiene el 36% de las muestras se caracterizan por `flipper_length` mayor o igual a 0.47 e `island` igual a `Biscoe`.

Las reglas de clasificaci贸n completas se encuentran en la tabla siguiente:

```{r}
#| label: dt-046
#| message: false
#| warning: false
#| results: asis

rpart.rules(modelo)
```

Para finalizar analizamos la matriz de confusi贸n asociada con el nuevo modelo.

```{r}
#| label: dt-047
#| warning: false
#| message: false

# Predicci贸n de la muestra de entrenamiento y validaci贸n
pred_train = ldt_classif_penguins$predict(tsk_train_penguins)
pred_test = ldt_classif_penguins$predict(tsk_test_penguins)
# scores de validaci贸n
measures = msrs(c("classif.acc", "classif.bacc"))
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci贸n
pred_test$score(measures)
# Matriz de confusi贸n
cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

Todos los errores de clasificaci贸n (2.9%) est谩n asociados con la especie `Gentoo` original, ya que el modelo obtenido clasifica esas muestras como pertenecientes a la especie `Adelie`.

Se puede finalizar el an谩lisis mediante el estudio de validaci贸n y la construcci贸n de la curva de aprendizaje.

### Electricity {#sec-120.7.3}

Finalizamos con la optimizaci贸n para el banco de datos `Electricity`. Recordemos que el `sMAPE` para la muestra de validaci贸n obtenido era del 0.00925. Empezamos definiendo la configuraci贸n del proceso de optimizaci贸n cambiando algunos de los par谩metros dado que el n煤mero de predictoras en este caso es muy bajo y no podemos considerar 谩rboles muy profundos.

```{r}
#| label: dt-048
#| message: false
#| warning: false
#| results: hide

ldt_regr_electricity = lrn("regr.rpart", keep_model = TRUE,
                         cp = to_tune(1e-03, 1, logscale = TRUE),
                         maxdepth = to_tune(2, 4),
                         minsplit = to_tune(5, 50)
                         )

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci贸n fijando el n煤mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_electricity,
  learner = ldt_regr_electricity,
  resampling = rsmp("cv", folds = 5),
  measures = msr("regr.smape"),
  term_evals = 30
)
```

Veamos los resultados obtenidos:

```{r}
#| label: dt-049
#| message: false
#| warning: false

# Veamos si converge el algoritmo
instance$is_terminated
# Resultados del proceso de optimizaci贸n
instance$result_learner_param_vals
# Valor de la m茅trica para resultado 贸ptimo
instance$result_y
```

El modelo optimizado reduce el `sMAPE` hasta el valor 0.0082. Analizamos ahora e 谩rbol obtenido con dichos valores.

```{r}
#| label: dt-050
#| message: false
#| warning: false

# Nuevo modelo
ldt_regr_electricity = lrn("regr.rpart", keep_model = TRUE, 
                         cp = instance$result_x_domain$cp,
                         maxdepth = instance$result_x_domain$maxdepth,
                         minsplit = instance$result_x_domain$minsplit
                         )
# Entrenamiento del modelo
ldt_regr_electricity$train(tsk_train_electricity)
```

Representamos la soluci贸n:

```{r}
#| label: dt-051
#| message: false
#| warning: false

modelo = ldt_regr_electricity$model
rpart.plot(modelo, type = 5)
```

En este caso la informaci贸n de los nodos terminales es:

-   Predicci贸n de la respuesta en el nodo terminal.
-   Porcentaje de muestras en el nodo terminal con respecto del total de muestras.

Podemos ver adem谩s que las ramas del 谩rbol hacen uso recursivo de las mismas predictores con diferentes scores de divisi贸n.

Para finalizar realizamos un estudio de validaci贸n cruzada de la soluci贸n obtenida.

```{r}
#| label: dt-052
#| message: false
#| warning: false

# Fijamos semilla
set.seed(135)
# Definimos proceso de validaci贸n cruzada kfold con k=10
resamp = rsmp("cv", folds = 10)
# Remuestreo
rr = resample(tsk_electricity, ldt_regr_electricity, resamp, store_models=TRUE)
```

Analizamos los resultados obtenidos:

```{r}
#| label: dt-053
#| message: false
#| warning: false

measure = msr("regr.smape")
# Resumen Scores individuales
scores = rr$score(measure)
skim(scores)
```

El valor estimado del `sMAPE` se sit煤a en el 0.0082 con una desviaci贸n del 0.0001. Para finalizar analizamos la curva de aprendizaje:

```{r}
#| label: dt-054
#| message: false
#| warning: false
#| fig-cap: "Curva de aprendizaje"

ptr = seq(0.1, 0.9, 0.1)
lcurve = learningcurve(tsk_electricity, ldt_regr_electricity, "regr.smape", ptr = ptr, rpeats = 10)
# Gr谩fico
ggplot(lcurve, aes(ptr, BACC, color = Sample)) + 
    geom_line() +
    labs(x ="Proporci贸n tama帽o muestra entrenamiento", y = "sMAPE",color = "Muestra") +
    scale_color_hue(labels = c("Validaci贸n", "Entrenamiento")) +
    scale_x_continuous(breaks=ptr)
```

Se aprecia como el `sMAPE` disminuye cuando aumentamos el tama帽o de la muestra de entrenamiento. El valor 贸ptimo se sit煤a en un tama帽o del 60%.

## Otros modelos de 谩boles en mlr3

En `mlr3` existen otro algoritmos para la obtenci贸n de 谩rboles de decisi贸n:

-   `classif.C50`, para construir 谩rboles de decisi贸n en problemas de clasificaci贸n utilizando el criterio de ganancia de informaci贸n.
-   `classif.ctree`, para construir 谩rboles de decisi贸n en problemas de clasificaci贸n donde se utiliza un test de comparaci贸n para encontrar la divisi贸n de cada rama.
-   `regr.ctree`, para construir 谩rboles de decisi贸n en problemas de regresi贸n donde se utiliza un test de comparaci贸n para encontrar la divisi贸n de cada rama.

## Ejercicios {#sec-120.8}

1.  Ajustar un modelo de aprendizaje autom谩tico basado en un modelo de 谩rbol de decisi贸n para el banco de datos `Mushroom`[-@sec-mushroom].
2.  Ajustar un modelo de aprendizaje autom谩tico basado en un modelo de 谩rbol de decisi贸n para el banco de datos `Water potability`[-@sec-waterpot].
3.  Ajustar un modelo de aprendizaje autom谩tico basado en un modelo de 谩rbol de decisi贸n para el banco de datos `Hepatitis`[-@sec-hepatitis].
4.  Ajustar un modelo de aprendizaje autom谩tico basado en un modelo de 谩rbol de decisi贸n para el banco de datos `Abalone`[-@sec-abalone].
5.  Ajustar un modelo de aprendizaje autom谩tico basado en un modelo de 谩rbol de decisi贸n para el banco de datos `Us economic time series`[-@sec-usaets].
6.  Ajustar un modelo de aprendizaje autom谩tico basado en un modelo de 谩rbol de decisi贸n para el banco de datos `QSAR`[-@sec-qsar].
