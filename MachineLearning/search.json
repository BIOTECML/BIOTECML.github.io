[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MachineLearning",
    "section": "",
    "text": "Prefacio\nEste compendio de materiales es una introducción a los problemas con los que se enfrenta todo analista de datos, científico de datos o bioinformático. Los materiales están orientados en un sentido muy practico dejando los conceptos teóricos más avanzados para otro tipo de cursos más teóricos.\nEntre los muchos materiales referidos al aprendizaje automático recomendar el libro de Mohri, Rostamizadeh, and Talwalkar (2018).\nLa comunidad de aprendizaje automático es muy dinámica y en continua evolución, y ha generado (y sigue generando) infinidad de recursos de aprendizaje. A continuación destacamos una lista de cursos online sobre aprendizaje automático, que han obtenido altas calificaciones en la comunidad, así como un listado de libros recomendados.\nCursos\n\nMachine Learning, por Andrew Ng en Coursera. Es uno de los cursos introductorios de aprendizaje automático más populares, con más de 4 millones de usuarios. El curso se centra más en los fundamentos de las técnicas y algoritmos de aprendizaje automático y es gratuito en Coursera.\nEspecialización en ingeniería de aprendizaje automático para la producción (MLOps), impartido por Andrew Ng, Laurence Moroney y Robert Crowe en Coursera. Es uno de los mejores cursos de ingeniería de Machine Learning. Enseña cómo diseñar sistemas de producción de aprendizaje automático de extremo a extremo, construyendo pipelines de datos y de modelado eficientes, y desplegando modelos en producción.\n\nLibros\n\nEl Libro de Aprendizaje Automático de Cien Páginas, escrito por Andriy Burkov, es uno de los libros más cortos pero concisos y bien escritos que encontrarás en Internet.\nMachine Learning Engineering, también escrito por Andriy Burkov, es otro gran libro de Machine Learning que descubre cada paso del flujo de trabajo en este tipo de análisis.\nHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, escrito por Aurelion Geron, es uno de los mejores libros de aprendizaje automático. Está escrito con claridad y repleto de ideas y ejemplos.\n\nA continuación se listan diferentes contenidos (texto y videos) de diferentes cursos sobre conceptos matemáticos, probabilidad y aprendizaje autmático que pueden complementar la formación de estos cuadernos (listado obtenido del enlace). Se indica la dificultad de los materiales y su relevancia sore los modelos de aprendizaje automático. la dificultad de los contenidos viene representada mediante tres niveles.\nA continuación se listan diferentes contenidos (texto y videos) de diferentes cursos sobre conceptos matemáticos, probabilidad y aprendizaje autmático que pueden complementar la formación de estos cuadernos (listado obtenido del enlace).\nÁlgebra Lineal\nEsta rama de las matemáticas es crucial para comprender el mecanismo de las técnicas de aprendizaje automático en cuanto al tratamiento y procesamiento de conjuntos de datos en estructura matricial.\n\nMIT Gilbert Strang 2005 Linear Algebra\nMathematics for Machine Learning Book: Chapter 2\n3Blue1Brown Essence of Linear Algebra\nMathematics For Machine Learning Specialization: Linear Algebra\nMatrix Methods for Linear Algebra for Gilber Strang UPDATED!\n\nProbabilidad\nLa mayoría de los algoritmos de aprendizaje automático se basan en la teoría de la probabilidad. Así que esta rama es extremadamente importante para comprender cómo funcionan.\n\nJoe Blitzstein Harvard Probability and Statistics Course\nMIT Probability Course 2011 Lecture videos\nMIT Probability Course 2018 short videos UPDATED!\nMathematics for Machine Learning Book: Chapter 6\nProbalistic Graphical Models CMU Advanced\nProbalistic Graphical Models Stanford Daphne Advanced\nA First Course In Probability Book by Ross\n\nCálculo y Optimización\n\nEssence of Calculus by 3Blue1Brown\nSingle Variable Calculus MIT 2007\nMathematics for Machine Learning Book: Chapter 5\nCMU optimization course 2018\nCMU Advanced optimization course\nStanford Famous optimization course\nBoyd Convex Optimization Book\n\nAprendizaje Automático\n\nMathematics for Machine Learning Part 2\nPattern Recognition and Machine Leanring\nElements of Statistical Learning\nMachine Learning: A Probalisitic Perspective\nBerkley CS188 Introduction to AI course\nMIT Classic AI course taught by Prof. Patrick H. Winston\nStanford AI course 2018\nCalifornia Instuite of Technology Learning from Data course\nCMU Machine Learning 2015 10-601\nCMU Statistical Machine Learning 10-702\nInformation Theory, Pattern Recognition ML course 2012\nLarge Scale Machine Learning Toronto University 2015\nAlgorithmic Aspects of Machine Learning MIT\nMIT Course 9.520 - Statistical Learning Theory and Applications, Fall 2015\nUndergraduate Machine Learning Course University of British Columbia 2013\n\n\n\n\n\nMohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. Foundations of Machine Learning. 2nd ed. Adaptive Computation and Machine Learning. Cambridge, MA: MIT Press."
  },
  {
    "objectID": "01_IntroCourse.html",
    "href": "01_IntroCourse.html",
    "title": "Parte 1. Introducción",
    "section": "",
    "text": "En esta primera parte de los materiales se introducen los conceptos básicos del análisis de datos y del aprendizaje automático. Veremos como instalar los programas y librerías necesarias para seguir los materiales, así como los bancos de datos con los que iremos trabajando. Para finalizar veremos el formato en el que debemos preparar nuestros bancos de datos para proceder con los modelos de aprendizaje automático que proporciona la librería mlr3 (Lang et al. 2019) de R.\n\n\n\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903."
  },
  {
    "objectID": "10_introAD.html#sec-10.1.1",
    "href": "10_introAD.html#sec-10.1.1",
    "title": "1  Introducción al análisis de datos",
    "section": "1.1 Conceptos básicos del diseño experimental",
    "text": "1.1 Conceptos básicos del diseño experimental\nEn esta sección presentamos los conceptos básicos que utilizaremos a lo largo de la materia. Se trata únicamente de un resumen muy esquemática, pero nos sirve para sentar las bases de los temas siguientes.\n\n1.1.1 Objetivo del diseño experimental\nEl objetivo de cualquier diseño experimental es aquellos que pretendemos estudiar en función del tipo de información que se ha recogido y del tipo de premisas establecidas antes de la recolección de los datos. Además es importante establecer el número de repeticiones del experimento que vamos a realizar, ya que eso condicionará el análisis de dichos datos. Si nuestro diseño experimental es muy complejo puede ocurrir que plantemos más de un objetivo.\n\nEjemplo 1.1 Degradación compuesto orgánico. Se va a realizar un experimento para conocer el tiempo que tarda en degradarse un compuesto orgánico. En este caso nuestro objetivo es el tiempo hasta la degradación. Si el experimento considera diferentes tipos de compuestos nuestro objetivo podría ser comparar el tiempo de degradación en función del tipo de compuesto.\n\n\n\n1.1.2 Población y muestra\nSe define la población como el conjunto de sujetos u objetos que son de interés para el objetivo u objetivos planteados en nuestro diseño experimental. EL problema principal es que la población de sujetos u objetos suele ser demasiado grande para poder analizarla de forma completa, y por tanto debemos acudir a un subconjunto de dicha población para llevar a cabo nuestro diseño experimental.\nSe define la muestra como el subconjunto de la población a la que accedemos para obtener la información necesaria de cara a responder de la forma más precisa posible al objetivo u objetivos planteados.\n\n\n1.1.3 Medidas y escalas de medida\nUna medida es un número o atributo que se puede calcular para cada uno de los miembros de la población que está relacionado directamente con el objetivo de interés de la investigación. El conjunto de medidas obtenidas para cada uno de los elementos muestrales se denominan datos muestrales.\nEL conjunto de medidas que se pueden observar y registrar para un conjunto de sujetos u objetos bajo investigación se denominan variables. Por tanto, una variable es el conjunto de valores que puede tomar cierta característica de la población sobre la que se realiza el estudio estadístico. Se distinguen dos tipos que pasamos a describir a continuación.\n\n1.1.3.1 Variables cualitativas\nSon el tipo de variables que como su nombre lo indica expresan distintas cualidades, características o modalidad. Cada modalidad que se presenta se denomina atributo o categoría, y la medición consiste en una clasificación de dichos atributos. Las variables cualitativas pueden ser dicotómicas cuando sólo pueden tomar dos valores posibles, como sí y no, hombre y mujer o ser politómicas cuando pueden adquirir tres o más valores. Dentro de ellas podemos distinguir:\n\nVariable cualitativa ordinal: La variable puede tomar distintos valores ordenados siguiendo una escala establecida, aunque no es necesario que el intervalo entre mediciones sea uniforme, por ejemplo: leve, moderado, fuerte.\nVariable cualitativa nominal: En esta variable los valores no pueden ser sometidos a un criterio de orden, como por ejemplo los colores.\n\n\n\n1.1.3.2 Variables cuantitativas\nSon las variables que toman como argumento cantidades numéricas. Las variables cuantitativas además pueden ser:\n\nVariable discreta: Es la variable que presenta separaciones o interrupciones en la escala de valores que puede tomar. Estas separaciones o interrupciones indican la ausencia de valores entre los distintos valores específicos que la variable pueda asumir. Ejemplo: El número de hijos (1, 2, 3, 4, 5). En muchas ocasiones una variable cualitativa ordinal puede ser interpretada como una variable discreta asociando a las categorías de la variable valores numéricos respetando el orden o escala establecida. Por ejemplo a la escala leve, moderado y fuerte le podríamos asociar la escala 1, 2 y 3 para mantener el orden.\n\nVariable continua: Es la variable que puede adquirir cualquier valor dentro de un intervalo especificado de valores. Por ejemplo el peso (2,3 kg, 2,4 kg, 2,5 kg,…), la altura (1,64 m, 1,65 m, 1,66 m,…), o el salario. Solamente se está limitado por la precisión del aparato medidor, en teoría permiten que existan valores infinitos entre dos valores observados.\n\nDe forma habitual, la estructura de cualquier banco de datos (asociado a un diseño experimental) tiene una estructura matricial donde en las filas se colocan los sujetos bajo estudio y en las columnas se sitúan las variables medidas para cada uno de ellos.\nAsociada a cada variable de nuestro banco de datos se puede establecer lo que conocemos como parámetro o parámetros de interés de la variable.\n\nEjemplo 1.2 Variable de interés. Para el diseño experimental del estudio de la degradación del compuesto orgánico presentado en el ejemplo Ejemplo 1.1, la variable de interés es de tipo continuo y viene dada por el tiempo de degradación asociado a cada repetición del experimento. Sin embargo, a la hora de extraer conclusiones no podemos presentar todo el conjunto de datos sino que recurrimos a un resumen de dichos datos.\n\n\n\n\n1.1.4 Parámetros poblacionales y estadísticos\nAsociado a cada variable se puede establecer lo que conocemos como parámetro o parámetros de interés de la variable. En el ejemplo anterior el parámetro de interés es el tiempo medio de degradación. Dado que generalmente no es posible examinar toda la población y debemos recurrir a una muestra de dicha población, es imposible conocer el verdadero valor del parámetro asociado con dicha variable. Para sortear este problema definimos el estadístico como una realización del parámetro para los datos muestrales observados. Por tanto el valor del estadístico (denominado estimación) varia entre dos muestras de las misma población. Cuanto mayor es la muestra más se parecerá el valor del estadístico al del parámetro.\nEn ocasiones ocurrirá que el número de parámetros asociado con una variable no es único, ya que se pueden establecer varios parámetros para estudiar el comportamiento de una variable. En el caso de variables de tipo cuantitativo siempre existen dos parámetros de interés: la media y la desviación típica. El primero nos indica como se sitúan los datos mientras que el segundo nos indica como se reparten los datos muestrales alrededor de la media.\n\nEjemplo 1.3 Para el diseño experimental del estudio de la degradación del compuesto orgánico presentado en el ejemplo Ejemplo 1.1, el parámetro poblacional de interés es el tiempo medio de degradación, mientras que el estadístico es la media del tiempo de degradación observado para los sujetos de la muestra. Distinguimos entonces entre media poblacional (parámetro) y media muestral (estadístico)."
  },
  {
    "objectID": "20_introAA.html",
    "href": "20_introAA.html",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "",
    "text": "3 Problemas típicos del AA\nEl aprendizaje automático ha transformado muchos sectores, desde la banca, la fabricación, el streaming, los vehículos autónomos, la agricultura, etc. De hecho, la mayoría de los productos y servicios tecnológicos que utilizamos a diario poseen algún tipo de algoritmo de aprendizaje automático en su interior.\nEstos son los casos de uso más comunes del aprendizaje automático:\nHay muchas más aplicaciones del aprendizaje automático y la lista podría resultar infinita. Esperamos que esta introducción te haya servido para percibir, a groso modo, el potencial del aprendizaje automático en el mundo actual.\nEn líneas generales, existen 5 tipos principales de sistemas de aprendizaje automático, que son:\nVamos a repasar todos estos tipos para conseguir una comprensión alta de lo que realmente significan.\nA continuación, utilizamos el problema de la detección de spam como ejemplo práctico para ilustrar algunas definiciones básicas y describir el uso y la evaluación de los algoritmos de aprendizaje automático en la práctica, incluyendo sus diferentes etapas.\nLa detección de spam consiste en clasificar automáticamente los mensajes de correo electrónico como spam o correo regular.\nEn nuestro problema de spam, las muestras son la colección de mensajes de correo electrónico disponibles para el análisis.\nEn el ejemplo de spam, algunas características relevantes pueden ser la longitud del mensaje, el nombre o dominio del remitente, diversas características del encabezado, la presencia de ciertas palabras clave en el cuerpo del mensaje, etc.\nEn el problema de correo spam la respuesta es una variable categórica binaria con dos posibles resultados spam y no spam, cuyos valores han sido confirmados a través de una revisión manual de los mensajes disponibles. El objetivo será predecirla en nuevos correos, sin necesidad de hacer un reconocimiento explícito, sino a través de aquellas características con las que se relaciona.\nEn el problema del correo spam los parámetros se corresponderían con los pesos que el modelo asigna a cada una de las predictoras a la hora de establecer su influencia en la clasificación de un correo como spam o no.\nEn nuestro problema de spam, la muestra de entrenamiento consiste en un conjunto de correos electrónicos que proporcionan datos sobre todas las variables predictoras identificadas como relevantes, y también sobre la variable respuesta, esto es, han sido identificados explícitamente como spam o correo regular.\nEn el problema del spam, la muestra de test consiste en una serie de muestras de correo electrónico para los que el algoritmo de aprendizaje debe predecir etiquetas basándose en las características. Estas predicciones se comparan con las etiquetas de la muestra de test para medir el rendimiento del algoritmo.\nAunque cada problema de aprendizaje automático es único, todos siguen un flujo de trabajo similar. En esta sección, aprenderemos a abordar los problemas de aprendizaje automático de forma sistemática.\nEn general, el flujo de trabajo típico de un proyecto de aprendizaje automático consiste en:\nA continuación describimos brevemente cada etapa.\nLas métricas de evaluación se utilizan para medir el rendimiento de los modelos de aprendizaje automático. Al principio de esta introducción al aprendizaje automático comentamos que la mayoría de los problemas se refieren a regresión y clasificación; en función del objetivo y tipo de respuesta, la evaluación del modelo será diferente. Veamos las alternativas.\nAl igual que cualquier otra tecnología, el aprendizaje automático ofrece desafíos relacionados tanto con la mejora de los datos como con la mejora de los modelos de ajuste.\nNo puede haber buenos modelos con malos datos, pero es un hecho que en el mundo real es raro encontrar “buenos datos” a la primera. Por lo general los datos presentan desorden, errores, incoherencias, y se convierte en todo un arte la labor de procesarlos (depurar, etiquetar, e incluso crear nuevas variables) para conseguir una base de datos apta para iniciar el ajuste de un modelo.\nPor otro lado, una vez disponemos de una base de datos razonablemente buena, el ajuste del modelo no es una labor trivial. En principio se inicia la labor de elegir qué modelo o modelos son razonables para los objetivos y el tipo de datos. A continuación hay que ajustarlos y compararlos para tomar una decisión.\nCuando ajustamos un modelo lo haremos sobre una submuestra del banco de datos, la muestra de entrenamiento, que si no está bien seleccionada y contiene todas las características de la población que representa, ocasionará sesgos. Otra parte de la muestra se dedica a la validación, es decir, a evaluar el error que genera el modelo ajustado en la predicción, y utilizarlo para volver sobre el ajuste y afinarlo en pos de reducirlo. Se puede optar también por utilizar otra parte de la muestra para la evaluación o testado del modelo que se ha ajustado. En la bibliografía encontramos diferentes recomendaciones sobre el porcentaje de la muestra que ha de dedicarse a cada parte. Una opción puede ser 70/20/10, pero es el científico de datos el que decide, en función de utilizar el mayor número de datos para entrenar el modelo (y conseguir así una buena representación de las características de la población a ajustar), y también en función de no sobrecargar el aprendizaje con los cálculos del error de validación. La muestra final de testado podría tener una representación mayor si el objetivo es concluir sobre la bondad de un modelo en la predicción de datos que no ha visto antes, esto es, que no ha utilizado para entrenar o validar.\nYa en el ajuste se nos pueden presentar dos tipos de problemas: el infra-ajuste (underfitting) y el sobre-ajuste (overfitting). Veamos en qué consisten.\nEl infra-ajuste o infra-adaptación del modelo se produce cuando el modelo da malos resultados (en términos de métricas de error) con los datos de entrenamiento; por ejemplo, una clasificación correcta de sólo el 50% de la muestra. Un problema de infra-ajuste ocasiona un problema de sesgo en la predicción. Esto puede ser causado porque el modelo es demasiado simple para los datos de entrenamiento o los datos no contienen las características que se están tratando de predecir. Para reducir este tipo de problema se puede optar por:\nEl sobre-ajuste o sobre-adaptación se produce cuando un modelo funciona muy bien con los datos de entrenamiento, pero no con datos nuevos que el modelo no utilizó para ajustar. Esto repercute en un inflado de la varianza de la muestra de validación.\nEl sobre-ajuste se puede deber a que el modelo es demasiado complejo para los datos disponibles o el volumen de la muestra de entrenamiento es demasiado pequeño. Para aliviar este tipo de problema, podemos intentar:\nTanto el sobre-ajuste como el infra-ajuste lo podemos detectar evaluando el error en las muestras de entrenamiento y de validación cuando hacemos crecer el tamaño de la muestra de entrenamiento progresivamente, y en consecuencia reducimos la de validación. Lo lógico es que cuando incrementamos la muestra de entrenamiento reduzcamos el error del ajuste en dicha muestra, a cambio de que provoquemos un aumento en el error de validación. Si ambos errores se encuentran (convergen) en un punto, significa que tenemos un problema de infra-ajuste y la curva de aprendizaje se detiene pronto. Si al aumentar el tamaño llega un punto en que ambos se estabilizan pero se mantienen alejados entre sí, tenemos un problema de sobre-ajuste.\nPara terminar, hemos de comentar la necesidad de tener una buena formación en aprendizaje automático. Seremos más capaces de ajustar buenos modelos de aprendizaje automático si tenemos un amplio conocimiento de las distintas opciones en cuanto a modelos, y una comprensión avanzada de cómo funciona cada uno de ellos. Podremos proponer entonces alternativas (quizás a través de hiperparámetros del modelo que trabajamos) que mejoren los resultados previos que vamos consiguiendo. Aunque hay técnicas específicas que nos ayudan y simplifican la búsqueda y elección de hiperparámetros (como el grid search, random search, Keras tuner), es fundamental comprender el modelo para tomar buenas decisiones al respecto.\nHasta aquí la introducción sobre los fundamentos del aprendizaje automático. A medida que vayamos desarrollando contenidos, entraremos en detalle en los procedimientos y descubriremos sus aplicaciones."
  },
  {
    "objectID": "20_introAA.html#sec-useAA",
    "href": "20_introAA.html#sec-useAA",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "3.1 Cuándo usar y no usar el AA",
    "text": "3.1 Cuándo usar y no usar el AA\nEl aprendizaje automático es una tecnología increíble y ha demostrado muchos éxitos en la resolución de diversos problemas del mundo real. Sin embargo, como cualquier otra tecnología, el aprendizaje automático no es adecuado para resolver todo tipo de problemas. Por lo tanto, es igualmente importante saber cuándo utilizarlo y cuándo no.\n¿Cuándo utilizar el aprendizaje automático? El uso de este tipo de aprendizaje es preferible cuando se abordan:\n\nProblemas que son demasiado complejos para ser resueltos con programación ordinaria, como por ejemplo el reconocimiento de rostros o la detección de correos spam. Es excesivamente complejo confeccionar todas las reglas de inclusión/exclusión. Para este tipo de problemas, probablemente sea seguro probar el aprendizaje automático.\nProblemas que implican el razonamiento visual y la comprensión del lenguaje, como el reconocimiento de imágenes, el reconocimiento del habla, la traducción automática, etc. Como veremos más adelante, los problemas de percepción a gran escala o los problemas visuales y de lenguaje suelen ser manejados por sistemas de aprendizaje profundo.\nProblemas que cambian rápidamente y cuyas características cambian con el tiempo, y es necesario que el sistema siga funcionando bien. El aprendizaje automático es adecuado para este tipo de problemas porque estos algoritmos pueden re-entrenarse con nuevos datos.\nLos problemas que son claros y tienen objetivos sencillos, como una pregunta de sí/no (ante un problema de clasificación: enfermo/no enfermo) o la predicción de un único número continuo (como la cantidad de electricidad que se consumirá en un día).\n\nDicho esto, es poco recomendable utilizar el aprendizaje automático en:\n\nProblemas de predicción en los que precisas conocer el efecto concreto que tienen las variables que están relacionadas con una respuesta sobre ella. La mayoría de los modelos de aprendizaje automático se consideran cajas negras y no proporcionan reglas fijas para relacionar distintas variables.\nProblemas de predicción de fenómenos variables y cambiantes con el tiempo y que no siguen patrones preestablecidos fijos, como por ejemplo el comportamiento de la bolsa, que si bien tiene ciertos patrones identificados, su capacidad de fluctuar e invertir de repente una tendencia dada es un fenómeno frecuente.\nProblemas que puedan ser resueltos con programación ordinaria o con métodos heurísticos simples.\nProblemas que requieran de una solución única y estable que no precise ser actualizada. Si no va a ser posible capturar de modo continuado información con la que abastecer, re-entrenar los modelos y actualizar las predicciones realizadas por los modelos de aprendizaje automático, carece de sentido utilizarlos. Es preferible acudir a modelos estadísticos clásicos.\n\nEl aprendizaje automático sigue transformando procesos que nunca se imaginaron y, sin lugar a dudas, continuará creciendo y aplicándose en más y más ámbitos y problemas."
  },
  {
    "objectID": "20_introAA.html#sec-Asupervised",
    "href": "20_introAA.html#sec-Asupervised",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "4.1 Aprendizaje supervisado",
    "text": "4.1 Aprendizaje supervisado\nLa mayoría de las tareas de aprendizaje automático pertenecen al tipo de aprendizaje supervisado. Un modelo de aprendizaje supervisado se entrena con datos etiquetados, esto es, datos que se han observado junto a algún tipo de resultado a predecir, al que llamamos etiqueta (cuando es de tipo categórico) o respuesta (en sentido general o numérico). En otras palabras, un modelo de aprendizaje supervisado utiliza los datos de entrada para establecer una solución que aproxime la respuesta observada (también proporcionada en la entrada).\nEl ejemplo de reconocimiento de caras que hemos mencionado anteriormente es un buen ejemplo de aprendizaje supervisado. En general, hay dos tipos principales de problemas de aprendizaje supervisado, a saber:\n\nLos problemas de clasificación, en los que la tarea a resolver consiste en clasificar, esto es, en atribuir una categoría determinada a un sujeto/registro, de entre varias categorías posibles. Por ejemplo, en salud diagnosticar a un sujeto como enfermo/no enfermo; en reconocimiento de textos, identificar su idioma, o si un correo entrante es spam o no. En este caso la variable respuesta es de tipo categórico.\nProblemas de regresión, en los que el objetivo es predecir un valor numérico de alguna característica o variable. Por ejemplo, predecir el precio de un coche usado según sus características (marca, antigüedad, prestaciones, …). En este caso la variable respuesta es de tipo numérico.\n\nLos algoritmos de aprendizaje supervisado incluyen algoritmos poco profundos como la regresión lineal y logística, los árboles de decisión (conocidos como regression trees, decision trees o classification trees), los bosques aleatorios (random forests), los K-vecinos más próximos, (K-Nearest Neighbors, KNN) y las máquinas de vectores de apoyo (Super Vector Machine, SVM).\nDicho esto, hay otros problemas “avanzados” que pueden resolverse con técnicas de aprendizaje supervisado, como:\n\nSubtitulado de imágenes, para predecir el título de una imagen determinada.\nDetección de objetos, para reconocer un objeto en una imagen y dibujar un contorno delimitador a su alrededor.\nSegmentación de imágenes, para identificar los píxeles que componen cada objeto en una imagen.\n\nAlgunas de estas tareas pueden incluir tanto técnicas de clasificación como de regresión. Así por ejemplo, para la detección de objetos es precisa una tarea de clasificación, con la que reconocer el objeto entre los otros en una imagen, y la regresión, para predecir sus coordenadas y poder definir su contorno delimitador."
  },
  {
    "objectID": "20_introAA.html#sec-Anosupervised",
    "href": "20_introAA.html#sec-Anosupervised",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "4.2 Aprendizaje no supervisado",
    "text": "4.2 Aprendizaje no supervisado\nEn este caso disponemos exclusivamente de datos genéricos con los que entrenar el modelo, pero no se ha observado a la par una variable respuesta a predecir, y con la que juzgar la bondad del modelo. El objetivo en este tipo de problemas es establecer o identificar patrones de comportamiento en el conjunto de datos disponible. El resultado de estos modelos suele ser una etiqueta de clasificación, esto es, una clase o categoría, para cada uno de los registros en la muestra, en base a que se haya identificado un patrón de comportamiento común a todos aquellos que comparten la misma etiqueta.\nLos algoritmos de aprendizaje no supervisado se utilizan principalmente para:\n\nAgrupar datos por patrones, como los algoritmos de K-Medias y de agrupación jerárquica.\nReducir la dimensión del banco de datos y mejorar su visualización, como el análisis de componentes principales (PCA), o el t-Distributed Stochastic Neighbor Embedding (t-SNE)."
  },
  {
    "objectID": "20_introAA.html#sec-Asemisupervised",
    "href": "20_introAA.html#sec-Asemisupervised",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "4.3 Aprendizaje semi-supervisado",
    "text": "4.3 Aprendizaje semi-supervisado\nEl aprendizaje semi-supervisado se sitúa entre el aprendizaje supervisado y el no supervisado. En el aprendizaje semi-supervisado se etiqueta una pequeña parte de los datos de entrenamiento, mientras que el resto de los datos no se etiquetan.\nEl etiquetado de los datos es uno de los mayores retos del aprendizaje automático, dado que requiere de una toma de datos específica para la etiqueta/respuesta, a veces automatizada, pero en otras ocasiones manual, y que obviamente ocasiona costes y puede conllevar errores. Por lo tanto, disponer de procedimientos analíticos que permitan minimizar el número de datos etiquetados, y a la vez utilizar datos no etiquetados para mejorar el ajuste puede generar interesantes beneficios relacionados con la reducción de tiempos y costes vinculados al etiquetado.\nEl aprendizaje semi-supervisado es más notable en los problemas que implican trabajar con conjuntos de datos masivos, como las búsquedas de imágenes en Internet, el reconocimiento de imágenes y audio, y la clasificación de páginas web. Como se puede imaginar, nadie en su sano juicio se va a dedicar a etiquetar las millones de imágenes que se suben a diario a plataformas de medios sociales como Instagram, o los miles de páginas web que se crean cada día (252.000 según Sitefy). En estos casos, el aprendizaje semi-supervisado puede generar aproximaciones más eficientes en términos de recursos."
  },
  {
    "objectID": "20_introAA.html#sec-Aautosupervised",
    "href": "20_introAA.html#sec-Aautosupervised",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "4.4 Aprendizaje auto-supervisado",
    "text": "4.4 Aprendizaje auto-supervisado\nEl aprendizaje auto-supervisado es uno de los tipos de aprendizaje automático más emocionantes por cuanto al tipo de aplicaciones que tiene relacionadas con la visión por ordenador y la robótica. Mientras que el aprendizaje semi-supervisado utiliza una pequeña porción de datos etiquetados, el aprendizaje auto-supervisado utiliza todos los datos sin etiquetar y no requiere de anotaciones manuales, lo que elimina la necesidad de los humanos en el proceso.\nLa motivación del aprendizaje auto-supervisado es aprovechar la gran cantidad de datos sin etiquetar, generando con ellos etiquetas según su estructura o características y luego entrenar sobre estos datos etiquetados artificialmente, con técnicas de aprendizaje supervisado."
  },
  {
    "objectID": "20_introAA.html#sec-Areforced",
    "href": "20_introAA.html#sec-Areforced",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "4.5 Aprendizaje por refuerzo",
    "text": "4.5 Aprendizaje por refuerzo\nEl aprendizaje por refuerzo es un tipo especial de aprendizaje automático que se aplica sobre todo en robótica y juegos. En el aprendizaje por refuerzo, un sistema de aprendizaje llamado agente puede percibir el entorno, realizar algunas acciones y ser recompensado o penalizado en función de su rendimiento. El objetivo principal del agente es acumular tantas recompensas como sea posible. El agente aprende así la mejor estrategia (política) buscando obtener la máxima recompensa.\nEl aprendizaje por refuerzo ha protagonizado algunos de los momentos más históricos de la IA. En 2016, DeepMind AlphaGo, un sistema de aprendizaje por refuerzo, ganó jugando a Go, un complejo juego de mesa que requiere intuición, a Lee Sedol -uno de los mejores jugadores a nivel mundial- en el Google DeepMind Challenge Match.\nMuchos de nosotros puede que no saquemos el máximo partido al aprendizaje por refuerzo, normalmente debido a la limitación de los recursos y su ámbito de aplicación, pero es una herramienta poderosa en los ámbitos de la robótica y los juegos."
  },
  {
    "objectID": "20_introAA.html#sec-problemaAA",
    "href": "20_introAA.html#sec-problemaAA",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.1 Definir un problema",
    "text": "6.1 Definir un problema\nTodo comienza aquí. La definición del problema es el paso inicial crucial en cualquier proyecto de aprendizaje automático. Aquí es donde te aseguras de entender el problema realmente bien. Entender el problema te dará las intuiciones adecuadas sobre los pasos a seguir, algoritmos a utilizar, etc. Pero espera, ¿qué significa entender el problema?\nEntender el problema consiste en profundizar en los detalles del problema en cuestión y formular las preguntas adecuadas. En primer lugar, siempre será importante simplificar el problema o seccionarlo en varios problemas más sencillos que nos permitan concretar objetivos claros y abordables. He aquí ejemplos de objetivos sencillos: clasificar productos en diferentes categorías, predecir el precio de un coche usado dadas sus características (como la marca, la edad, etc…), reconocer si una persona lleva una máscara facial, dividir a los clientes en diferentes grupos que comparten comportamientos similares, etc… Formular el objetivo generalmente nos conducirá a formular el problema como un problema de clasificación, de regresión, de agrupación, etc.\nEn esta fase es fundamental evitar expresiones vagas para la formulación de las preguntas a responder. Cuanto más simple sea la formulación del problema, mejor irán las cosas en el futuro. También es preciso evaluar si el proyecto es abordable, o no, mediante aprendizaje automático.\nLa definición del problema también conlleva reflexionar sobre los datos que se necesitan para resolverlo. Los modelos de aprendizaje automático se basan en datos, y con datos defectuosos sólo conseguiremos modelos defectuosos. ¿Qué información tenemos o de cuál podemos disponer?, ¿tenemos datos para cada una de las preguntas que hemos formulado? Hacernos estas preguntas y reajustar los objetivos o la recogida de datos conforme a ellas será fundamental para garantizar el éxito. Modelizar y resolver problemas con modelos NO es magia. Sólo podremos identificar patrones y predecir eficientemente si los datos que utilizamos contienen información significativamente relevante para ello."
  },
  {
    "objectID": "20_introAA.html#sec-recogerdatos",
    "href": "20_introAA.html#sec-recogerdatos",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.2 Recoger datos",
    "text": "6.2 Recoger datos\nEsta suele ser la siguiente etapa tras la formulación de un problema (no podemos obviar que en ocasiones se recopilan datos y después se plantea qué se puede hacer con ellos). Antes de entrar en detalle sobre la recogida de datos, vamos a repasar el significado de “dato”. Según Wikipedia, “los datos son un conjunto de valores de variables cualitativas o cuantitativas sobre una o varias personas u objetos”. En nuestro caso, cuando hablemos de datos estaremos hablando de una serie de registros recopilados para un conjunto de variables, y a veces también para alguna variable respuesta (que ya identificamos también como “etiqueta”, cuando describimos el aprendizaje supervisado).\nHay 2 tipos principales de datos que son:\n\nDatos estructurados, que se pueden organizar/registrar en formato tabular u hoja de cálculo. Ejemplos de datos tabulares son los registros de clientes, las ventas de coches, etc.\nDatos no estructurados, como imágenes, textos, sonidos y vídeos. Los datos no estructurados no están organizados como los anteriores.\n\nHoy en día hay muchas bases de datos en abierto, en plataformas como Kaggle, Conjuntos de datos de Google, UCI Machine Learning Repository y muchos otros sitios web gubernamentales (fuera de España fundamentalmente). Así que, si estás resolviendo un problema que alguien resolvió antes, o alguno similar, es muy probable que encuentres información en algún lugar de esas plataformas o en otras fuentes públicas. Inicia pues tu análisis, buscando.\nDicho esto, hay veces que tendrás que recoger tus propios datos, especialmente si estás resolviendo un problema específico que nadie ha resuelto antes. En este caso, ten en cuenta el tiempo a dedicar en la recogida de datos y sus costes. Ten también en cuenta que a veces no es necesario tener todos los registros deseados para poder empezar a resolver; adopta la dinámica de Machine Learning desde el inicio para ir aprendiendo si necesitas más datos.\nAdemás, al recopilar los datos, es siempre preferible la calidad a la cantidad. Hay veces que pocos datos buenos pueden generar soluciones superiores a las que generan muchos datos pobres. La cantidad de datos que necesites va a depender del problema que a resolver y de su alcance, pero ten presente siempre como objetivo el conseguir datos de la mejor calidad posible."
  },
  {
    "objectID": "20_introAA.html#sec-modelobasal",
    "href": "20_introAA.html#sec-modelobasal",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.3 Establecer un modelo de partida",
    "text": "6.3 Establecer un modelo de partida\nSin un modelo de partida será inviable evaluar los resultados, y mucho menos las mejoras que se consigan. Un modelo de partida es el modelo más sencillo que puede resolver un problema dado, con unos requisitos mínimos. No tiene por qué ser un modelo estrictamente hablando, y podría ser una aplicación de código abierto existente, un análisis estadístico, o incluso meras intuiciones que se obtienen de los datos a partir de un análisis preliminar.\nEl objetivo principal del modelo base o de partida es servir como punto de referencia para comparar el modelo de aprendizaje y evaluar las mejoras que genera. El objetivo final es superar al modelo base, en términos de reducción de pérdidas que se cuantificarán, como veremos más adelante, con la función de pérdida, y nos permitirán evaluar la bondad de un modelo. En caso de no poder mejorar el modelo de partida, deberemos entender que el modelo de aprendizaje no merece la pena, y bastará con asumir el modelo de partida. En ocasiones no serán posibles las mejoras del modelo base porque la recogida de datos no haya sido la adecuada y haya generado datos de mala calidad, o simplemente no se haya identificado ninguna variable predictora que realmente influya en la respuesta. Reiteramos que la calidad de los modelos de aprendizaje que generemos dependerá, en buena parte, de la calidad de los datos disponibles."
  },
  {
    "objectID": "20_introAA.html#sec-aedenAA",
    "href": "20_introAA.html#sec-aedenAA",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.4 Análisis exploratorio de datos (AED)",
    "text": "6.4 Análisis exploratorio de datos (AED)\nAntes de manipular los datos, es muy importante inspeccionarlos. Esto podría obviarse, pero hacerlo y hacerlo bien ayudará a identificar mejor estrategias eficaces para limpiar los datos e incrementar su calidad.\nEl análisis exploratorio de los datos consiste en revisar los valores para descubrir si hay\n\nincoherencias o errores\nvalores faltantes\ncarencias\nvalores corrompidos o con formatos no soportados (por ejemplo una imagen en un fichero .txt. y con 0Kb)\ndesequilibrios de clase\nduplicados\nsesgos\n\nRealizar un análisis exploratorio va incluso más allá de estas cuestiones y utiliza representaciones gráficas y numéricas que permitan descubrir cómo son los datos (cómo se distribuyen, qué variabilidad tienen, …), qué tipo de relación y correlación hay entre las variables disponibles, etc.\nDado que en los modelos de aprendizaje automático se seccionan los datos en tres subconjuntos o muestras, entrenamiento, validación y prueba o testado, hay que asegurar, mediante un análisis exploratorio, que estas tres muestras compartan la misma distribución estadística, pues de no ser así, los resultados obtenidos con los datos de entrenamiento no serán extrapolables al resto y el error se disparará."
  },
  {
    "objectID": "20_introAA.html#sec-preprocesado",
    "href": "20_introAA.html#sec-preprocesado",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.5 Preprocesar los datos",
    "text": "6.5 Preprocesar los datos\nEn muchos libros de texto y cursos, el preprocesamiento de datos también se denomina limpieza de datos o preparación de datos.\nEl preprocesado de datos es quizás el proceso que consume la mayor parte del tiempo en cualquier proyecto de aprendizaje automático. No es inusual que esta parte consuma alrededor del 80% del tiempo total de la modelización, y esto siempre será así porque los datos del mundo real están desordenados y contienen errores.\nPreprocesar los datos implica convertir los datos en bruto en un formato que pueda ser aceptado por los algoritmos de aprendizaje automático.\nEl preprocesamiento de datos es difícil porque hay diferentes tipos de datos y diferentes circunstancias de recogida de datos, y la forma de procesar cada uno generalmente será diferente al resto. Por ejemplo, en datos estructurados, la forma de procesar las características numéricas va a ser diferente a las características categóricas. También en los datos no estructurados, la forma de manipular las imágenes va a ser diferente a la forma de manipular los textos o los sonidos.\nTratamientos específicos para el procesado de datos los iremos encontrando a lo largo del módulo formativo. En términos generales hay una serie de tareas de preprocesado muy comunes en todos los problemas, que son las que comentamos a continuación.\n\nImputación de valores perdidos: los valores perdidos o faltantes en un banco de datos pueden rellenarse, eliminarse o dejarse como están. Con la excepción de los modelos basados en árboles, la mayoría de los modelos de aprendizaje automático no aceptan valores perdidos, con lo cual el hecho de que un banco de datos tenga valores faltantes puede ser un problema. Surge así la imputación como un método interesante para evitar los valores perdidos. Hay varias estrategias de imputación como la media o la mediana de los valores restantes, el relleno hacia atrás y hacia delante con valores colindantes cuando los datos tienen un carácter temporal, y las imputaciones iterativas. La técnica de imputación adecuada dependerá del problema y de las características de los datos, y habitualmente con la práctica el profesional adquiere experiencia para utilizar la técnica óptima.\nCodificación de características categóricas: en los modelos de aprendizaje automático las variables de tipo categórico han de contener etiquetas numéricas para identificar las categorías. Esto hace necesario recodificar las variables categóricas, que generalmente no vienen expresadas en términos de números. Las técnicas más comunes de codificación son la codificación por etiquetas y la codificación en caliente. Por ejemplo, la variable de sexo, con categorías Hombre y Mujer, se puede codificar con etiqueta 0 para el Hombre y 1 para la Mujer, o en caliente, en la que se elige una categoría que recibe el 1 (por ejemplo Hombre) y se asigna 0 a todas las restantes.\nEscalar las características numéricas: la mayoría de los modelos de aprendizaje automátio funcionan bien cuando los valores de entrada se escalan a valores pequeños, pues de este modo se entrenan y convergen más rápido a una solución. Hay dos técnicas principales de escalado, que son la normalización y la estandarización. La normalización reescala las características a valores entre 0 y 1, mientras que la estandarización reescala las características para que tengan una media de 0 y una desviación estándar de 1. Si los datos tienen una distribución normal, la estandarización puede ser una buena opción; en otro caso, o simplemente si no se sabe cuál es la distribución, la normalización funcionará bien.\nDefinir nuevas variables: la definición de nuevas variables a partir de las existentes, que contengan información relevante, incluso a veces más significativa que la que contienen algunas variables originales, también forma parte del preprocesamiento de datos. Se trata de una tarea creativa y requiere un conocimiento y experiencia adicional del modelador o analista."
  },
  {
    "objectID": "20_introAA.html#sec-entrenamiento",
    "href": "20_introAA.html#sec-entrenamiento",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.6 Seleccionar y entrenar un modelo",
    "text": "6.6 Seleccionar y entrenar un modelo\nSeleccionar, crear y entrenar, esto es, ajustar, un modelo de aprendizaje automático es la parte menor, en cuanto a carga de trabajo, en un proyecto de aprendizaje automático. Hay diferentes tipos de modelos, pero en líneas generales, la mayoría de ellos entran en estas categorías: modelos lineales como la regresión lineal y logística, modelos basados en árboles como los árboles de clasificación, modelos de conjunto como los bosques aleatorios y, por último, las redes neuronales.\nDependiendo del problema, se puede elegir uno de estos modelos, o probar varios. En general hay que experimentar con diferentes modelos alternativos para conseguir, al final, uno que funcione razonablemente bien para el problema y conjunto de datos con el que se está trabajando.\nPara reducir la curva de modelado, esto es, el tiempo que se tarda en conseguir un modelo óptimo, existen una serie de cuestiones a considerar para elegir cómo abordar el aprendizaje automático:\n\nEl objetivo y tipo de datos pueden dar señales importantes sobre qué algoritmo de aprendizaje utilizar. Por ejemplo, si se pretende conseguir un clasificador de imágenes, las redes neuronales (concretamente las redes neuronales convolucionales) son las herramientas preferidas.\nEl tamaño de la base de datos. Los modelos lineales tienden a funcionar bien en problemas con pocos datos, mientras que los modelos de conjunto y las redes neuronales pueden funcionar bien cuando se dispone de bases con muchos datos y variables.\nEl nivel de interpretabilidad deseado. Si se pretende que las predicciones del modelo sean explicables, un modelo lineal o de árbol puede dar una buena solución, pero no será así si se elige un modelo de redes neuronales.\nEl tiempo de entrenamiento y la capacidad computacional para ajustar un modelo. Los modelos complejos, como las redes neuronales y los modelos de conjunto, requieren importantes recursos de memoria y suelen tardar más tiempo en entrenarse. En cambio los modelos lineales suelen entrenarse más rápidamente."
  },
  {
    "objectID": "20_introAA.html#sec-errores",
    "href": "20_introAA.html#sec-errores",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.7 Análisis de errores",
    "text": "6.7 Análisis de errores\nEl análisis de los errores nos debe guiar en el proceso de ajuste para mejorar los resultados del modelo. Las mejoras pueden provenir de los datos o del modelo.\nUna de las mejores maneras de realizar el análisis de errores es trazar la curva de aprendizaje e identificar dónde está fallando el modelo y cuál podría ser la razón, así como las acciones correctas que se pueden tomar para reducir los errores.\nPara mejorar el modelo se pueden probar diferentes alternativas para la configuración del modelo a través de los hiperparámetros. También se pueden probar diferentes modelos hasta encontrar uno que funcione mejor.\nPero además, hay que tener en cuenta que no puede haber un buen modelo sin unos buenos datos, por lo que es importante dedicar tiempo a examinar los resultados del modelo con respecto a los datos de entrada, inspeccionar si va mal en general o sólo para un subconjunto de los datos, y en general cuál es el margen de mejora.\nA menudo, las mejoras no vendrán de afinar el modelo, sino de dedicar tiempo a aumentar el volumen de la muestra de entrenamiento y la calidad de los datos. Para mejorar los datos también se pueden crear variables artificiales más informativas a partir de las disponibles o aumentar los datos (por ejemplo aplicación de filtros en imágenes), entre otros.\nCon todo es importante tener presente que el análisis de errores es un proceso iterativo basado en el ensayo error hasta conseguir un modelo mejor o quizás simplemente aceptable."
  },
  {
    "objectID": "20_introAA.html#sec-evaluar",
    "href": "20_introAA.html#sec-evaluar",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.8 Evaluar el modelo",
    "text": "6.8 Evaluar el modelo\nEn un modelo de aprendizaje automático, los datos se suelen dividir en varios subconjuntos, cada uno de los cuales se utiliza en una etapa de la modelización. El conjunto de datos de entrenamiento se utiliza para entrenar un modelo, el conjunto de validación para evaluar el rendimiento del modelo durante el entrenamiento, con el que sugerir mejoras, y el conjunto de prueba, que se utiliza para evaluar el rendimiento final y la mejora del modelo.\nCuando se ha seleccionado un modelo que funciona, es habitual evaluarlo con la muestra de validación. Si el modelo ofrece buenos resultados sobre estos datos, ajenos a aquellos con los que se ha ajustado (entrenado) el modelo, es el momento de pasar a la fase de test, si se dispone de una muestra de test, o de implementar el modelo de aprendizaje para que continúe aprendiendo conforme se recoja nueva información.\nLos sistemas de recomendación en plataformas de comercio online son un buen ejemplo de modelos que continúan aprendiendo conforme se nutren de más datos de los usuarios."
  },
  {
    "objectID": "20_introAA.html#sec-automatizar",
    "href": "20_introAA.html#sec-automatizar",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "6.9 Automatizar el modelo",
    "text": "6.9 Automatizar el modelo\nLa automatización o implementación del modelo es la última parte del flujo de trabajo en un análisis abordado con aprendizaje automático. Cuando todos los pasos anteriores han ido bien y se ha alcanzado un modelo con buenos resultados en la muestra de validación, el siguiente paso será implementar el modelo para que los usuarios puedan empezar a hacer uso de él, retroalimentando con nuevos datos con los que obtener mejores predicciones y servicios."
  },
  {
    "objectID": "20_introAA.html#sec-metricasregresion",
    "href": "20_introAA.html#sec-metricasregresion",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "7.1 Métricas para problemas de regresión",
    "text": "7.1 Métricas para problemas de regresión\nEn las tareas de regresión el objetivo es predecir el valor numérico de la variable respuesta (identificada también como target en la literatura). Así pues, las métricas que surgen de manera natural para evaluar estos modelos son las basadas en distancias entre las observaciones y las predicciones.\nLa diferencia entre el valor real de la respuesta (\\(y\\)) y el valor predicho (\\(\\hat{y}\\)) se llama error de predicción:\n\\[Error = y - \\hat{y} \\tag{7.1}\\]\nEl cuadrado del error de predicción sobre los datos observados \\(\\{y_1,y_2,..., y_n\\}\\), se llama error cuadrático medio (MSE o Mean Squared Error, en inglés), y se calcula como:\n\\[MSE = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n} \\tag{7.2}\\]\nOtra métrica común es la raíz cuadrada del error cuadrático medio (RMSE). El RMSE es la métrica de regresión más utilizada.\n\\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n}} \\tag{7.3}\\]\nHay veces que se trabaja con conjuntos de datos que contienen valores atípicos. Una métrica adecuada para este tipo de conjuntos de datos es el error medio absoluto (MAE, Mean Absolute Error) que se calcula como:\n\\[MAE = \\frac{\\sum_{i=1}^n |y_i - \\hat{y}_i|}{n} \\tag{7.4}\\]"
  },
  {
    "objectID": "20_introAA.html#sec-metricasclasif",
    "href": "20_introAA.html#sec-metricasclasif",
    "title": "2  Introducción al Aprendizaje Automático (AA)",
    "section": "7.2 Métricas para problemas de clasificación",
    "text": "7.2 Métricas para problemas de clasificación\nEn los problemas de clasificación el objetivo es predecir correctamente la categoría de clasificación de cada observación. Así pues, las métricas naturales para evaluar estos modelos estarán basadas en contabilizar las coincidencias entre la clasificación correcta y la conseguida o predicha con el modelo.\nPara definir estos indicadores partimos de identificar las clasificaciones posibles, cuestión que salvamos con una matriz de confusión, esto es, una tabla que muestra el número de predicciones correctas e incorrectas realizadas por un clasificador en todas las clases disponibles. En el caso de una respuesta binaria, 0=Negativo y 1=Positivo en relación al hecho de que se dé determinada característica, las clasificaciones/predicciones posibles sobre el total de datos o registros disponibles son las que se muestran en la siguiente matriz:\n\n\n\n\n\nValor Predicho\n\n\n\n\n\n\n\nNegativo\nPositivo\n\n\nValor observado\nNegativo\nTN\nFP\n\n\n\nPositivo\nFN\nTP\n\n\n\nSe identifican entonces, las siguientes cantidades de interés:\n\nTP, True Positive o Verdaderos positivos, que es el número de registros que se clasifican correctamente como positivos.\nFP, False Positive o Falsos positivos es el número de registros que siendo negativos (0), se clasifican de modo incorrecto como positivos (1).\nTN, True Negative o Verdaderos negativos es el número de registros negativos (0) que con correctamente clasificados como negativos.\nFN, False Negative o Falsos negativos es el número de registros positivos (1) que son clasificados incorrectamente (con 0).\n\nEn base a estos números se definen las siguientes métricas de evaluación de los modelos de aprendizaje basados en clasificación:\n\nLa exactitud (representado en la literatura por A, de accuracy) es la métrica más utilizada y muestra la capacidad del modelo para hacer predicciones correctas, es decir, el ratio de observaciones clasificadas correctamente. Se calcula a partir de la proporción de observaciones clasificadas correctamente (TP + TN sobre el total), esto es,\n\n\\[Exactitud (A)= \\frac{TP + TN}{TP + TN + FP + FN}. \\tag{7.5}\\]\nLa exactitud es un indicador de la bondad del modelo para hacer predicciones correctas, por lo que sólo será útil cuando el conjunto de datos esté repartido de forma equilibrada entre las categorías de respuesta.\nSupongamos que hemos construido un clasificador de imágenes que distingue entre caballos y humanos, con 250 imágenes de caballos y otras 250 de humanos. Si el modelo predice correctamente 400 imágenes en total (caballos y humanos), su exactitud es de 0.8, o lo que es lo mismo, del 80% en términos porcentuales.\nCuando tenemos un conjunto de datos sesgado o desequilibrado hacia alguna categoría, necesitamos una perspectiva diferente sobre cómo evaluar el modelo. Por ejemplo, si tenemos 450 imágenes de caballos y 50 imágenes de humanos, hay una probabilidad del 90% (450/500) de que el caballo se prediga correctamente porque el conjunto de datos está dominado por los caballos. Y aunque la predicción sea nefasta para los humanos, la exactitud seguirá siendo buena.\nSurgen pues otras métricas que, en estas circunstancias, serán más útiles que la exactitud, como la precisión, el recuerdo y la puntuación F1.\nLa precisión (representado habitualmente como P, de precision) representa el porcentaje de observaciones correctamente clasificadas, de entre las que se han clasificado como positivas.\n\\[Precisión (P)= \\frac{TP}{TP + FP}. \\tag{7.6}\\]\nPor ejemplo, nos podría interesar especialmente saber, de las imágenes que se clasificaron como humanos, cuántas eran realmente de humanos.\nPor otro lado, el recuerdo (representado por R, de recall) muestra el porcentaje de las observaciones que siendo positivas se clasificaron correctamente, y nos resulta útil cuando nos interesa especialmente la clasificación correcta de una determinada clase.\n\\[Recuerdo (R)= \\frac{TP}{TP+FN}. \\tag{7.7}\\]\nEn el ejemplo de caballos y humanos, descubriríamos, con el recuerdo, de las imágenes que eran de humanos, qué porcentaje fueron correctamente clasificadas como humanos.\nDado que existe cierta relación entre la precisión y la recuperación, y a menudo el aumento de una conlleva la disminución de la otra, se propuso otra métrica construida a partir de las dos, para conjugar las ventajas de ambas.\nLa puntuación F1 (o F1 Score) es la media armónica de la precisión y la recuperación, y muestra lo bueno que es el modelo a la hora de clasificar todas las clases sin tener que equilibrar la precisión y la recuperación. Si la precisión o la recuperación son muy bajas, la puntuación F1 también lo será.\n\\[F1 = \\frac{2 \\cdot P \\cdot R}{P + R}. \\tag{7.8}\\]"
  },
  {
    "objectID": "30_RandRstudio.html#sec-whatisR",
    "href": "30_RandRstudio.html#sec-whatisR",
    "title": "3  Introducción a R y RStudio",
    "section": "3.1 R y RStudio",
    "text": "3.1 R y RStudio\nR es un lenguaje de programación estadística y un entorno de software libre y de código abierto. Fue creado por Ross Ihaka y Robert Gentleman en 1993 (R Core Team (2021)). Este lenguaje es ampliamente utilizado en estadísticas, análisis de datos, investigación científica y campos relacionados. R cuenta con una gran cantidad de paquetes y librerías que facilitan el análisis y manipulación de datos, así como la generación de gráficos y visualizaciones.\nR es especialmente popular en el análisis estadístico debido a su flexibilidad, potencia y comunidad activa de usuarios y desarrolladores. Permite realizar operaciones matemáticas y estadísticas avanzadas y se utiliza en áreas como la bioinformática, la economía, la ciencia de datos y la investigación social.\nPara instalar el programa R para el que existen versiones tanto para Windows, macOS o Linux se puede acudir a la página web https://cran.r-project.org/.\nRStudio es un entorno de desarrollo integrado (IDE) para el lenguaje R. Es una aplicación que proporciona una interfaz gráfica amigable y eficiente para trabajar con R. RStudio mejora la experiencia del usuario al programar en R al ofrecer diversas funcionalidades, como un editor de código con resaltado de sintaxis, herramientas para la visualización de objetos y datos, una consola interactiva, y la posibilidad de generar informes en formato RMarkdown, que combinan código, texto y visualizaciones en un solo documento.\nLa interfaz de RStudio está organizada de manera intuitiva, lo que facilita la programación, depuración y análisis de datos en R. Además, RStudio permite gestionar proyectos y trabajar con diversos archivos y scripts de forma eficiente.\nLa instalación de RStudio siempre debe ser posterior a la instalación de R y como para este existen versiones para diferentes sistemas operativos. En este caso tenemos también versiones del programa gratuitas y de pago (sólo para usuarios muy avanzados) que se pueden descargar de la página web https://posit.co/products/open-source/rstudio/"
  },
  {
    "objectID": "30_RandRstudio.html#sec-resourcesR",
    "href": "30_RandRstudio.html#sec-resourcesR",
    "title": "3  Introducción a R y RStudio",
    "section": "3.2 Recursos online para R y RStudio",
    "text": "3.2 Recursos online para R y RStudio\nAunque existen una cantidad de recursos inmensos sobre R y Rstudio, en este material hemos hecho una pequeña selección de recursos online que pueden servir de apoyo para la utilización de R y Rstudio. Se recomienda la consulta de los materiales electrónicos siguientes para iniciar la formación sobre ambos programas:\n\nChilds, D. Z. 2017. APS 135: Introduction to Exploratory Data Analysis with R. [Versión electrónica](https://dzchilds.github.io/eda-for-bio/).\nGrosser, M. 2017. Tidyverse Cookbook. [Versión electrónica incompleta](https://bookdown.org/Tazinho/Tidyverse-Cookbook/).\nWickham, H. 2015. Advanced R. CRC Press. [Versión electrónica resumida](https://adv-r.hadley.nz/).\nWickham, H. 2010. ggplot2. Third Edition. Springer. [Recursos electrónicos](http://ggplot2.org/).\nWickham, H. & Grolemund, G. 2016. R for Data Science. O’Reilly. [Versión electrónica resumida](http://r4ds.had.co.nz/).\n\nA continuación se presentan unas pequeñas recomendaciones de lectura para los primeros pasos tanto con R como RStudio.\n\nPrimeros pasos en R y RStudio. Leer los capítulos 1, 2, y 3 de Childs (2017), el capítulo 4 de Wickham (2015), y los capítulos 4 y 6 de Wickham (2016) para un desarrollo más amplio. Realizar los ejercicios que van a apareciendo a lo largo de los capítulos.\nEstructuras de datos. Leer los capítulos 4, 5, 6 y 9 de Childs (2017) para una breve introducción y los capítulos 2 y 3 de Wickham (2015) para completar la información. Realiza los ejercicios que van apareciendo.\nInstalación y uso de librerías en RStudio. Leer el capítulo 8 de Childs (2017) para ver como descargar e instalar en RStudio. En el punto siguiente presentamos las librerías iniciales que utilizaremos y veremos como instalarlas y hacerlas accesibles de forma directa en RStudio.\nCreación de proyectos y entornos de trabajo. Leer el capítulo 8 de Wickham (2016). Crea un proyecto para esta unidad, selecciona el directorio de trabajo donde se encuentra situado el proyecto, y guarda el entorno de trabajo.\n\nEn los últimos tiempos se ha puesto de modo la creación de informes directos a partir del código utilizado en Rstudio mediante la creación de documentos específicos. Su puede consultar una guía sencilla de uso en enlace. Un desarrollo más completo se puede ver en este enlace. También se puede consultar este vídeo."
  },
  {
    "objectID": "30_RandRstudio.html#sec-libraryR",
    "href": "30_RandRstudio.html#sec-libraryR",
    "title": "3  Introducción a R y RStudio",
    "section": "3.3 Librerias de R",
    "text": "3.3 Librerias de R\nEn R, las “librerías” (también conocidas como “paquetes”) son conjuntos de funciones, datos y documentación que extienden las capacidades del lenguaje y proporcionan herramientas adicionales para realizar tareas específicas. Los paquetes son desarrollados por la comunidad de usuarios de R y se encuentran disponibles de forma gratuita en el repositorio oficial de R llamado CRAN (Comprehensive R Archive Network) y en otros repositorios especializados.\nPara usar una librería en R, primero se debe instalar y luego hacerla accesible. para instalarla podemos usar el menú Build de Rstudio, que accede al repositorio CRAN para la búsqueda de la librería de interés. Una vez instalada usamos el código `library(nombre_del paquete)` para acceder a las funciones y características proporcionadas por el paquete para realizar tareas específicas.\nEn estos momentos la librería más relevante para la ciencia de datos es tidyverse (Wickham et al. 2019) que es una colección de paquetes R diseñados para la ciencia de datos. Entre los paquetes que forman parte de la colección podemos destacar:\n\nggplot2 (Wickham et al. 2019): Esta librería es ampliamente utilizada para crear gráficos elegantes y personalizados. Es una parte fundamental del “Grammar of Graphics” y proporciona una forma fácil y flexible de visualizar datos.\ndplyr (Wickham et al. 2023): Es una librería para manipulación de datos que ofrece funciones eficientes y rápidas para filtrar, ordenar, agrupar y resumir datos.\ntidyr (Wickham, Vaughan, and Girlich 2023): Esta librería se utiliza para transformar datos en un formato “tidy”, que es un formato estándar para datos donde cada variable se encuentra en una columna y cada observación se encuentra en una fila.\nreadr (Wickham, Hester, and Bryan 2023): Proporciona funciones para leer datos desde diferentes formatos de archivo, como CSV, Excel, y bases de datos, de manera rápida y sencilla.\nlubridate (Grolemund and Wickham 2011): Es útil para el manejo de fechas y horas, facilitando su manipulación y cálculos.\nstringr (Wickham 2022): Ofrece funciones para manipulación de cadenas de texto, como búsqueda, reemplazo, y extracción de patrones.\nforcats (Wickham 2023): Ofrece un conjunto de herramientas que resuelven problemas comunes con factores, incluido el cambio del orden de los niveles o los valores.\n\nPara instalar todo el compendio de librerías se puede utilizar el código siguiente directamente en la consola de RStudio:\n\ninstall.packages(\"tidyverse\")\n\nPara hacer accesible todos lo recursos debemos cargar dicha librería (en la consola de RStudio o el documento de código que estemos creando):\n\nlibrary(tidyverse)\n\no podemos optar por cargar un paquete únicamente:\n\nlibrary(ggplot2)\n\nOtros paquetes muy interesantes son tidymodels (Kuhn and Wickham 2020) y sjPlot (Lüdecke 2023) que nos permiten extraer información numérica o gráfica de nuestros modelos estadísticos de forma mucho más accesible. El código siguiente instala y carga dichas librerías.\n\ninstall.packages(c(\"tidymodels\", \"sjPlot\"))\n\n\nlibrary(tidymodels)\nlibrary(sjPlot)\n\nUtilizamos está última librería para configurar el entorno gráfico que utilizaremos para usar todos los gráficos.\n\ntheme_set(theme_sjplot2())\n\nPor último siempre es recomendable indicar la versión tanto del programa R que se está utilizando como de las versiones de las librerías disponibles. Esto se puede hacer con le código siguiente:\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6.8\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Madrid\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] sjPlot_2.8.15      yardstick_1.2.0    workflowsets_1.0.1 workflows_1.1.3   \n [5] tune_1.1.2         rsample_1.2.0      recipes_1.0.8      parsnip_1.1.1     \n [9] modeldata_1.2.0    infer_1.0.5        dials_1.2.0        scales_1.2.1      \n[13] broom_1.0.5        tidymodels_1.1.1   lubridate_1.9.3    forcats_1.0.0     \n[17] stringr_1.5.0      dplyr_1.1.3        purrr_1.0.2        readr_2.1.4       \n[21] tidyr_1.3.0        tibble_3.2.1       ggplot2_3.4.4      tidyverse_2.0.0   \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.1         magrittr_2.0.3      furrr_0.3.1        \n [4] compiler_4.3.0      vctrs_0.6.4         lhs_1.1.6          \n [7] pkgconfig_2.0.3     fastmap_1.1.1       backports_1.4.1    \n[10] utf8_1.2.4          rmarkdown_2.25      prodlim_2023.08.28 \n[13] tzdb_0.4.0          nloptr_2.0.3        xfun_0.40          \n[16] jsonlite_1.8.7      sjmisc_2.8.9        ggeffects_1.3.2    \n[19] parallel_4.3.0      R6_2.5.1            stringi_1.7.12     \n[22] parallelly_1.36.0   boot_1.3-28.1       rpart_4.1.21       \n[25] estimability_1.4.1  Rcpp_1.0.11         iterators_1.0.14   \n[28] knitr_1.45          modelr_0.1.11       future.apply_1.11.0\n[31] Matrix_1.6-1.1      splines_4.3.0       nnet_7.3-19        \n[34] timechange_0.2.0    tidyselect_1.2.0    rstudioapi_0.15.0  \n[37] yaml_2.3.7          timeDate_4022.108   codetools_0.2-19   \n[40] sjlabelled_1.2.0    listenv_0.9.0       lattice_0.22-5     \n[43] withr_2.5.2         bayestestR_0.13.1   evaluate_0.22      \n[46] future_1.33.0       survival_3.5-7      pillar_1.9.0       \n[49] foreach_1.5.2       insight_0.19.6      generics_0.1.3     \n[52] hms_1.1.3           munsell_0.5.0       minqa_1.2.6        \n[55] globals_0.16.2      xtable_1.8-4        class_7.3-22       \n[58] glue_1.6.2          emmeans_1.8.9       tools_4.3.0        \n[61] data.table_1.14.8   lme4_1.1-34         gower_1.0.1        \n[64] mvtnorm_1.2-3       grid_4.3.0          ipred_0.9-14       \n[67] colorspace_2.1-0    nlme_3.1-163        performance_0.10.8 \n[70] cli_3.6.1           DiceDesign_1.9      fansi_1.0.5        \n[73] lava_1.7.2.1        sjstats_0.18.2      gtable_0.3.4       \n[76] GPfit_1.0-8         digest_0.6.33       htmlwidgets_1.6.2  \n[79] htmltools_0.5.6.1   lifecycle_1.0.3     hardhat_1.3.0      \n[82] MASS_7.3-60        \n\n\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nKuhn, Max, and Hadley Wickham. 2020. Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles. https://www.tidymodels.org.\n\n\nLüdecke, Daniel. 2023. sjPlot: Data Visualization for Statistics in Social Science. https://CRAN.R-project.org/package=sjPlot.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2022. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr."
  },
  {
    "objectID": "40_DataBases.html#sec-40.1",
    "href": "40_DataBases.html#sec-40.1",
    "title": "4  Bases de datos",
    "section": "4.1 Introducción",
    "text": "4.1 Introducción\nEn esta unidad se detallan brevemente las bases de datos que se van a usar a lo largo de estos materiales. Se realiza una breve descripción de cada una, se indica el número de muestras y variables contenidas en ellas, así como si existen valores perdidos. En todos los casos se proporciona el código correspondiente para su carga en R.\nPara facilitar la visualización de las bases de datos, estas se han organizados según su objetivo principal: regresión, clasificación, y agrupación. Sin embargo, hay que tener en cuenta que en muchas situaciones prácticas los problemas de clasificación pueden ser vistos también como problemas de regresión, lo que se identificara durante la presentación de los materiales. En principio los problemas de reducción de la dimensión suelen estar integrados dentro de los otras tres situaciones como parte del preprocesado de los datos, y no se considera ninguna base de datos específica para ese problema.\nTodos los bancos de datos están alojados en GitHub (https://github.com/) para que resulten más accesibles. Sin embargo, para que resulte mas fácil la carga de los datos generamos un fichero rds con los datos cargados (que se crea en nuestro directorio de trabajo) que iremos utilizando en el resto de temas.\nAntes de comenzar hay que acordarse de cargar las librerías necesarias para la lectura de datos:\n\nlibrary(tidyverse)\nlibrary(readr)\n\nEn todos los bancos de datos preparamos los datos para que puedan ser interpretados adecuadamente en nuestros modelos de aprendizaje automático."
  },
  {
    "objectID": "40_DataBases.html#sec-40.1.1",
    "href": "40_DataBases.html#sec-40.1.1",
    "title": "4  Bases de datos",
    "section": "4.2 Aprendizaje supervisado: Regresión",
    "text": "4.2 Aprendizaje supervisado: Regresión\nConsultar Sección 4.1 de la Unidad 2 para los detalles de este tipo de problemas.\n\n4.2.1 Anime ratings\nEste conjunto de datos contiene información sobre las preferencias de los usuarios de diferentes animes. Cada usuario puede agregar el anime a su lista de preferencias y darle una calificación.\nCaracterísticas del banco de datos:\n\ntarget: rating\nValores perdidos: sí\nNúmero de registros: 12294\nNúmero de características: 7\n\nVariables contenidas:\n\nanime_id: identificación única de myanimelist.net que identifica un anime.\nname: nombre completo del anime.\ngenre: lista de géneros separada por comas para el anime.\ntype: película, TV, OVA, etc.\nepisodes: cuántos episodios tiene el programa. (1 si es película).\nrating: calificación promedio sobre 10 para el anime.\nmembers: número de miembros de la comunidad que están en el grupo de este anime.\n\nReferencias bibliográficas:\n\nKaggle: enlace\n\nEl código para la lectura de este banco de datos es:\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/anime.csv\"\nanime = read_csv(url, col_types = \"ccccidi\")\nwrite_rds(anime, \"anime.rds\")\n\n\n\n4.2.2 Diabetes\nEn un estudio sobre la diabetes se obtuvieron diez variables basales, edad, sexo, índice de masa corporal, presión arterial media y seis mediciones de suero sanguíneo para 442 pacientes diabéticos, así como la respuesta de interés, una medida cuantitativa de la progresión de la enfermedad un año después de la línea de base. Cada una de estas 10 variables predictoras se ha centrado en la media y se ha escalado por la desviación estándar multiplicada por la raíz cuadrada de n_muestras (es decir, la suma de los cuadrados de cada columna suma 1).\nCaracterísticas del banco de datos:\n\nTarget: Y (progresión de la enfermedad)\nValores perdidos: no\nNúmero de registros: 442\nNúmero de características: 11\n\nVariables contenidas:\n\nAGE: edad (en años)\nSEX: sexo\nBMI: índice de masa corporal\nBP: promedio de la presión sanguínea\nS1: colesterol sérico total\nS2: lipoproteínas de baja densidad\nS3: lipoproteínas de alta densidad\nS4: colesterol total\nS5: registro del nivel de triglicéridos en suero\nS6: nivel de azúcar en sangre\n\nReferencias bibliográficas:\n\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) “Least Angle Regression,” Annals of Statistics (with discussion), 407-499.\n\nEn este caso el fichero de datos es de tipo txt (no csv como el resto) y el código para la carga de este banco de datos es:\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/diabetes.tab.txt\"\ndiabetes = read_delim(url, col_types = \"dfddddddddd\")\n# usamos la función read_delim en lugar de read_csv\nwrite_rds(diabetes, \"diabetes.rds\")\n\n\n\n4.2.3 Electricity\nUna central eléctrica de ciclo combinado (CCPP) está compuesta por turbinas de gas (GT), turbinas de vapor (ST) y generadores de vapor de recuperación de calor. En una CCPP, la electricidad es generada por turbinas de gas y vapor, que se combinan en un ciclo, y se transfiere de una turbina a otra. Mientras que el Vacío se recolecta y tiene efecto sobre la Turbina de Vapor, las otras tres variables ambientales afectan el desempeño del GT. Este conjunto de de datos recogidos se ha recogido de una central eléctrica de ciclo combinado a lo largo de 6 años (2006-2011), cuando la central eléctrica estaba configurada para funcionar a plena carga. Las características consisten en variables ambientales promedio por hora Temperatura (AT), Presión ambiental (AP), Humedad relativa (RH) y Vacío de escape (V) para predecir la producción de energía eléctrica neta por hora (PE) de la planta.\nCaracterísticas del banco de datos:\n\nVariable respuesta: PE\nTipo de variable respuesta: numérica\nTipo de problema que se quiere resolver: regresión\nValores perdidos: no\nNúmero de registros: 9568\nNúmero de variables: 5\n\nVariables contenidas:\nSe recogen la promedios por hora de:\n\nAT: temperatura (en grados centígrados)\nV: vacío de escape (en cm Hg)\nAP: presión ambiental (en milibares)\nRH: humedad relativa (en %)\nPE: producción de energía eléctrica neta (en MW)\n\nReferencias bibliográficas:\n\nPınar Tüfekci, Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods, International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615.\nHeysem Kaya, Pınar Tüfekci , Sadık Fikret Gürgen: Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine, Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai).\nUCi Machine learning repository: https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant\n\nA continuación tenemos el código para la carga de estos datos\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/mult_linear_reg.csv\"\nelectricity = read_csv(url, col_types = \"ddddd\")\nwrite_rds(electricity, \"electricity.rds\")\n\n\n\n4.2.4 Housing in California\nEn este ejemplo vamos a utilizar la base de datos HousingCA, que recoge la información sobre el censo viviendas de California en el año 1990. Se está interesado en predecir el valor medio de la vivienda (median_house_value) medido en dólares americanos en función de las predictoras.\nCaracterísticas del banco de datos:\n\nTarget: median_house_value\nValores perdidos: sí\nNúmero de registros: 20640\nNúmero de variables: 10\n\nVariables contenidas:\n\nmedian_house_value: valor medio de la vivienda en dólares.\nlongitude: medida de la distancia al oeste de una casa; un valor más alto es más al oeste.\nlatitude: medida de la distancia al norte de una casa; un valor más alto es más al norte.\nhousing_median_age: mediana de edad de una vivienda dentro de una manzana; un número más bajo es un edificio más nuevo (en años)\ntotal_rooms: número total de habitaciones dentro de un bloque.\ntotal_bedrooms: número total de dormitorios dentro de un bloque.\npopulation: Número total de personas que residen en un bloque.\nhouseholds: Número total de hogares, grupo de personas que residen en una unidad de vivienda, para un bloque.\nmedian_income: Ingreso medio de los hogares dentro de un bloque de casas (medido en decenas de miles de dólares estadounidenses)\noceanProximity: Ubicación de la casa con respecto al océano/mar (<1H OCEAN, INLAND, ISLAND, NEAR BAY, NEAR OCEAN).\n\nReferencias bibliográficas:\n\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297\nKaggle: enlace\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/housing.csv\"\nhousingCA = read_csv(url, col_types = \"dddddddddf\")\nwrite_rds(housingCA, \"housingCA.rds\")\n\n\n\n4.2.5 Meat spec\nEl departamento de calidad de una empresa de alimentación se encarga de medir el contenido en grasa de la carne que comercializa. Este estudio se realiza mediante técnicas de analítica química, un proceso relativamente costoso en tiempo y recursos. Una alternativa que permitiría reducir costes y optimizar tiempo es emplear un espectrofotómetro (instrumento capaz de detectar la absorbancia que tiene un material a diferentes tipos de luz en función de sus características) e inferir el contenido en grasa a partir de sus medidas. Por lo tanto, el objetivo que se persigue es predecir el contenido en grasa (fat) a partir de los valores dados por el espectrofotómetro.\nCaracterísticas del banco de datos:\n\nTarget: fat\nValores perdidos: no\nNúmero de registros: 215\nNúmero de variables: 101\n\nVariables contenidas:\n\nVariables desde V1 hasta V100: cada una de las variables hace referencia al valor de la muestra para una longitud de onda específica.\nfat: contenido en grasa de la carne (en gramos)\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/meatspec.csv\"\nmeatspec = read_csv(url)\nwrite_rds(meatspec, \"meatspec.rds\")\n\n\n\n4.2.6 Penguins\nPenguins es un banco de datos que contiene las mediciones de tamaño, observaciones de nidadas y proporciones de isótopos en sangre de 344 pingüinos adultos de Adelia, Barbijo y Papúa. Estos han sido observados en islas del archipiélago Palmer cerca de la estación Palmer en la Antártida. Los datos fueron recogidos y puestos a disposición por la Dra. Kristen Gorman y el Programa de Investigación Ecológica a Largo Plazo (LTER) de la Estación Palmer, Antártida.\nCaracterísticas del banco de datos:\n\nTarget: flipper_length_mm\nValores perdidos: si\nNúmero de registros: 344\nNúmero de variables: 9\n\nVariables contenidas:\n\nId: identificador del animal\nspecies: especie de pingüino (Adélie, Chinstrap y Gentoo)\nisland: isla en el Archipiélago Palmer, Antártida (Biscoe, Dream o Torgersen)\nbill_length_mm: longitud del pico (en milímetros)\nbill_depth_mm: profundidad del pico (en milímetros)\nflipper_length_mm: longitud de la aleta (en milímetros)\nbody_mass_g: masa corporal (en gramos)\nsex: sexo del pingüino (hembra, macho)\nyear: año de la toma de información\n\nReferencias bibliográficas:\n\nData were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\nKaggle: https://www.kaggle.com/datasets/larsen0966/penguins\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/penguins.csv\"\npenguins = read_csv(url)\n\n# Convertimos variable carácter a factor\npenguins = penguins |> mutate_if(sapply(penguins, is.character), as.factor) \nwrite_rds(penguins, \"penguins.rds\")\n\n\n\n4.2.7 US economic time series\nEsta base de datos contiene datos de series temporales económicas de EE. UU. El objetivo que se persigue es predecir el desempleo (unemploy) dentro la economía de US. El desempleo es una gran preocupación socioeconómica y política para cualquier país y, por lo tanto, gestionarlo es una tarea primordial para todos los gobiernos.\nCaracterísticas del banco de datos:\n\nTarget: unemploy\nValores perdidos: no\nNúmero de registros: 574\nNúmero de variables: 6\n\nVariables contenidas:\n\ndate: mes de recogida de los datos.\npce: gastos de consumo personal (en miles de millones de dólares)\npop: población total (en miles)\npsavert: tasa de ahorro personal.\nuempmed: duración mediana del desempleo (en semanas)\nunemploy: número de desempleados (en miles)\n\nReferencias bibliográficas:\n\nLas series de datos se pueden consultar en https://fred.stlouisfed.org/. Mas concretamente:\n\npce: https://fred.stlouisfed.org/series/PCE\npop: https://fred.stlouisfed.org/series/POP\npsavert: https://fred.stlouisfed.org/series/PSAVERT/\nuempmed: https://fred.stlouisfed.org/series/UEMPMED\nunemploy: https://fred.stlouisfed.org/series/UNEMPLOY\n\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/US_economic_time_series.csv\"\nusaets = read_csv(url)\n\n# Eliminamos las dos primeras variables para quedarnos con el fichero final\nusaets = usaets[,-(1:2)]\nwrite_rds(usaets, \"usaets.rds\")\n\n\n\n4.2.8 QSAR\nEste conjunto de datos se utilizó para desarrollar modelos QSAR de regresión cuantitativa para predecir la toxicidad acuática aguda hacia el pez Pimephales promelas (pececillo de cabeza plana) sobre un conjunto de 908 sustancias químicas. Como variable a predecir se consideraron los datos de la LC50, que es la concentración que provoca la muerte del 50% de sujetos sometidas a prueba durante 48 horas.\nCaracterísticas del banco de datos:\n\nTarget: LC50\nValores perdidos: no\n\nVariables contenidas:\n\nTPSA (propiedades moleculares),\nSAacc (propiedades moleculares),\nH-050 (fragmentos centrados en átomos),\nMLOGP (propiedades moleculares),\nRDCHI (índices de conectividad),\nGATS1p (autocorrelaciones 2D),\nnN (índices constitucionales),\nC-040 (fragmentos centrados en átomos),\nLC50\n\n\nurl = \"https://raw.githubusercontent.com/jmsocuellamos/DatosBIOTEC/master/CaseStudies/Qsar/qsar_aquatic_toxicity.csv\"\nqsar = read_csv(url)\nwrite_rds(qsar, \"qsar.rds\")"
  },
  {
    "objectID": "40_DataBases.html#sec-40.1.2",
    "href": "40_DataBases.html#sec-40.1.2",
    "title": "4  Bases de datos",
    "section": "4.3 Aprendizaje supervisado: Clasificación",
    "text": "4.3 Aprendizaje supervisado: Clasificación\nConsultar Sección 4.1 de la Unidad 2 para los detalles de este tipo de problemas.\n\n4.3.1 Abalone\nEn este conjunto de datos se recoge información sobre los abulones, de la familia de los moluscos. Se está interesado en medir su desarrollo, que viene determinado principalmente por su desarrollo sexual. Concretamente se consideran tres estados de desarrollo asociados con el atributo Sex: M (machos), F (hembras), e I (infantil o sin desarrollo sexual). Para clasificar cada sujeto se utiliza un conjunto de características que son de tipo numérico.\nCaracterísticas del banco de datos:\n\ntarget: Sex\nValores perdidos: no\nNúmero de registros: 4177\nNúmero de características: 8\n\nVariables contenidas:\n\nSex: sexo (M = masculino, F = femenino e I = intantil)\nLength: longitud de la carcasa más larga (en mm)\nDiameter: diámetro perpendicular a la longitud (en mm)\nHeight: altura con carne en cáscara (en mm)\nWhole weight: peso completo (en gramos)\nShucked weight: peso de la carne (en gramos)\nViscera weight: peso de las vísceras después del sangrado (en gramos)\nShell weight: peso de la cáscara después de secarse (en gramos)\nRings: anillos (+1,5 da la edad en años)\n\nReferencias bibliográficas:\n\nKaggle: enlace\n\nEl código para la lectura de este banco de datos es:\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/abalone.csv\"\nabalone = read_csv(url, col_types = \"cdddddddd\")\n\n# Convertimos variable carácter a factor\nabalone = abalone |> mutate_if(sapply(abalone, is.character), as.factor) \nwrite_rds(abalone, \"abalone.rds\")\n\n\n\n4.3.2 Breast Cancer Wisconsin\nEn esta base de datos se recoge información sobre los cánceres de mama en la ciudad de Wisconsin. Las características de la base de datos se calculan a partir de una imagen digitalizada de un aspiración de aguja fina (FNA) de una masa mamaria. Describen las características de los núcleos celulares presentes en la imagen y el objetivo que se persigue es clasificar un tumor como benigno o maligno en función de las variables predictoras.\nCaracterísticas del banco de datos:\n\nVariable respuesta: diagnosis\nTipo de variable respuesta: categórica (binaria)\nTipo de problema que se quiere resolver: clasificación y regresión\nValores perdidos: no\nNúmero de registros: 569\nNúmero de variables: 32\n\nVariables contenidas:\n\nid: identificador.\ndiagnosis: diagnóstico de tejidos mamarios (M = maligno, B = benigno)\nradius_mean: media de las distancias del centro a los puntos del perímetro.\ntexture_mean: desviación estándar de los valores de la escala de grises.\nperimeter_mean:tamaño medio del tumor central.\narea_mean:\nsmoothness_mean: media de variación local en longitudes de radio\ncompactness_mean: (media de perímetro)^2 / área - 1,0\nconcavity_mean: media de gravedad de las porciones cóncavas del contorno.\nconcave points_mean: media para el número de porciones cóncavas del contorno.\nsymmetry_mean:\nfractal_dimension_mean: media para “aproximación de la costa” - 1.\nradius_se: error estándar para la media de distancias desde el centro hasta los puntos en el perímetro.\ntexture_se: error estándar para la desviación estándar de los valores de escala de grises.\nperimeter_se:\narea_se:\nsmoothness_se: error estándar para la variación local en las longitudes del radio.\ncompactness_se: (error estándar para perímetro)^2 / área - 1,0.\nconcavity_se: error estándar para la gravedad de las partes cóncavas del contorno.\nconcave points_se: error estándar para el número de porciones cóncavas del contorno.\nsymmetry_se:\nfractal_dimension_se: error estándar para “aproximación de la costa” - 1.\nradius_worst: “peor” o mayor valor medio para la media de distancias desde el centro hasta los puntos del perímetro (en cm).\ntexture_worst: “peor” o mayor valor medio para la desviación estándar de los valores de escala de grises.\nperimeter_worst:\narea_worst:\nsmoothness_worst: “peor” o mayor valor medio para la variación local en longitudes de radio.\ncompactness_worst: “peor” o mayor valor medio para el perímetro^2 / área - 1,0.\nconcavity_worst: “peor” o mayor valor medio para la gravedad de las porciones cóncavas del contorno.\nconcave points_worst: “peor” o mayor valor medio para el número de porciones cóncavas del contorno.\nsymmetry_worst:\nfractal_dimension_worst: “peor” o mayor valor medio para “aproximación de la costa” - 1.\n\nReferencias bibliográficas:\n\nKaggle: enlace\n\nEl código para la lectura de este banco de datos es:\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/cancer.csv\"\nbreastcancer = read_csv(url, col_types = \"ccdddddddddddddddddddddddddddddd\")\n\n# Cambíamos solo el tipo de la variable diagnosis\nbreastcancer$diagnosis = as.factor(breastcancer$diagnosis)\nwrite_rds(breastcancer, \"breastcancer.rds\")\n\n\n\n4.3.3 Iris\nLa base de datos iris consta de 50 muestras de cada una de las tres especies de Iris: Iris Setosa, Iris virginica e Iris versicolor. Se escogieron cuatro características de cada muestra: la longitud y el ancho de los sépalos y pétalos, en centímetros. El objetivo que se persigue es predecir la clase de especie (species) que es en función de sus características.\nCaracterísticas del banco de datos:\n\nTarget: species\nValores perdidos: no\nNúmero de registros: 150\nNúmero de variables: 5\n\nVariables contenidas:\n\nsepal_length: longitud del sépalo (en cm)\nsepal_width: anchura del sépalo (en cm)\npetal_length: longitud del pétalo (en cm)\npetal_width: anchura del pétalo (en cm)\nspecies: especie de la flor (Iris Setosa, Iris virginica e Iris versicolor)\n\nReferencias bibliográficas:\n\nKaggle: enlace\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/IRIS.csv\"\niris = read_csv(url, col_types = \"ddddc\")\n\n# Convertimos carácter en factor\niris$species = as.factor(iris$species)\nwrite_rds(iris, \"iris.rds\")\n\n\n\n4.3.4 Mushroom\nLa “caza de setas” (también conocida como “shrooming” en inglés) está disfrutando de nuevos picos de popularidad. Conocer qué características significan una muerte segura y cuáles son más apetecibles es un aspecto muy importante en este proceso. El conjunto de datos Mushrooms muestra las características de una muestra muy extensa de diferentes tipos de setas. En concreto se incluyen descripciones de 23 especies de setas con agallas de la familia Agaricus y Lepiota, extraídas de The Audubon Society Field Guide to North American Mushrooms (1981). Cada especie se identifica como: “definitivamente comestible”, “definitivamente venenosa”, o “de comestibilidad desconocida y no recomendada”. Esta última clase se combinó con la venenosa.\nCaracterísticas del banco de datos:\n\nTarget: class\nValores perdidos: no\nNúmero de registros: 8124\nNúmero de variables: 23\n\nVariables contenidas:\n\nclass: clase de seta (“e” = comestible, “p” = venenosa)\ncap-shape: forma del sombrero (“b” = campana, “c” = cónica, “x” = convexa, “f”= plana, “k” = nudosa, “s” = hundida)\ncap-surface: superficie del sombrero (“f” = fibrosa, “g” = surcos, “y” = escamosa, “s” = lisa)\ncap-color: color del sombrero (“n” = marrón, “b” = beige, “c” = canela, “g” = gris, “r” = verde, “p” = rosa, “u” = morado, “r” = rojo, “w” = blanco y “y” = amarillo)\nbruises: cicatrices (“t” = con rasguños y “f” = sin rasguños)\nodor: olor (“a” = almendra, “l” = anís, “c” = creosota, “y” = pescado, “f” = asqueroso, “m” = mohoso, “n” = ninguno, “p” = acre= y “s” = especiado)\ngill-attachment: accesorio branquial (“a” = adjunto, “d” = descendente, “f” = libre y “m” = con muescas)\ngill-spacing: espacio entre branquias(“c” = cerca, “w” = abarrotado y “d” = distante)\ngill-size: tamaño branquial (“b” = ancho y “n” = estrecho)\ngill-color: color de las branquias (“k” = negro, “n” = marrón, “b” = beige, “h” = chocolate, “g” = gris, “r” = verde, “o” = naranja, “p” = rosa, “u” = morado, “r” = rojo, “w” = blanco y “y” = amarillo)\nstalk-shape: forma de tallo (“e” = agrandando y “t” = disminuyendo)\nstalk-root: raíz del tallo (“b” = bulboso, “c” = club,“u” = copa, “e” = igual, “z” = rizomorfos y “r” = enraizado)\nstalk-surface-above-ring: superficie del tallo sobre el anillo (“f” = fibrosa, “y” = escamosa, “k” = sedosa y “s” = lisa)\nstalk-surface-below-ring: superficie del tallo debajo del anillo (“f” = fibrosa, “y” = escamosa, “k” = sedosa y “s” = lisa)\nstalk-color-above-ring: color del tallo sobre el anillo (“n” = marrón, “b” = beige, “c” = canela, “g” = gris, “o” = naranja, “p” = rosa, “r” = rojo, “w” = blanco y “y” = amarillo)\nstalk-color-below-ring: color del tallo debajo del anillo (“n” = marrón, “b” = beige, “c” = canela, “g” = gris, “o” = naranja, “p” = rosa, “r” = rojo, “w” = blanco y “y” = amarillo)\nveil-type: tipo de velo (“p” = parcial y “u” = universal)\nveil-color: color del velo (“n” = marrón, “o” = naranja, “w” = blanco y “y” = amarillo)\nring-number: número de anillos (“n” = ninguno, “o” = uno y “t” = dos)\nring-type: tipo de anillo (“c” = telaraña, “e” = evanescente, “f” = resplandeciente, “l” = grande, “n” = ninguno, “p” = colgante, “s” = revestimiento y “z” = zona)\nspore-print-color: color de impresión de esporas (“k” = negro, “n” = marrón, “b” = beige, “h” = chocolate, “r” = verde, “o” = naranja, “u” = morado, “w” = blanco y “y” = amarillo)\npopulation: población (“a” = abundante, “c” = agrupada, “n” = numerosa, “s” = dispersa, “v” = varia y “y” = solitaria)\nhabitat: hábitat (“g” = pastos, “l” = hojas, “m” = prados, “p” = caminos, “u” = urbano, “w” = desechos y “d” = bosques)\n\nReferencias bibliográficas:\n\nMushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf.\nUCI Machine learning repository: https://archive.ics.uci.edu/ml/datasets/mushroom\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/mushrooms.csv\"\nmushroom = read_csv(url)\n\n# Convertimos variables carácter a factor\nmushroom = mushroom |> mutate_if(sapply(mushroom, is.character), as.factor) \nwrite_rds(mushroom, \"mushroom.rds\")\n\n\n\n4.3.5 Spam\nLa base de datos trata sobre la detección de spam en una bandeja de entrada de correo electrónico. El banco de datos dispone de dos variables donde la primera es el texto del mail, y la segunda el indicador que nos dice si el correo se ha clasificado como spam o no spam de forma manual.\nCaracterísticas del banco de datos:\n\nTarget: label\nValores perdidos: sí (variable email)\nNúmero de registros: 3000\nNúmero de variables: 2\n\nVariables contenidas:\n\nlabel: etiqueta que indica si es spam o no (0 = no spam, 1 = spam)\nemail: contenido del correo electrónico.\n\nReferencias bibliográficas:\n\nKaggle: enlace\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/spam_or_not_spam.csv\"\nspam = read_csv(url)\nwrite_rds(spam, \"spam.rds\")\n\n\n\n4.3.6 Stroke\nSegún la Organización Mundial de la Salud (OMS), el ictus es la segunda causa de muerte en el mundo, responsable de aproximadamente el 11% del total de fallecimientos. El banco de datos Stroke se utiliza para predecir si es probable que un paciente sufra un ictus en función de los parámetros de entrada como el sexo, la edad, diversas enfermedades y estatus de fumador. Cada fila de los datos proporciona información relevante sobre el paciente.\nCaracterísticas del banco de datos:\n\nTarget: stroke\nValores perdidos: sí (en la variable bmi)\nNúmero de registros: 5110\nNúmero de variables: 12\n\nVariables contenidas:\n\nid: identificador único del sujeto\ngender: sexo (“Male” = masculino, “Female” = femenino y “Other” = otro)\nage: edad del paciente (en años)\nhypertension: el paciente tiene hipertensión (“Yes” = sí y “No”)\nheart_disease: el paciente tiene una enfermedad del corazón (“Yes” = sí y “No”)\never_married: el paciente se ha casado alguna vez (“Yes” = sí y “No”)\nwork_type: tipo de trabajo (“children” = niños, “Govt_jov” = Gubernamental, “Never_worked” = nunca ha trabajado, “Private” = privado y “Self-employed” = autónomo)\nResidence_type: tipo de residencia (“Rural” y “Urban” = urbana)\navg_glucose_level: media de glucosa en sangre.\nbmi: índice de masa corporal.\nsmoking_status: nivel de fumador (“formerly smoked” = fumaba, “never smoked” = nunca ha fumado, “smokes” = fuma y “Unknown” = desconocido).\nstroke: el paciente sufre un ictus (“Yes” = sí y “No”)\n\nReferencias bibliográficas:\n\nkaggle: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/stroke_ori.csv\"\nstroke = read_csv(url)\n\n# Convertimos variables carácter a factor\nstroke = stroke |> mutate_if(sapply(stroke, is.character), as.factor) \nwrite_rds(stroke, \"stroke.rds\")\n\n\n\n4.3.7 Water potability\nEl agua potable es el derecho humano más básico y un factor importante para la salud. El conjunto de datos Water potability, tiene por objetivo estudiar la potabilidad del agua utilizando varias propiedades químicas debido a su importancia como cuestión de salud y desarrollo a nivel nacional, regional y local. En algunas regiones, se ha demostrado que las inversiones en abastecimiento de agua y saneamiento pueden producir un beneficio económico neto, ya que la reducción de los efectos adversos para la salud y los costes de la atención sanitaria superan los costes de las intervenciones.\nCaracterísticas del banco de datos:\n\nTarget: potability\nValores perdidos: sí (variables ph, Sulfate y Trihalomethanes)\nNúmero de registros: 3276\nNúmero de variables: 10\n\nVariables contenidas:\n\npH: valor del pH.\nHardness: dureza o capacidad del agua para precipitar el jabón causado por el calcio y el magnesio.\nSolids: sólidos disueltos totales (en partes por millón)\nChloramines: cantidad de cloraminas (en partes por millón)\nSulfate: cantidad de sulfatos disueltos (en mg/L)\nConductivity: conductividad eléctrica del agua (en μS/cm)\nOrganic_carbon: cantidad de carbono orgánico (en partes por millón)\nTrihalomethanes: cantidad de trihalometanos (en μg/L)\nTurbidity: medida de la propiedad de emisión de luz del agua en NTU.\nPotability: indica si el agua es segura para el consumo humano (1 = potable y 0 = no potable)\n\nReferencias bibliográficas:\n\nKaggle: https://www.kaggle.com/datasets/adityakadiwal/water-potability\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/water_potability.csv\"\nwaterpot = read_csv(url)\nwrite_rds(waterpot, \"waterpot.rds\")\n\n\n\n4.3.8 Wine quality\nEl conjunto de datos Winequality está relacionado con las variantes rojas del vino portugués “Vinho Verde”. Debido a cuestiones de privacidad y logística, sólo se dispone de variables fisicoquímicas (entradas) y sensoriales (la salida). Por ejemplo, no hay datos sobre tipos de uva, marca de vino, precio de venta del vino, etc. Las variables provienen de tests psicoquímicos y el objetivo es, mediante las predictoras, establecer la calidad del vino (quality), medida en la escala discreta de 0 a 10.\nCaracterísticas del banco de datos:\n\nTarget: quality\nValores perdidos: no\nNúmero de registros: 1599\nNúmero de variables: 12\n\nVariables contenidas:\n\nfixed acidity: acidez fija.\nvolatile acidity: acidez volátil.\ncitric acid: acidez cítrica.\nresidual sugar: azúcar residual (cantidad de azúcar que queda después de que se detiene la fermentación)\nchlorides: cloruros (cantidad de sal en el vino)\nfree sulfur dioxide: dióxido de azufre libre.\ntotal sulfur dioxide: dióxido de azufre total.\ndensity: densidad.\npH: ph.\nsulphates: sulfatos.\nalcohol: alcohol (porcentaje de contenido de alcohol del vino)\nquality: calidad (entre 0 y 10)\n\nReferencias bibliográficas:\n\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\nKaggle: https://www.kaggle.com/datasets/sh6147782/winequalityred\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/winequality-red.csv\"\nwinequality = read_csv(url)\nwrite_rds(winequality, \"winequality.rds\")\n\n\n\n4.3.9 Hepatitis\nDatos de síntomas de hepatitis y resultados en varias personas de ambos sexos de varios grupos de edad. Los síntomas de la hepatitis incluyen fatiga, anorexia, hígado grande, etc. Este archivo informa sobre los diversos síntomas en caso de hepatitis y si la persona que padece hepatitis vivió o murió.\nCaracterísticas del banco de datos:\n\nTarget: Class\nValores perdidos: si\nNúmero de registros: 142\nNúmero de variables: 20\n\nVariables contenidas:\n\nClass: DIE, LIVE\nAGE: 10, 20, 30, 40, 50, 60, 70, 80\nSEX: male, female\nSTEROID: no, yes\nANTIVIRALS: no, yes\nFATIGUE: no, yes\nMALAISE: no, yes\nANOREXIA: no, yes\nLIVER BIG: no, yes\nLIVER FIRM: no, yes\nSPLEEN PALPABLE: no, yes\nSPIDERS: no, yes\nASCITES: no, yes\nVARICES: no, yes\nBILIRUBIN: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00\nALK PHOSPHATE: 33, 80, 120, 160, 200, 250\nSGOT: 13, 100, 200, 300, 400, 500,\nALBUMIN: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0\nPROTIME: 10, 20, 30, 40, 50, 60, 70, 80, 90\nHISTOLOGY: no, yes\n\nReferencias bibliográficas:\n\nKaggle: https://www.kaggle.com/datasets/harinir/hepatitis\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/hepatitis.csv\"\nhepatitis = read_csv(url)\nwrite_rds(hepatitis, \"hepatitis.rds\")\n\n\n\n4.3.10 Gene expression breast cancer\nEste archivo contiene los niveles de expresión génica de 54676 genes (columnas) de 151 muestras (filas). Hay 5 tipos diferentes de cáncer de mama (más tejido sano) representados en este conjunto de datos (columna “type”). Más información sobre este conjunto de datos, así como otros formatos de archivo como TAB y ARFF, visualización de datos y puntos de referencia de clasificación y agrupación están disponibles gratuitamente en el sitio web oficial de CuMiDa con el id GSE45827: http://sbcb.inf.ufrgs.br/cumida\nCaracterísticas del banco de datos:\n\nTarget: type\nValores perdidos: no\nNúmero de registros: 151\nNúmero de variables: 54676\n\nReferencias bibliográficas:\n\nKaggle: https://www.kaggle.com/datasets/brunogrisci/breast-cancer-gene-expression-cumida\n\nEn este caso el banco de datos es muy grande y no es posible importarlo desde github. Es necesario descargarlo a nuestro máquina y cargarlo desde allí directamente. Para ello se ha generado un fichero en formato rds que se puede cargar fácilmente.\n\ngeneexp = read_rds(\"geneexpression.rds\")\n\n\n\n4.3.11 Gene expression leukemia\nEste archivo que contiene los niveles de expresión génica de 22284 genes (columnas) de 64 muestras (filas). Hay 5 tipos diferentes de leucemia representados en este conjunto de datos (columna “type”). Más información sobre este conjunto de datos, así como otros formatos de archivo como TAB y ARFF, visualización de datos y puntos de referencia de clasificación y agrupación están disponibles gratuitamente en el sitio web oficial de CuMiDa con el id GSE9476: http://sbcb.inf.ufrgs.br /cumida\nCaracterísticas del banco de datos:\n\nTarget: type\nValores perdidos: no\nNúmero de registros: 64\nNúmero de variables: 22284\n\nReferencias bibliográficas:\n\nKaggle: https://www.kaggle.com/datasets/brunogrisci/leukemia-gene-expression-cumida\n\nComo antes el fichero es muy grande y procedemos de igual forma al ejemplo anterior.\n\ngeneexpleu = read_rds(\"geneexpressionleukemia.rds\")"
  },
  {
    "objectID": "40_DataBases.html#sec-40.1.3",
    "href": "40_DataBases.html#sec-40.1.3",
    "title": "4  Bases de datos",
    "section": "4.4 Aprendizaje no supervisado: Agrupación",
    "text": "4.4 Aprendizaje no supervisado: Agrupación\nConsultar Sección 4.2 de la Unidad 2 para los detalles de este tipo de problemas.\n\n4.4.1 Sales\nContiene las cantidades compradas semanalmente de 800 productos a lo largo de 52 semanas. Todos los atributos son numéricos sin valores perdidos y van identificados mediante W(número de la semana).\nCaracterísticas del banco de datos:\n\nNúmero de registros: 527\nNúmero de variables: 38\n\nVariables contenidas:\n\nVariables desde W0 hasta W51: cada una de las variables hace referencia a las cantidades compradas semanalmente.\n\nReferencias bibliográficas:\n\nTan, Swee Chuan and San Lau, Jess Pei (2014). Time series clustering: A superior alternative for market basket analysis. Proceedings of the First International Conference on Advanced Data and Information Engineering (DaEng-2013), 241–248. Springer.\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/Sales_Transactions_Weekly.csv\"\nsales = read_csv(url)\nwrite_rds(sales, \"sales.rds\")\n\n\n\n4.4.2 Vehicle silhouettes\nEste conjunto de datos recoge información de cuatro tipos diferentes de vehículos, utilizando un conjunto de características extraídas de su silueta. Para el experimento se utilizaron cuatro vehículos modelo “Corgie”: un bus de dos pisos, una camioneta Cheverolet, un Saab 9000 y un Opel Manta 400. El objetivo del estudio es clasificar una silueta dada como uno de cuatro tipos diferentes de vehículos. Todos los atributos son numéricos discretos salvo la primera variable que lleva el registro de la observación y muchos de ellos contienen valores perdidos.\nCaracterísticas del banco de datos:\n\nValores perdidos: si\nNúmero de registros: 946\nNúmero de variables: 19\n\nVariables contenidas:\n\nComp: compactness, se calcula mediante la fórmula \\((perímetro.medio)^2/área\\)\nCirc: circularity, se calcula mediante la fórmula \\((radio. medio)^2/área\\)\nD.Circ: distance_circularity, se calcula mediante la fórmula \\(área/(distancia. media. desde. el. borde)^2\\)\nRad.Ra: radius_ratio, se calcula mediante la fórmula \\((max.rad-min.rad)/av.radius\\)\nPr.axis.Ra: se calcula mediante la fórmula \\((eje. menor)/(eje. mayor)\\)\nMax.L.Ra: max.length_aspect_ratio, se calcula mediante la fórmula \\((longitud. perp. longitud. máxima)/(longitud. máxima)\\)\nScat.Ra: scatter_ratio, se calcula mediante la fórmula $ (inercia. sobre. el. eje. menor)/(inercia. sobre. el. eje. mayor)$\nElong: elongatedness, se calcula mediante la fórmula \\(área/(ancho. de. contracción)^2\\)\nPr.Axis.Rect: pr.axis_rectangularity, se calcula mediante la fórmula \\(área/(longitud. del. eje. pr. * ancho. del. eje. pr.)\\)\nMax.L.Rect: max.length_rectangularity, se calcula mediante la fórmula \\(área/(long. max. * long. perp.)\\)\nSc.var.Maxis: se calcula mediante la fórmula \\((momento. de. segundo. orden. sobre. el. eje. menor)/área. a. lo. largo. del. eje. mayor\\)\nSc.var.maxis: se calcula mediante la fórmula \\((momento. de. segundo. orden. sobre. el. eje. mayor)/área. a. lo. largo. del. eje. menor\\)\nRa.Gyr: scaled_radius_of_gyration, se calcula mediante la fórmula \\((mavar+mivar)/área\\)\nSkew.Maxis: se calcula mediante la fórmula \\((mavar+mivar)/área\\)\nSkew.maxis: se calcula mediante la fórmula \\((momento. de. tercer. orden. sobre. el. eje. principal.)/sigma-min^3 * eje. principal\\)\nKurt.maxis: se calcula mediante la fórmula (momento. de. tercer. orden. sobre. el. eje. menor.)/sigma-maj^3 * eje. menor$\nKurt.Maxis: se calcula mediante la fórmula (momento. de. tercer. orden. sobre. el. eje. menor.)/sigma-maj^3 * eje. menor$\nHoll.Ra: se calcula mediante la fórmula \\((área. de. huecos)/(área. del. polígono. delimitador)\\)\nClass: tipo de vehículo (van, saab, bus y opel)\n\nReferencias bibliográficas:\n\nTuring Institute Research Memorandum TIRM-87-018 “Vehicle Recognition Using Rule Based Methods” by Siebert,JP (March 1987).\nNewman, D.J. & Hettich, S. & Blake, C.L. & Merz, C.J. (1998). UCI Repository of machine learning databases [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University of California, Department of Information and Computer Science.\n\nEn este banco de datos se encuentra disponible dentro de la librería mlbench (puede ser necesario instalarla), y para acceder al banco de datos basta con usar el código siguiente:\n\nlibrary(mlbench)\ndata(\"Vehicle\")\nwrite_rds(Vehicle, \"vehicle.rds\")\n\n\n\n4.4.3 Water treatment\nEste conjunto de datos procede de las medidas diarias de los sensores de una planta de tratamiento de aguas residuales urbanas. El objetivo es clasificar el estado operativo de la planta para predecir fallos a través de las variables de estado de la planta en cada una de las etapas del proceso de tratamiento. Todos los atributos son numéricos y continuos salvo la primera variable que lleva el registro de la observación y muchos de ellos contienen valores perdidos.\nCaracterísticas del banco de datos:\n\nValores perdidos: sí (en casi todas las variables)\nNúmero de registros: 527\nNúmero de variables: 38\n\nVariables contenidas:\n\nQ-E: caudal de entrada a la planta\nZN-E: zinc de entrada a la planta\nPH-E: pH de entrada a la planta\nDBO-E: demanda biológica de oxígeno de entrada a la planta\nDQO-E: demanda química de oxígeno de entrada a la planta\nSS-E: sólidos en suspensión de entrada a la planta\nSSV-E: entrada de sólidos volátiles en suspensión a la planta\nSED-E: entrada de sedimentos a la planta\nCOND-E: entrada de conductividad a la planta\nPH-P: entrada de pH al decantador primario\nDBO-P: entrada de demanda biológica de oxígeno al decantador primario\nSS- P: entrada de sólidos en suspensión al decantador primario\nSSV-P: entrada de sólidos volátiles en suspensión al decantador primario\nSED-P: entrada de sedimentos al decantador primario\nCOND-P: entrada de conductividad al decantador primario\nPH-D: entrada de pH al decantador secundario\nDBO-D: entrada de demanda biológica de oxígeno al decantador secundario\nDQO-D: entrada de demanda química de oxígeno al decantador secundario\nSS-D: entrada de sólidos en suspensión al decantador secundario\nSSV-D: entrada de sólidos volátiles en suspensión al decantador secundario\nSED-D: entrada de sedimentos al decantador secundario\nCOND-D: entrada de conductividad al decantador secundario\nPH-S: salida de pH\nDBO-S: salida de demanda biológica de oxígeno\nDQO-S: salida de demanda química de oxígeno\nSS-S: salida de sólidos en suspensión\nSSV-S: salida de sólidos volátiles en suspensión)\nSED-S: salida de sedimentos\nCOND-S: salida de conductividad\nRD-DBO-P: entrada de rendimiento Demanda biológica de oxígeno en el decantador primario\nRD-SS-P: entrada de rendimiento de sólidos en suspensión en el decantador primario\nRD- SED-P: rendimiento de entrada de sedimentos al decantador primario\nRD-DBO-S: rendimiento de entrada de demanda biológica de oxígeno al decantador secundario\nRD-DQO-S: rendimiento de entrada de demanda química de oxígeno al decantador secundario\nRD-DBO-G: rendimiento global de entrada de demanda biológica de oxígeno\nRD-DQO-G: rendimiento global de entrada de demanda química de oxígeno\nRD-SS-G: rendimiento global de entrada de sólidos en suspensión\nRD-SED-G: rendimiento global de entrada de sedimentos\n\nReferencias bibliográficas:\n\nJ. De Gracia. “Avaluacio de tecniques de classificacio per a la gestio de Bioprocessos: Aplicacio a un reactor de fangs activats’’ Master Thesis. Dept. de Quimica. Unitat d’Enginyeria Quimica. Universitat Autonoma de Barcelona. Bellaterra (Barcelona). 1993.\nJ. Bejar, U. Cortés and M. Poch. “LINNEO+: A Classification Methodology for Ill-structured Domains’’. Research report RT-93-10-R. Dept. Llenguatges i Sistemes Informatics. Barcelona. 1993.\nLl. Belanche, U. Cortes and M. Sánchez. “A knowledge-based system for the diagnosis of waste-water treatment plant’’. Proceedings of the 5th international conference of industrial and engineering applications of AI and Expert Systems IEA/AIE-92. Ed Springer-Verlag. Paderborn, Germany, June 92.\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/water-treatment.csv\"\nwatertre = read_csv(url)\n\n# Eliminamos la variable de registro\nwatertre = watertre[,-1]\nwrite_rds(watertre, \"watertre.rds\")\n\n\n\n4.4.4 Wine recognition\nEn este banco de datos se recoge el resultado de un análisis químico de vinos cultivados en la misma región de Italia pero procedentes de tres cultivos distintos. El análisis determinó las cantidades de 13 características que se encuentran en cada una de las muestras de vinos. El objetivo que perseguimos es clasificar cada muestra en una de estas tres clases de vino (Class label) en función de sus características de tipo numérico.\nCaracterísticas del banco de datos:\n\nTarget: Class label\nValores perdidos: no\nNúmero de registros: 178\nNúmero de variables: 14\n\nVariables contenidas:\n\nClass label: etiqueta que indica el tipo de vino (1, 2 o 3)\nAlcohol: alcohol\nMalic acid: ácido málico\nAsh: ceniza\nAlcalinity of ash: alcalinidad de la ceniza\nMagnesium: magnesio\nTotal phenols: fenoles totales\nFlavanoids: flavonoides\nNonflavanoid phenols: fenoles no flavonoides\nProanthocyanins: proantocianinas\nColor intensity: intensidad de color\nHue: matiz\nOD280/OD315 of diluted wines: OD280/OD315 de vinos diluidos\nProline: prolina\n\nReferencias bibliográficas:\n\nKaggle: enlace\n\n\nurl = \"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/data/Wine.csv\"\nwinerecognition = read_csv(url)\nwrite_rds(winerecognition, \"winerecognition.rds\")"
  },
  {
    "objectID": "02_FirstStepsAA.html",
    "href": "02_FirstStepsAA.html",
    "title": "Parte 2. Primeros pasos",
    "section": "",
    "text": "Antes de comenzar con la presentación de los diferentes modelos de aprendizaje automático que veremos a lo largo de estos materiales, y de acuerdo al flujo de trabajo establecido en la sección Unidad 6 de la unidad Unidad 2, resulta necesario dedicar algo de tiempo al análisis exploratorio de datos, al establecimiento de un modelo de partida, y al preprocesamiento de los datos para proceder con el entrenamiento del modelo de aprendizaje automático que finalmente utilicemos.\nEn la unidad siguiente nos centramos en el análisis exploratorio de datos introduciendo algunas nuevas librerías que resultan de utilidad para este trabajo.\nEn la última unidad de este bloque presentamos en detalle la librería mlr3 (Lang et al. 2019), así como todas las ampliaciones disponibles que nos permiten ampliar las posibilidades de construcción de modelos de aprendizaje automático. En este caso usaremos nuestros algoritmos sin prestar mucho detalle a los aspectos técnicos que analizaremos en las unidades siguientes.\n\n\n\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903."
  },
  {
    "objectID": "50_AED.html#sec-50.1",
    "href": "50_AED.html#sec-50.1",
    "title": "5  Introducción al análisis de datos",
    "section": "5.1 Librerías para el AED",
    "text": "5.1 Librerías para el AED\nLos tres paquetes que vamos a utilizar son:\n\nskimr (Waring et al. 2022), que está diseñado para proporcionar estadísticas resumidas sobre variables en marcos de datos, tibbles, tablas de datos y vectores. Su valores predeterminados proporcionan buenos resultados, pero resultan fácil de modificar si es necesario.\nDataExplorer(Cui 2020), que tiene como objetivo automatizar la mayor parte del manejo y la visualización de datos, para que los usuarios puedan concentrarse en estudiar los datos y extraer información.\nGGally(Schloerke et al. 2021), que extiende ggplot2` agregando varias funciones para reducir la complejidad de combinar geoms con datos transformados. Algunas de estas funciones incluyen una matriz de gráficos por pares, una matriz de gráficos de dispersión, un gráfico de coordenadas paralelas, un gráfico de supervivencia y varias funciones para trazar redes.\n\nAntes de comenzar vamos a instalar los paquetes:\n\ninstall.packages(c(\"skimr\",\"DataExplorer\",\"GGally\"))\n\nCargamos ahora los paquetes y vemos la versión que estamos utilizando:\n\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\n\npackageVersion(\"skimr\")\n\n[1] '2.1.5'\n\npackageVersion(\"DataExplorer\")\n\n[1] '0.8.2'\n\npackageVersion(\"GGally\")\n\n[1] '2.1.2'\n\n\nCargamos también los paquetes básicos.\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\ntheme_set(theme_sjplot2())\n\nYa estamos en disposición de poder usar estos paquetes para el análisis exploratorio de datos. Para ejemplificar su uso utilizaremos los bancos de datos Penguin e Iris que introdujimos en la Unidad 4. Más concretamente en los apartados 4.2.6 y 4.3.3.\nAntes de mostrar el uso de los paquetes anteriores vamos a cargar los datos que utilizaremos:\n\npenguins = read_rds(\"penguins.rds\")\n\nVamos a desechar las variables Id y year (por el momento) y generamos un nuevo marco de datos para no modificar el archivo original.\n\n# indicamos las variables que debemos eliminar\nborrar = c(\"Id\",\"year\")\n\n# generamos el nuevo banco de datos eliminando esas variables\ndata_penguins = penguins |> select(-borrar)\n# vemos los primeros casos\nkable(head(data_penguins))\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n\n\n\n\n\nCargamos ahora el banco de datos iris:\n\niris = read_rds(\"iris.rds\")\n\nGeneramos el nuevo banco de datos:\n\n# generamos el nuevo banco de datos\ndata_iris = iris\n# vemos los primeros casos\nkable(head(data_iris))\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n5.4\n3.9\n1.7\n0.4\nIris-setosa\n\n\n\n\n\n\n5.1.1 Análisis exploratorio con skimr\nLa función central de skimr es skim(), que está diseñada para trabajar con data frames (agrupados). Al igual que summary(), que es la función por defecto para resúmenes numéricos que tenemos a nuestra disposición en R, la función skim() presenta un informe de resultados para cada columna del data frame; cuyas estadísticas dependen de la clase de cada una de las variables. Los resultados se estructuran según el tipo de variable.\n\n5.1.1.1 Datos iris\nComenzamos con los datos iris, donde la variable de interés para nuestro modelo de aprendizaje automático es `Species`, con un análisis completo de todas las variables:\n\nskim(data_iris)\n\n\nData summary\n\n\nName\ndata_iris\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1\nFALSE\n3\nIri: 50, Iri: 50, Iri: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nsepal_width\n0\n1\n3.05\n0.43\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▅▇▂▁\n\n\npetal_length\n0\n1\n3.76\n1.76\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\npetal_width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\n\n\n\n\nLos resultados se estructuran en tres tablas:\n\nEn la primera tenemos información sobre el nombre y dimensiones del data frame, así como el número de variables numéricas y factores.\nEn la segunda tabla tenemos la información sobre las variables de tipo factor:\n\nskim_variable: nombre de la variable.\nn_missing: número de valores perdidos.\ncomplete_rate: ratio de valores completos (1 = sin valores perdidos).\nordered: valor boleano para indicar si los niveles del factor están ordenados.\nn_unique: valores únicos de la variable.\ntop_counts: frecuencia de cada uno de los valores de la variable.\n\nEn la última tabla tenemos la información sobre las variables de tipo numéricas:\n\nskim_variable: nombre de la variable.\nn_missing: número de valores perdidos.\ncomplete_rate: ratio de valores completos (1 = sin valores perdidos).\nmean: media de la variable.\nsd: desviación típica de la variable.\np0: mínimo de la variable.\np25: percentil 25 de la variable.\np50: percentil 50 (mediana) de la variable.\np75: percentil 75 de la variable.\np100: máximo de la variable.\nhist: histograma de valores de la variable.\n\n\nLos resultado obtenidos nos permiten tener una primera visión del comportamiento de cada una de las variables que componen el banco de datos. Sin embargo, dado que nuestro objetivo es conocer el poder de clasificación de cada una de las variables predictoras para establecer la clasificación de especies, parece lógico que orientemos nuestro análisis descriptivo preliminar en esta línea. Para ello podemos utilizar una modificación de la función anterior donde establecemos el análisis para cada predictora en función de la variable species.\n\ndata_iris |>\n  dplyr::group_by(species) |>\n  skim()\n\n\nData summary\n\n\nName\ndplyr::group_by(data_iris…\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsepal_length\nIris-setosa\n0\n1\n5.01\n0.35\n4.3\n4.80\n5.00\n5.20\n5.8\n▃▃▇▅▁\n\n\nsepal_length\nIris-versicolor\n0\n1\n5.94\n0.52\n4.9\n5.60\n5.90\n6.30\n7.0\n▂▇▆▃▃\n\n\nsepal_length\nIris-virginica\n0\n1\n6.59\n0.64\n4.9\n6.23\n6.50\n6.90\n7.9\n▁▃▇▃▂\n\n\nsepal_width\nIris-setosa\n0\n1\n3.42\n0.38\n2.3\n3.12\n3.40\n3.68\n4.4\n▁▅▇▃▂\n\n\nsepal_width\nIris-versicolor\n0\n1\n2.77\n0.31\n2.0\n2.52\n2.80\n3.00\n3.4\n▁▅▆▇▂\n\n\nsepal_width\nIris-virginica\n0\n1\n2.97\n0.32\n2.2\n2.80\n3.00\n3.18\n3.8\n▂▆▇▅▁\n\n\npetal_length\nIris-setosa\n0\n1\n1.46\n0.17\n1.0\n1.40\n1.50\n1.58\n1.9\n▁▃▇▃▁\n\n\npetal_length\nIris-versicolor\n0\n1\n4.26\n0.47\n3.0\n4.00\n4.35\n4.60\n5.1\n▂▂▇▇▆\n\n\npetal_length\nIris-virginica\n0\n1\n5.55\n0.55\n4.5\n5.10\n5.55\n5.88\n6.9\n▃▇▇▃▂\n\n\npetal_width\nIris-setosa\n0\n1\n0.24\n0.11\n0.1\n0.20\n0.20\n0.30\n0.6\n▇▂▂▁▁\n\n\npetal_width\nIris-versicolor\n0\n1\n1.33\n0.20\n1.0\n1.20\n1.30\n1.50\n1.8\n▅▇▃▆▁\n\n\npetal_width\nIris-virginica\n0\n1\n2.03\n0.27\n1.4\n1.80\n2.00\n2.30\n2.5\n▂▇▆▅▇\n\n\n\n\n\nAhora podemos comparar numéricamente el efecto de cada predictora en función de las diferentes especies de flor. Podemos ver que:\n\nPara sepal_length la especie con mayor valor en media es virginica con bastante diferencia de setosa, que es la que tiene el valor más bajo. Algo similar ocurre con las variabilidades, en términos de desviaciones típicas.\nPara sepal_width todas las especies son muy parecidas (medias y desviaciones típicas similares).\nTanto para petal_length y petal_width se observan diferencias entre las especies, tanto en términos de los valores medios como de las variabilidades.Veamos ahora\n\nVeamos ahora con el otro conjunto de datos.\n\n\n5.1.1.2 Datos Penguins\nComo en el caso anterior realizamos un análisis completo de todo el banco de datos. En este caso el target es una variable numérica por lo que no resulta posible distinguir los resultados por dicho valor, aunque en el conjunto de predictoras si tenemos variables de agrupación. Lo habitual en este tipo de situaciones es evaluar como aumenta o disminuye el valor del target en función de las posibles predictoras. En los puntos siguientes veremos como utilizar los otros dos paquetes para analizar esta situación.\n\nskim(data_penguins)\n\n\nData summary\n\n\nName\ndata_penguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\n\n\n\nAdemás de la interpretación de las frecuencias para las variables factores, y de las medias y desviaciones típicas de las numéricas, lo más relevante es la presencia de observaciones missing. Estas muestras pueden afectar a nuestros modelos de aprendizaje automático, así que además de identificar su existencia hay que decidir que hacer con ellas. Si su número es pequeño en comparación con el tamaño total (número de muestras) de la base de datos una opción es eliminar dichas muestras, pero generalmente lo más habitual es realizar un proceso de imputación (rellenar los valores perdidos con nuevos valores) como los que estudiaremos más adelante.\n\n\n\n5.1.2 Análisis exploratorio con DataExplorer\nAunque el paquete skimr nos proporciona una primera aproximación numérica a nuestro banco de datos, también es cierto que no esta preparado para visualizar las posibles relaciones entre un target (numérico o factor) y las posibles variables predictoras. Vamos a ver como el paquete DataExplorer nos proporciona visualizaciones gráficas muy interesantes y fáciles de implementar. El paquete nos proporciona una primera toma de contacto con todo el conjunto de datos, de forma que podemos realizar un estudio más exhaustivo utilizando el paquete GGally que veremos en el punto siguiente.\nLos gráficos que podemos obtener con este paquete son accesibles mediante las funciones:\n\nplot_intro(), que nos proporciona la información general de base de datos\n\nDiscrete Columns: Porcentaje de variables de tipo factor.\nContinuous Columns: Porcentaje de variables de tipo numérico.\nAll Missing Columns: Porcentaje de variables con todos sus valores missing.\nComplete Rows: Porcentaje de muestras sin valores missing.\nMissing Observations: Porcentaje de valores missing con respecto al total de datos recogidos.\n\nplot_missing(), que nos proporciona la información del porcentaje de valores missing para cada una de las variables del banco de datos. también proporciona una escala de medida sobre la consideración del porcentaje de valores missing en cada una de las variables. Dicha escala se puede modificar con el parámetro group.\nplot_bar(), que proporciona gráficos de barras para cada una de las variables de tipo factor de forma individual. Podemos modificar dichos gráficos introduciendo el target de interés mediante los parámetros with, cuando es de tipo numérico, o by cuando es de tipo factor. En al práctica solo tiene sentido usar target factor para este tipo de gráficos.\nplot_histogram y plot_density, para representar la distribución de variables de tipo numérico. En este caso tenemos el parámetro scale_x que nos permite realizar de forma automática transformacionales sobre la variable de interés para estudiar su comportamiento, sobre todo para tratar de normalizar su comportamiento.\nplot_qq, para analizar si las variables numéricas se comportan según una distribución normal. En este caso tenemos disponible el parámetro by para analizar la posible normalidad mediante la variable factor identificada con este parámetro.\nplot_boxplot(), para analizar el comportamiento de las variables numéricas con respecto a un factor identificado mediante el parámetro by. Al igual que en otras funciones resulta posible introducir transformacionales de las variables numéricas mediante el parámetro scale_y.\nplot_correlation, para obtener una mapa de intensidad con las correlaciones entre las variables del banco de datos. Es necesario especificar type = \"c\" para tener en cuenta únicamente las variables numéricas, y así poder interpretar de forma adecuada los valores obtenidos.\nplot_scatterplot(), para representar la nube de puntos entre todas las viables numéricas y la variable de interés. La variable de interés se identifica con el parámetro by, y para una mejor interpretación se deberían utilizar únicamente variables de tipo numérico. Este gráfico permite la introducción de cambios de escala en ambos ejes mediante los parámetros scale_x y scale_y.\n\n\n5.1.2.1 Datos Iris\nComenzamos el análisis de este banco de datos recordando que el target (`species`) es de tipo factor, lo que condiciona parte de nuestras visualizaciones. En primer lugar tenemos una visualización general del banco de datos:\n\nplot_intro(data_iris, \n           ggtheme = theme_bw()) # Fondo blanco del gráfico\n\n\n\n\nFigura 5.1: Análisis estructura datos Iris\n\n\n\n\nTenemos un 20% de variables de tipo factor y un 80% de tipo numérico. No tenemos ninguna variable con todos los valores missing. De hecho el 100% de las muestras están completas, es decir, sin valores perdidos. verificamos este hecho con el gráfico de valores missing:\n\nplot_missing(data_iris,\n             ggtheme = theme_bw())\n\n\n\n\nFigura 5.2: Porcentaje de missings en los datos Iris\n\n\n\n\nComenzamos ahora con los gráficos individuales de cada variable donde podremos analizar su distribución. Comenzamos con las de tipo factor:\n\nplot_bar(data_iris, \n         ggtheme = theme_bw())\n\n\n\n\nFigura 5.3: Gráficos de barras factores datos Iris\n\n\n\n\nPodemos ver que todos los niveles del factor tienen el mismo numero de observaciones, es lo que llamamos un diseño equilibrado, pero echamos en falta que no aparezcan los valores observados sobre cada una de las barras. Continuamos con el análisis de las variables numéricas. En este caso utilizamos el histograma para representar su comportamiento. para configurar adecuadamente el gráfico introducimos un elemento lista donde indicamos el tamaño y color de las etiquetas de los gráficos, y donde borramos las etiquetas de los ejes x e y.\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank(),\n     axis.title.y = element_blank())\n\n# gráfico\n# fijamos ncol = 2 para que aparezcan dos gráficos por fila\nplot_histogram(data_iris, \n               ncol=2,\n               ggtheme = theme_bw(),\n               theme_config = lista) \n\n\n\n\nFigura 5.4: Histogramas variables numéricas datos Iris\n\n\n\n\nApreciamos claramente comportamientos bastante extraños en algunas de las variables. Por ejemplo, en petal_length podemos ver como dos poblaciones diferenciadas para valores inferior y superior a 2. Desde luego en ninguna de las variables se aprecia una distribución Normal en su comportamiento.\nVeamos que ocurre cuando queremos estudiar el comportamiento de cada predictora con respecto al target.\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank(),\n     axis.title.y = element_blank())\n\n# gráfico\n# fijamos ncol = 2 para que aparezcan dos gráficos por fila\nplot_boxplot(data_iris, by = \"species\",\n               ncol=2,\n               ggtheme = theme_bw(),\n               theme_config = lista) \n\n\n\n\nFigura 5.5: Gráficos de cajas numéricas vs target datos Iris\n\n\n\n\nClaramente se aprecia que los diferentes tipos de flor se diferencian en sus medidas de pétalo (length y width), pero no existen tantas diferencias entre las medidas de sépalo. Dado que nuestro objetivo es la clasificación parece que las medidas de pétalos ayudarán mejor a clasificar entre las tres especies de Iris.\nPor último podemos estudiar el nivel de asociación entre las predictoras numéricas mediante el gráfico de correlaciones. En este caso hay que tener en cuenta que si la relación entre numéricas no es lineal la interpretación de los valores obtenidos puede resultar errónea.\n\n# gráfico\nplot_correlation(data_iris,\n                 type = \"c\",  \n                 ggtheme = theme_bw()) \n\n\n\n\nFigura 5.6: Gráfico de correlación variables numéricas datos Iris\n\n\n\n\nSe producen correlaciones positivas muy altas entre las medidas de pétalo, indicando que cuanto mayor es una de ellas la otra también aumenta sus valores, es decir tenemos una relación directa entre ellas. La variable sepal_width tiene correlaciones negativas con el resto indicando que cuando las otras aumentan esta última disminuye, es decir, tenemos una relación inversa entre ellas.\n\n\n5.1.2.2 Datos Penguins\nEn este caso nuestro target es una variable numérica y debemos tener en cuenta esto a la hora de definir nuestro gráficos de asociación. Comenzamos con los gráficos general del banco de datos y los individuales de cada variable.\n\nplot_intro(data_penguins, \n           ggtheme = theme_bw()) \n\n\n\n\nFigura 5.7: Gráfico de estructura datos Penguins\n\n\n\n\nEn este caso tenemos un 0.79% de valores missing en nuestro banco de datos. tratamos de identificar donde se encuentran dichos valores evaluando cada variable por separado.\n\nplot_missing(data_penguins,\n             ggtheme = theme_bw())\n\n\n\n\nFigura 5.8: Porcentaje missings datos Penguins\n\n\n\n\nTenemos valores missing en todas las variables numéricas y en el factor sexo, siendo está última la que presenta un porcentaje mayor (3.2%). Ninguno de los porcentajes es muy relevante, y un proceso de imputación podría proporcionar una buena solución para nuestros modelos de aprendizaje automático.\nAnalizamos ahora según el tipo de variable comenzando con lo factores:\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank(),\n     axis.title.y = element_blank())\n\nplot_bar(data_penguins, \n               ncol=2, \n               ggtheme = theme_bw(),\n               theme_config = lista) \n\n\n\n\nFigura 5.9: Gráficos barras factores datos Penguins\n\n\n\n\nPodemos destacar que la especie más numerosa es Adelie, mientras que la menos es Chinistrap. Por otro lado, la isla de la que tenemos más información es Biscoe, y tenemos un diseño bastante equilibrado por sexo.\nVeamos ahora las variables numéricas:\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank(),\n     axis.title.y = element_blank())\n\nplot_histogram(data_penguins, \n               ncol=2, \n               ggtheme = theme_bw(),\n               theme_config = lista) \n\n\n\n\nFigura 5.10: Histogramas variables numéricas datos Penguins\n\n\n\n\nEn la target de este banco de datos (flipper_length_mm) se aprecian como dos poblaciones distintas separadas por un valor algo superior a los 200 mm. En todas las distribuciones se aprecia cierto sesgo hacia un lado (las distribuciones no están centradas) por lo que podríamos probar alguna transformación para tratar de corregir en parte dicho comportamiento. Dejamos esta tarea para más adelante.\nVeamos el gráfico de densidades para visualizar de forma más clara los posibles sesgos:\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank(),\n     axis.title.y = element_blank())\n\nplot_density(data_penguins, \n               ncol=2, \n               ggtheme = theme_bw(),\n               theme_config = lista) \n\n\n\n\nFigura 5.11: Densidades variables numéricas datos Penguins\n\n\n\n\nComenzamos ahora con los estudios de asociación. en primer lugar analizamos el comportamiento de cada uno de los factores con respecto a la característica de interés.\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank(),\n     axis.title.y = element_blank())\n\n# gráfico\n# fijamos ncol = 2 para que aparezcan dos gráficos por fila\nplot_boxplot(data_penguins, by = \"species\",\n               ncol=2, \n               ggtheme = theme_bw(),\n               theme_config = lista) \n\nWarning: Removed 8 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigura 5.12: Gráficos de cajas variables numéricas vs especies. Datos Penguins\n\n\n\n\nLa solución proporciona todos los gráficos para las variables numéricas, pero nos vamos a fijar en el de la variable target. Podemos ver que la especie Gentoo es la que muestra mayores valores de target, mostrando además diferencias con las otras dos especies. Veamos que ocurre con el sexo y la isla.\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank(),\n     axis.title.y = element_blank())\n\n# gráfico\n# fijamos ncol = 2 para que aparezcan dos gráficos por fila\nplot_boxplot(data_penguins, by = \"sex\",\n               ncol=2, \n               ggtheme = theme_bw(),\n               theme_config = lista) \n\nWarning: Removed 8 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigura 5.13: Gráficos de cajas variables numéricas vs sexo Datos Penguins\n\n\n\n\nNo se aprecian diferencias por sexo aunque los machos muestran una valor medio más alto.\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank(),\n     axis.title.y = element_blank())\n\n# gráfico\n# fijamos ncol = 2 para que aparezcan dos gráficos por fila\nplot_boxplot(data_penguins, by = \"island\",\n               ncol=2, \n               ggtheme = theme_bw(),\n               theme_config = lista) \n\nWarning: Removed 8 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nFigura 5.14: Gráficos de cajas variables numéricas vs isla Datos Penguins\n\n\n\n\nEn este caso las islas Torgensen y Dream muestran valores más pequeños del target que las muestras de la isla Biscoe.\nPor último vemos los gráficos de dispersión de las variables numéricas con respecto al target.\n\n# lista de configuraciones gráficas\nlista = list(plot.title = element_text(size = 16, face = \"bold\"),\n     strip.text = element_text(colour = \"black\", size = 10, face = 2),\n     axis.title.x = element_blank())\n\n# gráfico\n# fijamos ncol = 2 para que aparezcan dos gráficos por fila\nplot_scatterplot(data = data_penguins |> select_if(is.numeric), \n               by = \"flipper_length_mm\",\n               ncol=2, \n               ggtheme = theme_bw(),\n               theme_config = lista) \n\nWarning: Removed 6 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFigura 5.15: Gráficos de dispersión variables numéricas vs target. Datos Penguins\n\n\n\n\nEn la Figura 5.15 podemos destacar tres aspectos:\n\nNo se aprecia una relación entre bill_depth y flipper_length dado que incluso se ven dos nubes de puntos separadas. Parece que existe algún tipo de factor que provoca esa diferencia.\nEntre bill_length y flipper_length se aprecia cierta dependencia, ya que conforme aumenta la primera parece que aumenta la segunda. Sin embargo, la nube de puntos es bastante dispersa dando a entender que el nivel de asociación no es muy alto.\nPor último, entre body_mass y flipper_length se aprecia una tendencia bastante lineal, indicando que ambas están muy relacionadas, de forma que si estamos interesados en predecir un valor de flipper_length parece factible utilizar el valor de body_mass.\n\nPodemos ver como todas las posibles predictoras no tienen la misma relevancia o relación con el target a lo hora de tratar de predecir su comportamiento. Para analizar el grado de asociación podemos hacer el mapa de intensidad de correlación. Dado que tenemos valores missing en nuestras variables debemos indicar a la función que utiliza para los cálculos aquellas muestras sin valores missing en ninguna de las variables numéricas.\n\n# gráfico\nplot_correlation(data_penguins,\n                 type = \"c\",  \n                 cor_args = list(\"use\" = \"pairwise.complete.obs\"),\n                 ggtheme = theme_bw()) \n\n\n\n\nFigura 5.16: Gráfico de correlación variables numéricas datos Penguins\n\n\n\n\nEn los resultados de Figura 5.16 se aprecia el grado de asociación entre las predictoras y el target (segunda fila de la matriz), tal y como hemos comentado anteriormente. De hecho, para bill_depth proporciona un valor negativo que en este caso solamente indica que no hay asociación lineal entre ellas debido a la separación de las dos nubes de puntos que veíamos en la Figura 5.15.\n\n\n\n5.1.3 Análisis exploratorio con GGally\nPara finalizar este apartado se presentan algunas de las funciones contenidas en el paquete GGally. Para conocer todas las funciones disponibles se recomienda consultar este enlace.\nEn concreto nos vamos a centrar en todas las funciones que nos permiten extraer información del comportamiento del target de nuestro datos frente al conjunto de predictoras. Dichas funciones son:\n\nggbivariate(), que nos permite representar la información de un target con respecto a un conjunto de predictoras (numéricas o factores). El gráfico de asociación que aparece depende del tipo del target (numérica o factor) y del tipo de cada una de las predictoras.\nggpairs(), que representa de forma bivariada toda la información contenida en una matriz de datos en función del tipo de cada una de las variables allí contenidas.\nggscatmat(), que representa de forma bivariada toda la información contenida en una matriz de datos donde todas las variables son de tipo numérico.\nggduo(), que generaliza los pairs plot representando la información de dos conjuntos de datos distintos. En este caso podemos poner en uno de los grupos el target deseado y en otro grupo todas las posibles predictoras.\n\nA continuación, vemos diferentes versiones de estos gráficos para los conjuntos de datos que hemos venido trabajando hasta ahora.\n\n5.1.3.1 Datos Iris\n\n# gráfico\nggbivariate(data_iris, \"species\", names(data_iris)[1:4])\n\n\n\n\nFigura 5.17: Gráfico de asociación individual entre predictora y target datos Iris\n\n\n\n\nDado que todas las predictoras son numéricas el gráfico por defecto es un gráfico de cajas donde podemos comparar el comportamiento de cada especie con respecto a cada una de las variables numéricas.\nLa Figura 5.17 nos permite ver la contribución individual de cada predictora al problema de clasificación de las tres especies, pero no resulta de utilidad para ver como dos de ellas nos pueden ayudar en dicha tarea. Introducimos ahora un gráfico que nos permite ver la contribución dos a dos de las predictoras en nuestro problema de clasificación:\n\n# gráfico\nggscatmat(data_iris, names(data_iris)[1:4], color = \"species\")\n\n\n\n\nFigura 5.18: Gráfico de asociación bivariante entre predictora y target datos Iris\n\n\n\n\nEsta figura nos proporciona tres análisis distintos:\n\nEn la diagonal aparecen los gráficos de densidad individuales de cada predictora en las tres especies. Este gráfico es similar al anterior del gráfico de cajas pero permite una mejor visualización para diferencia entre especies. Por ejemplo, podemos distinguir entre iris-setosa y las otras dos especies tanto para petal_length y petal_width.\nEn la parte superior aparecen los coeficientes de correlación entre las predictoras para cada una de las especies. Si tomamos petal_width podemos ver que las correlaciones más latas se producen siempre con las iris-versicolor y las más bajas con la iris-setosa.\nEn la parte inferior se presentan las nubes de puntos bivaraintes identificando cada punto de acuerdo a la especie a la que pertenecen. Desde le punto de vista de clasificación las especies se identificarán más fácilmente cuando las nubes de puntos estén más separadas. Por ejemplo, si tomamos petal_length y petal_width se aprecia claramente al separación entre iris-setosa y las otras dos especies. De hecho podemos ver que también hay cierta separación para las otras dos especies (iris-versicolor con valores más bajos) pero no tan claramente. Esto implica directamente que será más fácil clasificar las iris-setosa, y que habrá cierta confusión en la clasificación para las otras dos cuando utilizamos las variables de pétalo.\n\n\n\n5.1.3.2 Datos Penguins\nLos gráficos y su interpretación varían un poco con respecto al ejemplo anterior ya que nuestro target en este caso es una variable de tipo numérico. Comenzamos como antes con los gráficos individuales para cada variable.\n\n# gráfico\nggbivariate(data_penguins, \"flipper_length_mm\", names(data_penguins)[-5])\n\n\n\n\nFigura 5.19: Gráfico asociación individual datos Penguins\n\n\n\n\nDado que nuestro target es continuo los gráficos disponibles son diagramas de cajas cuando la predictora es factor, y diagramas de dispersión (con tendencia lineal aproximada) para las numéricas.\nPor ejemplo, podemos ver que la especie Gentoo tiene los valores de la predictora más grandes y diferenciados respecto de las otras dos especies. Para la variable body_mass podemos ver una tendencia lineal creciente indicando que cuanto más aumenta dicha variable mayor será el valor de target. Estos gráficos son similares a los que obtuvimos con el paquete DataExplorer.\nComo en el caso anterior podemos obtener los gráficos del target con respecto a dos predictoras de forma conjunta para analizar como varían los valores del target. Seleccionamos las predictoras numéricas y añadimos colores de función de cada una de las predictoras factor para estudiar su comportamiento conjunto. Visualizamos también la tendencia lineal entre target y predictora numérica diferenciado por el factor para analizar su comportamiento.\n\nggduo(\n   data_penguins,\n   # target\n   \"flipper_length_mm\",\n   # predictoras numéricas\n   names(data_penguins |> select_if(is.numeric))[-3],\n   # Coloreamos por factor y visualizamos tendencia lineal\n   mapping = ggplot2::aes(color = species),\n   types = list(continuous = wrap(\"smooth_lm\", se=FALSE)),\n   legend = 1\n   )\n\n\n\n\nFigura 5.20: Gráfico bivariante datos Penguins\n\n\n\n\n¿Qué conclusiones podemos extraer de este gráfico? ¿qué ocurre si asumimos una tendencia no lineal?\n\nggduo(\n   data_penguins,\n   # target\n   \"flipper_length_mm\",\n   # predictoras numéricas\n   names(data_penguins |> select_if(is.numeric))[-3],\n   # Coloreamos por factor y visualizamos tendencia lineal\n   mapping = ggplot2::aes(color = species),\n   types = list(continuous = wrap(\"smooth_loess\", se=FALSE)),\n   legend = 1\n   )\n\n\n\n\nFigura 5.21: Gráfico bivariante datos Penguins\n\n\n\n\n¿Cambian muchos nuestras conclusiones?\nVeamos ahora que ocurre con la isla (probamos únicamente la tendencia no lineal).\n\nggduo(\n   data_penguins,\n   # target\n   \"flipper_length_mm\",\n   # predictoras numéricas\n   names(data_penguins |> select_if(is.numeric))[-3],\n   # Coloreamos por factor y visualizamos tendencia lineal\n   mapping = ggplot2::aes(color = island),\n   types = list(continuous = wrap(\"smooth_loess\", se=FALSE)),\n   legend = 1\n   )\n\n\n\n\nFigura 5.22: Gráfico bivariante datos Penguins\n\n\n\n\n¿Qué conclusiones podemos extraer en este caso teniendo en cuenta que nuestro objetivo es predecir flipper_length?\nLa cantidad de gráficos que podemos realizar en nuestro bancos de datos es inmensa y depende como hemos visto del tipo de target y predictoras. Cada vez que introducimos un nuevo banco de datos debemos realizar el análisis descriptivo correspondiente para tener una mejor compresión del comportamiento de nuestros datos.\nEn el punto siguiente empezamos a introducir el paquete que utilizaremos para nuestros modelos de aprendizaje automático. Por el momento presentaremos dos modelos de aprendizaje básicos para los bancos de datos utilizados, y más adelante iremos detallando los aspectos técnicos de cada uno de los modelos que estudiaremos durante este curso."
  },
  {
    "objectID": "50_AED.html#sec-50.2",
    "href": "50_AED.html#sec-50.2",
    "title": "5  Introducción al análisis de datos",
    "section": "5.2 Librerías para el AA",
    "text": "5.2 Librerías para el AA\nDentro del programa R existen diferentes paquetes para trabajar con modelos de aprendizaje automático, como por ejemplo Caret y H20, pero en estos materiales nos vamos a centrar en el paquete mlr3 ya que emplea programación orientada a objetos mediante el uso de la clase R6.\n\n5.2.1 Ecosistema mlr3\nEl paquete mlr3 (Lang et al. 2019) y el ecosistema mlr3 más amplio proporcionan un marco genérico, orientado a objetos y extensible para la regresión, la clasificación y otras tareas de aprendizaje automático para la Lenguaje R. En el nivel más básico, la interfaz unificada proporciona funcionalidad para entrenar, probar y evaluar muchos algoritmos de aprendizaje automático. También puede llevar esto un paso más allá con la optimización de hiperparámetros, canalizaciones computacionales, interpretación de modelos y mucho más. mlr3 tiene objetivos generales similares a caret y tidymodels para R, scikit-learn para Python y MLJ para Julia. En general, mlr3 está diseñado para proporcionar más flexibilidad que otros marcos de AA y, al mismo tiempo, ofrecer formas sencillas de usar la funcionalidad avanzada. Si bien tidymodels en particular hace que sea muy fácil realizar tareas simples de AA, mlr3 está más orientado a AA avanzado.\nEl primer paso es instalar y hacer accesible el paquete mlr3verse (Lang and Schratz 2023) que nos permite acceder al entorno básico de mlr3. Este paquete está destinado a simplificar tanto la instalación como la carga de paquetes del ecosistema mlr3. En lugar de depender de los paquetes de extensión, las funciones requeridas para el análisis de datos se reexportan, lo que brinda una visión general de la funcionalidad más importante del ecosistema mlr3. A continuación aparece el código para su instalación:\n\ninstall.packages(\"mlr3verse\")\n\nPodemos utilizarlo ahora cargando el paquete correspondiente:\n\nlibrary(mlr3verse)\nlibrary(mlr3)\n\nAl cargar el paquete mlr3verse,se cargan los modelos básicos de aprendizaje automático para problemas de regresión, clasificación, cluster y supervivencia. Las funciones y objetos de Los importados por este metapaquete son:\n\nmlr3, que es el paquete principal con las funcionalidades necesarias para crear los objetos necesarios para el ecosistema mlr3.\nmlr3cluster, que es el paquete diseñado para los modelos de aprendizaje no supervisado de agrupación.\nmlr3data, que contiene bancos de datos adicionales.\nmlr3filters, que es el paquete utilizado para la selección de predictores.\nmlr3fselect, que es el paquete para la selección de funciones basada en contenedores. - mlr3learners, que es el paquete de definición de los modelos básicos de aprendizaje automático.\nmlr3pipelines, que es el paquete para realizar el preprocesado de los datos y modelado en un único objeto.\nmlr3tuning, que el paquete utilizado para probar diferentes configuraciones de los modelos de aprendizaje automático.\nmlr3tuningspaces, que contiene una colección de espacios de búsqueda para la optimización de hiperparámetros en el ecosistema mlr3.\nmlr3viz, que el paquete utilizado para visualizar los objetos mlr3.\nparadox, que es el paquete para crear y manejar el valor de los hiperparámetros de los modelos.\n\nPodemos ver todos los paquetes (junto con su versión) integrados mediante el código siguiente:\n\nmlr3verse_info()\n\n             package    version\n 1:            bbotk      0.7.2\n 2:      mlr3cluster      0.1.8\n 3:         mlr3data      0.7.0\n 4:      mlr3filters      0.7.1\n 5:      mlr3fselect     0.11.0\n 6:    mlr3hyperband      0.4.5\n 7:     mlr3learners      0.5.6\n 8:          mlr3mbo      0.2.1\n 9:         mlr3misc     0.13.0\n10:    mlr3pipelines    0.5.0.1\n11:       mlr3tuning     0.19.0\n12: mlr3tuningspaces      0.4.0\n13:          mlr3viz 0.6.1.9000\n14:          paradox     0.11.1\n\n\nSin embargo, el ecosistema mlr3 contiene muchos otros paquetes que podemos ver en la imagen siguiente:\n\nAlgunos de estos paquetes necesitan su propia instalación por lo que se debe consultar la página web de cada uno de ellos para realizar dicha tarea. A lo largo de los materiales se indicara como instalar y utilizar todos aquellos que no están dentro del paquete básico.\n\n\n5.2.2 Crear un modelo con mlr3\nEncontrar el mejor modelo de aprendizaje automático capaz de representar los patrones presentes en los datos de entrenamiento y generalizarlos a nuevas observaciones no es fácil, dado que existen multitud de algoritmos para una misma tarea, cada uno con unas características propias y con distintos parámetros que deben ser ajustados. Es a lo largo de todo este proceso donde más destacan las funcionalidades ofrecidas por mlr3, permitiendo emplear la misma sintaxis para ajustar, optimizar, evaluar y predecir un amplio abanico de modelos variando únicamente el nombre del algoritmo. Aunque mlr3 permite todo esto con apenas unas pocas líneas de código, son muchos los argumentos que pueden ser adaptados, cada uno con múltiples posibilidades. De momento en esta unidad mostraremos las funcionalidades básicas para los bancos de datos que hemos venido presentando.\nLos desarrolladores de mlr3 han dividido las etapas de la creación de un modelo de machine learning en 6 “bloques”. Dentro cada uno de estos “bloques” se llevan a cabo una o varias acciones (preprocesado, resampling, entrenamiento, evaluación, predicción, etc). Al trabajar de esta forma, se consigue que cada “bloque” transmita al siguiente, no solo los datos, sino también toda la información de lo ha ocurrido hasta el momento. La secuencia de pasos con los que se crea un modelo predictivo en mlr3 puede parecer extraña para aquellos lectores que estén acostumbrados a otros paquetes de aprendizaje automático. En mlr3, sin embargo, primero se crean los objetos en los que se definen cada una las acciones para finalmente ejecutarlas de forma conjunta e interconectada. A continuación se detallan cada uno de los pasos a seguir para construir un modelo predictivo.\n\n5.2.2.1 Task\nEl objeto task o tarea contiene datos (generalmente tabulares) y metadatos adicionales para definir un problema de aprendizaje automático. Los metadatos son, por ejemplo, el nombre de la variable de destino para problemas de aprendizaje automático supervisado, o el tipo de conjunto de datos (por ejemplo, una tarea espacial o de supervivencia). Esta información es utilizada por operaciones específicas que se pueden realizar en una tarea. Podemos crear un task a partir de un data.frame(), data.table(), o Matrix().\nUna vez creado el objeto podemos realizar todo el preprocesado necesario para pasar al paso siguiente. Los tipos task disponibles son:\n\nTask de clasificación: el objetivo es una etiqueta (almacenada como carácter o factor) con solo relativamente pocos valores distintos.\nTask de regresión: el objetivo es una cantidad numérica (almacenada como número entero o numérico) .\nTask de supervivencia: el objetivo es el tiempo (censurado por la derecha) hasta un evento. Actualmente se están desarrollando más tipos de censura. Para utilizar esta task debemos instalar el paquete mlr3proba. El código siguiente nos permite esto:\n\n\ninstall.packages(\"mlr3proba\", repos = \"https://mlr-org.r-universe.dev\")\n\n\nTask de densidad: una tarea no supervisada para estimar la densidad . Se encuentra disponible también en el paquete mlr3proba.\nTask de clúster o agrupación: un tipo de tarea no supervisada; no hay un target y el objetivo es identificar grupos similares en términos de distancia. En este caso utilizamos el paquete mlr3cluster.\nTask espacio-temporal: las observaciones en la tarea tienen información espacio-temporal (por ejemplo, coordenadas). En este caso es necesario instalar el paquete mlr3spatiotempcv. El código siguiente nos permite esto:\n\n\ninstall.packages(\"mlr3spatiotempcv\")\n\nUn aspecto a tener en cuenta una vez hemos creado una task es la representación gráfica de la información contenida en ella. Se trata de una análisis exploratorio gráfico del target, de cada predictoras vs el target, y el gráfico conjunto de predictoras vs target. Las soluciones obtenidas son similares a los de las librerías gráficas presentadas en la Sección 5.1. para realizar estos análisis debemos cargar en primer lugar la librería mlr3viz. para conseguir las tres soluciones gráficas para una task debemos utilizar la función autoplot() con los parámetros siguientes:\n\nautoplot(task, type = \"target\"), para el gráfico individual de target.\nautoplot(task, type = \"duo\"), para el gráfico individual de cada predictora vs target.\nautoplot(task, type = \"pairs\"), para el gráfico conjunto bivariante de target y predictoras.\n\nA continuación, vemos el código necesario para crear las task correspondientes a los banco de datos iris y penguins, y representamos gráficamente la información contenida en ellos.\n\nDatos Iris\nEn este caso estamos en un problema de clasificación y vamos a utilizar la función as_task_classif para definir la task de clasificación a partir del dataframe de datos iris. El código necesario se presenta a continuación:\n\n# creamos la tarea\ntask_iris = as_task_classif(data_iris, target = \"species\")\n# información de la tarea\nprint(task_iris)\n\n<TaskClassif:data_iris> (150 x 5)\n* Target: species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): petal_length, petal_width, sepal_length, sepal_width\n\n\nLa información proporcionada nos indica el target, si tenemos un problema de clasificación binario o multiclase, y el tipo de predictoras contenidas.\n\n\n\n\n\n\nEn un problema de clasificación binario otro parámetro de interés de la función as_task_classif() es positive con el que podemos indicar que nivel del factor es el identificado como “exitus”. Por ejemplo, en un problema donde el target es si se tiene o no se tiene cierta enfermedad podríamos identificar como positive=\"Si\" al grupo de interés.\n\n\n\nAdemás, mlr3 incorpora toda una serie de atributos y métodos con los que extraer o modificar información de un objeto task:\n\n$col_info, que proporciona información de todas las variables.\n$col_roles, que proporciona los tipos de todas las variables.\n$missings(), que proporciona los valores perdidos por variable.\n$levels(), que proporciona los niveles de cada variable de tipo factor.\n$select(), que nos permite seleccionar un conjunto de variables.\nfilter$(), que nos permite seleccionar un conjunto de muestras.\n\nHay que tener en cuenta que al utilizar los atributos $select(), y filter$() estamos modificamos la task definida.\n\n\n\n\n\n\nAl utilizar los atributos $select(), y filter$() estamos modificamos la task definida. Más adelante veremos como realizar una copia para modificarla sin alterar la task original.\n\n\n\nVeamos el código para usar los atributos principales:\n\n# información variables\ntask_iris$col_info\n\n             id    type                                     levels label\n1:     ..row_id integer                                             <NA>\n2: petal_length numeric                                             <NA>\n3:  petal_width numeric                                             <NA>\n4: sepal_length numeric                                             <NA>\n5:  sepal_width numeric                                             <NA>\n6:      species  factor Iris-setosa,Iris-versicolor,Iris-virginica  <NA>\n   fix_factor_levels\n1:             FALSE\n2:             FALSE\n3:             FALSE\n4:             FALSE\n5:             FALSE\n6:             FALSE\n\n# información de roles\ntask_iris$col_roles\n\n$feature\n[1] \"petal_length\" \"petal_width\"  \"sepal_length\" \"sepal_width\" \n\n$target\n[1] \"species\"\n\n$name\ncharacter(0)\n\n$order\ncharacter(0)\n\n$stratum\ncharacter(0)\n\n$group\ncharacter(0)\n\n$weight\ncharacter(0)\n\n# missings\ntask_iris$missings()\n\n     species petal_length  petal_width sepal_length  sepal_width \n           0            0            0            0            0 \n\n# Niveles de los factores\ntask_iris$levels()\n\n$species\n[1] \"Iris-setosa\"     \"Iris-versicolor\" \"Iris-virginica\" \n\n\nUn rol muy importante dentro de los problemas de clasificación es el de los pesos de las categorías que deseamos clasificar. Las ponderaciones se utilizan para ponderar los puntos de datos de manera diferente y que estos sean tenidos en cuenta en la predicción del modelo de aprendizaje. Un ejemplo de por qué haríamos esto es en tareas de clasificación donde las clases están desequilibradas. Aunque en este ejemplo todas las clases están equilibradas vamos a ver como realizaríamos este proceso en otros ejemplos:\n\n# En primer lugar veamos la distribución del target\ndf = task_iris$data()\nsummary(df$species)\n\nEn este caso tenemos una relación 1-1-1 entre las especies, es decir, hay los mismos elementos de las tres especies.\n\n# Asignamos pesos a las muestras todos iguales\ndf$weights = rep(1, nrow(df))\n# Creamos nueva tarea e introducimos los pesos \ntask_iris_ponderado = as_task_classif(df, target = \"species\")\ntask_iris_ponderado$set_col_roles(\"weights\", roles = \"weight\")\n\nSi por ejemplo tuviéramos dos clases donde en una hay el doble de observaciones que la otra nuestra estructura de pesos debería ser 1-2, asignando el 2 a la clase que tiene la mitad de observaciones que la otra. De esta forma equilibramos los datos en la construcción de nuestro modelo de aprendizaje automático.\nVeamos ahora las diferentes opciones gráficas para esta tarea. Comenzamos con el gráfico del target:\n\n# Gráfico target\nautoplot(task_iris, type =\"target\")\n\n\n\n\nFigura 5.23: Autoplot task Iris. Gráfico target\n\n\n\n\nObtenemos un gráfico de barras con los conteos de cada una de las clases del target. Esto nos permite saber si dichas clases están equilibradas o no.\n\n# gráfico predictora vs target\nautoplot(task_iris, type =\"duo\")\n\n\n\n\nFigura 5.24: Autoplot task Iris. Gráfico duo\n\n\n\n\nEn este caso obtenemos un gráfico de cajas para cada variable de tipo numérico respecto del factor respuesta donde podemos ver como se diferencian las clases para cada una de las predictoras. Por último vemos el gráfico conjunto de todas las predictoras y la respuesta.\n\n# gráfico predictora vs target\nautoplot(task_iris, type =\"pairs\")\n\n\n\n\nFigura 5.25: Autoplot task Iris. Gráfico pairs\n\n\n\n\nLa solución en este caso nos aporta gráficos diferentes en función de tipo de las variables involucradas en cada combinación. Tenemos desde histogramas o gráficos de densidad para el análisis individual de las variables numéricas, diagramas de barras para el análisis individual de factores, gráficos de dispersión y análisis de correlación para representar a información de dos variables numéricas, y diagramas de caja para una variable numérica y otra categórica. En este caso todos los gráficos identifican el target en sus soluciones mediante una escala de colores.\n\n\nDatos Penguins\nEn este caso estamos en un problema de regresión y vamos a utilizar la función as_task_regr. Sin embargo, dado que como comprobaremos más tarde dicho conjunto contiene valores perdidos en la respuesta realizamos un proceso de limpieza inicial. En concreto se platean dos tareas que no se encuentran de lo que habitualmente se conoce como preprocesado:\n\nDetección de muestras con todos los valores perdidos.\nDetección de muestras con valores perdidos en la respuesta o target.\n\nEn ambos casos la solución pasa por eliminar dichas muestras. En el primer caso no tenemos ningún dato y por tanto no tiene sentido utilizar esa muestra. En el segundo eliminamos la muestra porque no podemos imputar valores en la respuesta para evitar posibles sesgos en el modelo. Para este proceso utilizamos funciones habituales para el tratamiento de dataframes, y una vez eliminadas dichas muestras procedemos con la creación del task correspondiente.\nComenzamos detectando todas las muestras donde todos sus valores son missing. Utilizamos la función complete.cases\n\n# valoramos si las muestras tiene todas sus observaciones pérdidas\nids = complete.cases(data_penguins)\n# identificamos dichas muestras\npos = which(ids==\"FALSE\")\npos\n\n [1]   4   9  10  11  12  48 179 219 257 269 272\n\n# Eliminamos esas observaciones\ndata_penguins = data_penguins[-pos,]\n\nAhora nos hemos asegurado que no hay muestras con todos los valores perdidos. Veamos si todavía quedan valores perdidos en la respuesta.\n\n# valoramos si hay valores perdidos en la repuesta\nsum(is.na(data_penguins$flipper_length_mm))\n\n[1] 0\n\n\nNo quedan valores perdidos y podemos construir la task correspondiente.\n\n# creamos la tarea\ntask_penguins = as_task_regr(data_penguins, target = \"flipper_length_mm\")\n# información de la tarea\nprint(task_penguins)\n\n<TaskRegr:data_penguins> (333 x 7)\n* Target: flipper_length_mm\n* Properties: -\n* Features (6):\n  - dbl (3): bill_depth_mm, bill_length_mm, body_mass_g\n  - fct (3): island, sex, species\n\n\n\n# información variables\ntask_penguins$col_info\n\n                  id    type                  levels label fix_factor_levels\n1:          ..row_id integer                          <NA>             FALSE\n2:     bill_depth_mm numeric                          <NA>             FALSE\n3:    bill_length_mm numeric                          <NA>             FALSE\n4:       body_mass_g numeric                          <NA>             FALSE\n5: flipper_length_mm numeric                          <NA>             FALSE\n6:            island  factor  Biscoe,Dream,Torgersen  <NA>             FALSE\n7:               sex  factor             female,male  <NA>             FALSE\n8:           species  factor Adelie,Chinstrap,Gentoo  <NA>             FALSE\n\n# información de roles\ntask_penguins$col_roles\n\n$feature\n[1] \"bill_depth_mm\"  \"bill_length_mm\" \"body_mass_g\"    \"island\"        \n[5] \"sex\"            \"species\"       \n\n$target\n[1] \"flipper_length_mm\"\n\n$name\ncharacter(0)\n\n$order\ncharacter(0)\n\n$stratum\ncharacter(0)\n\n$group\ncharacter(0)\n\n$weight\ncharacter(0)\n\n# missings\ntask_penguins$missings()\n\nflipper_length_mm     bill_depth_mm    bill_length_mm       body_mass_g \n                0                 0                 0                 0 \n           island               sex           species \n                0                 0                 0 \n\n# Niveles de los factores\ntask_penguins$levels()\n\n$island\n[1] \"Biscoe\"    \"Dream\"     \"Torgersen\"\n\n$sex\n[1] \"female\" \"male\"  \n\n$species\n[1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"   \n\n\nVeamos ahora las soluciones gráficas para esta task. Para targets de tipo numérico la opción \"duo\" no se encuentra disponible. Comenzamos por representar la información del target:\n\n# Gráfico target\nautoplot(task_penguins, type =\"target\")\n\n\n\n\nFigura 5.26: Autoplot task penguins. Gráfico target\n\n\n\n\nEn este caso dado que el target es numérico obtenemos el gráfico de cajas que no da información sobre su distribución.\n\n# gráfico predictora vs target\nautoplot(task_penguins, type =\"pairs\")\n\n\n\n\nFigura 5.27: Autoplot task penguins. Gráfico pairs\n\n\n\n\n¿Cómo interpretamos estos gráficos? ¿cómo nos pueden ayudar estos gráficos en nuestra tarea de predicción?\n\n\n\n5.2.2.2 Division de muestras\nComo ya vimos anteriormente evaluar la capacidad predictiva de un modelo consiste en comprobar cómo de próximas son sus predicciones a los verdaderos valores de la variable respuesta. Para poder cuantificarlo de forma correcta necesitamos dividir nuestro conjunto de muestras en train y test. Con las muestras train entrenamos el modelo, mientras que con las muestras de test evaluamos la capacidad predictiva del modelo con observaciones que no han participado en la obtención del modelo.\nEl tamaño adecuado de las particiones depende en gran medida de la cantidad de datos disponibles y la seguridad que se necesite en la estimación del error, 80%-20% suele dar buenos resultados. El reparto debe hacerse de forma aleatoria o aleatoria-estratificada.\n\n\n\n\n\n\nPara asegurar la estratificación y evitar posibles sesgo en el modelo, la muestra de test debe compartir características similares a la muestra de entrenamiento. En mlr3, y siempre que el target sea categórico, esto se puede hacer modificando el task indicando que tenemos una variable como stratum. El código para la task_iris sería el siguiente:\ntask_iris$col_roles$stratum <- \"species\"\n\n\n\nEl paquete mlr3 dispone del objeto resampling = rsmp(), con las parámetros \"holdout\" y ratio = p, para repartir en los conjuntos train (p%) y test ((1-p)%) los datos contenidos en un task. Una vez creado el objeto de muestreo los atributos train_set() y test_set() nos permiten acceder a los índices de las observaciones que corresponden a cada bloque. Para asegurar la reproducibilidad de nuestros modelos es necesario fijar la semilla de números aleatorios. Cuando se emplea un resampling, si existe una o varias columnas con el role stratum, se realiza un reparto estratificado, garantizando así una distribución aproximada (por el momento, esto solo es válido para variables cualitativas). El reparto estratificado es muy importante cuando hay grupos minoritarios en alguna variable. Por ejemplo, en un set de datos con 100 observaciones, un predictor binario que tenga 90 observaciones de un grupo y solo 10 de otro, tiene un alto riesgo de que, en alguna de las particiones, el grupo minoritario no tenga representantes. Si esto ocurre en el conjunto de entrenamiento, algunos algoritmos darán error al aplicarlos al conjunto de test, ya que no entenderán el valor que se les está pasando. Este problema puede evitarse eliminando variables con varianza próxima a cero.\nEl código siguiente nos permite establecer de forma genérica el proceso de división de muestras mediante la identificación de las observaciones que asignaremos a la muestra de entrenamiento y test:\n\n# Fijamos semilla\nset.seed(1234)\n# Creamos el objeto para la separación de muestras\nrsmp_holdout <- rsmp(\"holdout\", ratio = 0.8)\nrsmp_holdout\n# Estratificación para una task dada\ntask_datos$col_roles$stratum <- \"precio\"\n# Incicializamos el objeto de división\nrsmp_holdout$instantiate(task = task_datos)\n# Obtenemos índices de muestras\n# Índices train\nid_train <- rsmp_holdout$train_set(i = 1)\n# Índices test\nid_test <- rsmp_holdout$test_set(i = 1)\n# Se crean dos nuevas task, una con los datos de train y otra con los de test.\n# Dado que se va a aplicar un filtrado, y para no alterar task_datos, se emplea\n# antes del filtro el método $clone() para hacer una copia.\ntask_train <- task_datos$clone()$filter(id_train)\ntask_test  <- task_datos$clone()$filter(id_test)\n\n\nDatos Iris\nVamos a obtener la muestra de entrenamiento y test para la task iris:\n\n# Fijamos semilla\nset.seed(1234)\n# Objeto de muestreo\nrsmp_holdout <- rsmp(\"holdout\", ratio = 0.8)\n# Estratificación\ntask_iris$col_roles$stratum <- \"species\"\n# inicialización\nrsmp_holdout$instantiate(task = task_iris)\n# Índices de muestras\nid_train <- rsmp_holdout$train_set(i = 1)\nid_test <- rsmp_holdout$test_set(i = 1)\n# Muestras\ntask_iris_train <- task_iris$clone()$filter(id_train)\ntask_iris_test  <- task_iris$clone()$filter(id_test)\n\nDado que todas las predictoras son numéricas nos resulta muy fácil verificar si las muestras de entrenamiento y test son similares sin más que comparar las medianas de las predictoras para el target en cada uno de los grupos.\n\nkable(task_iris_train$data() |> group_by(species) |> summarise_if(is.numeric, median, na.rm = TRUE))\n\n\n\nTabla 5.1: División muestras datos Iris.Medianas muestra entrenamiento\n\n\nspecies\npetal_length\npetal_width\nsepal_length\nsepal_width\n\n\n\n\nIris-setosa\n1.50\n0.2\n5.00\n3.4\n\n\nIris-versicolor\n4.35\n1.3\n5.95\n2.8\n\n\nIris-virginica\n5.55\n2.0\n6.70\n3.0\n\n\n\n\n\n\n\nkable(task_iris_test$data() |> group_by(species) |> summarise_if(is.numeric, median, na.rm = TRUE))\n\n\n\nTabla 5.2: División muestras datos Penguins. Medianas muestra test\n\n\nspecies\npetal_length\npetal_width\nsepal_length\nsepal_width\n\n\n\n\nIris-setosa\n1.45\n0.2\n5.05\n3.45\n\n\nIris-versicolor\n4.30\n1.3\n5.75\n2.75\n\n\nIris-virginica\n5.45\n2.1\n6.35\n2.80\n\n\n\n\n\n\nSe puede ver que las medianas para los dos grupos son muy similares indicando que la separación de muestras no parece incurrir en ningún sesgo de partida.\n\n\nDatos Penguins\nVamos a obtener la muestra de entrenamiento y test para la task penguins:\n\n# Fijamos semilla\nset.seed(1234)\n# Objeto de muestreo\nrsmp_holdout <- rsmp(\"holdout\", ratio = 0.8)\n# inicialización\nrsmp_holdout$instantiate(task = task_penguins)\n# Índices de muestras\nid_train <- rsmp_holdout$train_set(i = 1)\nid_test <- rsmp_holdout$test_set(i = 1)\n# Muestras\ntask_penguins_train <- task_penguins$clone()$filter(id_train)\ntask_penguins_test  <- task_penguins$clone()$filter(id_test)\n\nEn este caso podemos comparar la división mediante al representación gráfica de cada task:\n\n# gráfico predictora vs target\nautoplot(task_penguins_train, type =\"pairs\")\n\n\n\n\nFigura 5.28: Autoplot task-train penguins. Gráfico pairs\n\n\n\n\n\n# gráfico predictora vs target\nautoplot(task_penguins_test, type =\"pairs\")\n\n\n\n\nFigura 5.29: Autoplot task-test penguins. Gráfico pairs\n\n\n\n\n\n\n\n5.2.2.3 Preprocesado de datos\nComo ya vimos en la Unidad 5 el preprocesado de datos engloba todas aquellas transformaciones realizadas sobre los datos con el objetivo que puedan ser interpretados por los algoritmos de aprendizaje automático lo más eficientemente posible. Todo preprocesado de datos debe realizarse con las observaciones de entrenamiento y luego aplicarse al conjunto de entrenamiento y al de test. Esto es muy importante para no incluir sesgos artificiales en los modelos y evitar violar la condición de que ninguna información procedente de las observaciones de test participe o influya en el ajuste del modelo. Este principio debe aplicarse también si se emplea validación cruzada como veremos más adelante. Los pasos principales de cualquier preprocesado se pueden englobar en cinco grandes apartados: limpieza de datos. imputación de valores ausentes, exclusión de variables con varianza próxima a cero, estandarización y/o escalado de variables numéricas, y codificación de factores en variables dummy.\nTan solo la tarea de limpieza de datos no requiere de funciones especiales ya que sus objetivo principales son:\n\nDetectar factores con un único nivel (función summary()).\nDetectar niveles del factor con nombres distintos pero con los mismos valores (función identical()).\nDetectar variables numéricas con los mismos valores en diferentes escalas (función cor()).\n\nEn esta situación la opción más sencilla es eliminar dichas variables (to_remove()). Sin embargo, existen otros problemas habituales en los que no podemos eliminar variables:\n\nLas columnas de ID, es decir, las columnas que son únicas para cada observación deben eliminarse o etiquetarse.\nLos NA no están codificados correctamente como “NA” o “”\nErrores semánticos en los datos, por ejemplo, una variable que sólo puede tomar valores positivos con valores negativos.\nFunciones numéricas codificadas como categóricas para los alumnos que no pueden manejar dichas funciones.\n\nEn estas situaciones hay que identificar las muestras donde se producen estos errores y corregirlos si es posible.\nPara el resto de tarea de preprocesado de los datos dentro del ecosistema mlr3 es mediante los paquetes mlr3pipelines y mlrfilters. Estos paquetes permite encadenar operadores PipeOp y Filters, donde cada uno aplica una transformación concreta, y ejecutarlos todos de forma secuencial. Una vez que han sido definidos, pueden ser utilizados para entrenar con $train() y luego aplicados a nuevos conjuntos de datos con $pred(). La cantidad de operadores es muy grande y todavía se siguen añadiendo más por lo que es recomendable acudir al manual para ver los detalles de todos ellos. Para una demostración detallada del uso de estos paquetes para el preprocesado de datos se deben consultar los capítulos 7 a 9 de este libro.\nA continuación listamos los operadores disponibles:\n\n# Operadores pipeop disponibles \nkable(as.data.table(po())[, 1:3])\n\n\nOperadores pipeop disponibles \n\n\n\n\n\n\n\nkey\nlabel\npackages\n\n\n\n\nboxcox\nBox-Cox Transformation of Numeric Features\nmlr3pipelines, bestNormalize\n\n\nbranch\nPath Branching\nmlr3pipelines\n\n\nchunk\nChunk Input into Multiple Outputs\nmlr3pipelines\n\n\nclassbalancing\nClass Balancing\nmlr3pipelines\n\n\nclassifavg\nMajority Vote Prediction\nmlr3pipelines, stats\n\n\nclassweights\nClass Weights for Sample Weighting\nmlr3pipelines\n\n\ncolapply\nApply a Function to each Column of a Task\nmlr3pipelines\n\n\ncollapsefactors\nCollapse Factors\nmlr3pipelines\n\n\ncolroles\nChange Column Roles of a Task\nmlr3pipelines\n\n\ncopy\nCopy Input Multiple Times\nmlr3pipelines\n\n\ndatefeatures\nPreprocess Date Features\nmlr3pipelines\n\n\nencode\nFactor Encoding\nmlr3pipelines, stats\n\n\nencodeimpact\nConditional Target Value Impact Encoding\nmlr3pipelines\n\n\nencodelmer\nImpact Encoding with Random Intercept Models\nmlr3pipelines, lme4 , nloptr\n\n\nfeatureunion\nAggregate Features from Multiple Inputs\nmlr3pipelines\n\n\nfilter\nFeature Filtering\nmlr3pipelines\n\n\nfixfactors\nFix Factor Levels\nmlr3pipelines\n\n\nhistbin\nSplit Numeric Features into Equally Spaced Bins\nmlr3pipelines, graphics\n\n\nica\nIndependent Component Analysis\nmlr3pipelines, fastICA\n\n\nimputeconstant\nImpute Features by a Constant\nmlr3pipelines\n\n\nimputehist\nImpute Numerical Features by Histogram\nmlr3pipelines, graphics\n\n\nimputelearner\nImpute Features by Fitting a Learner\nmlr3pipelines\n\n\nimputemean\nImpute Numerical Features by their Mean\nmlr3pipelines\n\n\nimputemedian\nImpute Numerical Features by their Median\nmlr3pipelines, stats\n\n\nimputemode\nImpute Features by their Mode\nmlr3pipelines\n\n\nimputeoor\nOut of Range Imputation\nmlr3pipelines\n\n\nimputesample\nImpute Features by Sampling\nmlr3pipelines\n\n\nkernelpca\nKernelized Principle Component Analysis\nmlr3pipelines, kernlab\n\n\nlearner\nWrap a Learner into a PipeOp\nmlr3pipelines\n\n\nlearner_cv\nWrap a Learner into a PipeOp with Cross-validated Predictions as Features\nmlr3pipelines\n\n\nmissind\nAdd Missing Indicator Columns\nmlr3pipelines\n\n\nmodelmatrix\nTransform Columns by Constructing a Model Matrix\nmlr3pipelines, stats\n\n\nmultiplicityexply\nExplicate a Multiplicity\nmlr3pipelines\n\n\nmultiplicityimply\nImplicate a Multiplicity\nmlr3pipelines\n\n\nmutate\nAdd Features According to Expressions\nmlr3pipelines\n\n\nnmf\nNon-negative Matrix Factorization\nmlr3pipelines, MASS , NMF\n\n\nnop\nSimply Push Input Forward\nmlr3pipelines\n\n\novrsplit\nSplit a Classification Task into Binary Classification Tasks\nmlr3pipelines\n\n\novrunite\nUnite Binary Classification Tasks\nmlr3pipelines\n\n\npca\nPrinciple Component Analysis\nmlr3pipelines\n\n\nproxy\nWrap another PipeOp or Graph as a Hyperparameter\nmlr3pipelines\n\n\nquantilebin\nSplit Numeric Features into Quantile Bins\nmlr3pipelines, stats\n\n\nrandomprojection\nProject Numeric Features onto a Randomly Sampled Subspace\nmlr3pipelines\n\n\nrandomresponse\nGenerate a Randomized Response Prediction\nmlr3pipelines\n\n\nregravg\nWeighted Prediction Averaging\nmlr3pipelines\n\n\nremoveconstants\nRemove Constant Features\nmlr3pipelines\n\n\nrenamecolumns\nRename Columns\nmlr3pipelines\n\n\nreplicate\nReplicate the Input as a Multiplicity\nmlr3pipelines\n\n\nscale\nCenter and Scale Numeric Features\nmlr3pipelines\n\n\nscalemaxabs\nScale Numeric Features with Respect to their Maximum Absolute Value\nmlr3pipelines\n\n\nscalerange\nLinearly Transform Numeric Features to Match Given Boundaries\nmlr3pipelines\n\n\nselect\nRemove Features Depending on a Selector\nmlr3pipelines\n\n\nsmote\nSMOTE Balancing\nmlr3pipelines, smotefamily\n\n\nspatialsign\nNormalize Data Row-wise\nmlr3pipelines\n\n\nsubsample\nSubsampling\nmlr3pipelines\n\n\ntargetinvert\nInvert Target Transformations\nmlr3pipelines\n\n\ntargetmutate\nTransform a Target by a Function\nmlr3pipelines\n\n\ntargettrafoscalerange\nLinearly Transform a Numeric Target to Match Given Boundaries\nmlr3pipelines\n\n\ntextvectorizer\nBag-of-word Representation of Character Features\nmlr3pipelines, quanteda , stopwords\n\n\nthreshold\nChange the Threshold of a Classification Prediction\nmlr3pipelines\n\n\ntunethreshold\nTune the Threshold of a Classification Prediction\nmlr3pipelines, bbotk\n\n\nunbranch\nUnbranch Different Paths\nmlr3pipelines\n\n\nvtreat\nInterface to the vtreat Package\nmlr3pipelines, vtreat\n\n\nyeojohnson\nYeo-Johnson Transformation of Numeric Features\nmlr3pipelines, bestNormalize\n\n\n\n\n\nLos filtros nos permiten utilizar diferentes criterios de selección de variables para identificar aquellas con un mayor peso sobre el target, y así poder descartar todas aquellas que tengan poca relevancia para el problema planteado. Podemos conocer todos los filtros disponibles con el código siguiente o acudiendo al manual directamente:\n\n# Filtros disponibles \nkable(as.data.table(flt())[, 1:3])\n\n\nOperadores filter disponibles \n\n\n\n\n\n\n\nkey\nlabel\ntask_types\n\n\n\n\nanova\nANOVA F-Test\nclassif\n\n\nauc\nArea Under the ROC Curve Score\nclassif\n\n\ncarscore\nCorrelation-Adjusted coRrelation Score\nregr\n\n\ncarsurvscore\nCorrelation-Adjusted coRrelation Survival Score\nsurv\n\n\ncmim\nMinimal Conditional Mutual Information Maximization\nclassif, regr\n\n\ncorrelation\nCorrelation\nregr\n\n\ndisr\nDouble Input Symmetrical Relevance\nclassif, regr\n\n\nfind_correlation\nCorrelation-based Score\nclassif, regr\n\n\nimportance\nImportance Score\nclassif\n\n\ninformation_gain\nInformation Gain\nclassif, regr\n\n\njmi\nJoint Mutual Information\nclassif, regr\n\n\njmim\nMinimal Joint Mutual Information Maximization\nclassif, regr\n\n\nkruskal_test\nKruskal-Wallis Test\nclassif\n\n\nmim\nMutual Information Maximization\nclassif, regr\n\n\nmrmr\nMinimum Redundancy Maximal Relevancy\nclassif, regr\n\n\nnjmim\nMinimal Normalised Joint Mutual Information Maximization\nclassif, regr\n\n\nperformance\nPredictive Performance\nclassif\n\n\npermutation\nPermutation Score\nclassif\n\n\nrelief\nRELIEF\nclassif, regr\n\n\nselected_features\nEmbedded Feature Selection\nclassif\n\n\nvariance\nVariance\nNA\n\n\n\n\n\nA continuación vamos a aplicar los operadores presentados sobre los bancos de datos que venimos trabajando. La función para pipeops es po(), mientras que para los filtros es flt().\nPipeOps representa los pasos computacionales individuales en canalizaciones de aprendizaje automático. Estas canalizaciones en sí mismas están definidas por objetos Graph. Un Graph es una colección de PipeOps con “ejes” que guían el flujo de datos.\nLa forma más conveniente de construir un Graph es conectar una secuencia de PipeOps usando el operador %>>%. Cuando se le dan dos PipeOps, este operador crea un Graph que primero ejecuta el PipeOps de la izquierda, seguido por el de la derecha. También se puede usar para conectar un Graph con un PipeOps o con otro Graph. En lugar de usar %>>%, también puede crear un gráfico explícitamente usando los métodos $add_pipeop() y $add_edge() para crear PipeOps y los bordes que los conectan. En los ejemplos siguientes presentamos diferentes usos de estos recursos.\n\nDatos Iris\nComo ya vimos en la Sección 5.2.2.1 la task de este conjunto de datos no contiene valores perdidos y todas las predictoras son de tipo numérico. La única tarea de preprocesado es la estandarización de predictoras. Definimos el objeto de preprocesado para este conjunto de datos:\n\n# Objeto preprocesado\npp_iris = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n\nLos resultados de un operador mlr3pipelines siempre son una lista, en la que el primer elemento ([[1]]) contiene los datos transformados.\n\n# Creamos un nuevo objeto con los datos preprocesados\ntask_iris_train_pp = pp_iris$train(list(task_iris_train))[[1]]$clone()\ntask_iris_test_pp = pp_iris$train(list(task_iris_test))[[1]]$clone()\n\nPodemos ver la transformación realizada listando los primeros casos de los nuevos objetos\n\nhead(task_iris_train_pp)\n\n       species petal_length petal_width sepal_length sepal_width\n1: Iris-setosa    -1.326694   -1.316694   -1.1298717 -0.14263325\n2: Iris-setosa    -1.382712   -1.316694   -1.3622361  0.31996106\n3: Iris-setosa    -1.270676   -1.316694   -1.4784183  0.08866391\n4: Iris-setosa    -1.326694   -1.316694   -1.0136896  1.24514969\n5: Iris-setosa    -1.158640   -1.053355   -0.5489608  1.93904115\n6: Iris-setosa    -1.326694   -1.185025   -1.4784183  0.78255538\n\nhead(task_iris_test_pp)\n\n       species petal_length petal_width sepal_length sepal_width\n1: Iris-setosa    -1.358415   -1.256005   -0.9185233  1.07200685\n2: Iris-setosa    -1.299863   -1.382448   -1.2116690  0.17242068\n3: Iris-setosa    -1.299863   -1.256005   -0.4788047  1.52179994\n4: Iris-setosa    -1.358415   -1.382448   -1.3582418 -0.05247586\n5: Iris-setosa    -1.416968   -1.003118   -0.4788047  1.97159302\n6: Iris-setosa    -1.358415   -1.129561   -0.9185233  1.07200685\n\n\nValoramos ahora la capacidad de clasificación de cada una de las predictoras mediante el uso de filtros para estudiar la relevancia de cada una de ellas. Comenzamos utilizando el estadístico F para la comparación de los tres grupos de flores en cada una de las predictoras individualmente.\n\n# Construimos el objeto que define el filtro\nfilter = flt(\"anova\")\n\n# Aplicamos el filtro sobre los datos de entrenamiento\nfilter$calculate(task_iris_train_pp)\n# Vemos los resultados\nfilter\n\n<FilterAnova:anova>: ANOVA F-Test\nTask Types: classif\nProperties: -\nTask Properties: -\nPackages: stats\nFeature types: integer, numeric\n        feature    score\n1: petal_length 72.04517\n2:  petal_width 67.43149\n3: sepal_length 26.62265\n4:  sepal_width 11.58676\n\n\nPodemos ver como los valores más altos aparecen en las predictoras relacionadas con el pétalo, tal y como habíamos visto en el análisis exploratorio inicial. En realidad el score obtenido se puede pasar al p-valor de comparación de los tres grupos con:\n\n10^(-filter$scores)\n\npetal_length  petal_width sepal_length  sepal_width \n9.012243e-73 3.702597e-68 2.384219e-27 2.589643e-12 \n\n\n\n\nDatos Penguins\nEn esta banco de datos hemos observado valores perdidos tanto en las variables numéricas como en los factores, de forma que antes de pasar al preprocesado vamos a ver las opciones posibles de imputación con la función po():\n\nimputeconstant: que imputa valores con una valor constante.\nimputehist: imputa valores en variables numéricas mediante su histograma.\nimputemean: imputa valores en variables numéricas mediante su media.\nimputemedian: imputa valores en variables numéricas mediante su mediana.\nimputemode: imputa valores mediante su moda.\nimputeoor: imputa valores en factores mediante la creación de un nuevo nivel .MISSING.\nimputesample: imputa valores muestreando sobre los datos no perdidos de entrenamiento.\n\nEn este caso seleccionamos imputmedian para las variables numéricas e imputesample para las de tipo categórico.\nAdemás, como tenemos variables categóricas como posibles predictoras es necesario convertirlas a variables numéricas mediante un método de codificación. Las codificaciones posibles son:\n\none-hot: que crea variables ficticias con valores 0-1 en función del nivel de la categórica de cada variable. De esta forma se asocian tantas variables numéricas como niveles de un factor.\ncontr.helmert: devuelve una matriz de variables numéricas con los contrastes de Helmert, que contrastan el segundo nivel con el primero, el tercero con el promedio de los dos primeros, y así sucesivamente.\ncontr.poly: devuelve una matriz de variables numéricas con los contrastes basados en polinomios ortogonales.\ncontr.sum: devuelve una matriz de variables con los contrastes ‘sum to zero contrats’.\ncontr.treatment: contrasta cada nivel con el nivel de referencia (especificado por base): se omite el nivel de referencia.\n\nA continuación podemos ver las variables asociadas a cada tipo de contraste para un factor con tres niveles:\n\ncontr.helmert(3)\n\n  [,1] [,2]\n1   -1   -1\n2    1   -1\n3    0    2\n\ncontr.poly(3)\n\n                .L         .Q\n[1,] -7.071068e-01  0.4082483\n[2,] -7.850462e-17 -0.8164966\n[3,]  7.071068e-01  0.4082483\n\ncontr.sum(3)\n\n  [,1] [,2]\n1    1    0\n2    0    1\n3   -1   -1\n\ncontr.treatment(3)\n\n  2 3\n1 0 0\n2 1 0\n3 0 1\n\n\nEn nuestro caso utilizaremos la codificación one-hot que es muy habitual dentro del aprendizaje automático. A continuación se muestra el código necesario para llevar a cabo la imputación, estandarización y codificación para el banco de datos penguins.\n\npreprocesado = \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) %>>%\n   po(\"imputeoor\", affect_columns = selector_type(\"factor\")) %>>%\n   po(\"encode\", param_vals = list(method = \"one-hot\"))\n\nPodemos aplicar el procedimiento de preprocesado haciendo uso del método train sobre dicho objeto aplicado sobre una task específica. Veamos su efecto sobre los datos de entrenamiento penguin:\n\npreprocesado$train(task_penguins_train)\n\n$encode.output\n<TaskRegr:data_penguins> (266 x 12)\n* Target: flipper_length_mm\n* Properties: -\n* Features (11):\n  - dbl (11): bill_depth_mm, bill_length_mm, body_mass_g,\n    island.Biscoe, island.Dream, island.Torgersen, sex.female,\n    sex.male, species.Adelie, species.Chinstrap, species.Gentoo\n\n\nEn el nuevo objeto aparecen definidas las variables obtenidas a partir de la codificación e imputación de los factores.\nLos resultados (output) de un operador mlr3pipelines siempre son una lista, en la que el primer elemento ([[1]]) contiene los datos transformados. En este caso vamos a obtener los valores preprocesados tanto para la muestra de entrenamiento como la de test. Creamos nuevas task para no machacar los datos anteriores.\n\ntask_penguins_train_pp <- preprocesado$predict(task_penguins_train)[[1]]$clone()\ntask_penguins_test_pp  <- preprocesado$predict(task_penguins_test)[[1]]$clone()\n\nVeamos como han quedado los datos para la muestra de entrenamiento:\n\nkable(task_penguins_train_pp$data() %>% head())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflipper_length_mm\nbill_depth_mm\nbill_length_mm\nbody_mass_g\nisland.Biscoe\nisland.Dream\nisland.Torgersen\nsex.female\nsex.male\nspecies.Adelie\nspecies.Chinstrap\nspecies.Gentoo\n\n\n\n\n186\n0.0559552\n-0.7699838\n-0.4643987\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n193\n1.0311198\n-1.2704497\n-0.8992543\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n190\n1.6983376\n-0.8057313\n-0.6507654\n0\n0\n1\n0\n1\n1\n0\n0\n\n\n181\n0.2612530\n-0.8772265\n-0.6818265\n0\n0\n1\n1\n0\n1\n0\n0\n\n\n195\n1.1850931\n-0.8236051\n0.6227403\n0\n0\n1\n0\n1\n1\n0\n0\n\n\n191\n2.0062843\n-0.9308478\n-0.4643987\n0\n0\n1\n0\n1\n1\n0\n0\n\n\n\n\n\n\n\n\n5.2.2.4 Modelo (learner)\nEl siguiente paso tras definir los datos de entrenamiento, es seleccionar el algoritmo que se va a emplear. En mlr3, esto se hace mediante la creación de un objeto learner. Los objetos de la clase Learner proporcionan una interfaz unificada para muchos algoritmos populares de aprendizaje automático en R. El paquete mlr3learners contiene solo los principales algoritmos empleados para clasificación y regresión (listado).\n\n# Listado de learners disponibles en mlr3learners\nmlr_learners$keys()\n\n  [1] \"classif.abess\"            \"classif.AdaBoostM1\"      \n  [3] \"classif.bart\"             \"classif.C50\"             \n  [5] \"classif.cforest\"          \"classif.ctree\"           \n  [7] \"classif.cv_glmnet\"        \"classif.debug\"           \n  [9] \"classif.earth\"            \"classif.featureless\"     \n [11] \"classif.fnn\"              \"classif.gam\"             \n [13] \"classif.gamboost\"         \"classif.gausspr\"         \n [15] \"classif.gbm\"              \"classif.glmboost\"        \n [17] \"classif.glmer\"            \"classif.glmnet\"          \n [19] \"classif.IBk\"              \"classif.imbalanced_rfsrc\"\n [21] \"classif.J48\"              \"classif.JRip\"            \n [23] \"classif.kknn\"             \"classif.ksvm\"            \n [25] \"classif.lda\"              \"classif.liblinear\"       \n [27] \"classif.lightgbm\"         \"classif.LMT\"             \n [29] \"classif.log_reg\"          \"classif.lssvm\"           \n [31] \"classif.mob\"              \"classif.multinom\"        \n [33] \"classif.naive_bayes\"      \"classif.nnet\"            \n [35] \"classif.OneR\"             \"classif.PART\"            \n [37] \"classif.priority_lasso\"   \"classif.qda\"             \n [39] \"classif.randomForest\"     \"classif.ranger\"          \n [41] \"classif.rfsrc\"            \"classif.rpart\"           \n [43] \"classif.svm\"              \"classif.xgboost\"         \n [45] \"clust.agnes\"              \"clust.ap\"                \n [47] \"clust.cmeans\"             \"clust.cobweb\"            \n [49] \"clust.dbscan\"             \"clust.diana\"             \n [51] \"clust.em\"                 \"clust.fanny\"             \n [53] \"clust.featureless\"        \"clust.ff\"                \n [55] \"clust.hclust\"             \"clust.kkmeans\"           \n [57] \"clust.kmeans\"             \"clust.MBatchKMeans\"      \n [59] \"clust.mclust\"             \"clust.meanshift\"         \n [61] \"clust.pam\"                \"clust.SimpleKMeans\"      \n [63] \"clust.xmeans\"             \"dens.kde_ks\"             \n [65] \"dens.locfit\"              \"dens.logspline\"          \n [67] \"dens.mixed\"               \"dens.nonpar\"             \n [69] \"dens.pen\"                 \"dens.plug\"               \n [71] \"dens.spline\"              \"regr.abess\"              \n [73] \"regr.bart\"                \"regr.cforest\"            \n [75] \"regr.ctree\"               \"regr.cubist\"             \n [77] \"regr.cv_glmnet\"           \"regr.debug\"              \n [79] \"regr.earth\"               \"regr.featureless\"        \n [81] \"regr.fnn\"                 \"regr.gam\"                \n [83] \"regr.gamboost\"            \"regr.gausspr\"            \n [85] \"regr.gbm\"                 \"regr.glm\"                \n [87] \"regr.glmboost\"            \"regr.glmnet\"             \n [89] \"regr.IBk\"                 \"regr.kknn\"               \n [91] \"regr.km\"                  \"regr.ksvm\"               \n [93] \"regr.liblinear\"           \"regr.lightgbm\"           \n [95] \"regr.lm\"                  \"regr.lmer\"               \n [97] \"regr.M5Rules\"             \"regr.mars\"               \n [99] \"regr.mob\"                 \"regr.nnet\"               \n[101] \"regr.priority_lasso\"      \"regr.randomForest\"       \n[103] \"regr.ranger\"              \"regr.rfsrc\"              \n[105] \"regr.rpart\"               \"regr.rsm\"                \n[107] \"regr.rvm\"                 \"regr.svm\"                \n[109] \"regr.xgboost\"             \"surv.akritas\"            \n[111] \"surv.aorsf\"               \"surv.blackboost\"         \n[113] \"surv.cforest\"             \"surv.coxboost\"           \n[115] \"surv.coxtime\"             \"surv.ctree\"              \n[117] \"surv.cv_coxboost\"         \"surv.cv_glmnet\"          \n[119] \"surv.deephit\"             \"surv.deepsurv\"           \n[121] \"surv.dnnsurv\"             \"surv.flexible\"           \n[123] \"surv.gamboost\"            \"surv.gbm\"                \n[125] \"surv.glmboost\"            \"surv.glmnet\"             \n[127] \"surv.loghaz\"              \"surv.mboost\"             \n[129] \"surv.nelson\"              \"surv.obliqueRSF\"         \n[131] \"surv.parametric\"          \"surv.pchazard\"           \n[133] \"surv.penalized\"           \"surv.priority_lasso\"     \n[135] \"surv.ranger\"              \"surv.rfsrc\"              \n[137] \"surv.svm\"                 \"surv.xgboost\"            \n\n\nPodemos identificar el tipo de tarea de cada algoritmo de aprendizaje viendo el prefijo de cada uno de ellos:\n\nclassif para tareas de clasificación.\nclust para tareas de agrupación.\ndens para tareas de estimación de densidades.\nregr para tareas de regresión.\nsurv para tareas de supervivencia.\n\nEl listado completo de learners disponibles (incluidos en el paquete mlr3extralearners) se pueden consultar en este enlace. Es necesario instalar y cargar dicho paquete para acceder a todos esos modelos de aprendizaje. Todos los objeto learner incluyen los metadatos siguientes:\n\nfeature_types: tipo de variables que el algoritmo es capaz de aceptar (importante para identificar aquellos que no pueden tratar con factores.\npackages: el paquete al que pertenece el algoritmo.\nproperties: propiedades adicionales del algoritmo. Por ejemplo, “missings” significa que puede tratar con valores ausentes, y “importance” significa que el algoritmo es capaz de calcular la importancia de los predictores.\npredict_types: tipo de predicción.\n\nClasificación: “response” devuelve únicamente la clase con mayor probabilidad y “prob” devuelve además la probabilidad de cada clase.\nRegresión: “response” devuelve el valor predicho y “se” devuelve además el error estándar de la predicción.\nSupervivencia: “response” devuelve una medida de “risk” y “prob” devuelve la probabilidad en función del tiempo.\nClustering: “response” devuelve el id del cluster al que se ha asignado cada observación y “prob” devuelve la probabilidad de asignación a cada cluster (solo para fuzzy clustering).\n\nparam_set: conjunto de hiperparámetros disponibles.\n\nAl igual que con las tareas, podemos acceder a un modelo de aprendizaje específico mediante la función lrn(). Para ejecutar cualquier modelo de aprendizaje debemos pasar por dos fases:\n\nEntrenamiento: Los datos de entrenamiento (características y objetivo) se pasan a la función $train() del learner que entrena y almacena un modelo, es decir, la relación entre el objetivo y las características.\nPredicción: Los datos de test se pasan al método $predict() del learner para predecir los valores objetivo. Con dichos datos debemos valorar la capacidad predictiva mediante cualquiera de las métricas disponibles. A continuación se muestran las métricas disponibles:\n\n\n# Métricas disponibles\nmlr_measures\n\n<DictionaryMeasure> with 66 stored values\nKeys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,\n  classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,\n  classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n  classif.logloss, classif.mauc_au1p, classif.mauc_au1u,\n  classif.mauc_aunp, classif.mauc_aunu, classif.mbrier, classif.mcc,\n  classif.npv, classif.ppv, classif.prauc, classif.precision,\n  classif.recall, classif.sensitivity, classif.specificity, classif.tn,\n  classif.tnr, classif.tp, classif.tpr, clust.ch, clust.dunn,\n  clust.silhouette, clust.wss, debug_classif, oob_error, regr.bias,\n  regr.ktau, regr.mae, regr.mape, regr.maxae, regr.medae, regr.medse,\n  regr.mse, regr.msle, regr.pbias, regr.rae, regr.rmse, regr.rmsle,\n  regr.rrse, regr.rse, regr.rsq, regr.sae, regr.smape, regr.srho,\n  regr.sse, selected_features, sim.jaccard, sim.phi, time_both,\n  time_predict, time_train\n\n\nComo ocurre con la task o el learner para poder utilizar una métrica debemos definirla en primer lugar mediante la función msr(). La descripción de cada una de estas medidas de validación se puede consultar en este enlace.\nEn la imagen siguiente podemos ver la representación del proceso de aprendizaje básico en un modelo de aprendizaje:\n\n\n\n\n\nA continuación aplicamos modelos de aprendizaje basal para cada uno de los ejemplos que hemos ido trabajando.\nPodemos acceder a todos los modelos de aprendizaje de cierto tipo con el código siguiente:\n\n# Lista de algoritmos de clasificación\nclassif_learners = mlr_learners$keys()[startsWith(mlr_learners$keys(),\"classif\")]\nclassif_learners\n\n [1] \"classif.abess\"            \"classif.AdaBoostM1\"      \n [3] \"classif.bart\"             \"classif.C50\"             \n [5] \"classif.cforest\"          \"classif.ctree\"           \n [7] \"classif.cv_glmnet\"        \"classif.debug\"           \n [9] \"classif.earth\"            \"classif.featureless\"     \n[11] \"classif.fnn\"              \"classif.gam\"             \n[13] \"classif.gamboost\"         \"classif.gausspr\"         \n[15] \"classif.gbm\"              \"classif.glmboost\"        \n[17] \"classif.glmer\"            \"classif.glmnet\"          \n[19] \"classif.IBk\"              \"classif.imbalanced_rfsrc\"\n[21] \"classif.J48\"              \"classif.JRip\"            \n[23] \"classif.kknn\"             \"classif.ksvm\"            \n[25] \"classif.lda\"              \"classif.liblinear\"       \n[27] \"classif.lightgbm\"         \"classif.LMT\"             \n[29] \"classif.log_reg\"          \"classif.lssvm\"           \n[31] \"classif.mob\"              \"classif.multinom\"        \n[33] \"classif.naive_bayes\"      \"classif.nnet\"            \n[35] \"classif.OneR\"             \"classif.PART\"            \n[37] \"classif.priority_lasso\"   \"classif.qda\"             \n[39] \"classif.randomForest\"     \"classif.ranger\"          \n[41] \"classif.rfsrc\"            \"classif.rpart\"           \n[43] \"classif.svm\"              \"classif.xgboost\"         \n\n\ny a los hiperparámetros de cada algoritmo con:\n\n# Hiperparámetros para árboles de clasificación\nlrn(\"classif.rpart\")$param_set$ids()\n\n [1] \"cp\"             \"keep_model\"     \"maxcompete\"     \"maxdepth\"      \n [5] \"maxsurrogate\"   \"minbucket\"      \"minsplit\"       \"surrogatestyle\"\n [9] \"usesurrogate\"   \"xval\"          \n\n\n\nDatos Iris\nEn este caso nos planteamos una tarea de clasificación utilizando el learner \"classif.featureless\" que es el modelo de aprendizaje basal para tareas de clasificación, que solo analiza las etiquetas durante el entrenamiento, ignorando todas las características. El método de hiperparámetro determina el modo de operación durante la predicción:\n\nmodo: Predice la etiqueta más frecuente. Si hay dos o más etiquetas empatadas, selecciona aleatoriamente una por predicción. Las probabilidades corresponden a la frecuencia relativa de las etiquetas de clase en el conjunto de entrenamiento.\nmuestra: Predice aleatoriamente una etiqueta de manera uniforme. Las probabilidades corresponden a una distribución uniforme de etiquetas de clases, es decir, 1 dividido por el número de clases.\nmuestra.ponderada: Predice aleatoriamente una etiqueta, con la probabilidad estimada a partir de la distribución de entrenamiento. Para mantener la coherencia, las probabilidades son 1 para la etiqueta muestreada y 0 para todas las demás etiquetas.\n\nCargamos el modelo y vemos los metadatos que contiene.\n\n# learner\nlearner_iris = lrn(\"classif.featureless\")\n# metadatos\nprint(learner_iris)\n\n<LearnerClassifFeatureless:classif.featureless>: Featureless Classification Learner\n* Model: -\n* Parameters: method=mode\n* Packages: mlr3\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered,\n  POSIXct\n* Properties: featureless, importance, missings, multiclass,\n  selected_features, twoclass\n\n\nPodemos ver que:\n\nEl método utilizado es “mode” (Parameters).\nLos tipos de predicción (Predict Types) disponibles son “response” (para obtener la clase predicha) y “prob” (para obtener la probabilidad de cada clase).\nEste modelo nos permite cualquier tipo de variable predictora (Feature Types).\n\nComenzamos el proceso de aprendizaje aplicando el modelo sobre la muestra de entrenamiento.\n\n# Proceso de aprendizaje\nlearner_iris$train(task_iris_train_pp)\n# Descripción del modelo\nlearner_iris$model\n\n$tab\n\n    Iris-setosa Iris-versicolor  Iris-virginica \n             40              40              40 \n\n$features\n[1] \"petal_length\" \"petal_width\"  \"sepal_length\" \"sepal_width\" \n\nattr(,\"class\")\n[1] \"classif.featureless_model\"\n\n\nEstudiamos ahora la capacidad de predicción del modelo, es decir, si el modelo es capaz de indicar correctamente la clase de cada una de las muestras a partir del modelo construido. para este proceso utilizamos dos procedimientos:\n\nLa matriz de confusión, que nos proporciona una tabla donde se representan los valores observados frente a los valores predichos por le modelo. Cuanto menores sean los valores fuera de la diagonal de dicha matriz mayor será la capacidad predictiva del modelo.\nLas medidas de valoración o scores. En este caso utilizamos el porcentaje de clasificación correcta (classif.acc), que nos proporciona el porcentaje de muestras clasificadas correctamente, y el error de clasificación (classif.ce), que nos proporciona el porcentaje de observaciones clasificadas incorrectamente. Ambas medidas son complementarias y cuando mayor es el porcentaje de clasificación correcta mejor es la capacidad predictiva del modelo.\n\nPara valorar el modelo es necesario extraer en primer lugar la predicción tanto de la muestra de entrenamiento como de validación para el modelo considerado:\n\n# Predicción de la muestra de entrenamiento\npred_train = learner_iris$predict(task_iris_train_pp)\n# Predicción de la muestra de validación\npred_test = learner_iris$predict(task_iris_test_pp)\n\nObtenemos la matriz de confusión para los datos de validación:\n\n# Matriz de confusión para la muestra de validación\npred_test$confusion\n\n                 truth\nresponse          Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa               4               2              5\n  Iris-versicolor           0               5              2\n  Iris-virginica            6               3              3\n\n\nClaramente se puede ver que el modelo de clasificación basal considerado no funciona muy bien dado que los valores obtenidos fuera de la diagonal son muy elevados. Valoramos ahora los porcentajes de clasificación correcta y de error asociados con la matriz de confusión. Para ello es necesario definir en primer lugar la medidas que vamos a utilizar.\n\n# Definimos los scores de validación. En este caso utilizamos la función msrs ya que utilizamos múltiples scores. \nmeasures = msrs(c('classif.acc', 'classif.ce'))\n# Evaluamos los scores en la muestra de entrenamiento\npred_train$score(measures)\n\nclassif.acc  classif.ce \n  0.3666667   0.6333333 \n\n# Evaluamos los scores en la muestra de validación\npred_test$score(measures)\n\nclassif.acc  classif.ce \n        0.4         0.6 \n\n\nPodemos ver como el porcentaje de clasificación correcta pasa del 28.3% para la muestra de entrenamiento al 20% para la muestra de test. Por supuesto el modelo tiene un poder de clasificación muy bajo como corresponde con un modelo basal que no tiene en cuenta las características o posibles predictoras de la muestras para dicho este conjunto de datos. De hecho ya habíamos visto antes que las predictoras relacionadas tanto con la longitud como la anchura de pétalo.\nA pesar de que todavía no hemos presentado los aspectos teóricos de otros algoritmos de clasificación vamos a utilizar uno de ellos para comparar los resultados de clasificación. En concreto utilizamos classif.rpart que nos proporciona los árboles de clasificación. más adelante entraremos en el detalle de este algoritmo. Comenzamos estableciendo el modelo de aprendizaje:\n\n# Librerías gráficas para modelos de árboles\nlibrary(partykit)\nlibrary(ggparty)\n# learner\nlearner2_iris = lrn(\"classif.rpart\", keep_model = TRUE)\n# Proceso de aprendizaje\nlearner2_iris$train(task_iris_train_pp)\n# Solución gráfica del modelo\nautoplot(learner2_iris)\n\n\n\n\nAutoplot algoritmo aprendizaje datos Iris\n\n\n\n\nLos modelos de árbol producen grupos donde se organizan las observaciones de acurdo al modelo construido. En este caso tenemos como resultado tres grupos. Podemos ver también cuales son las variables más relevantes en el proceso de clasificación, que en este caso se corresponde únicamente con la longitud de pétalo. Como resultado tenemos que en el último grupo podemos ver como se mezclan observaciones tanto de iris versicolor como iris virginica. Veamos los resultados de la clasificación para este modelo:\n\n# Predicción de la muestra de entrenamiento\npred_train = learner2_iris$predict(task_iris_train_pp)\n# Predicción de la muestra de validación\npred_test = learner2_iris$predict(task_iris_test_pp)\n# Evaluamos los scores en la muestra de entrenamiento\npred_train$score(measures)\n\nclassif.acc  classif.ce \n 0.96666667  0.03333333 \n\n# Evaluamos los scores en la muestra de validación\npred_test$score(measures)\n\nclassif.acc  classif.ce \n  0.8333333   0.1666667 \n\n\nPodemos ver que el porcentaje de clasificación correcta es del 96.67% para la muestra de entrenamiento y del 83.33% para la muestra de validación. Dado que entrenamos el modelo con los mismos datos con los que lo hemos construido es esperable que el porcentaje para el entrenamiento sea superior que en la validación. También podemos ver como mejoramos ostensiblemente los resultados frente al modelo basal que no consideraba ninguna predictora. De hecho el modelo obtenido es bastante bueno al alcanzar un porcentaje de clasificación correcta bastante alto para la muestra de validación. Por último vemos la matriz de confusión asociada:\n\n# Matriz de confusión para la muestra de validación\npred_test$confusion\n\n                 truth\nresponse          Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa              10               0              0\n  Iris-versicolor           0               6              1\n  Iris-virginica            0               4              9\n\n\nPara la muestra de validación tenemos una clasificación perfecta en las iris-setosa, y casi perfecta en las iris versicolor. La clasificación de las iris virginica es la que peor funciona, con un mayor error de clasificación (4/13 = 30.7%).\n\n\nDatos Penguins\nEn este conjunto de datos estamos interesados en predecir la característica numérica flipper_length en función del conjunto de predictoras, de forma que nos enfrentamos a un problema de regresión. Utilizamos de nuevo un modelo basal para mostrar el proceso de aprendizaje: regr.featureless. Se trata de un modelo simple que solo analiza la respuesta durante el entrenamiento, ignorando todas las predictoras Si el hiperparámetro robusto es FALSO (predeterminado), predice constantemente la media(y) como respuesta y sd(y) como error estándar. Si robusto es VERDADERO, se utilizan mediana(y) y mad(y) en su lugar respectivamente.\nLos metadatos y el proceso de aprendizaje de este tipo de modelos es similar al de los modelos de clasificación. Comenzamos definiendo el modelo de aprendizaje:\n\n# learner\nlearner_penguins = lrn(\"regr.featureless\")\n# Entrenamiento del modelo\nlearner_penguins$train(task_penguins_train_pp)\n# Descripción del modelo\nlearner_penguins$model\n\n$location\n[1] 200.3459\n\n$dispersion\n[1] 13.94385\n\n$features\n [1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"      \n [4] \"island.Biscoe\"     \"island.Dream\"      \"island.Torgersen\" \n [7] \"sex.female\"        \"sex.male\"          \"species.Adelie\"   \n[10] \"species.Chinstrap\" \"species.Gentoo\"   \n\nattr(,\"class\")\n[1] \"regr.featureless_model\"\n\n\nEl modelo proporciona una estimación de 200.34 para la variable de interés con una dispersión de 13.94. Aunque en temas posteriores estudiaremos con detalle le proceso de validación de los modelos de regresión, es este apartado vamos a realizar una pequeña aproximación a los aspectos fundamentales. Los procesos de validación más sencillos pasan por:\n\nValoración numérica, donde utilizamos criterios como el \\(R2^2\\), \\(MSE\\) o \\(MAPE\\) para valorar la capacidad predictiva del modelo.\nValoración gráfica, donde se representan las predicciones del modelo frente a los valores observados realmente para ver lo cerca que están las predicciones de nuestro modelo frente a los valores reales.\n\nPara poder realizar dichas valoraciones debemos obtener en primer lugar las predicciones de nuestro modelo tanto en la muestra de entrenamiento como de validación.\n\n# Predicción de la muestra de entrenamiento\npred_train = learner_penguins$predict(task_penguins_train_pp)\n# Predicción de la muestra de validación\npred_test = learner_penguins$predict(task_penguins_test_pp)\n\nDefinimos ahora el conjunto de medidas numéricas que vamos a utilizar para valorar nuestro modelo:\n\n# Definimos los scores de validación.\nmeasures = msrs(c('regr.rsq', 'regr.mse', 'regr.mape'))\n# Evaluamos los scores en la muestra de entrenamiento\npred_train$score(measures)\n\n    regr.rsq     regr.mse    regr.mape \n  0.00000000 193.69992651   0.05955929 \n\n\nLos scores de valoración proporcionan valores muy bajos para el \\(R^2\\) y muy altos para el \\(MSE\\) y \\(MAPE\\) indicando que el moldeo basal propuesto es bastante deficiente, como era de esperar. Veamos que ocurre con la muestra de validación.\n\n# Evaluamos los scores en la muestra de entrenamiento\npred_test$score(measures)\n\n    regr.rsq     regr.mse    regr.mape \n -0.04842593 206.31219334   0.06021553 \n\n\nPodemos ver como los resultados empeoran con la muestra de validación. Analizamos ahora la solución gráfica del modelo propuesto. Para ello tenemos dos gráficos:\n\nvalores predichos frente a valores observados. Si el moldeo es adecuado la nube de puntos debería situarse sobre la diagonal del gráfico. En otro caso el modelo tiene poco poder predictivo.\nResiduos del modelo (predicción menos valor observado) frente a cada una de las muestras. En este caso la nube de puntos debería ser dispersa sin ningún tipo de tendencia\n\n\nlibrary(ggplot2)\n# Representación gráfica de observados frente a predichos\nggplot(pred_train, aes(x = truth, y = response)) +\n       geom_point() +\n       geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n       labs(title = \"Valor predicho vs valor real\") +\n       theme_bw()\n\n\n\n\nFigura 5.30: Valores reales vs predichos. Modelo Penguins.\n\n\n\n\nComo se puede ver el modelo es bastante malo ya que predice todas las observaciones con un único valor. Veamos ahora el gráfico de residuos:\n\n# Representación ids vs errores\nggplot(pred_train, aes(x = row_ids, y = response - truth)) +\n    geom_point() +\n    geom_hline(yintercept =  0, color = \"firebrick\") +\n    labs(title = \"Residuos del modelo\")\n\n\n\n\nFigura 5.31: Residuos. Modelo Penguins.\n\n\n\n\nDe nuevo podemos ver que el modelo es bastante malo, ya que los residuos observados no tiene un comportamiento aleatorio. De hecho, se aprecia cierta tendencia en el comportamiento de los residuos.\nComo hicimos con la tarea de clasificación vamos a proponer un modelo de regresión más avanzado para mostrar la mejora con respecto al modelo basal. En concreto utilizamos el algoritmo del modelo de regresión lineal que es accesible con el learner regr.lm. Comenzamos definiendo el proceso de aprendizaje.\n\n# learner\nlearner2_penguins = lrn(\"regr.lm\")\n# Entrenamiento del modelo\nlearner2_penguins$train(task_penguins_train_pp)\n# Descripción del modelo\nlearner2_penguins$model\n\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n      (Intercept)      bill_depth_mm     bill_length_mm        body_mass_g  \n          214.917              1.561              2.675              4.040  \n    island.Biscoe       island.Dream   island.Torgersen         sex.female  \n           -1.955             -1.114                 NA             -0.214  \n         sex.male     species.Adelie  species.Chinstrap     species.Gentoo  \n               NA            -19.839            -18.703                 NA  \n\n\nEn este caso el algoritmo de aprendizaje nos proporcionan los coeficientes del modelo (los estudiaremos más adelante). Utilizando los filtros de características podemos establecer una primer valoración de las variables más relevantes para el modelo planteado. En este caso utilizamos correlation como medida para dicha valoración. El listado completo de filtro se pueden consultar en este enlace.\n\n# Construimos el objeto que define el filtro\nfilter = flt(\"correlation\")\n# Aplicamos el filtro sobre los datos de entrenamiento\nfilter$calculate(task_penguins_train_pp)\n# Vemos los resultados\nfilter\n\n<FilterCorrelation:correlation>: Correlation\nTask Types: regr\nProperties: missings\nTask Properties: -\nPackages: stats\nFeature types: integer, numeric\n              feature     score\n 1:       body_mass_g 0.8755042\n 2:    species.Gentoo 0.8705125\n 3:    species.Adelie 0.6986344\n 4:    bill_length_mm 0.6816958\n 5:     island.Biscoe 0.5913543\n 6:     bill_depth_mm 0.5495039\n 7:      island.Dream 0.4180206\n 8:        sex.female 0.2782721\n 9:          sex.male 0.2782721\n10:  island.Torgersen 0.2640905\n11: species.Chinstrap 0.1473346\n\n# o los representamos gráficamente\nautoplot(filter)\n\n\n\n\nFiltro de correlación. Datos penguins\n\n\n\n\nEl modelo indica que la variable más relevante para obtener la predicción de la respuesta es body_mass_g seguida de las variables codificadas que identifican las especies Gentoo y Adelie. La isla y el sexo el pingüino parecen estar menos relacionados con la respuesta.\nValoramos ahora la capacidad predictiva de este nuevo modelo utilizando los mismos scores que en el modelo anterior.\n\n# Predicción de la muestra de entrenamiento\npred_train = learner2_penguins$predict(task_penguins_train_pp)\n# Predicción de la muestra de validación\npred_test = learner2_penguins$predict(task_penguins_test_pp)\n# Evaluamos los scores en la muestra de entrenamiento\npred_train$score(measures)\n\n   regr.rsq    regr.mse   regr.mape \n 0.87724404 23.77781960  0.01885683 \n\n# Evaluamos los scores en la muestra de validación\npred_test$score(measures)\n\n   regr.rsq    regr.mse   regr.mape \n 0.81102066 37.18788329  0.02339544 \n\n\nClaramente este modelo mejora el anterior. Aumentamos claramente el valor de \\(R^2\\) y reducimos las medidas del error. Como siempre los valores para la muestra de validación son algo peores que los de la muestra de entrenamiento. El valor de \\(R^2\\) del 81.1% indica que las variables predictoras contribuyen a explicar un 81% de la variabilidad de la respuesta lo que es un valor bastante alto. Veamos ahora la solución gráfica:\n\nlibrary(ggpubr)\n# Representación gráfica de observados frente a predichos\ng1 = ggplot(pred_train, aes(x = truth, y = response)) +\n       geom_point() +\n       geom_abline(slope = 1, intercept = 0, color = \"firebrick\") +\n       labs(title = \"Valor predicho vs valor real\") +\n       theme_bw()\n# Representación ids vs errores\ng2 = ggplot(pred_train, aes(x = row_ids, y = response - truth)) +\n      geom_point() +\n      geom_hline(yintercept =  0, color = \"firebrick\") +\n      labs(title = \"Residuos del modelo\")\nggarrange(g1,g2)\n\n\n\n\nFigura 5.32: Análisis gráfico del modelo. Datos Penguins.\n\n\n\n\nSe puede ver gráficamente como el modelo plateado tiene un comportamiento mucho mejor que el basal para predecir el comportamiento de la respuesta. En el caso de los residuos observamos un comportamiento bastante aleatorio sin ningún tipo de tendencia.\n\n\n\n5.2.2.5 Validación\nLa finalidad última de un modelo es predecir la variable respuesta en observaciones futuras o en observaciones que el modelo no ha “visto” antes. El error mostrado por defecto tras entrenar un modelo suele ser el error de entrenamiento, el error que comete el modelo al predecir las observaciones que ya ha “visto”. Si estos errores son útiles para entender cómo está aprendiendo el modelo (estudio de residuos), no es una estimación realista de cómo se comporta el modelo ante nuevas observaciones (el error de entrenamiento suele ser demasiado optimista). Para conseguir una estimación más certera, y antes de recurrir al conjunto de test, se pueden emplear estrategias de validación basadas en resampling o remuestreo.\nLas estrategias de remuestreo dividen repetidamente todos los datos disponibles en múltiples conjuntos de entrenamiento y prueba, y una repetición corresponde a lo que se llama una “iteración de remuestreo” en mlr3. Luego se entrena un modelo intermedio en cada conjunto de entrenamiento y el conjunto de prueba se utiliza para medir el rendimiento en cada iteración de remuestreo. El rendimiento de la generalización finalmente se estima agregando las puntuaciones de rendimiento en múltiples iteraciones de remuestreo (ver figura siguiente). Al repetir el proceso de división de datos, los datos se utilizan repetidamente tanto para el entrenamiento como para las pruebas, lo que permite un uso más eficiente de todos los datos disponibles para la evaluación del modelo en referencia a su capacidad predictiva. Además, una gran cantidad de iteraciones de remuestreo puede reducir la varianza en nuestras medidas de evaluación (scores) y, por lo tanto, dar como resultado una estimación más confiable. Esto significa que es menos probable que la evaluación del modelo de aprendizaje se vea afectada por una única división “desafortunada”.\n\n\n\nEsquema de resampling para un modelo de AA\n\n\nExiste una variedad de estrategias de remuestreo, cada una con sus ventajas y desventajas, que dependen del número de muestras disponibles, la complejidad de la tarea y el tipo de modelo.\nUna estrategia muy común es la validación cruzada (CV) k veces, que divide aleatoriamente los datos en subconjuntos que no se superponen, llamados pliegues o folds. En la imagen siguiente podemos ver el esquema de remuestreo con k=3.\n\n\n\nRemuestreo 3-fold\n\n\nEn esta situación los modelos siempre son entrenados en cada fold, utilizándose el fold restante como datos de validación, repitiendo este proceso hasta que cada fold haya actuado exactamente una vez como conjunto de validación. Finalmente, lo scores de cada fold se agregan, generalmente mediante un promedio, para dar un score global del modelo. La validación cruzada garantiza que cada observación se utilizará exactamente una vez en un conjunto de validación, haciendo un uso eficiente de los datos disponibles para la valoración de la capacidad predictora del modelo. Valores comunes para k son 5 y 10, lo que significa que cada conjunto de entrenamiento constará de 4/5 o 9/10 de los datos originales, respectivamente. Existen varias variaciones de CV, incluida la validación cruzada repetida de k veces, donde el proceso de k veces se repite varias veces, y la validación cruzada de dejar uno fuera (LOO-CV), donde el número de fold es igual al número de observaciones, lo que lleva al conjunto de pruebas en cada fold que consta de una sola observación.\nEl submuestreo y el bootstrapping son dos estrategias de remuestreo relacionadas. El submuestreo selecciona aleatoriamente una proporción determinada (4/5 y 9/10 son comunes) de los datos para el conjunto de datos de entrenamiento donde cada observación en el conjunto de datos se extrae sin reemplazo del conjunto de datos original. El modelo se entrena con estos datos y luego se prueba con los datos restantes, y este proceso se repite k veces. Bootstrapping sigue el mismo proceso que el submuestreo, pero los datos se extraen y se reemplazan del conjunto de datos original. Por lo general, el número de muestras de bootstrapping es igual al tamaño del conjunto de datos original. Esto significa que una observación podría seleccionarse varias veces (y, por lo tanto, duplicarse) en los datos de entrenamiento (pero nunca más de una vez por conjunto de datos de validación). De media, el 63.2% de los datos estarán contenidos en el conjunto de entrenamiento durante el bootstrapping, denominados muestras “in-bag” (el otro 36,8% se conocen como muestras “out-of-bag”).\nHay que tener en cuenta que la terminología relativa a las estrategias de remuestreo no es consistente en toda la literatura; por ejemplo, el submuestreo a veces se denomina “repeat holdout” o “Monte Carlo cross-validation”.\nLa elección de la estrategia de remuestreo generalmente depende de la tarea específica en cuestión y de los objetivos de la evaluación del desempeño, pero existen algunas reglas generales. Si los datos disponibles son bastante pequeños ($N \\leq 500$), se puede utilizar una validación cruzada repetida con una gran cantidad de repeticiones para mantener baja la varianza de los scores de evaluación (10 folds y 10 repeticiones es un buen lugar para comenzar). Tradicionalmente, también se ha recomendado LOO-CV para estos regímenes de tamaño de muestra pequeño, pero este esquema de estimación es bastante costoso (excepto en casos especiales donde existen atajos computacionales) y sufre de una varianza bastante alta. Además, LOO-CV también es problemático en tareas de clasificación binaria desequilibradas, ya que no se pueden aplicar conceptos como la estratificación. Para tamaños $500 \\leq N \\leq 50000$ se recomienda un CV de 5 a 10 veces mayor. En general, cuanto mayor es el conjunto de datos, menos divisiones se requieren; sin embargo, aún pueden ocurrir problemas con el tamaño de la muestra, por ejemplo, debido a datos desequilibrados.\nEl paquete mlr3 incorpora las siguientes estrategias: Bootstrap (mlr_resamplings_bootstrap), V-Fold Cross-Validation (mlr_resamplings_cv), Repeated V-Fold Cross-Validation (mlr_resamplings_repeated_cv) y Monte Carlo Cross-Validation (mlr_resamplings_subsampling). Para crear un objeto de remuestreo utilizamos la función rsmp(). En la tabla siguiente se muestran todos los métodos de remuestreo disponibles:\n\nas.data.table(mlr_resamplings)\n\n           key                         label        params iters\n1:   bootstrap                     Bootstrap ratio,repeats    30\n2:      custom                 Custom Splits                  NA\n3:   custom_cv Custom Split Cross-Validation                  NA\n4:          cv              Cross-Validation         folds    10\n5:     holdout                       Holdout         ratio     1\n6:    insample           Insample Resampling                   1\n7:         loo                 Leave-One-Out                  NA\n8: repeated_cv     Repeated Cross-Validation folds,repeats   100\n9: subsampling                   Subsampling ratio,repeats    30\n\n\nA continuación se muestran diferentes ejemplos de estrategias de validación cruzada:\n\n# three-fold CV\ncv3 = rsmp(\"cv\", folds = 3)\n# Subsampling with 3 repeats and 9/10 ratio\nss390 = rsmp(\"subsampling\", repeats = 3, ratio = 0.9)\n# 2-repeats 5-fold CV\nrcv25 = rsmp(\"repeated_cv\", repeats = 2, folds = 5)\n\nun aspecto a tener muy en cuenta es que las tareas de preprocesado del banco de datos deben estar incluidas dentro de cada iteración del remuestreo. Para realizar esta tarea se emplea el objeto GraphLearner, que permite combinar un mlr3pipeline de preprocesado con un learner. Una vez creado el objeto GraphLearner, este puede emplearse para como si de un learner se tratase (entrenamiento, predicción, validación…).\nEn las subsecciones siguientes vemos como funcionan los métodos de remuestreo sobre los bancos de datos que hemos venido trabajando. Para mostrar resultados razonables vamos a utilizar los modelos de aprendizaje más avanzados que presentamos en el punto anterior. Se ajusta de nuevo el modelo, esta vez con validación cruzada repetida para estimar su error. En primer lugar, se crea un objeto Resampling en el que se define el tipo de validación y el número de repeticiones. Con la función resample() se combinan un task, un learner y la estrategia de Resampling. De esta forma el modelo se ajusta y evalúa con cada una de las particiones de forma automática, calculando y almacenando en cada iteración la métrica o score de interés.\n\nDatos Iris\nComenzamos con los datos iris para la tarea de clasificación de flores. En primer lugar definimos el objeto de remuestreo. En este caso consideramos la validación cruzada k-fold con \\(k = 5\\).\n\nset.seed(123)\nresampling_cv = rsmp(\"cv\", folds = 5)\nresampling_cv\n\n<ResamplingCV>: Cross-Validation\n* Iterations: 5\n* Instantiated: FALSE\n* Parameters: folds=5\n\n\nDefinimos la métrica que vamos a utilizar, que en este caso es el porcentaje de clasificación correcta para cada fold.\n\nmetrica = msr(\"classif.acc\")\n\nGeneramos el objeto GraphLearner uniendo el objeto de preprocesado y el algoritmo de aprendizaje automático.\n\n# Unión pipeline procesado y learner\ngraph = pp_iris %>>% learner2_iris\n# Representamos el proceso de aprendizaje\ngraph$plot()\n\n\n\n# Generamos el modelo de aprendizaje \ngraph_learner <- GraphLearner$new(graph)\n\nPodemos definir ahora el objeto de remuestreo a partir del modelo de aprendizaje y de los datos originales:\n\nset.seed(135)\nrr = resample(task_iris, graph_learner, resampling_cv, store_models=TRUE)\n\nINFO  [17:32:24.239] [mlr3] Applying learner 'scale.classif.rpart' on task 'data_iris' (iter 1/5)\nINFO  [17:32:24.527] [mlr3] Applying learner 'scale.classif.rpart' on task 'data_iris' (iter 2/5)\nINFO  [17:32:24.797] [mlr3] Applying learner 'scale.classif.rpart' on task 'data_iris' (iter 3/5)\nINFO  [17:32:25.219] [mlr3] Applying learner 'scale.classif.rpart' on task 'data_iris' (iter 4/5)\nINFO  [17:32:25.478] [mlr3] Applying learner 'scale.classif.rpart' on task 'data_iris' (iter 5/5)\n\nrr\n\n<ResampleResult> with 5 resampling iterations\n   task_id          learner_id resampling_id iteration warnings errors\n data_iris scale.classif.rpart            cv         1        0      0\n data_iris scale.classif.rpart            cv         2        0      0\n data_iris scale.classif.rpart            cv         3        0      0\n data_iris scale.classif.rpart            cv         4        0      0\n data_iris scale.classif.rpart            cv         5        0      0\n\n\nCada fila de la tabla anterior corresponde con una de las iteraciones de la estrategia de remuestreo considerada. Ahora podemos obtener la información correspondiente a cada uno de los remuestreos, y más concretamente el score asociado con cada una de ellas para la muestra de validación correspondiente.\n\n# Seleccionamos las columnas de interés\nacc = rr$score(metrica)\nacc[, .(iteration, classif.acc)]\n\n   iteration classif.acc\n1:         1   0.9333333\n2:         2   0.9666667\n3:         3   0.9000000\n4:         4   0.9333333\n5:         5   1.0000000\n\n\nPodemos ver que en todas las iteraciones el porcentaje de clasificación correcta es superior al 90%. De hecho se observa una gran estabilidad en los resultados independientemente de la iteración. Obtenemos ahora el valor agregado para las cinco iteraciones para dar una medida global del porcentaje de clasificación correcta.\n\n# valor agregado\nrr$aggregate(metrica)\n\nclassif.acc \n  0.9466667 \n\n\nEl valor agregado del porcentaje de clasificación correcta se sitúa casi en el 95% indicando que le modelo clasifica muy bien los diferentes tipos de flores.\nContinuando con el análisis individual de cada fold vamos a representar gráficamente la matriz de confusión asociada a cada uno de ellos. Para ello necesitamos instalar la librería cvms y definir una pequeña función que tome los datos de un fold específico y nos permita representar gráficamente la matriz de confusión.\n\n# Cargamos la librería para representar la matriz de confusión\nlibrary(cvms)\n# Definimos la función necesaria para la representación gráfica\ncm_plot_resampling = function(data)\n{\n  cm = confusion_matrix(data$truth, data$response)\n   plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n}\n\nRepresentamos ahora la matriz de confusión para un fold específico. En este caso tomamos el fold 2.\n\n# Plot para cada fold\ncm_plot_resampling(rr$predictions()[[2]])\n\n\n\n\nFigura 5.33: Matriz de confusión para el el remuestreo. Datos Iris.\n\n\n\n\nEl gráfico proporciona los conteos de cada combinación y diferentes porcentajes que pasamos a explicar. Para la combinación Iris-virginica vs Iris-virginica tenemos que:\n\nHay 9 casos en la combinación y los clasificamos todos correctamente.\nEl porcentaje de casos dentro de esta combinación frente al total de casos evaluados es del 30% (porcentaje central).\nTenemos un porcentaje marginal por columnas de clasificación correcta del 90% (porcentaje pequeño en horizontal) indicando que todas de las flores iris-virgnica originales son clasificadas correctamente el 90% con nuestro modelo.\nTenemos un porcentaje marginal por filas del 100% indicando que todas las predicciones como Iris virginica que proporciona nuestro modelo son originalmente Iris virginica.\nSi sumamos todos los porcentajes centrales tendríamos el porcentaje global de clasificación correcta para este fold. En este caso es del 30 + 33.3 + 33.3 = 96.6%.\n\n\n\nDatos Penguins\nProponemos ahora un método de resampling para el banco de datos penguins. Como en el caso anterior definimos el proceso de resampling, la métrica que utilizaremos y el graphlearner que combina el preprocesado con le moldeo de aprendizaje utilizado.\nComenzamos con el proceso de remuestreo\n\nset.seed(123)\nresampling_cv = rsmp(\"cv\", folds = 5)\nresampling_cv\n\n<ResamplingCV>: Cross-Validation\n* Iterations: 5\n* Instantiated: FALSE\n* Parameters: folds=5\n\n\nAhora la métrica a utilizar\n\nmetrica = msr(\"regr.mse\")\n\nRecargamos el objeto de preprocesado para este banco de datos\n\npp_penguins = \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) %>>%\n   po(\"imputeoor\", affect_columns = selector_type(\"factor\")) %>>%\n   po(\"encode\", param_vals = list(method = \"one-hot\"))\n\nDefinimos el Grpahlearner asociado a este modelo:\n\n# Unión pipeline procesado y learner\ngraph = pp_penguins %>>% learner2_penguins\n# Representamos el proceso de aprendizaje\ngraph$plot()\n\n\n\n# Generamos el modelo de aprendizaje \ngraph_learner <- GraphLearner$new(graph)\n\nPodemos definir ahora el objeto de remuestreo a partir del modelo de aprendizaje y de los datos originales:\n\nset.seed(135)\nrr = resample(task_penguins, graph_learner, resampling_cv, store_models=TRUE)\n\nINFO  [17:32:29.506] [mlr3] Applying learner 'scale.imputemedian.imputeoor.encode.regr.lm' on task 'data_penguins' (iter 1/5)\nINFO  [17:32:30.683] [mlr3] Applying learner 'scale.imputemedian.imputeoor.encode.regr.lm' on task 'data_penguins' (iter 2/5)\nINFO  [17:32:31.288] [mlr3] Applying learner 'scale.imputemedian.imputeoor.encode.regr.lm' on task 'data_penguins' (iter 3/5)\nINFO  [17:32:31.810] [mlr3] Applying learner 'scale.imputemedian.imputeoor.encode.regr.lm' on task 'data_penguins' (iter 4/5)\nINFO  [17:32:32.476] [mlr3] Applying learner 'scale.imputemedian.imputeoor.encode.regr.lm' on task 'data_penguins' (iter 5/5)\n\nrr\n\n<ResampleResult> with 5 resampling iterations\n       task_id                                  learner_id resampling_id\n data_penguins scale.imputemedian.imputeoor.encode.regr.lm            cv\n data_penguins scale.imputemedian.imputeoor.encode.regr.lm            cv\n data_penguins scale.imputemedian.imputeoor.encode.regr.lm            cv\n data_penguins scale.imputemedian.imputeoor.encode.regr.lm            cv\n data_penguins scale.imputemedian.imputeoor.encode.regr.lm            cv\n iteration warnings errors\n         1        0      0\n         2        0      0\n         3        0      0\n         4        0      0\n         5        0      0\n\n\nAhora podemos obtener la información correspondiente a cada uno de los remuestreos, y más concretamente el score asociado con cada una de ellas para la muestra de validación correspondiente.\n\n# Seleccionamos las columnas de interés\nmse = rr$score(metrica)\nmse[, .(iteration, regr.mse)]\n\n   iteration regr.mse\n1:         1 33.68020\n2:         2 23.40316\n3:         3 27.71888\n4:         4 21.59297\n5:         5 34.29915\n\n\nTodas las muestras producen errores cuadráticos medios muy similares. Evaluamos ahora el valor agregado para todas las submuestras.\n\n# valor agregado\nrr$aggregate(metrica)\n\nregr.mse \n28.13887 \n\n\nEl valor agregado es muy inferior al del modelo basal que estudiamos en puntos anteriores. Podemos utilizar otra métrica (como \\(R^2\\)) para evaluar cada uno de los reumuestreos sin mucha dificultad, ya que tan solo debemos cambiar el score que deseamos utilizar sin necesidad de ejecutar todo el código.\n\n# Seleccionamos las columnas de interés\nrsquared = rr$score(msr(\"regr.rsq\"))\nrsquared[, .(iteration, regr.rsq)]\n\n   iteration  regr.rsq\n1:         1 0.8065669\n2:         2 0.8911105\n3:         3 0.8747264\n4:         4 0.8694097\n5:         5 0.8283350\n\n\nTodos los \\(R^2\\) son superiores al 80% indicando que el modelo tiene un buen comportamiento.\n\n# valor agregado\nrr$aggregate(msr(\"regr.rsq\"))\n\n regr.rsq \n0.8540297 \n\n\nLa variabilidad explicada de la respuesta de forma global para el modelo propuesto alcanza el 85%, lo que indica su buen comportamiento.\n\n\n\n5.2.2.6 Benchmarking\nEl benchmarking o evaluación comparativa en el aprendizaje automático supervisado se refiere a la comparación de diferentes learners o modelos de aprendizaje en una o más tareas. Cuando se comparan varios learners en una sola tarea o en un dominio que consta de varias tareas similares, el objetivo principal suele ser clasificar a los learners según una medida de desempeño predefinida e identificar al mejor para la tarea o dominio considerado. Cuando se comparan varios learners en múltiples tareas, el objetivo principal suele ser más de naturaleza científica, por ejemplo, obtener información sobre cómo se comportan los diferentes modelos de aprendizaje en diferentes situaciones o si existen ciertas propiedades de los datos que afectan en gran medida a su comportamiento.\nEs una práctica habitual que los científicos de datos analicen el rendimiento a la hora de generalizar el funcionamiento de un algoritmo o el tiempo de ejecución del mismo frente a otros propuestos en la literatura con el objeto de mejorar y/o agilizar el aprendizaje propuesto en comparación con los alumnos existentes en un experimento de referencia.\nLos experimentos de benchmarking en mlr3 se llevan a cabo con benchmark(), que simplemente ejecuta resample() en cada tarea y learner por separado, y luego recopila los resultados. La estrategia de remuestreo proporcionada se crea automáticamente en cada tarea para garantizar que todos los learner se comparen con los mismos datos de entrenamiento y prueba.\nPara utilizar la función benchmark() primero llamamos a benchmark_grid(), que construye un diseño exhaustivo para describir todas las combinaciones de learner, tareas y resampling que se utilizarán en un experimento de referencia. A modo de ejemplo, a continuación configuramos un diseño con un bosque aleatorio, un árbol de decisión o un modelo basal sin características funcionan para comparar su comportamiento en dos tareas de clasificación.\n\n# Tareas\ntasks = tsks(c(\"german_credit\", \"sonar\"))\n# Learners\nlearners = lrns(c(\"classif.rpart\", \"classif.ranger\",\n  \"classif.featureless\"), predict_type = \"prob\")\n# Resampling\nrsmp_cv5 = rsmp(\"cv\", folds = 5)\n\n# Grid de combinaciones\ndesign = benchmark_grid(tasks, learners, rsmp_cv5)\nhead(design)\n\n            task             learner resampling\n1: german_credit       classif.rpart         cv\n2: german_credit      classif.ranger         cv\n3: german_credit classif.featureless         cv\n4:         sonar       classif.rpart         cv\n5:         sonar      classif.ranger         cv\n6:         sonar classif.featureless         cv\n\n\nMás adelante veremos el uso de estos procedimientos para determinar el mejor modelo en diferentes para un banco de datos dado.\n\n\n5.2.2.7 Optimización de hiperparámetros\nLos algoritmos de aprendizaje automático suelen incluir parámetros e hiperparámetros. Los parámetros son los coeficientes o pesos del modelo u otra información que determina el algoritmo de aprendizaje en función de los datos de entrenamiento. Por el contrario, los hiperparámetros los configura el usuario y determinan cómo el modelo se ajustará a sus parámetros, es decir, cómo se construye el modelo.\nEl objetivo de la optimización de hiperparámetros (HPO) o ajuste de modelo es encontrar la configuración óptima de hiperparámetros de un algoritmo de aprendizaje automático para una tarea determinada. Para este proceso se sigue un enfoque de optimización de caja negra: se configura un algoritmo de aprendizaje automático con valores elegidos para uno o más hiperparámetros, luego se evalúa este algoritmo (utilizando un método de remuestreo) y se mide su rendimiento. Este proceso se repite con múltiples configuraciones y finalmente se selecciona la configuración con mejor rendimiento. En la figura siguiente se presenta un esquema de dicho proceso:\n\n\n\nOprimización hiperparámetros\n\n\nHPO se relaciona estrechamente con la evaluación del modelo ya que el objetivo es encontrar una configuración de hiperparámetro que optimice el rendimiento. En términos generales, podríamos pensar en encontrar la configuración óptima del modelo de la misma manera que seleccionar un modelo de un experimento de referencia, donde en este caso cada modelo del experimento es el mismo algoritmo pero con diferentes configuraciones de hiperparámetros. Sin embargo, el método de prueba y error humano requiere mucho tiempo, es subjetivo y a menudo sesgado, propenso a errores y computacionalmente ineficiente. En cambio, en las últimas décadas se han desarrollado muchos métodos sofisticados de optimización de hiperparámetros (o “tuners”) para lograr HPO robusto y eficiente. Además de enfoques simples como una búsqueda aleatoria o una búsqueda en cuadrícula, la mayoría de los métodos de optimización de hiperparámetros emplean técnicas iterativas que proponen diferentes configuraciones a lo largo del tiempo, mostrando a menudo un comportamiento adaptativo guiado hacia configuraciones de hiperparámetros potencialmente óptimas. Estos métodos proponen continuamente nuevas configuraciones hasta que se cumple un criterio de finalización, momento en el cual se devuelve la mejor configuración hasta el momento.\nmlr3tuning es el paquete de optimización de hiperparámetros del ecosistema mlr3. En el corazón del paquete están las clases R6:\n\nTuningInstanceSingleCrit, una “instancia” de ajuste que describe el problema de optimización y almacena los resultados; y\nTuner que se utiliza para configurar y ejecutar algoritmos de optimización.\n\nEn temas posteriores veremos como utilizar la optimización de hiperparámetros para cada uno de los algoritmos que iremos presentando. A modo de ejemplo vamos a ver los hiperparámetros configurables en los learner de clasificación y regresión vistos en este capítulo. Ignoramos los modelos basales pues no tienen hiperparámetros. Vamos as obtener un listado con los hiperparámetros disponibles indicando su nombre, clase valor más pequeño y más grande que puede tomar y número de niveles si se tratan de parámetros discretos.\n\n# Parámetros para el algoritmo de clasificación\nas.data.table(lrn(\"classif.rpart\")$param_set)[,\n  .(id, class, lower, upper, nlevels)]\n\n                id    class lower upper nlevels\n 1:             cp ParamDbl     0     1     Inf\n 2:     keep_model ParamLgl    NA    NA       2\n 3:     maxcompete ParamInt     0   Inf     Inf\n 4:       maxdepth ParamInt     1    30      30\n 5:   maxsurrogate ParamInt     0   Inf     Inf\n 6:      minbucket ParamInt     1   Inf     Inf\n 7:       minsplit ParamInt     1   Inf     Inf\n 8: surrogatestyle ParamInt     0     1       2\n 9:   usesurrogate ParamInt     0     2       3\n10:           xval ParamInt     0   Inf     Inf\n\n\nAhora con el modelo para la tarea de regresión:\n\n# Parámetros para el algoritmo de clasificación\nas.data.table(lrn(\"regr.lm\")$param_set)[,\n  .(id, class, lower, upper, nlevels)]\n\n             id    class lower upper nlevels\n 1:          df ParamDbl  -Inf   Inf     Inf\n 2:    interval ParamFct    NA    NA       3\n 3:       level ParamDbl  -Inf   Inf     Inf\n 4:       model ParamLgl    NA    NA       2\n 5:      offset ParamLgl    NA    NA       2\n 6:    pred.var ParamUty    NA    NA     Inf\n 7:          qr ParamLgl    NA    NA       2\n 8:       scale ParamDbl  -Inf   Inf     Inf\n 9: singular.ok ParamLgl    NA    NA       2\n10:           x ParamLgl    NA    NA       2\n11:           y ParamLgl    NA    NA       2\n\n\nMás adelante estudiaremos con detalle cada uno de los hiperparámetros vinculados con un modelo de aprendizaje y veremos cuales de ellos es necesario optimizar y cuales no. Estos últimos proporcionan información sobre el ajuste del modelo pero no controlan el proceso de entrenamiento en si. Hay que tener en cuenta que el número de hiperparámetros suele ser elevado y no resulta práctico llevar una búsqueda óptima sobre todos ellos a la vez."
  },
  {
    "objectID": "50_AED.html#sec-50.3",
    "href": "50_AED.html#sec-50.3",
    "title": "5  Introducción al análisis de datos",
    "section": "5.3 Consideraciones finales",
    "text": "5.3 Consideraciones finales\nuna vez hemos visto los pasos a seguir en el establecimiento de un modelo de aprendizaje automático ha llegado el momento de pasar a describir los algoritmos más habituales tanto en el aprendizaje supervisado como el no supervisado.\n\n\n\n\nCui, Boxuan. 2020. DataExplorer: Automate Data Exploration and Treatment. https://CRAN.R-project.org/package=DataExplorer.\n\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903.\n\n\nLang, Michel, and Patrick Schratz. 2023. Mlr3verse: Easily Install and Load the ’Mlr3’ Package Family.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally: Extension to ’Ggplot2’. https://CRAN.R-project.org/package=GGally.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr."
  },
  {
    "objectID": "03_SupervisedAA.html#modelos-de-regresión",
    "href": "03_SupervisedAA.html#modelos-de-regresión",
    "title": "Parte 3. Aprendizaje supervisado",
    "section": "Modelos de regresión",
    "text": "Modelos de regresión\nLos modelos de regresión más habituales son:\n\nRegresión lineal donde se resuelve la estimación de los parámetros o coeficientes que relacionan de modo lineal las características disponibles con la respuesta de tipo numérico. Los resultados -coeficientes estimados- explican el grado en que cada una de las variables predictoras contribuye a explicar la respuesta, o lo que es lo mismo, el grado de asociación lineal con ella. Los modelos de regresión son más rápidos de entrenar que otros modelos de aprendizaje automático, si bien son sensibles a valores atípicos, no dan buenos resultados si la relación entre predictores y respuesta no es de tipo lineal, y las predicciones no son buenas cuando hay pocos datos y muchos predictores. Aplicaciones populares de este tipo de modelo son la predicción del precio de la vivienda, la estatura de un adulto, la esperanza de vida, …, si bien se utiliza en infinidad de contextos.\nRegresión logística. Se modela una relación lineal entre los predictores o entradas y una transformación logit de la probabilidad de clasificar a un sujeto de la muestra en una de dos categorías posibles (identificadas 0/1), para predecir dicha probabilidad. Es interpretable, en la medida en que los parámetros o coeficientes estimados explican el peso que tiene cada característica sobre la probabilidad de clasificación. Se puede generalizar a modelos de clasificación con más categorías de respuesta. Como desventajas tiene las mismas que el modelo de regresión lineal: falla cuando falta linealidad con los predictores y con bancos de datos con pocas muestras y muchos predictores.\n\nOtros modelos de regresión que mejoran ciertas deficiencias de los anteriores son la regresión ridge y la regresión lasso."
  },
  {
    "objectID": "03_SupervisedAA.html#modelos-de-clasificación",
    "href": "03_SupervisedAA.html#modelos-de-clasificación",
    "title": "Parte 3. Aprendizaje supervisado",
    "section": "Modelos de clasificación",
    "text": "Modelos de clasificación\nSon modelos específicos de clasificación el modelo Naïve Bayes, muy utilizado en aplicaciones reales, y el modelo de K vecinos más cercanos.\n\nLos clasificadores Naïve Bayes son los primeros algoritmos de clasificación que se estudian habitualmente, puesto que se utilizan en muchas ocasiones como modelo de base o partida en los problemas de clasificación, dado que son extremadamente rápidos y sencillos y suelen ser adecuados para conjuntos de datos de muy alta dimensión.\nEl algoritmo de K vecinos más cercanos o (K-Nearest Neighbors (KNN)) es un clasificador que utiliza la proximidad entre las observaciones en las variables disponibles, para hacer clasificaciones o predecir agrupaciones de datos. Se caracteriza por ser fácil de aplicar sin necesidad de crear un modelo, configurar parámetros o formular hipótesis suplementarias. Como desventajas destacamos que el algoritmo se ralentiza cuando aumenta el número de datos y/o el de variables."
  },
  {
    "objectID": "03_SupervisedAA.html#modelos-de-clasificación-y-regresión",
    "href": "03_SupervisedAA.html#modelos-de-clasificación-y-regresión",
    "title": "Parte 3. Aprendizaje supervisado",
    "section": "Modelos de clasificación y regresión",
    "text": "Modelos de clasificación y regresión\nLas técnicas que se pueden aplicar tanto en tareas de clasificación como de regresión son:\n\nLas máquinas de vectores de soporte o Support Vector Machines (SVM) construyen un hiperplano en un espacio multidimensional para separar las observaciones en distintas clases, de modo que el hiperplano maximiza el margen entre los puntos en clases distintas. Genera también dimensiones adicionales a través de núcleos o kernels. Ofrecen una buena precisión y realizan predicciones más rápidas que el algorimo Naïve Bayes, y también utilizan menos memoria. Son muy versátiles para un buen número de problemas diversos, y en espacios dimensionales elevados (con muchas variables). No propociona buenos resultados sin embargo para grandes conjuntos de datos, tampoco con clases superpuestas, y es sensible al tipo de núcleo utilizado.\nLos árboles de decisión (Decision Trees) están basados en aplicar secuencialmente reglas de decisión sobre las características disponibles, para seccionar categorías o segmentos y producir predicciones. Entre las ventajas encontramos que el resultado es explicable e interpretable y que se puede utilizar con valores faltantes. Como desventajas destacamos que es sensible a los valores atípicos y que es propenso al sobreajuste. Aplicaciones de esta técnica son la creación de perfiles de clientes, predicción de pérdidas en carteras de seguros, etc.\nModelos de conjunto (Ensemble models) combinan múltiples modelos en uno nuevo con el objetivo de lograr un equilibro entre sesgo y varianza, tratando de obtener mejores predicciones o clasificaciones que cualquiera de los modelos individuales originales. En la práctica existen dos metodologías para la obtención de modelo de conjunto:\n\nMétodo a partir de modelos individuales independientes. Es la conocida como metodología Bagging, y en ella se ajustan múltiples modelos, cada uno con un subconjunto distinto de los datos de entrenamiento. En esta situación los modelos que forman el agregado participan aportando de forma individual su predicción o clasificación. Como valor final, se toma la media de todas las predicciones de los modelos individuales si estamos en un problema de regresión o la clase más frecuente del conjunto de soluciones aportadas por todos los clasificadores individuales. Los algoritmos más habituales dentro de este grupo son los de voto por mayoría, bosques aleatorios, y clasificadores Bagging.\nMétodo a partir de modelos secuenciales. El boosting es una técnica de modelado de conjunto que intenta construir un strong learner a partir de un número de weak learner secuenciales, todos basados en el mismo algoritmo de predicción o clasificación. El proceso de construcción del strong learner comienza fijando un modelo inicial sobre los datos de entrenamiento y obteniendo los errores de dicho modelo (errores de predicción o clasificación). A continuación, se construye un segundo modelo que intenta corregir los errores presentes en el primer modelo mediante la asignación de pesos a todos los datos de entrenamiento en función del error cometido en la primera etapa. Este procedimiento continúa y se añaden modelos hasta que se predice correctamente todo el conjunto de datos de entrenamiento o se añade el máximo número de modelos. Finalmente se combinan los resultados de los diferentes modelos secuenciales para obtener el modelo final. Los métodos de boosting más empleados son AdaBoost, Gradient Boosting, XGBoost y LightGBM. Casi todos ellos toman como weak learner basado en árboles de decisión, pero en teoría se pueden utilizar con otro tipo de algoritmos de aprendizaje automático."
  },
  {
    "objectID": "60_RegressionModels.html#sec-60.2",
    "href": "60_RegressionModels.html#sec-60.2",
    "title": "6  Modelos de Regresión",
    "section": "6.1 Modelos de Regresión Lineal",
    "text": "6.1 Modelos de Regresión Lineal\nA continuación se presentan brevemente los conceptos teóricos más relevantes de los modelos lineales de regresión con respuesta numérica.\n\n6.1.1 Modelo de regresión Lineal Simple (RLS)\nPara ilustrar las características de este modelo de aprendizaje comenzamos con el modelo más sencillo en el que únicamente consideramos una variable predictora de tipo numérico (\\(x\\)), conocido como modelo de regresión lineal simple y que se expresa matemáticamente como:\n\\[y = w_0 + w_1x + \\epsilon, \\tag{6.1}\\]\ndonde:\n\n\\(w_0\\) se conoce como interceptación o sesgo y representa el valor medio de la respuesta cuando la variable predictora no tiene influencia sobre ella, y se utiliza como modelo de partida.\n\\(w_1\\) se conoce como pendiente y representa la variación de la respuesta al aumentar en una unidad el valor de la predictora, es decir, el cambio que sufre \\(y\\) cuando pasamos de \\(x\\) a \\(x+1\\), de forma que el valor de \\(w_1\\) determina si la relación entre \\(x\\) e \\(y\\) es directa (valor positivo) o inversa (valor negativo).\n\\(\\epsilon\\) se conoce como error aleatorio y representa la diferencia entre el valor observado de la respuesta y el valor predicho por el modelo, teniendo en cuenta que los valores de la respuesta para un mismo valor de la predictora pueden ser diferentes debido a otras variables que no aparecen en nuestro modelo de aprendizaje.\n\nTanto \\(w_0\\) como \\(w_1\\) son las cantidades desconocidas que el algoritmo debe estimar para alcanzar el modelo de predicción:\n\\[\\hat{y} = \\hat{w_0} + \\hat{w_1}x, \\tag{6.2}\\]\ndonde el símbolo \\(^\\) indica los valores estimados mediante el modelo de \\(w_0\\) y \\(w_1\\), mientras que el error estimado viene dado por la diferencia entre el valor observado y el predicho por el modelo:\n\\[\\hat{\\epsilon} = y - \\hat{y}. \\tag{6.3}\\]\nEs evidente que cuanto menor sea el error estimado mejor será nuestro modelo, ya que más cerca estará el valor predicho del valor real observado. De hecho, las funciones de pérdida para este tipo de modelos se basan en estimar los valores de \\(w_0\\) y \\(w_1\\) de forma que el error cometido sea lo más pequeño posible.\nDe forma general, el modelo anterior se suele expresar en notación matricial como:\n\\[y = Xw + \\epsilon \\tag{6.4}\\]\ndonde la primera columna de \\(X\\) es toda de unos para representar el efecto asociado a \\(w_0\\), de forma que en realidad la matriz \\(X\\) viene dada por \\((1| x)\\), es decir una matriz con dos columnas.\nDado un conjunto de datos \\(\\{(y_i, x_i)\\}_{i=1}^n\\), la función de pérdida habitual es el error cuadrático medio definido como:\n\\[\\frac{1}{n}\\sum_{i_1}^n (y_i - \\hat{y}_i)^2 \\tag{6.5}\\] de donde se pueden obtener los valores estimados de \\(w_0\\) y \\(w_1\\) minimizando dicha cantidad para el rango de valores posibles de ambos parámetros. Gráficamente tenemos:\n\ndonde los puntos representan los pares de puntos \\((x_i, y_i)\\), la línea roja sería el modelo de regresión estimado, y la líneas verticales negras representan el error cometido por el modelo para cada muestra del conjunto de datos. Se trata pues de obtener el modelo que minimice el error conjunto para todos los elementos de la muestra.\nLa solución para \\(w = (w_0, w_1)\\) se obtiene entonces como el cuadrado de la norma 2 sobre los errores:\n\\[\\underset{w}{min} ||y-Xw||_2^2, \\tag{6.6}\\]\nque es la forma matemática de la minimización del error cuadrático medio.\n\n\n6.1.2 Modelo Lineal General (MLG)\nEl Modelo Lineal General es una generalización del modelo anterior donde se dispone de una variable respuesta (\\(y\\)) de tipo numérico y una matriz de variables predictoras de tipo numérico y/o categórico, a partir del cual podemos obtener la matriz \\(X\\) mediante la codificación de las variables de tipo categórico junto las variables de tipo numérico. Si disponemos de \\(p\\) posibles predictoras el conjunto de datos\n\\[\\{(y_i, x_{1i},...x_{pi})\\}_{i=1}^n \\tag{6.7}\\]\nnos proporciona los datos para la obtención de este modelo donde \\(x_{ji}\\) es el valor de la muestra \\(i\\) en la predictora \\(j\\), de forma que la ecuación del Modelo Lineal General viene dada por:\n\\[y = w_0 + w_1X_1+...+w_pX_p + \\epsilon \\tag{6.8}\\]\ndonde cada \\(w_j\\) representa la pendiente o variación de la respuesta con respecto a cada predictora, y \\(w_0\\) representa el sesgo del modelo.\nComo en el caso anterior el ajuste de este modelo se basa en obtener los valores de \\(\\hat{w}_0, \\hat{w}_1,...,\\hat{w}_p\\) que minimicen el error cuadrático medio de los errores del modelo obtenidos como:\n\\[y - \\hat{y} = y -  \\hat{w}_0 + \\hat{w}_1X_1+...+\\hat{w}_pX_p \\tag{6.9}\\]\nDe forma general, el modelo anterior se suele expresar en notación matricial como:\n\\[y = Xw + \\epsilon \\tag{6.10}\\]\ndonde la primera columna de \\(X\\) es toda de unos para representar el efecto asociado a \\(w_0\\), de forma que en realidad la matriz \\(X\\) viene dada por \\((1| X_1,...,X_p)\\), es decir una matriz con \\(p+1\\) columnas.\nAl igual que en el caso más simple, la solución de \\(w = (w_0, w_1,...,w_p)\\) se obtiene mediante la expresión:\n\\[\\underset{w}{min} ||y-Xw||_2^2 \\tag{6.11}\\]\nComo veremos más adelante, uno de los principales problemas que nos encontramos con este tipo de modelos es determinar que subconjunto de \\(X\\) es relevante para explicar el comportamiento de la respuesta. Dado que cada predictora numérica puede estar medida en escalas diferentes, es necesario expresar dichas variables en escala estandarizada para determinar la relevancia de cada una de ellas sobre la respuesta, ya que de lo contrario los pesos \\(w_j\\) no son comparables para dos predictoras medidas en escalas distintas.\nEl procedimiento habitual es corregir cada variable por su media y dividir por la norma 2 de sus valores para que las nuevas variables tengan media cero y varianza similar. Si todas las predictoras son de tipo numérico el modelo lineal se expresa como:\n\\[y' = w'_0 + w'_1X'_1+...+w'_pX'_p+ \\epsilon \\tag{6.12}\\]\ndonde \\(y', X'_1,...,X'_p\\) son las variables normalizadas y los coeficientes \\(w'_j\\) ahora si son comparables para el conjunto de predictoras, de forma que los valores más grandes en valor absoluto indican mayor peso sobre la respuesta, mientras que el signo del coeficiente indica si la relación es directa o inversa con la respuesta."
  },
  {
    "objectID": "60_RegressionModels.html#sec-60.3",
    "href": "60_RegressionModels.html#sec-60.3",
    "title": "6  Modelos de Regresión",
    "section": "6.2 Modelo Lineal General con mlr3",
    "text": "6.2 Modelo Lineal General con mlr3\nDe los diferentes learner con los que se puede obtener un modelo lineal general nos vamos a centrar por le momento en el más sencillo de todos que es el regr.lm. Este algoritmo asume que el target es de tipo numérico y todos los efectos que pueden afectarlo se pueden describir mediante una ecuación del tipo @eq-rm012. Podemos acceder tanto a los parámetros como los métodos disponibles para este algoritmo en este enlace.\nEn el punto siguiente estudiamos como especificar la ecuación de este tipo de modelos cuando las características que utilizamos para predecir la respuesta son numéricas y/o factores. Algo muy importante a tener en cuenta en este tipo de modelos es que no resulta necesaria la codificación de los factores en el preprocesado, ya que el algoritmo lleva a cabo su propia codificación cuando detecta que una de las predictoras es un factor. Esto facilita esta tarea y nos permite escribir modelos más complejos sin tener que especificar la codificación. En R la codificación de factores se realiza fijando una categoría como referencia y comparando el resto frente a ella. Por defecto se utiliza contr.treatment(). En los ejemplos que trataremos veremos como interpretar los resultados del modelo teniendo en cuenta dicha codificación.\n\n6.2.1 Especificación del modelo\nPara especificar un modelo de este tipo utiliza la formulación habitual de R. Veamos diferentes situaciones en modelos muy sencillos donde tenemos un target numérico \\(Y\\), y que podremos generalizar en situaciones más complejas más tarde:\n\nUna predictora de tipo numérico (\\(X_1\\)). En este caso la ecuación del modelo viene dada por:\n\n\\[Y \\sim X_1\\]\ndonde \\(X_1\\) representa el efecto de regresión asociado con ella, es decir, la respuesta y \\(X_1\\) están relacionadas mediante una recta con una pendiente que deberemos estimar. Este es el denominado modelo de regresión lineal simple.\n\nDos predictoras de tipo numérico (\\(X_1\\) y \\(X_2\\)). En este caso la ecuación del modelo viene dada por:\n\n\\[Y \\sim X_1 + X_2\\] donde \\(X_1\\) y \\(X_2\\) representan el efecto de regresión asociado con cada una de las variable numéricas, es decir, la respuesta y cada una de ellas están relacionadas mediante una recta con una pendiente que deberemos estimar. Este es el denominado modelo de regresión lineal múltiple.\n\nUna predictora de tipo categórico (\\(F_1\\)). En este caso la ecuación del modelo viene dada por:\n\n\\[Y \\sim F_1\\] donde \\(F_1\\) representa el efecto del factor que en este tipo de modelos lo que hace es comparar la media de la respuesta para cada uno de los niveles del factor considerado. En este caso no estimamos una pendiente sino una media directamente, es decir, es un modelo para la comparación de medias. Este es el denominado modelo de anova de una vía.\n\nDos predictoras de tipo categórico (\\(F_1\\) y \\(F_2\\)). En este caso la ecuación del modelo viene dada por:\n\n\\[Y \\sim F_1 + F_2 + F_1:F_2\\] donde \\(F_1\\) y \\(F_2\\) representan los denominados efectos principales de los factores, es decir la comparación de medias de los nivel de cada factor (de forma independiente). Por otro lado, \\(F_1:F_2\\) representa el efecto de interacción entre ambos factores, es decir, comparamos las medias de la respuesta para las diferentes combinaciones de los niveles de ambos factores. Este es el denominado modelo anova de dos vías.\n\nUna predictora de tipo numérico (\\(X_1\\)) y otra de tipo categórico (\\(F_1\\)). En este caso la ecuación del modelo viene dada por:\n\n\\[Y \\sim X_1 + F_1 + X_1:F_1\\]\ndonde \\(X_1\\) representa el efecto de regresión asociado con la variable numérica, es decir, la respuesta y \\(X_1\\) están relacionadas mediante una única pendiente que deberemos estimar. \\(F_1\\) representa el efecto del factor, es decir, comparamos si las medias de la respuesta para cada grupo pueden considerarse iguales. \\(X_1:F_1\\) representa el efecto de interacción entre predictoras, es decir, que la respuesta se relaciona con la predictora numérica mediante tantas curvas (generalmente líneas) como niveles tenga el factor \\(F_1\\). Este es el denominado modelo ancova con una vía de clasificación.\nEsto último modelo es el más habitual ya que generalmente en las predictoras podemos tener variables de diferentes tipos. Modelos que generalizan este último podrían ser:\n\nModelo para dos factores (\\(F_1\\), \\(F_2\\)) y una numérica (\\(X_1\\)):\n\n\\[Y \\sim F_1 + F_2 + F_1:F:2 + X_1 + F_1:X_1 + F_2:X_1 + F_1:F:2:X_1\\] o de forma resumida \\[Y \\sim F_1*F_2*X_1\\]\n\nModelo para un factor (\\(F_1\\)) y dos numéricas (\\(X_1\\), \\(X_2\\)):\n\n\\[Y \\sim F_1 + X_1 + X_2 + F_1:X_1 + F_1:X_2\\] o de forma resumida \\[Y \\sim F_1*(X_1 + X_2)\\]\n\nModelo para dos factores (\\(F_1\\), \\(F_2\\)) y dos numéricas (\\(X_1\\), \\(X_2\\)):\n\n\\[Y \\sim F_1 * F_2 *(X_1 + X_2)\\]\nComo se puede ver la complejidad del modelo aumenta sustancialmente con la consideración de más variables predictoras. En cualquier caso estas son las ecuaciones de os denominados modelos saturados, es decir, el más complejo que se puede establecer a partir de la información recogida. Como veremos en diferentes situaciones prácticas en ocasiones resulta más sencillo considerar modelos más sencillos que conocemos como modelos anidados. Para determinar el modelo de partida resulta obligatorio realizar un análisis descriptivo gráfico que nos permita establecer tendencias (gráficos de dispersión) o diferencias entre medias (gráficos de cajas).\n\n\n\n\n\n\nSi el modelo considerado no contiene efectos de interacción la especificación del modelo saturado es automática y no hace falta escribir la ecuación correspondiente. Si se consideran interacciones es necesario escribir la ecuación del modelo para proceder con su estimación.\n\n\n\n\n\n6.2.2 Bancos de datos\nPara ejemplificar el uso de estos modelos vamos a utilizar los bancos de datos Diabetes (4.2.2), Meat spec (4.2.5), Electricity (4.2.3), y Housing in California (4.2.4) que presentamos en el capítulo 4.\nA continuación se muestra el código para la carga de los diferentes bancos de datos y la creación de la correspondiente task de regresión para cada uno de ellos. A continuación definiremos los modelos saturados (cuando sea necesario) para cada uno de esos bancos de datos.\n\n6.2.2.1 Diabetes\nEn un estudio sobre la diabetes se obtuvieron diez variables basales, edad, sexo, índice de masa corporal, presión arterial media y seis mediciones de suero sanguíneo para 442 pacientes diabéticos, así como la respuesta de interés, una medida cuantitativa de la progresión de la enfermedad un año después de la línea de base. Cada una de estas 10 variables predictoras se ha centrado en la media y se ha escalado por la desviación estándar multiplicada por la raíz cuadrada de n_muestras (es decir, la suma de los cuadrados de cada columna suma 1).\nEl target viene identificado con \\(Y\\) (progresión de la enfermedad) y las posibles predictoras como AGE, SEX, BMI, BP, S1, S2, S3, S4, S5, S6.\n\n# Carga de datos\ndiabetes = read_rds(\"diabetes.rds\")\n# Creación de task\ntsk_diabetes = as_task_regr(diabetes, target = \"Y\")\n# información de la tarea\nprint(tsk_diabetes)\n\n<TaskRegr:diabetes> (442 x 11)\n* Target: Y\n* Properties: -\n* Features (10):\n  - dbl (9): AGE, BMI, BP, S1, S2, S3, S4, S5, S6\n  - fct (1): SEX\n\n\nVeamos la representación gráfica de la información contenida en el banco de datos.\n\nautoplot(tsk_diabetes, type =\"pairs\")\n\n\n\n\nFigura 6.1: Autoplot pairs para Diabetes\n\n\n\n\nEn la primera fila de la matriz podemos ver los valores de correlación de la respuesta con cada una de las predictoras numéricas, así como el gráfico de cajas con respecto a las predictoras categóricas. Aunque los coeficientes de correlación resultan significativos también es cierto que los valores obtenidos son bastante bajos salvo para BMI y S5. Tampoco parece apreciarse diferencias entre los niveles de SEX para la respuesta.\nEn la primera columna podemos ver los gráficos de dispersión entre predictoras y respuesta. Dado el elevado número de puntos no se aprecian tendencias en el comportamiento. Vamos a estudiar con algo más de detalle los gráficos de dispersión para las variables con mayores valores del coeficiente de correlación.\n\ng1 <- ggplot(tsk_diabetes$data(), aes(x = BMI, y= Y)) + geom_point() \ng2 <- ggplot(tsk_diabetes$data(), aes(x = S5, y= Y)) + geom_point()\nggarrange(g1, g2, nrow = 1)\n\n\n\n\nFigura 6.2: Diabetes BMI, SS vs Y\n\n\n\n\nSe aprecia cierta tendencia creciente en ambos gráficos, es decir, conforme aumentan los valores de la predictora aumenta el valor de la respuesta, pero como indicaban los valores de correlación esta no es muy elevada. Verificamos ahora que no tenemos valores perdidos en el conjunto de datos:\n\ntsk_diabetes$missings()\n\n  Y AGE BMI  BP  S1  S2  S3  S4  S5  S6 SEX \n  0   0   0   0   0   0   0   0   0   0   0 \n\n\nDado que no hay missings podemos pasar a especificar el modelo saturado que utilizaremos en el punto siguiente.\n\n# Modelo saturado para diabetes\ndiabetes_model = po(\"modelmatrix\", formula = ~ (AGE + BMI + BP + S1 + S2 + S3 + S4 + S5 + S6)*SEX)\n\n\n\n6.2.2.2 Meat spec\nEl departamento de calidad de una empresa de alimentación se encarga de medir el contenido en grasa de la carne que comercializa. Este estudio se realiza mediante técnicas de analítica química, un proceso relativamente costoso en tiempo y recursos. Una alternativa que permitiría reducir costes y optimizar tiempo es emplear un espectrofotómetro (instrumento capaz de detectar la absorbancia que tiene un material a diferentes tipos de luz en función de sus características) e inferir el contenido en grasa a partir de sus medidas. Por lo tanto, el objetivo que se persigue es predecir el contenido en grasa (fat) a partir de los valores dados por el espectrofotómetro.El resto de variables almacenan los valores de los 100 puntos del espectrofómetro para cada una de las muestras.\nEn este caso tampoco tenemos valores perdidos pero no resulta útil la representación gráfica de los datos debido al alto número de predictoras. Por el momento creamos únicamente la tara de interés.\n\n# Carga de datos\nmeatspec = read_rds(\"meatspec.rds\")\nmeatspec = meatspec[,-1]\n# Creación de task\ntsk_meatspec = as_task_regr(meatspec, target = \"fat\")\n# información de la tarea\nprint(tsk_meatspec)\n\n<TaskRegr:meatspec> (215 x 101)\n* Target: fat\n* Properties: -\n* Features (100):\n  - dbl (100): V001, V002, V003, V004, V005, V006, V007, V008, V009,\n    V010, V011, V012, V013, V014, V015, V016, V017, V018, V019, V020,\n    V021, V022, V023, V024, V025, V026, V027, V028, V029, V030, V031,\n    V032, V033, V034, V035, V036, V037, V038, V039, V040, V041, V042,\n    V043, V044, V045, V046, V047, V048, V049, V050, V051, V052, V053,\n    V054, V055, V056, V057, V058, V059, V060, V061, V062, V063, V064,\n    V065, V066, V067, V068, V069, V070, V071, V072, V073, V074, V075,\n    V076, V077, V078, V079, V080, V081, V082, V083, V084, V085, V086,\n    V087, V088, V089, V090, V091, V092, V093, V094, V095, V096, V097,\n    V098, V099, V100\n\n\nDado que no tenemos valores perdidos la única tarea de preprocesamiento es la estandarización de la variables numéricas. Definimos el pipeops necesario.\n\n# Objeto preprocesado\npp_meatspec = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n\nDado que todas las predictoras son de tipo numérico no existen interacciones y no hace falta especificar el modelo.\n\n\n6.2.2.3 Electricity\nUna central eléctrica de ciclo combinado (CCPP) está compuesta por turbinas de gas (GT), turbinas de vapor (ST) y generadores de vapor de recuperación de calor. En una CCPP, la electricidad es generada por turbinas de gas y vapor, que se combinan en un ciclo, y se transfiere de una turbina a otra. Mientras que el Vacío se recolecta y tiene efecto sobre la Turbina de Vapor, las otras tres variables ambientales afectan el desempeño del GT. Este conjunto de de datos recogidos se ha recogido de una central eléctrica de ciclo combinado a lo largo de 6 años (2006-2011), cuando la central eléctrica estaba configurada para funcionar a plena carga. Las características consisten en variables ambientales promedio por hora: Temperatura (AT), Presión ambiental (AP), Humedad relativa (RH) y Vacío de escape (V) para predecir la producción de energía eléctrica neta por hora (PE) de la planta.\nCreamos la tarea asociada, evaluamos la presencia de valores perdidos y representamos gráficamente la información contenida.\n\n# Carga de datos\nelectricity = read_rds(\"electricity.rds\")\n# Creación de task\ntsk_electricity = as_task_regr(electricity, target = \"PE\")\n# información de la tarea\nprint(tsk_electricity)\n\n<TaskRegr:electricity> (9568 x 5)\n* Target: PE\n* Properties: -\n* Features (4):\n  - dbl (4): AP, AT, RH, V\n\n\nEvaluamos la presencia de missings.\n\n# Creación de task\ntsk_electricity$missings()\n\nPE AP AT RH  V \n 0  0  0  0  0 \n\n\nNo hay missings. Realizamos el análisis gráfico.\n\nautoplot(tsk_electricity, type =\"pairs\")\n\n\n\n\nFigura 6.3: Autoplot pairs Electricity\n\n\n\n\nPodemos ver que hay correlaciones bastante altas entre AT y V con PE, aunque solo parece que el comportamiento lineal esta asociado con AT, mientras que con V la tendencia tiene una forma curvilínea. Definimos el objeto de preprocesado asociado con la estandarización de las predictoras.\n\n# Objeto preprocesado\npp_electricity = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n\nComo en el caso anterior no resulta necesario explicitar el modelo, ya que todas las predictoras son de tipo numérico.\n\n\n6.2.2.4 Housing in California\nEn este ejemplo vamos a utilizar la base de datos HousingCA, que recoge la información sobre el censo viviendas de California en el año 1990. Se está interesado en predecir el valor medio de la vivienda (median_house_value) medido en dólares americanos en función de las predictoras. Cargamos los datos y valoramos la presencia de missings.\n\n# Carga de datos\nhousingCA = read_rds(\"housingCA.rds\")\n# Creación de task\ntsk_housing = as_task_regr(housingCA, target = \"median_house_value\")\n# información de la tarea\nprint(tsk_housing)\n\n<TaskRegr:housingCA> (20640 x 10)\n* Target: median_house_value\n* Properties: -\n* Features (9):\n  - dbl (8): households, housing_median_age, latitude, longitude,\n    median_income, population, total_bedrooms, total_rooms\n  - fct (1): ocean_proximity\n\n\nEvaluamos la presencia de missings.\n\ntsk_housing$missings()\n\nmedian_house_value         households housing_median_age           latitude \n                 0                  0                  0                  0 \n         longitude      median_income    ocean_proximity         population \n                 0                  0                  0                  0 \n    total_bedrooms        total_rooms \n               207                  0 \n\n\nLa única característica donde aparecen valores perdidos es total_bedrooms. Podemos definir un objeto de preprocesado donde imputemos los valores missings para esa variable y a su vez estandarizamos los valores de las características numéricas. Como método de imputación utilizaremos la mediana de los valores de la característica.\n\npp_housing = \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) \n\nRealizamos el análisis descriptivo gráfico del conjunto de datos ignorando por el momento la presencia de valores perdidos.\n\nautoplot(tsk_housing, type =\"pairs\")\n\n\n\n\nFigura 6.4: Autoplot pairs Housing In California\n\n\n\n\nTan solo se aprecia una correlación alta entre la respuesta y median_income, mientras que para el resto todas las correlaciones son bastante bajas. Además no se aprecia ninguna tendencia clara en los gráficos de dispersión. Sin embargo, si podemos ver ciertas diferencias en el gráfico de cajas de la proximidad al océano.\nComo en este caso si disponemos de predictoras categóricas resulta necesario especificar el modelo saturado para tener en cuenta las interacciones entre las predictoras numéricas y el factor.\n\n# Modelo saturado para housing\nhousing_model = po(\"modelmatrix\", formula = ~ (households + housing_median_age + latitude + longitude + median_income + population + total_bedrooms + total_rooms)*ocean_proximity)\n\n\n\n\n6.2.3 Nuestros primeros modelos\nEn este punto presentamos los modelos saturados para cada uno de los bancos de datos de ejemplos que hemos presentado en el punto anterior. Para los primeros resultados utilizaremos una división de muestras 80% - 20% (entrenamiento - validación), para determinar las predictoras más relevantes, y más tarde realizaremos un estudio de estabilidad de la solución mediante validación cruzada.\n\n6.2.3.1 Diabetes\nEn este caso no tenemos tareas de preprocesado y nos vamos a centrar directamente en el establecimiento del proceso de aprendizaje a partir del modelo saturado que hemos definido en el punto anterior. La mayor dificultad con el algoritmo regr.lm es que no nos permite realzar un proceso de selección de variables de forma automática. La única opción que tenemos es definir un filter adecuado y obtener los valores para la muestra de entrenamiento. Sin embargo, en este tipo de modelos esta estrategia no es óptima dado que los filter no nos permiten evaluar directamente los efectos de interacción entre predictoras numéricas y factores.\nA continuación vamos a presentar un procedimiento para seleccionar los efectos relevantes el modelo basados en el estadístico AIC, y más tarde automatizaremos ese modelo para realizar un estudio de validación de la solución obtenida. En primer lugar obtenemos la muestra de entrenamiento y validación que utilizaremos para la valoración de efectos.\n\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_diabetes, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_diabetes = tsk_diabetes$clone()$filter(splits$train)\ntsk_test_diabetes  = tsk_diabetes$clone()$filter(splits$test)\n\nPara el ajuste del modelo lineal hacemos uso de la función lm() en la que únicamente debemos identificar el modelo deseado (en este caso el saturado), y el conjunto de datos sobre el que vamos a ajustar dicho modelo. A continuación utilizamos la función step() de la librería stats que toma el modelo saturado y mediante una búsqueda adelante-atrás determina el conjunto de efectos que más influye en la respuesta mediante el estadístico AIC. Este estadístico selecciona como mejor modelo aquel con un menor valor de AIC de todos los que se pueden construir al ir eliminando efectos en el modelo saturado. Por último utilizamos la función summary() para ver el modelo resultante (coeficientes, capacidad explicativa,…).\n\nfit = lm(Y ~ (AGE + BMI + BP + S1 + S2 + S3 + S4 + S5 + S6)*SEX, data = tsk_train_diabetes$data())\nfit = stats::step(fit)\n\nStart:  AIC=2821.13\nY ~ (AGE + BMI + BP + S1 + S2 + S3 + S4 + S5 + S6) * SEX\n\n          Df Sum of Sq    RSS    AIC\n- BP:SEX   1     453.6 932397 2819.3\n- S3:SEX   1     544.9 932489 2819.3\n- S4:SEX   1     664.5 932608 2819.4\n- S1:SEX   1    1784.0 933728 2819.8\n- S5:SEX   1    2167.4 934111 2819.9\n- S2:SEX   1    2592.7 934536 2820.1\n<none>                 931944 2821.1\n- BMI:SEX  1    5578.9 937523 2821.2\n- S6:SEX   1    7848.9 939793 2822.1\n- AGE:SEX  1   20919.9 952864 2827.0\n\nStep:  AIC=2819.3\nY ~ AGE + BMI + BP + S1 + S2 + S3 + S4 + S5 + S6 + SEX + AGE:SEX + \n    BMI:SEX + S1:SEX + S2:SEX + S3:SEX + S4:SEX + S5:SEX + S6:SEX\n\n          Df Sum of Sq    RSS    AIC\n- S3:SEX   1       564 932961 2817.5\n- S4:SEX   1       728 933125 2817.6\n- S1:SEX   1      1828 934225 2818.0\n- S5:SEX   1      2051 934448 2818.1\n- S2:SEX   1      2620 935018 2818.3\n<none>                 932397 2819.3\n- BMI:SEX  1      6940 939338 2819.9\n- S6:SEX   1      8548 940945 2820.5\n- AGE:SEX  1     22713 955110 2825.8\n- BP       1     65680 998077 2841.3\n\nStep:  AIC=2817.52\nY ~ AGE + BMI + BP + S1 + S2 + S3 + S4 + S5 + S6 + SEX + AGE:SEX + \n    BMI:SEX + S1:SEX + S2:SEX + S4:SEX + S5:SEX + S6:SEX\n\n          Df Sum of Sq    RSS    AIC\n- S4:SEX   1       209 933171 2815.6\n- S5:SEX   1      1574 934536 2816.1\n- S1:SEX   1      1816 934777 2816.2\n- S2:SEX   1      2427 935388 2816.4\n- S3       1      3667 936629 2816.9\n<none>                 932961 2817.5\n- BMI:SEX  1      6987 939948 2818.2\n- S6:SEX   1      8536 941497 2818.7\n- AGE:SEX  1     22535 955496 2823.9\n- BP       1     65706 998667 2839.5\n\nStep:  AIC=2815.6\nY ~ AGE + BMI + BP + S1 + S2 + S3 + S4 + S5 + S6 + SEX + AGE:SEX + \n    BMI:SEX + S1:SEX + S2:SEX + S5:SEX + S6:SEX\n\n          Df Sum of Sq    RSS    AIC\n- S3       1      3685 936855 2815.0\n- S1:SEX   1      4092 937263 2815.1\n- S5:SEX   1      5289 938459 2815.6\n<none>                 933171 2815.6\n- BMI:SEX  1      7008 940179 2816.2\n- S2:SEX   1      7413 940584 2816.4\n- S6:SEX   1      8400 941570 2816.8\n- S4       1     10729 943900 2817.6\n- AGE:SEX  1     22692 955862 2822.1\n- BP       1     65505 998676 2837.5\n\nStep:  AIC=2814.99\nY ~ AGE + BMI + BP + S1 + S2 + S4 + S5 + S6 + SEX + AGE:SEX + \n    BMI:SEX + S1:SEX + S2:SEX + S5:SEX + S6:SEX\n\n          Df Sum of Sq     RSS    AIC\n- S1:SEX   1      2331  939186 2813.9\n- S5:SEX   1      3781  940637 2814.4\n- S2:SEX   1      5109  941964 2814.9\n<none>                  936855 2815.0\n- BMI:SEX  1      7097  943953 2815.7\n- S4       1      7336  944191 2815.7\n- S6:SEX   1      7595  944451 2815.8\n- AGE:SEX  1     21790  958645 2821.1\n- BP       1     64952 1001807 2836.7\n\nStep:  AIC=2813.86\nY ~ AGE + BMI + BP + S1 + S2 + S4 + S5 + S6 + SEX + AGE:SEX + \n    BMI:SEX + S2:SEX + S5:SEX + S6:SEX\n\n          Df Sum of Sq     RSS    AIC\n- S5:SEX   1      1613  940799 2812.5\n- S2:SEX   1      4566  943753 2813.6\n<none>                  939186 2813.9\n- BMI:SEX  1      5600  944786 2814.0\n- S4       1      6746  945932 2814.4\n- S6:SEX   1      7628  946815 2814.7\n- S1       1     14039  953225 2817.1\n- AGE:SEX  1     23630  962816 2820.6\n- BP       1     63557 1002743 2835.0\n\nStep:  AIC=2812.47\nY ~ AGE + BMI + BP + S1 + S2 + S4 + S5 + S6 + SEX + AGE:SEX + \n    BMI:SEX + S2:SEX + S6:SEX\n\n          Df Sum of Sq     RSS    AIC\n- BMI:SEX  1      4477  945276 2812.2\n<none>                  940799 2812.5\n- S2:SEX   1      5709  946508 2812.6\n- S6:SEX   1      6258  947057 2812.8\n- S4       1      6515  947314 2812.9\n- S1       1     15012  955810 2816.1\n- AGE:SEX  1     22504  963303 2818.8\n- S5       1     55035  995834 2830.5\n- BP       1     63636 1004435 2833.6\n\nStep:  AIC=2812.15\nY ~ AGE + BMI + BP + S1 + S2 + S4 + S5 + S6 + SEX + AGE:SEX + \n    S2:SEX + S6:SEX\n\n          Df Sum of Sq     RSS    AIC\n- S2:SEX   1      4403  949679 2811.8\n<none>                  945276 2812.2\n- S4       1      7330  952607 2812.9\n- S6:SEX   1     11303  956579 2814.3\n- S1       1     13764  959040 2815.2\n- AGE:SEX  1     23748  969024 2818.9\n- S5       1     53777  999053 2829.7\n- BP       1     66586 1011863 2834.2\n- BMI      1    172186 1117462 2869.2\n\nStep:  AIC=2811.79\nY ~ AGE + BMI + BP + S1 + S2 + S4 + S5 + S6 + SEX + AGE:SEX + \n    S6:SEX\n\n          Df Sum of Sq     RSS    AIC\n- S2       1      4433  954111 2811.4\n<none>                  949679 2811.8\n- S4       1      7553  957231 2812.6\n- S6:SEX   1      8943  958621 2813.1\n- S1       1     14195  963874 2815.0\n- AGE:SEX  1     21094  970773 2817.5\n- S5       1     53738 1003416 2829.2\n- BP       1     66034 1015713 2833.5\n- BMI      1    175163 1124842 2869.5\n\nStep:  AIC=2811.43\nY ~ AGE + BMI + BP + S1 + S4 + S5 + S6 + SEX + AGE:SEX + S6:SEX\n\n          Df Sum of Sq     RSS    AIC\n<none>                  954111 2811.4\n- S6:SEX   1      8005  962116 2812.4\n- AGE:SEX  1     22251  976362 2817.6\n- S1       1     27635  981747 2819.5\n- S4       1     39145  993256 2823.6\n- BP       1     65368 1019480 2832.8\n- S5       1     78978 1033090 2837.5\n- BMI      1    192685 1146796 2874.4\n\nsummary(fit)\n\n\nCall:\nlm(formula = Y ~ AGE + BMI + BP + S1 + S4 + S5 + S6 + SEX + AGE:SEX + \n    S6:SEX, data = tsk_train_diabetes$data())\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-155.393  -37.026   -0.527   35.162  138.780 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -394.9460    41.3415  -9.553  < 2e-16 ***\nAGE            0.6397     0.3211   1.992 0.047143 *  \nBMI            6.1483     0.7398   8.311 2.27e-15 ***\nBP             1.1851     0.2448   4.841 1.96e-06 ***\nS1            -0.3285     0.1044  -3.147 0.001792 ** \nS4            12.1557     3.2451   3.746 0.000211 ***\nS5            40.6817     7.6460   5.321 1.87e-07 ***\nS6             0.5709     0.4125   1.384 0.167250    \nSEX1         165.6431    48.6522   3.405 0.000741 ***\nAGE:SEX1      -1.2606     0.4464  -2.824 0.005019 ** \nS6:SEX1       -0.9028     0.5329  -1.694 0.091187 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.82 on 342 degrees of freedom\nMultiple R-squared:  0.5399,    Adjusted R-squared:  0.5265 \nF-statistic: 40.14 on 10 and 342 DF,  p-value: < 2.2e-16\n\n\nEl modelo obtenido nos da una tabla con los efectos del modelo que son seleccionados por el AIC, así como los coeficientes que dichos efectos tienen en el modelo. En este caso podemos ver que los efectos más relevantes se corresponden con BMI y S5 con coeficientes positivos, indicando que cuando aumentan los valores de esas predictoras más aumenta el valor de la respuesta. Puesto que las variables están estandarizadas el coeficiente también refleja el peso de cada predictora en la respuesta. Por otro lado, aunque el coeficiente asociado con SEX es el más elevado no tiene sentido su interpretación ya que en el modelo están presentes diferentes efectos de interacción relacionados con dicho factor, En esa situación los coeficientes más relevantes son los de la interacción.\nEl modelo no tiene una capacidad explicada muy elevada dado que el \\(R^2\\) se sitúa en 53% y el MSE es de 2767.502 (residual standard error^2).\nLas ecuaciones del modelo resultante son:\nPara los sujetos de SEX = 1\n\\[\\hat{Y} = (-431.1117+134.9858) + (0.5030-1.0742)*AGE + (6.7845-2.3455)*BMI + 1.1940*BP\n-1.0872*S1 + 0.8818*S2 + 77.3292*S5\\] o lo que es o mismo\n\\[\\hat{Y} = -296.1259 -0.5712*AGE + 4.439*BMI + 1.1940*BP -1.0872*S1 + 0.8818*S2 + 77.3292*S5\\] En este caso el progreso de la enfermedad aumenta con BMI, BP, S2 y S5, y disminuye con AGE y S1 para los individuos codificados con SEX =1\nPara los sujetos de SEX = 2\n\\[\\hat{Y} = -431.1117 + 0.5030*AGE + 6.7845*BMI + 1.1940*BP -1.0872*S1 + 0.8818*S2 + 77.3292*S5\\] En este caso todos los indicadores son positivos salvo S1.\nUna vez hemos identificado los efectos que parecen mas relevantes para explicar el comportamiento de la respuesta, vamos a estudiar la validez construyendo un grpahlearner que nos permita entrenar el modelo de forma sencilla y evaluar individualmente y mediante validación cruzada k-fold el modelo propuesto. para ello definimos el nuevo modelo, establecemos el learner adecuado y utilizamos la medida de valoración del modelo basada en el MSE.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.lm\")\n# Modelo en términos de predictoras\ndiabetes_model = po(\"modelmatrix\", formula = ~ AGE + BMI + BP + S1 + S2 + S5 + SEX + AGE:SEX + BMI:SEX)\n\n# Graphlearner: Estructura del modelo y learner\ngr = diabetes_model %>>% learner\ngr = GraphLearner$new(gr)\n\n# Entrenamiento del modelo\ngr$train(tsk_train_diabetes)\n# Resumen del modelo\nsummary(gr$model$regr.lm$model)\n\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-157.498  -36.293    1.277   33.511  132.232 \n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -421.8006    35.4754 -11.890  < 2e-16 ***\n`(Intercept)`        NA         NA      NA       NA    \nAGE              0.6698     0.3191   2.099 0.036559 *  \nBMI              7.3311     1.0307   7.113 6.66e-12 ***\nBP               1.1522     0.2429   4.744 3.08e-06 ***\nS1              -0.9862     0.2424  -4.069 5.87e-05 ***\nS2               0.8962     0.2507   3.574 0.000402 ***\nS5              66.7037     7.9297   8.412 1.10e-15 ***\nSEX1           143.4395    37.6309   3.812 0.000164 ***\n`AGE:SEX1`      -1.3080     0.4385  -2.983 0.003059 ** \n`BMI:SEX1`      -2.2094     1.2862  -1.718 0.086753 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.85 on 343 degrees of freedom\nMultiple R-squared:  0.538, Adjusted R-squared:  0.5259 \nF-statistic: 44.38 on 9 and 343 DF,  p-value: < 2.2e-16\n\n\nComo era de esperar el modelo obtenido es el mismo, dado que el algoritmo regr.lm se basa en la función lm para su ajuste. Obtenemos la predicción del modelo tanto para la muestra de entrenamiento como de validación y representamos las soluciones obtenidas\n\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_diabetes)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_diabetes)\n# Scores de validación\nmeasures = msr(c(\"regr.mse\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n2714.173 \n\npred_test$score(measures)\n\nregr.mse \n 3080.09 \n\n\nComo era de esperar el MSE para la muestra de validación es superior al de la muestra de entrenamiento. A continuación vemos la solución gráfica para el modelo de aprendizaje que hemos entrenado. En concreto realizamos los gráficos:\n\nValores observados reales versus predichos por el modelo, donde pretendemos ver cuanto de buena es nuestra predicción con respecto a los valores observados realmente.\nValores observados reales frente a residuos del modelo, donde queremos observar is estos últimamente se comportan de forma adecuada, es decir, tiene un comportamiento aleatorio y se centran en cero.\nHistograma de los residuos del modelo para analizar su normalidad, es decir, comportamiento simétrico con respecto al cero y con forma de campana de Gauss.\n\nRealizamos los gráficos anteriores tanto para la muestra de entrenamiento como la de validación:\n\n# Muestra de entrenamiento\np1 = autoplot(pred_train, type = \"xy\") + labs(title = \"Observados vs predichos\")\np2 = autoplot(pred_train, type = \"residual\") + labs(title = \"Observados vs residuos\")\np3 = autoplot(pred_train, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\n# Muestra de validación\np4 = autoplot(pred_test, type = \"xy\") + labs(title = \"Observados vs predichos\")\np5 = autoplot(pred_test, type = \"residual\") + labs(title = \"Observados vs residuos\")\np6 = autoplot(pred_test, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\nggarrange(p1,p2,p3, p4, p5, p6, nrow = 2, ncol = 3)\n\n\n\n\nFigura 6.5: Gráficos del modelo (entrenamiento-validación). Task Penguins.\n\n\n\n\nEn los gráficos de observados vs predichos podemos ver que la nube de puntos se encuentra centrada sobre la diagonal (ajuste perfecto) pero con una gran variabilidad (dispersión) lo que provoca un MSE bastante alto e indica una bajo valor predictivo.\nEn los gráficos de observados frente a residuos podemos ver que se encuentran centrados en cero pero su comportamiento no es totalmente aleatorio, ya que se observa una mayor dispersión en los valores centrales de la respuesta, mientras que es menor en los extremos. Esto implica que le modelo lineal propuesto no es muy adecuado y es necesario una modificación o la consideración de otro algoritmo para predecir la repuesta de interés.\nEn los histogramas de los residuos podemos ver que se encuentran centrados en cero y su comportamiento se parece bastante al de la campana de Gauss, indicando la normalidad de los residuos del modelo.\nUna vez hemos analizado el modelo propuesto en detalle vamos a realizar un estudio de validación cruzada para valorar la estabilidad de la solución obtenida. utilizamos el modelo definido anteriormente y la misma métrica de validación.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_diabetes, gr, resamp, store_models=TRUE)\n\nINFO  [17:35:28.925] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:29.089] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:29.952] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:30.060] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:30.171] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:30.283] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:30.407] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:30.518] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:30.638] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:30.781] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\n\n\nEn primer lugar estudiamos los scores de validación (en este caso el MSE) tanto para cada fold como de forma agregada.\n\n# Scores individuales\nrr$score()$regr.mse\n\n [1] 3334.607 2955.360 2178.192 2561.333 4644.915 2770.456 2924.532 2070.048\n [9] 2718.506 2974.108\n\n# Gráfico de los scores individuales\nautoplot(rr, type = \"boxplot\")\n\n\n\n\nPodemos ver una gran dispersión en los valores de los scores lo que puede ser debido a la gran dispersión de los datos originales. Vamos a estudiar las soluciones individuales de todos los folds analizando los modelos estimados y la predicción obtenida con cada uno de ellos.\nEn primer lugar analizamos la ecuación del modelo para cada fold.\n\n# Número de folds\nnfolds = 10\n# Coeficientes primer fold\nfold_coef = rr$learners[[1]]$model$regr.lm$model$coefficients[-2]\n# Bucle con el resto de folds\nfor (i in 2:nfolds)\n{\n  pre = rr$learners[[i]]$model$regr.lm$model$coefficients[-2]\n  fold_coef = rbind(fold_coef, pre)\n}\nrownames(fold_coef) = paste(\"fold \", 1:nfolds)\nfold_coef\n\n         (Intercept)       AGE      BMI        BP         S1        S2       S5\nfold  1    -445.2340 0.8439358 7.603690 1.0619125 -0.9440420 0.7804093 70.68828\nfold  2    -412.3563 0.6042516 6.889477 0.9782751 -1.0823745 0.8894872 75.42517\nfold  3    -445.5676 0.6707123 6.944082 1.1762567 -1.0540675 0.8537913 76.67291\nfold  4    -422.9322 0.7105352 6.704040 1.1712944 -0.9833478 0.8107907 71.80353\nfold  5    -434.4493 0.7316720 7.237216 1.2056449 -0.9309912 0.6996666 70.26252\nfold  6    -420.8541 0.4394769 7.305076 1.0025348 -1.1581269 0.9838786 77.16000\nfold  7    -428.9777 0.6316744 7.113376 1.1264266 -0.9712969 0.7793862 72.39982\nfold  8    -439.9584 0.6814086 6.798735 1.1593487 -1.0309974 0.9018838 74.74262\nfold  9    -434.3076 0.7256529 7.084248 1.2269260 -1.0589913 0.8645149 71.99122\nfold  10   -441.9177 0.7355721 6.732934 1.2702042 -1.0249756 0.8833135 73.35674\n             SEX1 `AGE:SEX1` `BMI:SEX1`\nfold  1  158.5127 -1.6047008  -2.212433\nfold  2  122.6085 -1.1793687  -1.645068\nfold  3  155.6093 -1.2738598  -2.596898\nfold  4  133.6826 -1.3464280  -1.819396\nfold  5  131.7663 -1.1053187  -2.073289\nfold  6  131.1494 -0.7852138  -2.754880\nfold  7  145.4958 -1.2275106  -2.444391\nfold  8  148.9179 -1.3459060  -2.271558\nfold  9  146.1860 -1.3302285  -2.262392\nfold  10 151.5196 -1.4401144  -2.241624\n\n\nPodemos ver que las estimaciones de los modelos en cada fold son muy similares para todos los efectos del modelo. hay bastante estabilidad en la construcción del modelo dentro de cada fold. En la tabla siguiente podemos ver el resumen de cada coeficiente (media y desviación típica) para todos los folds.\n\ndescrip = function(x)\n{\n  c(mean(x),sd(x))\n}\n\ntabla = apply(fold_coef,2,descrip)\nrownames(tabla) = c(\"Media\", \"sd\")\ntabla\n\n      (Intercept)       AGE       BMI         BP         S1         S2\nMedia  -432.65548 0.6774892 7.0412874 1.13788240 -1.0239211 0.84471219\nsd       11.20222 0.1062489 0.2850842 0.09590241  0.0691617 0.07971934\n             S5      SEX1 `AGE:SEX1` `BMI:SEX1`\nMedia 73.450282 142.54482 -1.2638649 -2.2321929\nsd     2.437234  11.97597  0.2180697  0.3324235\n\n\nLos coeficientes siguen la misma interpretación que vimos en el modelo inicial, y la variabilidad de la estimación es pequeña en comparación con el valor estimado del coeficiente (sobre todo en los efectos más relevantes BMI, S5, y las interacciones de AGE y BMI con SEX).\nA continuación realizamos un análisis gráfico de cada uno de los modelos de cada fold. Para ello utilizamos las predicciones asociados a cada uno de ellos.\n\np1 = autoplot(rr$predictions()[[1]], type = \"xy\") + labs(title = \"Obs vs Pre (f1)\")\np2 = autoplot(rr$predictions()[[2]], type = \"xy\") + labs(title = \"Obs vs Pre (f2)\")\np3 = autoplot(rr$predictions()[[3]], type = \"xy\") + labs(title = \"Obs vs Pre (f3)\")\np4 = autoplot(rr$predictions()[[4]], type = \"xy\") + labs(title = \"Obs vs Pre (f4)\")\np5 = autoplot(rr$predictions()[[5]], type = \"xy\") + labs(title = \"Obs vs Pre (f5)\")\np6 = autoplot(rr$predictions()[[6]], type = \"xy\") + labs(title = \"Obs vs Pre (f6)\")\np7 = autoplot(rr$predictions()[[7]], type = \"xy\") + labs(title = \"Obs vs Pre (f7)\")\np8 = autoplot(rr$predictions()[[8]], type = \"xy\") + labs(title = \"Obs vs Pre (f8)\")\np9 = autoplot(rr$predictions()[[9]], type = \"xy\") + labs(title = \"Obs vs Pre (f9)\")\np10 = autoplot(rr$predictions()[[10]], type = \"xy\") + labs(title = \"Obs vs Pre (f10)\")\n\n\nggarrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, nrow = 3, ncol = 4)\n\n\n\n\nFigura 6.6: Gráfico de observados vs predichos para cada submuestro. Task Diabetes.\n\n\n\n\nComo era de esperar el comportamiento en todos los fold es similar. El objeto resampling nos permite combinar las predicciones de cada uno de los fold en un único objeto con el método prediction(). Esto nos da la posibilidad de realizar los gráficos para analizar el modelo obtenido:\n\np1 = autoplot(rr$prediction(), type = \"xy\") + labs(title = \"Observados vs predichos\")\np2 = autoplot(rr$prediction(), type = \"residual\") + labs(title = \"Observados vs residuos\")\np3 = autoplot(rr$prediction(), type = \"histogram\") + labs(title = \"Histograma residuos\")\n\nggarrange(p1, p2, p3, nrow = 1, ncol = 3)\n\n\n\n\nFigura 6.7: Gráficos del modelo obtenido. Task Diabetes.\n\n\n\n\nLos resultados obtenidos son similares a los del modelo inicial con la muestra de validación.\nPara finalizar nuestra primera aproximación al modelo propuesto vamos a realizar el análisis de la curva de aprendizaje asociada con este modelo. El análisis de la curva de aprendizaje nos permite estudiar como mejora el aprendizaje del modelo planteado conforme vamos aumentando el tamaño de la muestra de entrenamiento. Esto nos permite determinar el tamaño de la muestra de aprendizaje para que la solución sea estable.\nEl proceso es similar al de validación cruzada descrito en el punto anterior. Se utilizan subconjuntos del conjunto de entrenamiento con tamaños variables para entrenar el modelo y se calcula una puntuación para cada tamaño de subconjunto de entrenamiento y el conjunto de validación. Después, las puntuaciones obtenidas se promediarán para las k ejecuciones en cada tamaño de subconjunto de entrenamiento.\nPara realizar esta tarea nos apoyamos en la función rsmp() con la opción subsampling donde podemos fijar el número de repeticiones que deseamos para una partición de muestras de entrenamiento y validación. Con la función resample() nos resulta posible obtener los valores del score buscado tanto para la muestra de entrenamiento como de validación. En primer lugar definimos dos funciones learningcurve y plot_learningcurve que nos permiten obtener los valores de la curva de aprendizaje y su representación gráfica. Los parámetros de ambas funciones son los mismos y vienen definidos en la función.\n\n# Función que nos permite obtener los valores asociados a la curva de aprendizaje\nlearningcurve = function(task, learner, score, ptr, rpeats)\n{\n  # Parámetros de la función\n  # task: tarea\n  # learner: algoritmo de aprendizaje\n  # score: nombre del score a utilizar\n  # ptr: vector con las proporciones de tamaños de muestra de entrenamiento\n  # rpeats: número de repeticiones para cada proporción de tamaño de muestra de entrenamiento\n  \n  # Definimos los scores para cada conjunto de muestra\n  mtrain = msr(score, predict_sets = \"train\")\n  mtest = msr(score, predict_sets = \"test\")\n  # Configuramos el learner para que evalué los scores en la muestra de validación y test\n  learner$predict_sets = c(\"train\", \"test\")\n  # Incicializamos vector de scores agregados para la muestra de entrenamiento y validación\n  sco_train = c()\n  sco_test = c()\n  for(i in 1:length(ptr))\n  {\n    # estructura de muestreo: 5 repeticiones con porcentaje muestra entrenamiento ptr[i]\n    subsam = rsmp(\"subsampling\", repeats = rpeats, ratio = ptr[i])\n    # ejecución de remuestreo\n    rr = resample(task, learner, subsam)\n    sco_train[i] = rr$aggregate(mtrain)\n    sco_test[i] = rr$aggregate(mtest)\n  }\n  # Matriz de resultados\n  res = data.frame(ptr, sco_train, sco_test)\n  resdf = res %>% pivot_longer(!ptr, names_to = \"Sample\", values_to = \"MSR\")\n  return(resdf)\n}\n\n\n# Función que nos permite representar la curva de aprendizaje \nplot_learningcurve = function(task, learner, score, ptr, rpeats)\n{\n  # Parámetros de la función\n  # task: tarea\n  # learner: algoritmo de aprendizaje\n  # score: nombre del score a utilizar\n  # ptr: vector con las proporciones de tamaños de muestra de entrenamiento\n  # rpeats: número de repeticiones para cada proporción de tamaño de muestra de entrenamiento\n\n  lcurve = learningcurve(task, gr, score, ptr, rpeats)\n  # Gráfico\n  ggplot(lcurve, aes(ptr, MSR, color = Sample)) + \n    geom_line() +\n    labs(x =\"Proporción tamaño muestra entrenamiento\", y = \"MSE\",color = \"Muestra\") +\n    scale_color_hue(labels = c(\"Validación\", \"Entrenamiento\")) +\n    scale_x_continuous(breaks=ptr)\n}\n\nA continuación se muestra la curva de aprendizaje para un un grid de porcentajes que va desde el 10% al 90% del tamaño de la muestra de entrenamiento con diez repeticiones para cada tamaño.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.lm\")\n# Modelo en términos de predictoras\ndiabetes_model = po(\"modelmatrix\", formula = ~ AGE + BMI + BP + S1 + S2 + S5 + SEX + AGE:SEX + BMI:SEX)\n# Graphlearner: Estructura del modelo y learner\ngr = diabetes_model %>>% learner\ngr = GraphLearner$new(gr)\n\nplot_learningcurve(tsk_diabetes, gr, \"regr.mse\", ptr = seq(0.1, 0.9, 0.1), rpeats = 10)\n\nINFO  [17:35:38.585] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:38.879] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:39.331] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:39.564] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:39.827] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:40.041] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:40.287] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:40.535] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:40.778] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:41.017] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\nINFO  [17:35:41.352] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:41.547] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:41.760] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:41.959] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:42.134] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:42.336] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:42.528] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:42.697] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:42.892] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:43.072] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\nINFO  [17:35:43.365] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:43.591] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:43.770] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:43.959] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:44.137] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:44.325] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:44.515] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:44.725] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:45.061] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:45.359] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\nINFO  [17:35:45.747] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:45.953] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:46.138] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:46.362] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:46.557] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:46.736] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:47.022] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:47.198] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:47.376] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:47.548] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\nINFO  [17:35:47.833] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:48.004] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:48.180] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:48.359] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:48.566] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:48.736] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:48.909] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:49.084] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:49.285] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:49.461] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\nINFO  [17:35:49.717] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:49.891] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:50.110] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:50.286] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:50.460] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:50.632] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:50.846] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:51.026] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:51.200] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:51.376] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\nINFO  [17:35:51.705] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:51.935] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:52.128] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:52.359] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:52.624] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:52.861] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:53.075] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:53.281] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:53.488] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:53.878] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\nINFO  [17:35:55.063] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:35:55.707] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:35:56.183] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:35:56.445] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:35:56.639] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:35:56.855] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:35:57.337] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:35:57.977] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:35:58.574] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:35:59.095] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\nINFO  [17:36:00.357] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 1/10)\nINFO  [17:36:00.660] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 2/10)\nINFO  [17:36:01.059] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 3/10)\nINFO  [17:36:01.380] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 4/10)\nINFO  [17:36:01.860] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 5/10)\nINFO  [17:36:02.662] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 6/10)\nINFO  [17:36:03.361] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 7/10)\nINFO  [17:36:03.962] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 8/10)\nINFO  [17:36:04.572] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 9/10)\nINFO  [17:36:05.201] [mlr3] Applying learner 'modelmatrix.regr.lm' on task 'diabetes' (iter 10/10)\n\n\n\n\n\nFigura 6.8: Curva de calibración. Task Diabetes.\n\n\n\n\nSe puede ver como a ir aumentando el tamaño de la muestra de entrenamiento se va reduciendo el MSE de validación, obteniendo un valor óptimo para el 60% o 80%. Por tanto el tamaño de muestra de entrenamiento debería ser uno de esos dos. De hecho, se puede ver que el MSE para la muestra de entrenamiento tiene valores similares al de la muestra de validación para esos tamaños.\n\n\n6.2.3.2 Electricity\nEn este banco de datos no existen valores perdidos pero si es necesario estandarizar las variables para acometer el ajuste del modelo. Ese proceso de estandarización lo englobaremos dentro del graph learner correspondiente. Como todas la predictoras son de tipo numérico no resulta necesario detallar el modelo que vamos a ajustar. Además como su número es muy bajo no consideramos aquí un proceso de selección de variables.\nPara el estudio procedemos como en el ejemplo anterior:\n\nAjustamos un primer modelo con un tamaño de muestra de entrenamiento del 80%.\nRealizamos un estudio sobre la estabilidad de la solución.\nObtenemos la curva de aprendizaje.\n\nEn primer lugar realizamos la división de muestras:\n\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_electricity, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_electricity = tsk_electricity$clone()$filter(splits$train)\ntsk_test_electricity  = tsk_electricity$clone()$filter(splits$test)\n\nEstablecemos el proceso de aprendizaje\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.lm\")\n# Preprocesado\npp_electricity = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Graphlearner: Preprocesado y learner\ngr = pp_electricity %>>% learner\ngr = GraphLearner$new(gr)\n\n# Entrenamiento del modelo\ngr$train(tsk_train_electricity)\n# Resumen del modelo\nsummary(gr$model$regr.lm$model)\n\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.368  -3.143  -0.105   3.196  17.791 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 454.36149    0.05217 8709.99  < 2e-16 ***\nAP            0.36269    0.06308    5.75 9.27e-09 ***\nAT          -14.70309    0.12702 -115.76  < 2e-16 ***\nRH           -2.27720    0.06799  -33.49  < 2e-16 ***\nV            -3.00184    0.10309  -29.12  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.564 on 7650 degrees of freedom\nMultiple R-squared:  0.9286,    Adjusted R-squared:  0.9285 \nF-statistic: 2.487e+04 on 4 and 7650 DF,  p-value: < 2.2e-16\n\n\nLa ecuación del modelo obtenido viene dada por:\n\\[\\hat{PE} = 454.39 + 0.35*AP - 14.81*AT -2.36*RH -2.97*V\\] donde se puede ver que la variable más relevante es AT peso negativo hasta siete veces superior al de RH y V con peso negativo. Esto implica que PE aumenta cuando disminuyen AT, RH, y V al tratarse de efectos inversamente proporcionales. Además podemos ver que el \\(R^2\\) se sitúa en el 92% de capacidad explicativa, y con un pvalor del test F del modelo inferior a 0.05, lo que indica que las predictoras tiene capacidad explicativa sobre la respuesta.\nA continuación obtenemos la predicción del modelo para evaluar los scores correspondientes y realizar el análisis gráfico.\n\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_electricity)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_electricity)\n# Scores de validación\nmeasures = msr(c(\"regr.mse\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n20.81751 \n\npred_test$score(measures)\n\nregr.mse \n20.57926 \n\n\nEn este caso los MSE son casi del mismo orden y bastante pequeños indicando la posibilidad de un buen ajuste. Veamos los gráficos del modelo.\n\n# Muestra de entrenamiento\np1 = autoplot(pred_train, type = \"xy\") + labs(title = \"Observados vs predichos\")\np2 = autoplot(pred_train, type = \"residual\") + labs(title = \"Observados vs residuos\")\np3 = autoplot(pred_train, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\n# Muestra de validación\np4 = autoplot(pred_test, type = \"xy\") + labs(title = \"Observados vs predichos\")\np5 = autoplot(pred_test, type = \"residual\") + labs(title = \"Observados vs residuos\")\np6 = autoplot(pred_test, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\nggarrange(p1,p2,p3, p4, p5, p6, nrow = 2, ncol = 3)\n\n\n\n\nFigura 6.9: Gráficos del modelo para muestras de entrenamiento y validación. Task Electricity.\n\n\n\n\nEn los gráficos de observados versus predichos podemos ver que el ajuste del modelo es bastante bueno, dado que la nube de puntos se encuentra muy próxima a la diagonal. Solo se aprecia un grupo de puntos (muestra entrenamiento) donde el valor predicho es superior al verdadero valor. Este comportamiento se aprecia también en el gráfico de predichos versus residuos. Si embargo, los residuos del modelo son excesivamente grandes. Lo habitual es que se sitúen en rango [-3, 3] para indicar un buen modelo. En este caso tenemos valores muy por encima de ese rango indica mayor dispersión en los residuos de la que sería deseable. Más adelante estudiaremos diferentes posibilidades para corregir esta situación.\nComenzamos ahora con el estudio de validación mediante validación cruzada. En este caso consideramos 10 grupos para el análisis.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_electricity, gr, resamp, store_models=TRUE)\n\nINFO  [17:36:17.770] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:18.604] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:36:19.303] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:36:19.638] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:36:20.063] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:36:20.520] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:36:20.745] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:36:21.507] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:36:21.967] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:36:22.802] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\n\n\nEn primer lugar estudiamos los scores de validación (en este caso el MSE) tanto para cada fold como de forma agregada.\n\n# Scores individuales\nrr$score()$regr.mse\n\n [1] 21.28424 20.35541 20.56944 19.63743 23.19395 20.29066 20.78991 18.07989\n [9] 20.64482 22.99741\n\n\nPodemos ver muy poca dispersión en los valores de los scores indicando una gran estabilidad en la solución obtenida. Vamos a estudiar las soluciones individuales de todos los folds analizando los modelos estimados y la predicción obtenida con cada uno de ellos.\nEn primer lugar analizamos la ecuación del modelo para cada fold.\n\n# Número de folds\nnfolds = 10\n# Coeficientes primer fold\nfold_coef = rr$learners[[1]]$model$regr.lm$model$coefficients\n# Bucle con el resto de folds\nfor (i in 2:nfolds)\n{\n  pre = rr$learners[[i]]$model$regr.lm$model$coefficients\n  fold_coef = rbind(fold_coef, pre)\n}\nrownames(fold_coef) = paste(\"fold \", 1:nfolds)\nfold_coef\n\n         (Intercept)        AP        AT        RH         V\nfold  1     454.3224 0.3647719 -14.78313 -2.330588 -2.968159\nfold  2     454.3688 0.3753792 -14.73950 -2.317951 -2.982508\nfold  3     454.3277 0.3468655 -14.76228 -2.299491 -2.963939\nfold  4     454.3557 0.3462349 -14.79363 -2.320810 -2.914509\nfold  5     454.3494 0.3630205 -14.72871 -2.302844 -3.001273\nfold  6     454.3392 0.3821713 -14.71708 -2.302133 -2.956572\nfold  7     454.3624 0.3863806 -14.71080 -2.289330 -2.995456\nfold  8     454.4708 0.3785623 -14.63368 -2.277956 -3.014480\nfold  9     454.4552 0.3840943 -14.77773 -2.330544 -2.979969\nfold  10    454.2984 0.3597748 -14.72655 -2.304853 -2.949510\n\n\nComo se puede ver los coeficientes coinciden prácticamente hasta el primer decimal (redondeando) indicando una gran estabilidad en la solución. En la tabla siguiente podemos ver el resumen de cada coeficiente (media y desviación típica) para todos los folds.\n\ntabla = apply(fold_coef, 2, descrip)\nrownames(tabla) = c(\"Media\" , \"sd\")\ntabla\n\n       (Intercept)         AP           AT          RH           V\nMedia 454.36500913 0.36872552 -14.73730800 -2.30764988 -2.97263737\nsd      0.05576518 0.01483879   0.04663677  0.01723981  0.02888723\n\n\nPara finalizar el análisis preliminar analizamos la curva de aprendizaje asociada al modelo propuesto:\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.lm\")\n# Graphlearner: Estructura del modelo y learner\ngr = pp_electricity %>>% learner\ngr = GraphLearner$new(gr)\n\nplot_learningcurve(tsk_electricity, gr, \"regr.mse\", ptr = seq(0.1, 0.9, 0.1), rpeats = 10)\n\nINFO  [17:36:34.113] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:34.728] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:36:35.137] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:36:35.508] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:36:35.954] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:36:36.341] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:36:36.622] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:36:36.894] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:36:37.210] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:36:37.526] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\nINFO  [17:36:38.015] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:38.295] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:36:38.688] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:36:38.913] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:36:39.423] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:36:39.716] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:36:40.043] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:36:40.332] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:36:40.565] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:36:40.847] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\nINFO  [17:36:41.350] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:41.627] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:36:41.978] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:36:42.301] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:36:42.508] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:36:42.854] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:36:43.057] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:36:43.343] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:36:43.526] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:36:43.766] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\nINFO  [17:36:44.118] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:44.367] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:36:44.637] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:36:45.020] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:36:45.674] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:36:46.207] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:36:46.800] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:36:47.362] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:36:47.612] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:36:47.858] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\nINFO  [17:36:48.261] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:48.478] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:36:48.687] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:36:48.885] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:36:49.061] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:36:49.490] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:36:50.144] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:36:50.630] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:36:50.827] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:36:51.047] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\nINFO  [17:36:52.081] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:52.911] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:36:53.460] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:36:54.153] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:36:54.528] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:36:55.153] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:36:55.435] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:36:55.781] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:36:56.024] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:36:56.275] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\nINFO  [17:36:56.649] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:56.838] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:36:57.636] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:36:58.135] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:36:58.419] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:36:58.630] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:36:58.817] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:36:59.044] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:36:59.230] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:36:59.407] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\nINFO  [17:36:59.748] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:36:59.949] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:37:00.469] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:37:01.172] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:37:01.539] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:37:01.946] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:37:02.769] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:37:03.204] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:37:03.539] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:37:03.816] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\nINFO  [17:37:04.267] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 1/10)\nINFO  [17:37:04.499] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 2/10)\nINFO  [17:37:04.803] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 3/10)\nINFO  [17:37:05.128] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 4/10)\nINFO  [17:37:05.320] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 5/10)\nINFO  [17:37:05.559] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 6/10)\nINFO  [17:37:05.839] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 7/10)\nINFO  [17:37:06.436] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 8/10)\nINFO  [17:37:06.671] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 9/10)\nINFO  [17:37:06.855] [mlr3] Applying learner 'scale.regr.lm' on task 'electricity' (iter 10/10)\n\n\n\n\n\nFigura 6.10: Curva de aprendizaje. Task Electricity.\n\n\n\n\nA la vista del gráfico parece desprenderse que el tamaño de muestra de entrenamiento que proporciona mejores resultados se encentra sobre el 70%.\n\n\n6.2.3.3 Housing in California\nEn este caso el conjunto de datos dispone de valores perdidos, y además resulta necesario establecer el modelo saturado ya que disponemos de una predictora categórica y debemos introducir los efectos de interacción. En primer lugar procedemos con la división de muestras considerando el 80% de tamaño de la muestra de entrenamiento. para realizar esta tarea utilizamos la función partition() de la librería mlr3 donde debemos indicar la tarea y la proporción de elementos que tomaremos en la muestra de entrenamiento (ratio). Esta función nos proporciona los índices o posiciones de las muestras que debemos incluir tanto en la muestra de entrenamiento como de validación. A continuación se muestra como utilizar esta función y como construir la task de entrenamiento y validación a partir de la información que proporciona.\n\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_housing, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_housing = tsk_housing$clone()$filter(splits$train)\ntsk_test_housing  = tsk_housing$clone()$filter(splits$test)\n\nProponemos el modelo con interacciones y realizamos el proceso de selección de efectos del modelo. En principio ajustamos el modelo sin tener en cuenta la presencia de missings().\n\nfit = lm(median_house_value ~ (households + housing_median_age + latitude + longitude + median_income + population + total_bedrooms + total_rooms )*ocean_proximity, data = tsk_train_housing$data())\nfit = stats::step(fit)\n\nStart:  AIC=363012.1\nmedian_house_value ~ (households + housing_median_age + latitude + \n    longitude + median_income + population + total_bedrooms + \n    total_rooms) * ocean_proximity\n\n                                     Df  Sum of Sq        RSS    AIC\n<none>                                             7.1047e+13 363012\n- households:ocean_proximity          3 8.1396e+10 7.1128e+13 363025\n- total_bedrooms:ocean_proximity      3 1.7139e+11 7.1218e+13 363045\n- total_rooms:ocean_proximity         3 1.7511e+11 7.1222e+13 363046\n- median_income:ocean_proximity       3 2.6202e+11 7.1309e+13 363066\n- population:ocean_proximity          3 4.6576e+11 7.1512e+13 363113\n- housing_median_age:ocean_proximity  3 4.8000e+11 7.1527e+13 363116\n- latitude:ocean_proximity            3 2.2516e+12 7.3298e+13 363516\n- longitude:ocean_proximity           3 2.8368e+12 7.3883e+13 363646\n\nsummary(fit)\n\n\nCall:\nlm(formula = median_house_value ~ (households + housing_median_age + \n    latitude + longitude + median_income + population + total_bedrooms + \n    total_rooms) * ocean_proximity, data = tsk_train_housing$data())\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-536800  -38900   -9802   26016  770932 \n\nCoefficients: (4 not defined because of singularities)\n                                               Estimate Std. Error t value\n(Intercept)                                  -2.938e+07  1.419e+06 -20.708\nhouseholds                                    1.220e+02  5.179e+01   2.356\nhousing_median_age                            8.174e+02  1.389e+02   5.884\nlatitude                                     -1.962e+05  9.308e+03 -21.080\nlongitude                                    -3.015e+05  1.240e+04 -24.316\nmedian_income                                 3.643e+04  1.002e+03  36.339\npopulation                                   -7.222e+01  5.137e+00 -14.059\ntotal_bedrooms                                2.436e+01  4.499e+01   0.542\ntotal_rooms                                   1.326e+01  3.037e+00   4.366\nocean_proximity<1H OCEAN                      2.557e+07  1.438e+06  17.789\nocean_proximityINLAND                         2.814e+07  1.425e+06  19.745\nocean_proximityNEAR OCEAN                     2.528e+07  1.443e+06  17.520\nocean_proximityISLAND                         6.930e+08  2.737e+08   2.532\nhouseholds:ocean_proximity<1H OCEAN          -6.406e+01  5.542e+01  -1.156\nhouseholds:ocean_proximityINLAND             -1.135e+02  5.302e+01  -2.142\nhouseholds:ocean_proximityNEAR OCEAN         -2.021e+02  6.043e+01  -3.345\nhouseholds:ocean_proximityISLAND              2.737e+02  3.756e+02   0.729\nhousing_median_age:ocean_proximity<1H OCEAN   3.616e+02  1.573e+02   2.299\nhousing_median_age:ocean_proximityINLAND     -6.282e+02  1.631e+02  -3.851\nhousing_median_age:ocean_proximityNEAR OCEAN  7.536e+02  1.902e+02   3.961\nhousing_median_age:ocean_proximityISLAND      9.183e+03  7.199e+03   1.275\nlatitude:ocean_proximity<1H OCEAN             1.457e+05  9.795e+03  14.874\nlatitude:ocean_proximityINLAND                1.821e+05  9.404e+03  19.363\nlatitude:ocean_proximityNEAR OCEAN            1.504e+05  9.826e+03  15.307\nlatitude:ocean_proximityISLAND                2.192e+07  1.055e+07   2.078\nlongitude:ocean_proximity<1H OCEAN            2.545e+05  1.271e+04  20.015\nlongitude:ocean_proximityINLAND               2.866e+05  1.249e+04  22.954\nlongitude:ocean_proximityNEAR OCEAN           2.536e+05  1.278e+04  19.846\nlongitude:ocean_proximityISLAND               1.203e+07  5.239e+06   2.297\nmedian_income:ocean_proximity<1H OCEAN        2.088e+03  1.122e+03   1.860\nmedian_income:ocean_proximityINLAND          -3.056e+03  1.306e+03  -2.340\nmedian_income:ocean_proximityNEAR OCEAN       6.491e+03  1.389e+03   4.673\nmedian_income:ocean_proximityISLAND                  NA         NA      NA\npopulation:ocean_proximity<1H OCEAN           3.293e+01  5.359e+00   6.146\npopulation:ocean_proximityINLAND              5.295e+01  5.665e+00   9.346\npopulation:ocean_proximityNEAR OCEAN          3.016e+01  5.986e+00   5.039\npopulation:ocean_proximityISLAND                     NA         NA      NA\ntotal_bedrooms:ocean_proximity<1H OCEAN       7.887e+01  4.823e+01   1.635\ntotal_bedrooms:ocean_proximityINLAND          1.316e+01  4.653e+01   0.283\ntotal_bedrooms:ocean_proximityNEAR OCEAN      1.946e+02  5.324e+01   3.654\ntotal_bedrooms:ocean_proximityISLAND                 NA         NA      NA\ntotal_rooms:ocean_proximity<1H OCEAN         -1.923e+01  3.235e+00  -5.944\ntotal_rooms:ocean_proximityINLAND            -1.211e+01  3.606e+00  -3.359\ntotal_rooms:ocean_proximityNEAR OCEAN        -1.661e+01  4.033e+00  -4.118\ntotal_rooms:ocean_proximityISLAND                    NA         NA      NA\n                                             Pr(>|t|)    \n(Intercept)                                   < 2e-16 ***\nhouseholds                                   0.018491 *  \nhousing_median_age                           4.09e-09 ***\nlatitude                                      < 2e-16 ***\nlongitude                                     < 2e-16 ***\nmedian_income                                 < 2e-16 ***\npopulation                                    < 2e-16 ***\ntotal_bedrooms                               0.588112    \ntotal_rooms                                  1.28e-05 ***\nocean_proximity<1H OCEAN                      < 2e-16 ***\nocean_proximityINLAND                         < 2e-16 ***\nocean_proximityNEAR OCEAN                     < 2e-16 ***\nocean_proximityISLAND                        0.011358 *  \nhouseholds:ocean_proximity<1H OCEAN          0.247758    \nhouseholds:ocean_proximityINLAND             0.032236 *  \nhouseholds:ocean_proximityNEAR OCEAN         0.000824 ***\nhouseholds:ocean_proximityISLAND             0.466136    \nhousing_median_age:ocean_proximity<1H OCEAN  0.021499 *  \nhousing_median_age:ocean_proximityINLAND     0.000118 ***\nhousing_median_age:ocean_proximityNEAR OCEAN 7.49e-05 ***\nhousing_median_age:ocean_proximityISLAND     0.202151    \nlatitude:ocean_proximity<1H OCEAN             < 2e-16 ***\nlatitude:ocean_proximityINLAND                < 2e-16 ***\nlatitude:ocean_proximityNEAR OCEAN            < 2e-16 ***\nlatitude:ocean_proximityISLAND               0.037765 *  \nlongitude:ocean_proximity<1H OCEAN            < 2e-16 ***\nlongitude:ocean_proximityINLAND               < 2e-16 ***\nlongitude:ocean_proximityNEAR OCEAN           < 2e-16 ***\nlongitude:ocean_proximityISLAND              0.021646 *  \nmedian_income:ocean_proximity<1H OCEAN       0.062856 .  \nmedian_income:ocean_proximityINLAND          0.019302 *  \nmedian_income:ocean_proximityNEAR OCEAN      2.99e-06 ***\nmedian_income:ocean_proximityISLAND                NA    \npopulation:ocean_proximity<1H OCEAN          8.15e-10 ***\npopulation:ocean_proximityINLAND              < 2e-16 ***\npopulation:ocean_proximityNEAR OCEAN         4.73e-07 ***\npopulation:ocean_proximityISLAND                   NA    \ntotal_bedrooms:ocean_proximity<1H OCEAN      0.101995    \ntotal_bedrooms:ocean_proximityINLAND         0.777349    \ntotal_bedrooms:ocean_proximityNEAR OCEAN     0.000259 ***\ntotal_bedrooms:ocean_proximityISLAND               NA    \ntotal_rooms:ocean_proximity<1H OCEAN         2.84e-09 ***\ntotal_rooms:ocean_proximityINLAND            0.000784 ***\ntotal_rooms:ocean_proximityNEAR OCEAN        3.85e-05 ***\ntotal_rooms:ocean_proximityISLAND                  NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65990 on 16313 degrees of freedom\n  (157 observations deleted due to missingness)\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6737 \nF-statistic: 845.1 on 40 and 16313 DF,  p-value: < 2.2e-16\n\n\nEn este caso el procedimiento de selección basado en el estadístico AIC nos indica que no debemos eliminar ningún efecto del modelo. Pasamos ahora con el análisis preliminar del modelo.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.lm\")\n# Modelo en términos de predictoras\nhousing_model = po(\"modelmatrix\", formula = ~ (households + housing_median_age + latitude + longitude + median_income + population + total_bedrooms + total_rooms)*ocean_proximity)\n\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_housing %>>% housing_model%>>% learner\ngr = GraphLearner$new(gr)\n\n# Entrenamiento del modelo\ngr$train(tsk_train_housing)\n# Resumen del modelo\nsummary(gr$model$regr.lm$model)\n\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-532506  -38972   -9861   26086  774873 \n\nCoefficients: (5 not defined because of singularities)\n                                               Estimate Std. Error t value\n(Intercept)                                     -157768      31451  -5.016\n`(Intercept)`                                        NA         NA      NA\nhouseholds                                        61037       8197   7.446\nhousing_median_age                                10185       1738   5.859\nlatitude                                        -417178      19793 -21.077\nlongitude                                       -604275      24737 -24.428\nmedian_income                                     69493       1900  36.583\npopulation                                       -83926       5861 -14.319\ntotal_bedrooms                                    -3154       7352  -0.429\ntotal_rooms                                       28690       6703   4.280\n`ocean_proximity<1H OCEAN`                       364606      31488  11.579\nocean_proximityINLAND                            318049      31490  10.100\n`ocean_proximityNEAR OCEAN`                      368934      31554  11.692\nocean_proximityISLAND                          34831276   17557395   1.984\n`households:ocean_proximity<1H OCEAN`            -21627      10095  -2.142\n`households:ocean_proximityINLAND`               -57010       9260  -6.156\n`households:ocean_proximityNEAR OCEAN`           -72170      13821  -5.222\n`households:ocean_proximityISLAND`               104792     144793   0.724\n`housing_median_age:ocean_proximity<1H OCEAN`      4414       1967   2.244\n`housing_median_age:ocean_proximityINLAND`        -8000       2042  -3.918\n`housing_median_age:ocean_proximityNEAR OCEAN`     9334       2382   3.919\n`housing_median_age:ocean_proximityISLAND`       112406      90554   1.241\n`latitude:ocean_proximity<1H OCEAN`              308751      20830  14.823\n`latitude:ocean_proximityINLAND`                 386940      19998  19.349\n`latitude:ocean_proximityNEAR OCEAN`             317776      20894  15.209\n`latitude:ocean_proximityISLAND`               45971405   22513642   2.042\n`longitude:ocean_proximity<1H OCEAN`             509488      25367  20.085\n`longitude:ocean_proximityINLAND`                574494      24915  23.058\n`longitude:ocean_proximityNEAR OCEAN`            506638      25494  19.873\n`longitude:ocean_proximityISLAND`              23779886   10496367   2.266\n`median_income:ocean_proximity<1H OCEAN`           3503       2122   1.651\n`median_income:ocean_proximityINLAND`             -6394       2466  -2.593\n`median_income:ocean_proximityNEAR OCEAN`         11535       2627   4.391\n`median_income:ocean_proximityISLAND`                NA         NA      NA\n`population:ocean_proximity<1H OCEAN`             37415       6113   6.120\n`population:ocean_proximityINLAND`                61329       6476   9.470\n`population:ocean_proximityNEAR OCEAN`            33398       6847   4.878\n`population:ocean_proximityISLAND`                   NA         NA      NA\n`total_bedrooms:ocean_proximity<1H OCEAN`         29554       9219   3.206\n`total_bedrooms:ocean_proximityINLAND`            17050       8804   1.937\n`total_bedrooms:ocean_proximityNEAR OCEAN`        74830      13252   5.647\n`total_bedrooms:ocean_proximityISLAND`               NA         NA      NA\n`total_rooms:ocean_proximity<1H OCEAN`           -40637       7134  -5.696\n`total_rooms:ocean_proximityINLAND`              -24565       7909  -3.106\n`total_rooms:ocean_proximityNEAR OCEAN`          -32991       8880  -3.715\n`total_rooms:ocean_proximityISLAND`                  NA         NA      NA\n                                               Pr(>|t|)    \n(Intercept)                                    5.32e-07 ***\n`(Intercept)`                                        NA    \nhouseholds                                     1.01e-13 ***\nhousing_median_age                             4.75e-09 ***\nlatitude                                        < 2e-16 ***\nlongitude                                       < 2e-16 ***\nmedian_income                                   < 2e-16 ***\npopulation                                      < 2e-16 ***\ntotal_bedrooms                                 0.667889    \ntotal_rooms                                    1.88e-05 ***\n`ocean_proximity<1H OCEAN`                      < 2e-16 ***\nocean_proximityINLAND                           < 2e-16 ***\n`ocean_proximityNEAR OCEAN`                     < 2e-16 ***\nocean_proximityISLAND                          0.047289 *  \n`households:ocean_proximity<1H OCEAN`          0.032171 *  \n`households:ocean_proximityINLAND`             7.62e-10 ***\n`households:ocean_proximityNEAR OCEAN`         1.79e-07 ***\n`households:ocean_proximityISLAND`             0.469238    \n`housing_median_age:ocean_proximity<1H OCEAN`  0.024877 *  \n`housing_median_age:ocean_proximityINLAND`     8.97e-05 ***\n`housing_median_age:ocean_proximityNEAR OCEAN` 8.92e-05 ***\n`housing_median_age:ocean_proximityISLAND`     0.214507    \n`latitude:ocean_proximity<1H OCEAN`             < 2e-16 ***\n`latitude:ocean_proximityINLAND`                < 2e-16 ***\n`latitude:ocean_proximityNEAR OCEAN`            < 2e-16 ***\n`latitude:ocean_proximityISLAND`               0.041174 *  \n`longitude:ocean_proximity<1H OCEAN`            < 2e-16 ***\n`longitude:ocean_proximityINLAND`               < 2e-16 ***\n`longitude:ocean_proximityNEAR OCEAN`           < 2e-16 ***\n`longitude:ocean_proximityISLAND`              0.023493 *  \n`median_income:ocean_proximity<1H OCEAN`       0.098783 .  \n`median_income:ocean_proximityINLAND`          0.009533 ** \n`median_income:ocean_proximityNEAR OCEAN`      1.14e-05 ***\n`median_income:ocean_proximityISLAND`                NA    \n`population:ocean_proximity<1H OCEAN`          9.55e-10 ***\n`population:ocean_proximityINLAND`              < 2e-16 ***\n`population:ocean_proximityNEAR OCEAN`         1.08e-06 ***\n`population:ocean_proximityISLAND`                   NA    \n`total_bedrooms:ocean_proximity<1H OCEAN`      0.001350 ** \n`total_bedrooms:ocean_proximityINLAND`         0.052802 .  \n`total_bedrooms:ocean_proximityNEAR OCEAN`     1.66e-08 ***\n`total_bedrooms:ocean_proximityISLAND`               NA    \n`total_rooms:ocean_proximity<1H OCEAN`         1.25e-08 ***\n`total_rooms:ocean_proximityINLAND`            0.001899 ** \n`total_rooms:ocean_proximityNEAR OCEAN`        0.000204 ***\n`total_rooms:ocean_proximityISLAND`                  NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 66000 on 16470 degrees of freedom\nMultiple R-squared:  0.6743,    Adjusted R-squared:  0.6735 \nF-statistic: 852.4 on 40 and 16470 DF,  p-value: < 2.2e-16\n\n\nComo el modelo obtenido es bastante complejo resulta bastante difícil escribir su expresión. La capacidad explicativa se sitúa en el 67% (no es muy alto) y el test f de regresión resulta significativo (p-valor < 0.05). Para poder visualizar los efectos del modelo se presenta la imagen siguiente donde se identifican los coeficientes positivos y negativos para cada uno de los coeficientes del modelo.\n\n# Data frame con los coeficientes obtenidos y su codificación )positivo-negativo\ncoeficientes = na.omit(as.data.frame(gr$model$regr.lm$model$coefficients))\ncoeficientes = rownames_to_column(coeficientes)\ncolnames(coeficientes) = c(\"Coef\", \"Estimate\")\ncoeficientes$Value = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n# Gráfico de coeficientes\nggplot(coeficientes, aes(Estimate, Coef, color = Value)) + \n  geom_point() + \n  geom_vline(xintercept = 0, linetype = 2, color = \"black\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFigura 6.11: Coeficientes estimados del modelo. Task Housing in California\n\n\n\n\nPodemos ver como longitude y latitude tienen los efectos inversamente proporcionales más grandes con respecto a median_house_value. Por otro lado, la categoría ISLAND de ocean_proximity muestra el efecto proporcional directo más grande con respecto a la variable target. De hecho, hay muchas interacciones con coeficientes positivos indicando que las variables numéricas se combinan con la situación geográfica para determinar el precio medio de la vivienda.\nLlevamos a cabo ahora el análisis de los residuos del modelo propuesto.\n\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_housing)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_housing)\n# Scores de validación\nmeasures = msr(c(\"regr.mse\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\n  regr.mse \n4344876945 \n\npred_test$score(measures)\n\n  regr.mse \n4346674536 \n\n\nEn este caso los valores del MSE son muy grandes debido a la escala de la variable target. Veamos el comportamiento de los residuos.\n\n# Muestra de entrenamiento\np1 = autoplot(pred_train, type = \"xy\") + labs(title = \"Observados vs predichos\")\np2 = autoplot(pred_train, type = \"residual\") + labs(title = \"Observados vs residuos\")\np3 = autoplot(pred_train, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\n# Muestra de validación\np4 = autoplot(pred_test, type = \"xy\") + labs(title = \"Observados vs predichos\")\np5 = autoplot(pred_test, type = \"residual\") + labs(title = \"Observados vs residuos\")\np6 = autoplot(pred_test, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\nggarrange(p1,p2,p3, p4, p5, p6, nrow = 2, ncol = 3)\n\n\n\n\nFigura 6.12: Gráficos del modelo para muestras de entrenamiento y validación. Task Housing in California\n\n\n\n\nSe ve claramente el comportamiento incorrecto de los residuos indicando que debemos modificar este modelo o considerar un algoritmo diferente para poder analizar estos datos. Para analizar con algo más de detalle realizamos el proceso de validación cruzada para estudiar el comportamiento del MSE.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada follad con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_housing, gr, resamp, store_models=TRUE)\n\nINFO  [17:37:33.318] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 1/10)\nINFO  [17:37:35.035] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 2/10)\nINFO  [17:37:36.093] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 3/10)\nINFO  [17:37:37.390] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 4/10)\nINFO  [17:37:38.696] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 5/10)\nINFO  [17:37:39.712] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 6/10)\nINFO  [17:37:40.645] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 7/10)\nINFO  [17:37:41.466] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 8/10)\nINFO  [17:37:42.613] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 9/10)\nINFO  [17:37:43.775] [mlr3] Applying learner 'scale.imputemedian.modelmatrix.regr.lm' on task 'housingCA' (iter 10/10)\n\n# Análisis de los valores obtenidos\nskim(rr$score()$regr.mse)\n\n\nData summary\n\n\nName\nrr\\(score()\\)regr.mse\n\n\nNumber of rows\n10\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n4646847813\n644036963\n3940159396\n4301710797\n4413536532\n4741344015\n6019491017\n▇▆▂▂▂\n\n\n\n\n\nObservamos la gran dispersión en los valores obtenidos debido en parte a la escala de la respuesta, y por otro al modelo planteado."
  },
  {
    "objectID": "60_RegressionModels.html#sec-60.4",
    "href": "60_RegressionModels.html#sec-60.4",
    "title": "6  Modelos de Regresión",
    "section": "6.3 Actualizando el modelo de regresión",
    "text": "6.3 Actualizando el modelo de regresión\nUna vez considerado un modelo de partida para nuestro conjunto de datos nos debemos plantear la posibilidad de mejorarlo para obtener modelos más sencillos o con mayor capacidad explicativa.\nAntes de estudiar las diferentes posibilidades de mejora que podemos plantear en los modelos lineales del cuaderno anterior, debemos generalizar algunos conceptos ya presentados para aprender sobre el efecto que tienen en la construcción de los modelos de aprendizaje. Aunque aquí presentamos los resultados para los modelos de regresión, parte de las ideas son generales en cualquier modelo de aprendizaje automático.\nEn primer lugar analizamos con un poco más de profundidad los conceptos de sesgo del modelo (Bias) y variabilidad del modelo (Variance), así como su efecto sobre el error de predicción que cometemos al establecer nuestro modelo de aprendizaje.\nEn la situación ideal donde tuviéramos toda la información posible de las predictoras consideradas en nuestro modelo de aprendizaje podríamos escribir:\n\\[y = f(X).\\]\nSin embargo, ya que habitualmente sólo disponemos de un conjunto de muestras nuestro modelo de aprendizaje lineal se expresa mediante:\n\\[y = f(x) + \\epsilon\\]\ndonde \\(\\epsilon\\) hace referencia al error cometido con las muestras seleccionadas, de forma que la predicción de la respuesta viene dada por:\n\\[\\hat{y} = \\hat{f}(X).\\]\nEl sesgo del modelo es el error cometido entre el promedio de las predicciones de nuestro modelo de aprendizaje y el verdadero modelo para nuestros datos:\n\\[bias=E(\\hat{y})- y,\\]\nque representa la capacidad del modelo considerado para predecir los valores de la respuesta.\nLa variabilidad del modelo representa la variabilidad promedio de las predicciones obtenidas para nuestro conjunto de muestras específico, es decir:\n\\[variance=E[(\\hat{y}-E(\\hat{y})^2],\\]\nque indica cuánto puede ajustarse el modelo considerado al cambio en el conjunto de datos que se utilizan para entrenarlo.\nEsto nos lleva a la ecuación de compromiso entre sesgo y variabilidad para determinar el error del modelo que viene dado por la expresión:\n\\[Error = \\text{sesgo}^2 + \\text{variabilidad} + \\text{error irreducible},\\]\ndonde el error irreducible es aquel que no somos capaces de reducir o explicar aunque aumentemos el número de predictoras o de muestras. Dado que nuestro objetivo es quedarnos con el modelo con un error lo más pequeño posible, es evidente que tendremos que mantener un equilibrio entre sesgo y variabilidad para reducirlo.\n¿Cómo afectan el sesgo y la variabilidad a nuestro modelo de aprendizaje?\n\nUn sesgo alto proporciona un modelo excesivamente simplificado, con un ajuste insuficiente, y con un alto nivel de error tanto en los datos de prueba como en los de entrenamiento.\nUna variabilidad alta proporciona un modelo excesivamente complejo, con un ajuste excesivo, y con un error bajo en los datos de entrenamiento y alto en los de prueba. En este caso interesa reducir la variabilidad de entrada (técnicas de reducción de la dimensión) antes de comenzar el proceso de modelización.\n\nEn la imagen siguiente podemos ver el efecto de los cambios en el sesgo y la variabilidad, donde asumimos que el centro de la diana es el valor verdadero que deseamos predecir (imagen reproducida de https://www.cheatsheets.aqeel-anwar.com)\n\nPor otro lado, en la imagen siguiente representamos el efecto que tiene el comportamiento del sesgo y la variabilidad en el ajuste del modelo. La línea gris punteada indica el punto con menos error y por tanto con mejor ajuste. En la parte izquierda tenemos el denominado modelo infra-ajustado (Under-fitting) que coincide con baja variabilidad y sesgo alto, mientras que en la parte derecha tenemos el modelo sobre-ajustado (Over-fitting) que coincide con alta variabilidad y sesgo bajo. En el mínimo error tenemos el compromiso entre sesgo y variabilidad (Just Right).\n\nEntendemos por infra-ajuste cuando el modelo no explica bien los datos de entrenamiento (los datos que se usaron para ajustar el modelo), ni esperamos que se generalice a ningún dato no visto previamente del mismo proceso de generación de datos. El infra-ajuste suele ser el resultado de un modelo que no es lo suficientemente complejo para ajustarse a los datos dados. La forma más fácil de abordarlo es aumentando la complejidad del modelo, por ejemplo, agregando más predictoras.\nEl sobre-ajuste, por otro lado, es el caso en el que el modelo se ajusta muy bien a los datos de entrenamiento pero no logra generalizar sus resultados a la muestra de validación o a otro nuevo conjunto de datos.\n\n6.3.1 Modelos de regresión penalizada\nEn este punto se presentan diferentes modificaciones de los modelos lineales vistos hasta ahora para solucionar problemas como el sobreajuste. Para ello se plantean diferentes métodos de regularización que son modificaciones del algoritmo de estimación de los parámetros del modelo introduciendo cierto tipo de restricciones o penalizaciones para evitar los problemas de sobreajuste o la tendencia a obtener modelos muy complejos. Se trata pues de establecer cierto equilibrio entre la capacidad explicativa y la complejidad del modelo. Estas modificaciones eliminan parte de los problemas que aparecen cuando utilizamos el criterio de los mínimos cuadrados habitual.\nEl objetivo que se persigue con este tipo de métodos es estudiar cómo varía el valor de los coeficientes del modelo manteniendo su capacidad explicativa. Es decir, buscamos un modelo cuyos coeficientes reflejen de la mejor manera posible la importancia de las variables y muestre las variables que realmente son relevantes a la hora de explicar la respuesta.\n\n6.3.1.1 Regresión de Cresta (Ridge regression)\nEl primer método de regularización que se presenta es la Regresión de Cresta (Ridge Regression) que aborda algunos de los problemas de los mínimos cuadrados ordinarios imponiendo una penalización sobre los coeficientes en el proceso de estimación. El problema principal que se trata de resolver con este método de regularización es el de colinealidad entre las predictoras. Este problema es muy común y aparece cuando las predictoras están muy correlacionadas entre sí, lo que es muy habitual cuando disponemos de bancos de datos con muchas variables. Para un modelo lineal de la forma:\n\\[\\mathbf{y} = \\mathbf{X}\\mathbf{w} +\\mathbf{\\epsilon}\\]\nlos coeficientes de la cresta minimizan una suma de cuadrados residual penalizada:\n\\[\\underset{w}{min} (||\\mathbf{y} - \\mathbf{X}\\mathbf{w}||_2^2 + \\alpha ||\\mathbf{w}||_2^2)\\]\ndonde el parámetro de complejidad \\(\\alpha \\geq 0\\) controla la cantidad de contracción, es decir, cuanto mayor sea el valor de \\(\\alpha\\), mayor será la cantidad de contracción y, por tanto, los coeficientes serán más resistentes a la colinealidad, y \\(||.||_2\\) es la norma \\(L_2\\) del vector de coeficientes. En este caso el parámetro \\(\\alpha\\) controla el grado de dispersión de los coeficientes estimados.\n\n\n6.3.1.2 Regresión de Lazo (Lasso regression)\nLa Regresión de Lazo (Lasso Regression) es un modelo lineal que estima coeficientes dispersos. Resulta útil en algunos contextos debido a su tendencia a preferir soluciones con menos coeficientes distintos de cero, reduciendo de hecho el número de variables predictoras de las que depende la solución dada. Matemáticamente, consiste en un modelo lineal con un término de regularización añadido. La función objetivo a minimizar es:\n\\[\\underset{\\mathbf{w}}{min} \\frac{1}{2n}(||\\mathbf{y} - \\mathbf{X}\\mathbf{w}||_2^2 + \\alpha ||\\mathbf{w}||_1)\\]\nLa estimación Lasso resuelve así la minimización de la penalización por mínimos cuadrados añadiendo el término \\(\\alpha ||\\mathbf{w}||_1\\), donde \\(\\alpha\\) es una constante y \\(||.||_1\\) es la norma \\(L_1\\) del vector de coeficientes. En este caso el parámetro \\(\\alpha\\) controla el grado de dispersión de los coeficientes estimados.\nEste algoritmo está concebido para ir eliminando progresivamente predictores en el modelo, de forma que podemos estudiar cómo evoluciona el número de predictores en el modelo conforme varía el valor de la penalización (\\(\\alpha\\)). A mayor penalización menor número de predictores en el modelo.\n\n\n6.3.1.3 Regresión de Red Elástica (Elastic Net)\nLa Regresión de Red Elástica (Elastic Net) es una modificación de la regresión lineal que combina la penalización l1 y l2, es decir, la regresión de Lazo y la de Cresta. En el procedimiento, la estrategia habitual es primero encontrar el coeficiente de Regresión de Cresta y después realizar un algoritmo de Lazo sobre el coeficiente de regresión para reducirlo. La combinación de ambas penalizaciones suele dar lugar a buenos resultados.\nLa función objetivo a minimizar es una especie de mixtura entre ambas penalizaciones y viene dada por:\n\\[\\underset{\\mathbf{w}}{min} \\frac{1}{2n}(||\\mathbf{y} - \\mathbf{X}\\mathbf{w}||_2^2 + \\alpha*l1ratio*||\\mathbf{w}||_1 + 0.5*\\alpha*(1-l1ratio) ||\\mathbf{w}||_2^2\\]\nEl grado en que influye cada una de las penalizaciones está controlado por el hiperparámetro \\(l1ratio\\), cuyo valor está comprendido entre los valores 0 y 1, de forma que si toma el valor 0 estamos con la penalización l2 (Regresión de Lazo), mientras que si toma el valor 1 estamos con la penalización l1 (Regresión de Cresta).\nEn forma reducida podemos expresar el término de penalización como:\n\\[a||\\mathbf{w}||_1 + 0.5*b||\\mathbf{w}||_2^2,\\]\nde forma que\n\\[\\alpha = a+b \\quad \\text{y} \\quad l1ratio=\\frac{a}{a+b}.\\]\n\n\n6.3.1.4 Regresión penalizada en mlr3\nLa regresión penalizada con el paquete mlr3 se puede llevar a cabo con los learner regr.glmnet y regr.cv_glmnet.\nregr.glmnet utiliza para valores fijos de \\(\\alpha\\) y se optimiza el valor de lambda o valor que define la cantidad de contracción de los coeficientes. El valor de \\(\\alpha\\) determina el tipo de modelo utilizado:\n\n\\(\\alpha = 0\\) para la lasso regression.\n\\(\\alpha = 1\\) para ridge regression.\n\\(0 < \\alpha < 1\\) para elastic net.\n\nregr.cv_glmnet es similar al anterior pero realiza un estudio de validación cruzada para obtener el valor de `\\(\\lambda\\) que proporciona un score de validación más pequeño. Este ultimo se puede combinar con un grid search para determinar la combinación óptima de \\(\\alpha\\) y \\(\\lambda\\).\nPara entender el funcionamiento de estos modelos nos centraremos en el banco de datos Meat spec donde todas las características están altamente correlacionadas. Comenzaremos con modelos independientes y finalizaremos con el modelo de optimización de ambos parámetros de interés.\nEn primer lugar realizamos la división de muestras de entrenamiento y validación considerando la regla 80-20.\n\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_meatspec, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_meatspec = tsk_meatspec$clone()$filter(splits$train)\ntsk_test_meatspec  = tsk_meatspec$clone()$filter(splits$test)\n\n\nRidge regression\nComenzamos configurando el learner de aprendizaje teniendo en cuenta el algoritmo que vamos a utilizar y las tareas de preprocesado específicas para este banco de datos.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.cv_glmnet\", alpha = 0)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n\nComenzamos con el análisis del modelo obteniendo en primer lugar el valor de \\(\\lambda\\) óptimo.\n\n# Modelo obtenido para todos los lambda\nmodelo_AA = gr$model$regr.cv_glmnet$model\n# Lambda óptimo\nmodelo_AA$lambda.min\n\n[1] 0.8705676\n\n\nPodemos representar el proceso de validación cruzada a través del gráfico siguiente.\n\nplot(modelo_AA)\n\n\n\n\nFigura 6.13: Gráfico validación cruzada para estimación de \\(\\lambda\\). Task Meat Spec\n\n\n\n\nLos puntos rojos indican la curva de validación cruzada junto con las curvas de desviación estándar superior e inferior a lo largo de la secuencia de \\(log(\\lambda)\\). la líneas punteadas verticales representan lambda.min y lambda.1se que proporciona el modelo más regularizado de modo que el error de validación cruzada esté dentro de un error estándar del mínimo.\nVeamos las características del modelo ajustado utilizando el valor óptimo de \\(\\lambda\\) para obtener le modelo final haciendo uso del learner regr.glmnet. Entrenamos el nuevo modelo con esta configuración:\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.glmnet\", alpha = 0, lambda = modelo_AA$lambda.min)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n\n# modelo resultante\nmodelo_glmnet_AA = gr$model$regr.glmnet$model\n\nPara visualizar el efecto de los coeficientes sobre a respuesta resulta más útil la creación de un gráfico donde podemos ver su comportamiento:\n\n# Data frame con los coeficientes obtenidos y su codificación) positivo-negativo quitando intercept\ncoeficientes = as.data.frame(as.matrix(modelo_glmnet_AA$beta))\ncoeficientes = rownames_to_column(coeficientes)\ncolnames(coeficientes) = c(\"Coef\", \"Estimate\")\ncoeficientes$Value = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n# Gráfico de coeficientes\nggplot(coeficientes, aes(Coef, Estimate, color = Value)) + \n  geom_point() + \n  geom_hline(yintercept = 0, linetype = 2, color = \"black\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFigura 6.14: Coeficientes del modelo Ridge. Task Meat Spec\n\n\n\n\nEn este gráfico se aprecia claramente el efecto de la penalización. Los coeficientes próximos se penalizan para tener coeficientes similares, de forma que no se producen cambios bruscos entre dos coeficientes consecutivos. Además podemos detectar el conjunto de coeficientes que influye positivamente y negativamente sobre el contenido de grasa, es decir, detectamos de forma sencilla los rango del espectómetro que afectan más.\nEvaluamos los scores asociados a la muestra de entrenamiento y validación:\n\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_meatspec)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_meatspec)\n# Scores de validación\nmeasures = msr(c(\"regr.mse\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n25.21102 \n\npred_test$score(measures)\n\nregr.mse \n 18.1877 \n\n\nEstos valores los utilizaremos para comparar con el resto de modelos penalizados que vemos a continuación.\n\n\nLasso regression\nComenzamos configurando el learner de aprendizaje.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.cv_glmnet\", alpha = 1)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n\nUna vez entrenado el modelo obtenemos el valor de \\(\\lambda\\) óptimo y representamos gráficamente el proceso .\n\n# Modelo obtenido para todos los lambda\nmodelo_AA = gr$model$regr.cv_glmnet$model\n# Lambda óptimo\nmodelo_AA$lambda.min\n\n[1] 0.0117792\n\nplot(modelo_AA)\n\n\n\n\nFigura 6.15: Gráfico de validación cruzada para estimar \\(\\lambda\\). Task Meat Spec\n\n\n\n\nEntrenamos el modelo con el valor óptimo.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.glmnet\", alpha = 1, lambda = modelo_AA$lambda.min)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n\n# modelo resultante\nmodelo_glmnet_AA = gr$model$regr.glmnet$model\n\nPara visualizar el efecto de los coeficientes sobre a respuesta resulta más útil la creación de un gráfico donde podemos ver su comportamiento:\n\n# Data frame con los coeficientes obtenidos y su codificación) positivo-negativo quitando intercept\ncoeficientes = as.data.frame(as.matrix(modelo_glmnet_AA$beta))\ncoeficientes = rownames_to_column(coeficientes)\ncolnames(coeficientes) = c(\"Coef\", \"Estimate\")\ncoeficientes$Value = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n# Gráfico de coeficientes\nggplot(coeficientes, aes(Coef, Estimate, color = Value)) + \n  geom_point() + \n  geom_hline(yintercept = 0, linetype = 2, color = \"black\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFigura 6.16: Coeficientes de modelo Lasso. Task Meat Spec\n\n\n\n\nEn este caso el efecto de la penalización es muy relevante, porque se aprecia claramente la cantidad de coeficientes cero que establece el modelo. Este el efecto que se busca en estos modelos donde hay muchas predictoras y queremos reducir su número en el modelo final. Evaluamos los scores asociados a la muestra de entrenamiento y validación:\n\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_meatspec)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_meatspec)\n# Scores de validación\nmeasures = msr(c(\"regr.mse\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n10.13226 \n\npred_test$score(measures)\n\nregr.mse \n 6.91897 \n\n\nClaramente el modelo lasso es mucho mejor que el ridge ya que reduce claramente el mse tanto en la muestra de entrenamiento y validación. Además como el número de coeficientes distintos de cero es bastante pequeño tenemos una expresión del modelo lineal bastante sencilla que se pude usar con fines predictivos.\n\n\nElastic Net\nComenzamos configurando el learner de aprendizaje. En este caso tomamos un valor de \\(\\alpha = 0.3\\) para analizar su comportamiento.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.cv_glmnet\", alpha = 0.3)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n\nUna vez entrenado el modelo obtenemos el valor de \\(\\lambda\\) óptimo y representamos gráficamente el proceso .\n\n# Modelo obtenido para todos los lambda\nmodelo_AA = gr$model$regr.cv_glmnet$model\n# Lambda óptimo\nmodelo_AA$lambda.min\n\n[1] 0.005071139\n\nplot(modelo_AA)\n\n\n\n\nFigura 6.17: Validación cruzada estimación de \\(\\lambda\\). Task Meat Spec”\n\n\n\n\nEntrenamos el modelo con el valor óptimo.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.glmnet\", alpha = 1, lambda = modelo_AA$lambda.min)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n\n# modelo resultante\nmodelo_glmnet_AA = gr$model$regr.glmnet$model\n\nPara visualizar el efecto de los coeficientes sobre a respuesta resulta más útil la creación de un gráfico donde podemos ver su comportamiento:\n\n# Data frame con los coeficientes obtenidos y su codificación) positivo-negativo quitando intercept\ncoeficientes = as.data.frame(as.matrix(gr$model$regr.glmnet$model$beta))\ncoeficientes = rownames_to_column(coeficientes)\ncolnames(coeficientes) = c(\"Coef\", \"Estimate\")\ncoeficientes$Value = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n# Gráfico de coeficientes\nggplot(coeficientes, aes(Coef, Estimate, color = Value)) + \n  geom_point() + \n  geom_hline(yintercept = 0, linetype = 2, color = \"black\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFigura 6.18: Coeficientes del modelo Elastic-Net. Task Meat Spec”\n\n\n\n\nEn este caso se aprecia una mezcla entre los dos modelos anteriores. Hay bastantes coeficientes cero como en el modelo anterior. Evaluamos los scores asociados a la muestra de entrenamiento y validación:\n\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_meatspec)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_meatspec)\n# Scores de validación\nmeasures = msr(c(\"regr.mse\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n9.424514 \n\npred_test$score(measures)\n\nregr.mse \n6.287592 \n\n\nEste modelo combinado de las dos penalizaciones es el que muestra valores similares de los scores.\n\n\nOptmizando \\(\\alpha\\)\nPara finalizar este apartado vamos a ver el procedimiento para la obtención del \\(\\alpha\\) de forma semi automática, sin necesidad de especificar un valor desde el inicio. Para poder hacer esto debemos introducir nuevas funciones, que detallamos a continuación, que nos servirán de aquí en adelante en todos los procesos de optimización de los hiperpárametros de los algoritmos de aprendizaje. Para poder introducir y utilizar dichas funciones es necesario instalar en primer lugar el paquete mlr3tuning.\n\nlibrary(mlr3tuning)\n\nEl proceso de optimización comienza decidiendo qué hiperparámetros ajustar y en qué rango ajustarlos. Para conocer los posibles hiperparámetros asociados con una algoritmo de aprendizaje podemos utilizar el método $param_set asociado con él. En el caso que nos ocupa tenemos:\n\nas.data.table(lrn(\"regr.cv_glmnet\")$param_set)[,\n  .(id, class, lower, upper, nlevels)]\n\n                      id    class lower upper nlevels\n 1:            alignment ParamFct    NA    NA       2\n 2:                alpha ParamDbl     0     1     Inf\n 3:                  big ParamDbl  -Inf   Inf     Inf\n 4:               devmax ParamDbl     0     1     Inf\n 5:                dfmax ParamInt     0   Inf     Inf\n 6:                  eps ParamDbl     0     1     Inf\n 7:                epsnr ParamDbl     0     1     Inf\n 8:              exclude ParamInt     1   Inf     Inf\n 9:                 exmx ParamDbl  -Inf   Inf     Inf\n10:               family ParamFct    NA    NA       2\n11:                 fdev ParamDbl     0     1     Inf\n12:               foldid ParamUty    NA    NA     Inf\n13:                gamma ParamUty    NA    NA     Inf\n14:              grouped ParamLgl    NA    NA       2\n15:            intercept ParamLgl    NA    NA       2\n16:                 keep ParamLgl    NA    NA       2\n17:               lambda ParamUty    NA    NA     Inf\n18:     lambda.min.ratio ParamDbl     0     1     Inf\n19:         lower.limits ParamUty    NA    NA     Inf\n20:                maxit ParamInt     1   Inf     Inf\n21:                mnlam ParamInt     1   Inf     Inf\n22:                 mxit ParamInt     1   Inf     Inf\n23:               mxitnr ParamInt     1   Inf     Inf\n24:               nfolds ParamInt     3   Inf     Inf\n25:              nlambda ParamInt     1   Inf     Inf\n26:               offset ParamUty    NA    NA     Inf\n27:             parallel ParamLgl    NA    NA       2\n28:       penalty.factor ParamUty    NA    NA     Inf\n29:                 pmax ParamInt     0   Inf     Inf\n30:                 pmin ParamDbl     0     1     Inf\n31:                 prec ParamDbl  -Inf   Inf     Inf\n32:        predict.gamma ParamDbl  -Inf   Inf     Inf\n33:                relax ParamLgl    NA    NA       2\n34:                    s ParamDbl     0   Inf     Inf\n35:          standardize ParamLgl    NA    NA       2\n36: standardize.response ParamLgl    NA    NA       2\n37:               thresh ParamDbl     0   Inf     Inf\n38:             trace.it ParamInt     0     1       2\n39:        type.gaussian ParamFct    NA    NA       2\n40:        type.logistic ParamFct    NA    NA       2\n41:         type.measure ParamFct    NA    NA       5\n42:     type.multinomial ParamFct    NA    NA       2\n43:         upper.limits ParamUty    NA    NA     Inf\n                      id    class lower upper nlevels\n\n\nCon recursos infinitos, podríamos ajustar todos los hiperparámetros de forma conjunta, pero en realidad eso no es posible (o quizás necesario), por lo que normalmente solo se puede ajustar un subconjunto de hiperparámetros. Este subconjunto de posibles valores de hiperparámetros para optimizar se denomina espacio de búsqueda. En este ejemplo hay 43 hiperparámetros asociados con el algoritmo y nos vamos a centrar únicamente en optimizar el valor de \\(\\alpha\\). En espacios de búsqueda más complejos pueden requerir conocimientos expertos para definirlos. Una forma avanzada de optimización es definir espacios de búsqueda predefinidos mediante el uso del paquete mlr3tuningspaces. para utilizar dichos espacios de búsqueda predefinidos debemos utilizar la función lts(). A continuación se muestra como realizar la optimización definiendo nuestro propio espacio de búsqueda y posteriormente utilizando espacios de búsqueda predefinidos. La colección de algoritmos que permite espacios de búsqueda predefinidos se pueden consultar en este enlace.\n\n\n\n\n\n\nEn casos excepcionales, los conjuntos de parámetros pueden incluir hiperparámetros que no deben ajustarse. Por lo general, serán parámetros “técnicos” (o de “control”) que proporcionan información sobre cómo se ajusta el modelo pero no controlan el proceso de entrenamiento en sí.\n\n\n\n\nEspacio de búsqueda personalizado\nPara los hiperparámetros numéricos (exploraremos otros más adelante), se deben especificar los límites del proceso de optimización. Hacemos esto construyendo un learner y usando la función to_tune() para establecer los límites superior e inferior de los parámetros que queremos ajustar.\nEn nuestro caso para el ejemplo con el que venimos trabajando en este apartado vamos a optimizar únicamente el valor de \\(\\alpha\\) mediante la definición del correspondiente learner. Definimos los posibles valor del hiperparámetro en el intervalo \\([0.01, 1]\\).\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.cv_glmnet\", \n              alpha = to_tune(1e-10, 1))\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n\nEl paquete mlr3tuning incluye muchos métodos para especificar cuándo terminar el proceso de optimización, que se implementan en las clases Terminator. Los terminadores se almacenan en el diccionario mlr_terminators y se construyen con la función sugar trm(). La lista de terminadores disponibles se pueden consultar en la tabla siguiente:\n\n\n\n\n\n\n\nTerminator\nFunción y parámetros por defecto\n\n\n\n\nClock Time\ntrm(\"clock_time\")\n\n\nCombo\ntrm(\"combo\", any = TRUE)\n\n\nNone\ntrm(\"none\")\n\n\nNumber of Evaluations\ntrm(\"evals\", n_evals = 100, k = 0)\n\n\nPerformance Level\ntrm(\"perf_reached\", level = 0.1)\n\n\nRun Time\ntrm(\"run_time\", secs = 30)\n\n\nStagnation\ntrm(\"stagnation\", iters = 10, threshold = 0)\n\n\n\nLos terminadores más comúnmente utilizados son aquellos que detienen el ajuste después de un cierto tiempo (trm(\"run_time\")) o un número determinado de evaluaciones (trm(\"evals\")). La elección de un tiempo de ejecución suele basarse en consideraciones prácticas y en la intuición. Usar un límite de tiempo puede ser importante en clústeres de computación donde es posible que sea necesario especificar un tiempo de ejecución máximo para un trabajo de computación. trm(\"perf_reached\") detiene el ajuste cuando se alcanza un nivel de rendimiento específico, lo que puede ser útil si cierto rendimiento se considera suficiente para el uso práctico del modelo; sin embargo, si se establece de manera demasiado optimista, el ajuste puede nunca terminar. trm(\"stagnation\") se detiene cuando no se ha realizado ningún progreso mayor que el umbral durante un número determinado de iteraciones. El umbral puede ser difícil de seleccionar ya que la optimización podría detenerse demasiado pronto para espacios de búsqueda complejos a pesar de que hay margen para mejoras (posiblemente significativas). trm(\"none\") se usa para sintonizadores que controlan la terminación por sí mismos y, por lo tanto, este terminador no hace nada. Finalmente, cualquiera de estos terminadores se puede combinar libremente usando trm(\"combo\"), que se puede usar para especificar si HPO finaliza cuando se activa cualquier terminador (any = TRUE) o cuando todos (any = FALSE) se activan.\nLa instancia de ajuste recopila la información independiente del sintonizador necesaria para optimizar un modelo, es decir, toda la información sobre el proceso de ajuste, excepto el algoritmo de ajuste en sí. Esto incluye la tarea de ajustar, el algoritmo de aprendizaje, el método de remuestreo y la medida utilizados para comparar analíticamente las configuraciones de optimización de hiperparámetros, y el terminador para determinar cuándo la medida se ha optimizado “suficientemente”. Esto define implícitamente una función objetivo de “caja negra”, que asigna configuraciones de hiperparámetros a valores de rendimiento (estocásticos) que se optimizarán. Se puede construir una instancia con la función ti().\nPara el ejemplo que venimos trabajando, utilizaremos una partición 80-20 de la muestra de trabajo en la práctica esto supone que trabajamos con el reparto 80-20) y optimizaremos la medida del error de clasificación. En este caso el terminador establece 15 iteraciones del algoritmo de optimización. Al finalizar verificaremos si el algoritmo ha convergido o necesitamos aumentar el número de iteraciones.\n\n# Fijamos semilla para reproducibilidad\nset.seed(123)\n# Definimos instancia\ninstance = ti(\n  task = tsk_train_meatspec,\n  learner = gr,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measures = msr(\"regr.mse\"),\n  terminator = trm(\"evals\", n_evals = 15)\n)\n\ninstance\n\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:scale.regr.cv_glmnet_on_meatspec>\n* Search Space:\n                     id    class lower upper nlevels\n1: regr.cv_glmnet.alpha ParamDbl 1e-10     1     Inf\n* Terminator: <TerminatorEvals>\n\n\nCon todas las piezas de nuestro problema de optimización ensambladas, ahora podemos decidir cómo optimizar nuestro modelo. Hay múltiples clases \"Tuner en mlr3tuning, que implementan diferentes algoritmos HPO (o más generalmente hablando optimización de caja negra. A continuación se muestra una tabla con todos ellos, así como la librería que debe cargarse para poder ser utilizado el correspondiente algoritmo de optimización:\n\n\n\n\n\n\n\n\nTuner\nFunción\nLibrería\n\n\n\n\nRandom Search\ntnr(\"random_search\")\nmlr3tuning\n\n\nGrid Search\ntnr(\"grid_search\")\nmlr3tuning\n\n\nBayesian Optimization\ntnr(\"mbo\")\nmlr3mbo\n\n\nCMA-ES\ntnr(\"cmaes\")\nadagio\n\n\nIterated Racing\ntnr(\"irace\")\nirace\n\n\nHyperband\ntnr(\"hyperband\")\nmlr3hyperband\n\n\nGeneralized Simulated Annealing\ntnr(\"gensa\")\nGenSA\n\n\nNonlinear Optimization\ntnr(\"nloptr\")\nnloptr\n\n\n\nSe puede consultar la actualización de estos algoritmos en este enlace.\nLa Grid search (búsqueda en cuadrícula) y la Random search (búsqueda aleatoria) son los algoritmos más básicos y, a menudo, se seleccionan primero en los experimentos iniciales. La idea de la búsqueda en cuadrícula es evaluar exhaustivamente cada combinación posible de valores de hiperparámetros dados. Los hiperparámetros categóricos generalmente se evalúan sobre todos los valores posibles que pueden tomar. Luego, los valores de hiperparámetros numéricos y enteros se espacian equidistantemente en sus restricciones (límites superior e inferior) de acuerdo con una resolución determinada, que es el número de valores distintos que se deben probar por hiperparámetro.\nLa búsqueda aleatoria implica seleccionar aleatoriamente valores para cada hiperparámetro independientemente de una distribución predeterminada, generalmente uniforme. Ambos métodos no son adaptativos, lo que significa que cada configuración propuesta ignora el rendimiento de configuraciones anteriores. Debido a su simplicidad, tanto la búsqueda en cuadrícula como la búsqueda aleatoria pueden manejar espacios de búsqueda mixtos (es decir, los hiperparámetros pueden ser numéricos, enteros o categóricos), así como espacios de búsqueda jerárquicos. El resto de algoritmos son del tipo adaptativo en los que se aprende de configuraciones evaluadas previamente para encontrar buenas configuraciones rápidamente.\nComo regla general, si el espacio de búsqueda es pequeño o no tiene una estructura compleja, la búsqueda en cuadrícula puede evaluar exhaustivamente todo el espacio de búsqueda en un tiempo razonable. Sin embargo, generalmente no se recomienda debido a la maldición de la dimensionalidad (el tamaño de la cuadrícula “explota” muy rápidamente a medida que aumenta el número de parámetros a ajustar) y la cobertura insuficiente de los espacios de búsqueda numéricos. Por construcción, la búsqueda en cuadrícula no puede evaluar una gran cantidad de valores únicos por hiperparámetro, lo cual no es óptimo cuando algunos hiperparámetros tienen un impacto mínimo en el rendimiento mientras que otros sí. En tales escenarios, random search suele ser una mejor opción ya que considera más valores únicos por hiperparámetro en comparación con la búsqueda en cuadrícula.\nPara espacios de búsqueda de dimensiones superiores o espacios de búsqueda con estructura más compleja, los algoritmos de optimización más guiados, como las estrategias evolutivas o la optimización bayesiana, tienden a funcionar mejor y es más probable que den como resultado un rendimiento máximo. En este caso el coste de la evaluación de la función es muy relevante. Si las configuraciones de hiperparámetros se pueden evaluar rápidamente, las estrategias evolutivas suelen funcionar bien. Por otro lado, si las evaluaciones del modelo consumen mucho tiempo y el presupuesto de optimización es limitado, generalmente se prefiere la optimización bayesiana, ya que es bastante eficiente en comparación con otros algoritmos, es decir, se necesitan menos evaluaciones de funciones para encontrar buenas configuraciones. Por lo tanto, generalmente se recomienda la optimización bayesiana para HPO. Si bien la sobrecarga de optimización de la optimización bayesiana es comparativamente grande (por ejemplo, en cada iteración, entrenamiento del modelo sustituto y optimización de la función de adquisición), esto tiene un impacto menor en el contexto de evaluaciones de funciones relativamente costosas, como el remuestreo de modelos de ML.\nEn nuestro caso al tener un único parámetro para optimizar vamos a utilizar random_search. A continuación se muestra el código para el proceso de optimización:\n\n# Definimos el optimizador\ntuner = tnr(\"random_search\")\n# Optimización\ntuner$optimize(instance)\n\nINFO  [17:37:56.114] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=15, k=0]'\nINFO  [17:37:56.155] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:37:56.307] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:37:56.318] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:37:57.902] [mlr3] Finished benchmark\nINFO  [17:37:57.939] [bbotk] Result of batch 1:\nINFO  [17:37:57.943] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:37:57.943] [bbotk]             0.2162548 12.95327        0      0            1.573\nINFO  [17:37:57.943] [bbotk]                                 uhash\nINFO  [17:37:57.943] [bbotk]  efc37cf0-b170-46dd-9b91-5146cb938907\nINFO  [17:37:57.949] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:37:58.000] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:37:58.006] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:37:59.321] [mlr3] Finished benchmark\nINFO  [17:37:59.387] [bbotk] Result of batch 2:\nINFO  [17:37:59.389] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:37:59.389] [bbotk]             0.7008531 13.67589        0      0            1.293\nINFO  [17:37:59.389] [bbotk]                                 uhash\nINFO  [17:37:59.389] [bbotk]  af6319e9-0fb6-48f2-8815-421c17965845\nINFO  [17:37:59.395] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:37:59.445] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:37:59.451] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:00.439] [mlr3] Finished benchmark\nINFO  [17:38:00.474] [bbotk] Result of batch 3:\nINFO  [17:38:00.476] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:00.476] [bbotk]             0.8209513 12.35682        0      0            0.979\nINFO  [17:38:00.476] [bbotk]                                 uhash\nINFO  [17:38:00.476] [bbotk]  3bbdd0f1-41aa-44ff-a170-857c0bbf39a3\nINFO  [17:38:00.480] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:00.526] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:00.533] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:01.414] [mlr3] Finished benchmark\nINFO  [17:38:01.453] [bbotk] Result of batch 4:\nINFO  [17:38:01.454] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:01.454] [bbotk]             0.9611048 16.82053        0      0            0.872\nINFO  [17:38:01.454] [bbotk]                                 uhash\nINFO  [17:38:01.454] [bbotk]  0e9fd57e-d5b0-425c-b9ed-b6fd1a7ab1c2\nINFO  [17:38:01.457] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:01.499] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:01.506] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:03.173] [mlr3] Finished benchmark\nINFO  [17:38:03.205] [bbotk] Result of batch 5:\nINFO  [17:38:03.207] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:03.207] [bbotk]            0.05284394 14.10114        0      0             1.66\nINFO  [17:38:03.207] [bbotk]                                 uhash\nINFO  [17:38:03.207] [bbotk]  7f8a46e5-129d-46bd-905a-f6697ab6303b\nINFO  [17:38:03.210] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:03.256] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:03.262] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:04.141] [mlr3] Finished benchmark\nINFO  [17:38:04.171] [bbotk] Result of batch 6:\nINFO  [17:38:04.173] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:04.173] [bbotk]             0.5602533 16.48643        0      0             0.87\nINFO  [17:38:04.173] [bbotk]                                 uhash\nINFO  [17:38:04.173] [bbotk]  f3398338-c165-4301-9dee-4bb2b17a185c\nINFO  [17:38:04.177] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:04.220] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:04.226] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:04.926] [mlr3] Finished benchmark\nINFO  [17:38:04.964] [bbotk] Result of batch 7:\nINFO  [17:38:04.966] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:04.966] [bbotk]             0.6183512 12.73543        0      0            0.692\nINFO  [17:38:04.966] [bbotk]                                 uhash\nINFO  [17:38:04.966] [bbotk]  f4621b34-694c-4453-8811-220cda6d4551\nINFO  [17:38:04.969] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:05.016] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:05.022] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:06.707] [mlr3] Finished benchmark\nINFO  [17:38:06.738] [bbotk] Result of batch 8:\nINFO  [17:38:06.740] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:06.740] [bbotk]            0.05847849 14.47908        0      0            1.677\nINFO  [17:38:06.740] [bbotk]                                 uhash\nINFO  [17:38:06.740] [bbotk]  80fa73be-af36-40bc-8e72-8baa8f7b4f29\nINFO  [17:38:06.743] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:06.784] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:06.791] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:07.901] [mlr3] Finished benchmark\nINFO  [17:38:07.933] [bbotk] Result of batch 9:\nINFO  [17:38:07.935] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:07.935] [bbotk]             0.1977447 13.48541        0      0            1.101\nINFO  [17:38:07.935] [bbotk]                                 uhash\nINFO  [17:38:07.935] [bbotk]  7e09b013-9438-4f1e-98c3-ae22b644f480\nINFO  [17:38:07.938] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:07.983] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:07.990] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:08.748] [mlr3] Finished benchmark\nINFO  [17:38:08.779] [bbotk] Result of batch 10:\nINFO  [17:38:08.781] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:08.781] [bbotk]             0.8034185 15.94768        0      0             0.75\nINFO  [17:38:08.781] [bbotk]                                 uhash\nINFO  [17:38:08.781] [bbotk]  36ae17b5-3f3c-40e4-aaa4-c58e80d59465\nINFO  [17:38:08.784] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:08.828] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:08.834] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:10.129] [mlr3] Finished benchmark\nINFO  [17:38:10.160] [bbotk] Result of batch 11:\nINFO  [17:38:10.162] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:10.162] [bbotk]             0.1716985 14.42765        0      0            1.286\nINFO  [17:38:10.162] [bbotk]                                 uhash\nINFO  [17:38:10.162] [bbotk]  f3ee1a18-1bf6-4562-a532-36955880f932\nINFO  [17:38:10.165] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:10.210] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:10.217] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:11.036] [mlr3] Finished benchmark\nINFO  [17:38:11.067] [bbotk] Result of batch 12:\nINFO  [17:38:11.069] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:11.069] [bbotk]             0.7245543 12.53504        0      0            0.812\nINFO  [17:38:11.069] [bbotk]                                 uhash\nINFO  [17:38:11.069] [bbotk]  b2a14988-2a5b-47d2-97c8-6ea50ba0205a\nINFO  [17:38:11.072] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:11.116] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:11.122] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:11.965] [mlr3] Finished benchmark\nINFO  [17:38:11.997] [bbotk] Result of batch 13:\nINFO  [17:38:12.000] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:12.000] [bbotk]             0.9673984 15.30346        0      0            0.834\nINFO  [17:38:12.000] [bbotk]                                 uhash\nINFO  [17:38:12.000] [bbotk]  e72be6f8-4a3e-430b-a5d2-995ee81f181d\nINFO  [17:38:12.003] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:12.046] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:12.052] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:13.094] [mlr3] Finished benchmark\nINFO  [17:38:13.125] [bbotk] Result of batch 14:\nINFO  [17:38:13.127] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:13.127] [bbotk]             0.2217879 16.35465        0      0            1.033\nINFO  [17:38:13.127] [bbotk]                                 uhash\nINFO  [17:38:13.127] [bbotk]  c9975e2a-9b2d-43be-a582-a7fe9b9c606e\nINFO  [17:38:13.130] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:13.172] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:13.178] [mlr3] Applying learner 'scale.regr.cv_glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:14.027] [mlr3] Finished benchmark\nINFO  [17:38:14.059] [bbotk] Result of batch 15:\nINFO  [17:38:14.061] [bbotk]  regr.cv_glmnet.alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:14.061] [bbotk]             0.5310704 13.38162        0      0            0.839\nINFO  [17:38:14.061] [bbotk]                                 uhash\nINFO  [17:38:14.061] [bbotk]  a9495419-342b-4c50-9180-73ba2e945e58\nINFO  [17:38:14.071] [bbotk] Finished optimizing after 15 evaluation(s)\nINFO  [17:38:14.072] [bbotk] Result:\nINFO  [17:38:14.073] [bbotk]  regr.cv_glmnet.alpha learner_param_vals  x_domain regr.mse\nINFO  [17:38:14.073] [bbotk]             0.8209513          <list[5]> <list[1]> 12.35682\n\n\n   regr.cv_glmnet.alpha learner_param_vals  x_domain regr.mse\n1:            0.8209513          <list[5]> <list[1]> 12.35682\n\n\nEn primer lugar verificamos si el algoritmo de optimización ha convergido, es decir, ha obtenido una solución óptima.\n\ninstance$is_terminated\n\n[1] TRUE\n\n\nUna vez hemos verificado la convergencia podemos ver el valor obtenido en el proceso de optimización:\n\ninstance$result\n\n   regr.cv_glmnet.alpha learner_param_vals  x_domain regr.mse\n1:            0.8209513          <list[5]> <list[1]> 12.35682\n\n\nLa solución nos da un \\(\\alpha\\) de 0.82 con un MSE de 12.35. Este valor es casi un promedio entre la regresión de lazo y la de cresta. Podemos utilizar este valor para estimar el modelo de aprendizaje obteniendo el valor de \\(\\lambda\\):\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.cv_glmnet\", alpha = instance$result$regr.cv_glmnet.alpha)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n# Modelo obtenido para todos los lambda\nmodelo_AA = gr$model$regr.cv_glmnet$model\n# Lambda óptimo\nmodelo_AA$lambda.min\n\n[1] 0.001538513\n\n\nUtilizamos el valor de lambda óptimo para ajustar el modelo final de aprendizaje.\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.glmnet\", \n              alpha = instance$result$regr.cv_glmnet.alpha, \n              lambda = modelo_AA$lambda.min)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n# Modelo resultante\nmodelo_glmnet_AA = gr$model$regr.glmnet$model\nmodelo_glmnet_AA\n\n\nCall:  (if (cv) glmnet::cv.glmnet else glmnet::glmnet)(x = data, y = target,      family = \"gaussian\", alpha = 0.820951323974156, lambda = 0.00153851329409109) \n\n  Df  %Dev   Lambda\n1 74 95.05 0.001539\n\n\nEl modelo resultante tiene 74 parámetros (hay 26 que son penalizados a cero) y una variabilidad explicada del 95.05% proporcionando un modelo muy adecuado para la predicción de la respuesta. Podemos ver que la solución obtenida es intermedia entre las producidas por la regresión lazo y regresión cresta. A continuación podemos ver el efecto de los parámetros del modelo:\n\n# Data frame con los coeficientes obtenidos y su codificación) positivo-negativo quitando intercept\ncoeficientes = as.data.frame(as.matrix(gr$model$regr.glmnet$model$beta))\ncoeficientes = rownames_to_column(coeficientes)\ncolnames(coeficientes) = c(\"Coef\", \"Estimate\")\ncoeficientes$Value = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n# Gráfico de coeficientes\nggplot(coeficientes, aes(Coef, Estimate, color = Value)) + \n  geom_point() + \n  geom_hline(yintercept = 0, linetype = 2, color = \"black\") +\n  theme(legend.position = \"none\")\n\n\n\n\nCoeficientes del modelo tunnig. Task Meat Spec”\n\n\n\n\nVeamos los errores MSE para la muestra de entrenamiento y validación.\n\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_meatspec)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_meatspec)\n# Scores de validación\nmeasures = msr(c(\"regr.mse\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n8.352634 \n\npred_test$score(measures)\n\nregr.mse \n6.268222 \n\n\n\n\nEspacios de búsqueda predifinidos\nEn este punto utilizamos los espacios de búsqueda predefinidos para tratar de buscar una solución óptima del algoritmo de aprendizaje. A continuación se muestra el código correspondiente:\n\nlibrary(mlr3tuningspaces)\n\n# Algoritmo de aprendizaje con espacio de búsqueda predefinido\nlearner = lts(lrn(\"regr.glmnet\"))\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n\n### Definimos el proceso de optimización asegurando reproducibilidad\nset.seed(123)\ninstance = tune(\n  tnr(\"random_search\"),\n  task = tsk_train_meatspec,\n  learner = learner,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measure = msr(\"regr.mse\"),\n  term_evals = 10\n)\n\nINFO  [17:38:16.965] [bbotk] Starting to optimize 2 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=10, k=0]'\nINFO  [17:38:16.991] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:17.031] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:17.037] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:17.199] [mlr3] Finished benchmark\nINFO  [17:38:17.228] [bbotk] Result of batch 1:\nINFO  [17:38:17.230] [bbotk]         s      alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:17.230] [bbotk]  3.212132 0.04766363 188.1079        0      0            0.153\nINFO  [17:38:17.230] [bbotk]                                 uhash\nINFO  [17:38:17.230] [bbotk]  f713caba-8f58-4491-9cba-23d13a45c4fb\nINFO  [17:38:17.234] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:17.294] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:17.305] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:17.400] [mlr3] Finished benchmark\nINFO  [17:38:17.506] [bbotk] Result of batch 2:\nINFO  [17:38:17.514] [bbotk]          s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:17.514] [bbotk]  -1.677314 0.8209513 26.56825        0      0            0.066\nINFO  [17:38:17.514] [bbotk]                                 uhash\nINFO  [17:38:17.514] [bbotk]  9e8bedd1-d233-468d-a465-aeaa1a076a90\nINFO  [17:38:17.522] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:17.697] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:17.705] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:17.813] [mlr3] Finished benchmark\nINFO  [17:38:17.846] [bbotk] Result of batch 3:\nINFO  [17:38:17.848] [bbotk]         s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:17.848] [bbotk]  8.493864 0.7283944 215.7396        0      0            0.097\nINFO  [17:38:17.848] [bbotk]                                 uhash\nINFO  [17:38:17.848] [bbotk]  7c46b4be-6dc2-4aac-aac5-d6a39f7ad12d\nINFO  [17:38:17.852] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:17.887] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:17.894] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:17.995] [mlr3] Finished benchmark\nINFO  [17:38:18.040] [bbotk] Result of batch 4:\nINFO  [17:38:18.045] [bbotk]          s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:18.045] [bbotk]  -1.930116 0.4778454 20.78844        0      0            0.093\nINFO  [17:38:18.045] [bbotk]                                 uhash\nINFO  [17:38:18.045] [bbotk]  baf334e5-c34b-4e8c-bf45-407b8059f349\nINFO  [17:38:18.053] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:18.096] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:18.103] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:18.165] [mlr3] Finished benchmark\nINFO  [17:38:18.262] [bbotk] Result of batch 5:\nINFO  [17:38:18.265] [bbotk]         s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:18.265] [bbotk]  7.657174 0.6183512 215.7396        0      0            0.053\nINFO  [17:38:18.265] [bbotk]                                 uhash\nINFO  [17:38:18.265] [bbotk]  abc372aa-ddf9-43e1-83b9-f70689ad2000\nINFO  [17:38:18.270] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:18.446] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:18.453] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:18.590] [mlr3] Finished benchmark\nINFO  [17:38:18.627] [bbotk] Result of batch 6:\nINFO  [17:38:18.629] [bbotk]          s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:18.629] [bbotk]  -8.133127 0.2608569 11.02533        0      0            0.128\nINFO  [17:38:18.629] [bbotk]                                 uhash\nINFO  [17:38:18.629] [bbotk]  9b177472-fc73-4b2c-bba9-6cb960aca868\nINFO  [17:38:18.633] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:18.669] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:18.677] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:18.810] [mlr3] Finished benchmark\nINFO  [17:38:18.847] [bbotk] Result of batch 7:\nINFO  [17:38:18.849] [bbotk]         s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:18.849] [bbotk]  6.114332 0.1528872 215.7396        0      0            0.122\nINFO  [17:38:18.849] [bbotk]                                 uhash\nINFO  [17:38:18.849] [bbotk]  74f738ff-e7f0-4f63-ab34-0c9ca9a27f97\nINFO  [17:38:18.853] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:18.891] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:18.899] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:19.169] [mlr3] Finished benchmark\nINFO  [17:38:19.205] [bbotk] Result of batch 8:\nINFO  [17:38:19.207] [bbotk]         s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:19.207] [bbotk]  2.990001 0.1716985 192.1284        0      0            0.261\nINFO  [17:38:19.207] [bbotk]                                 uhash\nINFO  [17:38:19.207] [bbotk]  230a9987-4ba9-4765-8b96-ce22c834e466\nINFO  [17:38:19.212] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:19.269] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:19.281] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:19.382] [mlr3] Finished benchmark\nINFO  [17:38:19.422] [bbotk] Result of batch 9:\nINFO  [17:38:19.425] [bbotk]         s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:19.425] [bbotk]  4.136444 0.3989398 215.7396        0      0            0.092\nINFO  [17:38:19.425] [bbotk]                                 uhash\nINFO  [17:38:19.425] [bbotk]  1485e514-6615-4d39-b4a1-cdc5930c6b46\nINFO  [17:38:19.430] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:38:19.471] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:38:19.480] [mlr3] Applying learner 'regr.glmnet' on task 'meatspec' (iter 1/1)\nINFO  [17:38:19.622] [mlr3] Finished benchmark\nINFO  [17:38:19.656] [bbotk] Result of batch 10:\nINFO  [17:38:19.658] [bbotk]         s     alpha regr.mse warnings errors runtime_learners\nINFO  [17:38:19.658] [bbotk]  4.176015 0.2572167 215.7396        0      0            0.132\nINFO  [17:38:19.658] [bbotk]                                 uhash\nINFO  [17:38:19.658] [bbotk]  b9141802-691c-4986-989b-91ee89d31a7e\nINFO  [17:38:19.668] [bbotk] Finished optimizing after 10 evaluation(s)\nINFO  [17:38:19.669] [bbotk] Result:\nINFO  [17:38:19.671] [bbotk]          s     alpha learner_param_vals  x_domain regr.mse\nINFO  [17:38:19.671] [bbotk]  -8.133127 0.2608569          <list[3]> <list[2]> 11.02533\n\n# Valoramos si el proceso de optimización ha finalizado\ninstance$is_terminated\n\n[1] TRUE\n\n\nVemos la solución de los hiperparámetros obtenida en el proceso de optimización:\n\ninstance$result\n\n           s     alpha learner_param_vals  x_domain regr.mse\n1: -8.133127 0.2608569          <list[3]> <list[2]> 11.02533\n\n\nEl valor de \\(\\alpha\\) es 0.26 con un MSE de 11.02. la solución es muy parecida al del procedimiento más manual del punto anterior. Podemos ajustar ahora el modelo final con los hiperparámetros obtenidos:\n\n# Algoritmo de aprendizaje\nlearner = lrn(\"regr.glmnet\", \n              alpha = instance$result$alpha)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_meatspec %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_meatspec)\n# Modelo resultante\nmodelo_glmnet_AA = gr$model$regr.glmnet$model\nmodelo_glmnet_AA\n\n\nCall:  (if (cv) glmnet::cv.glmnet else glmnet::glmnet)(x = data, y = target,      family = \"gaussian\", alpha = 0.260856857057661) \n\n    Df  %Dev  Lambda\n1    0  0.00 25.2500\n2    6  3.18 23.0000\n3    6  6.31 20.9600\n4    8  9.09 19.1000\n5    8 11.47 17.4000\n6    9 13.53 15.8600\n7    9 15.30 14.4500\n8    9 16.80 13.1600\n9    9 18.08 11.9900\n10   9 19.18 10.9300\n11   9 20.11  9.9570\n12   9 20.90  9.0730\n13   9 21.58  8.2670\n14   8 22.15  7.5320\n15   8 22.64  6.8630\n16   8 23.06  6.2540\n17   8 23.41  5.6980\n18   8 23.71  5.1920\n19   7 23.97  4.7310\n20   7 24.19  4.3100\n21   7 24.38  3.9270\n22   7 24.54  3.5790\n23  15 26.18  3.2610\n24  21 30.93  2.9710\n25  23 35.75  2.7070\n26  24 41.27  2.4670\n27  24 45.56  2.2470\n28  26 49.89  2.0480\n29  26 53.70  1.8660\n30  27 57.33  1.7000\n31  27 60.30  1.5490\n32  29 63.79  1.4110\n33  30 66.22  1.2860\n34  30 68.56  1.1720\n35  37 71.46  1.0680\n36  46 73.61  0.9729\n37  48 75.89  0.8864\n38  48 77.80  0.8077\n39  49 79.48  0.7359\n40  50 81.06  0.6705\n41  50 82.46  0.6110\n42  52 83.55  0.5567\n43  54 84.52  0.5072\n44  54 85.49  0.4622\n45  54 86.17  0.4211\n46  54 86.89  0.3837\n47  54 87.52  0.3496\n48  56 88.04  0.3186\n49  55 88.54  0.2903\n50  55 88.94  0.2645\n51  54 89.35  0.2410\n52  55 89.67  0.2196\n53  55 89.97  0.2001\n54  55 90.25  0.1823\n55  55 90.46  0.1661\n56  53 90.72  0.1513\n57  53 90.99  0.1379\n58  50 91.16  0.1256\n59  49 91.34  0.1145\n60  46 91.53  0.1043\n61  46 91.68  0.0950\n62  45 91.84  0.0866\n63  48 91.86  0.0789\n64  45 92.09  0.0719\n65  48 92.26  0.0655\n66  48 92.44  0.0597\n67  49 92.59  0.0544\n68  50 92.74  0.0496\n69  54 92.86  0.0452\n70  54 93.04  0.0411\n71  54 93.16  0.0375\n72  55 93.28  0.0342\n73  57 93.37  0.0311\n74  58 93.47  0.0284\n75  59 93.55  0.0258\n76  60 93.65  0.0235\n77  61 93.74  0.0215\n78  62 93.81  0.0196\n79  61 93.87  0.0178\n80  61 93.93  0.0162\n81  61 93.98  0.0148\n82  62 94.02  0.0135\n83  61 94.06  0.0123\n84  63 94.10  0.0112\n85  65 94.13  0.0102\n86  66 94.16  0.0093\n87  67 94.18  0.0085\n88  70 94.20  0.0077\n89  70 94.23  0.0070\n90  70 94.25  0.0064\n91  73 94.26  0.0058\n92  73 94.28  0.0053\n93  74 94.29  0.0048\n94  76 94.31  0.0044\n95  76 94.32  0.0040\n96  81 94.33  0.0037\n97  81 94.33  0.0033\n98  82 94.34  0.0030\n99  82 94.34  0.0028\n100 83 94.34  0.0025\n\n\nEl modelo final tiene 83 coeficientes distintos de cero con una variabilidad explicada del 94.34%, y un valor de \\(\\lambda\\) de 0.0025. En este caso 100 iteraciones para alcanzar la solución.\n\n# Data frame con los coeficientes obtenidos (en la última iteración) y su codificación positivo-negativo \ncoeficientes = as.data.frame(as.matrix(coef(modelo_glmnet_AA)[,100]))\ncoeficientes = rownames_to_column(coeficientes)\ncolnames(coeficientes) = c(\"Coef\", \"Estimate\")\ncoeficientes$Value = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n# Gráfico de coeficientes\nggplot(coeficientes, aes(Coef, Estimate, color = Value)) + \n  geom_point() + \n  geom_hline(yintercept = 0, linetype = 2, color = \"black\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFigura 6.19: Coeficientes del modelo tunning con búsqueda predifinida. Task Meat Spec”\n\n\n\n\nEn el gráfico siguiente podemos ver la evolución de los coeficientes en el proceso de penalización:\n\nplot(modelo_glmnet_AA)\n\n\n\n\nFigura 6.20: Evolución de los coeficientes del modelo. Task Meat Spec”\n\n\n\n\nPor último, analizamos los MSE asociados con la muestra de entrenamiento y validación.\n\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_meatspec)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_meatspec)\n# Scores de validación\nmeasures = msr(c(\"regr.mse\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n9.904157 \n\npred_test$score(measures)\n\nregr.mse \n6.567436 \n\n\nLos MSE obtenidos son similares a los del procedimiento de búsqueda manual. En la práctica se puede utilizar el procedimiento basado en la búsqueda en espacios predefinidos para obtener una primera solución, y posteriormente realizar un búsqueda más fina con los valores obtenidos. Esto nos puede llevar a modelos de aprendizaje algo mejores con un tiempo de convergencia y ejecución algo menores.\n\n\n\n\n\n6.3.2 Modelos aditivos lineales\nLos modelos aditivos lineales surgen cuando la relación entre una predictora y la respuesta (en el caso de variables numéricas) no se puede escribir de forma lineal, sino más bien a través de una función desconocida. En la forma más simple se consideran funciones polinómicas en términos de la predictora. Sin embargo, se pueden utilizar funciones más complejas (funciones de suavizado) que permiten capturar todo tipo de comportamiento entre ambas.\nLa mayor dificultad en este tipo de modelos es que no tenemos una forma explícita para la función de suavizado, y por tanto es necesario utilizar las funciones específicas de predicción para obtener el modelo resultante. En este punto se pretende dar una versión introductoria de los modelos de suavizado por o que se recomienda la lectura de textos más avanzados para completar lo visto en este punto.\nEl modelo aditivo más básico con una variable predictora y una respuesta Normal viene dado por:\n\\[Y = f(X) + \\epsilon\\] donde \\(f()\\) se denomina función de suavizado para la predictora \\(X\\). Las ventajas de este tipo de modelos es que son muy flexibles ya que permiten modelizar, a través de dichas funciones suaves, relaciones de tipo no lineal entre la variable respuesta y las predictoras. Sin embargo, no todo son ventajas ya que el proceso de selección del mejor modelo se complica al añadir la elección de la función de suavizado a utilizar.\nLa forma habitual de proceder es obtener el modelo sin suavizado y comparar su capacidad explicativa y error de predicción con respecto a un modelo de suavizado. Para comparar esos modelos no podemos utilizar el estadístico AIC y se recurre al estadístico GCV para poder comparar las curvas de suavizado. Este estadístico selecciona el modelo con una valor más bajo.\nLas funciones de suavizado son los denominados splines que consisten en funciones definidas sobre bases de polinomios. En nuestro caso utilizaremos los denominados splines penalizados o p-splines. En este caso la función de suavizado tiene la estructura siguiente:\n\\[s(variable, k = , m = , bs = , by = factor)\\] donde \\(k\\) es el tamaño de la base de polinomios, \\(m\\) es el orden de los polinomios, \\(bs\\) es el tipo de la base de splines utilizados y \\(by\\) identifica un factor para el ajuste de las curvas de suavizado (efecto de interacción). Por defecto se utiliza la configuración \\(k=10, m = 2, bs = \"ps\"\\). La elección del grado de suavización de la función que ajusta la tendencia entre respuesta y predictora es un tema muy importante, y está asociado al grado de la base de polinomios utilizada. Las posibilidades que tenemos a la hora de elegir el grado de suavizado pasan por utilizar los denominados “splines penalizados” que son splines de regresión en los que se introduce una penalización al realizar el ajuste del modelo. Dicha penalización viene controlada por el parámetro de suavizado \\(\\lambda\\). Si \\(\\lambda = 0\\) estamos en el caso particular en el que no hay penalización y a medida que \\(\\lambda\\) aumenta, aumentamos la intensidad de la penalización. Cuando \\(\\lambda\\) tiende a 1 el modelo se convierte prácticamente en un modelo de regresión lineal simple.\n\n6.3.2.1 Modelos GAM en mlr3\nPara el análisis de los modelos GAM con la librería mlr3 debemos utilizar la función regr.gam(). Su funcionamiento y métodos son similares a los de los modelos presentados en puntos anteriores. Para mostrar el funcionamiento de estos modelos vamos a utilizar el banco de datos Electricity. Recordemos que este banco de datos no tenia valores perdidos y la única tarea de preprocesado era la estandarización de las predictoras numéricas. Pasamos a establecer el modelo de aprendizaje. Para ello debemos establecer la ecuación del modelo utilizando las funciones de suavizado. En este primer modelo consideramos una función de suavizado para cada predictora.\n\n# Modelo de aprendizaje\nlearner = mlr3::lrn(\"regr.gam\")\n# Ecuación del modelo\nlearner$param_set$values$formula = PE ~ s(AP, k=10, m=2, bs = \"ps\") + \n  s(AT, k=10, m=2, bs = \"ps\") + \n  s(RH, k=10, m=2, bs = \"ps\") + \n  s(V, k=10, m=2, bs = \"ps\")\n# Graphlearner: Estructura del modelo y learner\ngr = pp_electricity %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_electricity)\n\nUna vez entrenado el modelo podemos ver el resumen del modelo:\n\n# Resumen del modelo\nsummary(gr$model$regr.gam$model)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nPE ~ s(AP, k = 10, m = 2, bs = \"ps\") + s(AT, k = 10, m = 2, bs = \"ps\") + \n    s(RH, k = 10, m = 2, bs = \"ps\") + s(V, k = 10, m = 2, bs = \"ps\")\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 454.36149    0.04788    9490   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n        edf Ref.df       F p-value    \ns(AP) 6.428  6.977   34.35  <2e-16 ***\ns(AT) 6.727  7.126 1824.69  <2e-16 ***\ns(RH) 5.977  6.682   88.22  <2e-16 ***\ns(V)  8.607  8.881  157.95  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.94   Deviance explained =   94%\nGCV = 17.614  Scale est. = 17.548    n = 7655\n\n\nLos resultados del modelo muestran que las cuatro funciones de suavizado resultan significativas (pvalor \\(< 0.05\\)), con una variabilidad explicada del 94%, y con un valor del estadístico GCV de 17.614.\nPodemos ver los modelos de suavizado obtenidos con el gráfico siguiente\n\n# Curvas de suavizado\npar(mfrow=c(2,2))\nplot(gr$model$regr.gam$model)\n\n\n\n\nFigura 6.21: Modelo de suavizado. Task Electricity”\n\n\n\n\nA la vista de los gráficos podemos ver que:\n\nLa predictora AP esta inversamente relacionada con PE, pero la curva suavizada estimada es demasiado irregular, lo que puede ser debido a que el número de nodos no es el adecuado o que no es necesario una curva de suavizado ya que el modelo obtenido es más complejo.\nLa predictora AT esta inversamente relacionada con PE, con una curva suavizada estimada muy adecuada para esta situación. Claramente la relación entre ambas variables no es lineal.\nLa predictora RH esta un efecto combinado con PE. Para valores inferiores a -2 (estandarizados) de RH hay un efecto creciente de la respuesta, mientras que a partir de ese punto la curva es decreciente. La ecuación de suavizado es muy adecuada para explicar la relación entre esas variables.\nPor último, para la predictora V observamos un comportamiento similar al de AP. Hay demasiada fluctuación en la función de suavizado.\n\nPodemos evaluar ahora las métricas de ajuste para el modelo obtenido:\n\n# Métrica de evaluación\nmeasures = msr(\"regr.mse\")\n# Predicción muestra entrenamiento y validación\npred_train = gr$predict(tsk_train_electricity)\npred_test = gr$predict(tsk_test_electricity)\n# Scores para muestras de entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n17.48179 \n\npred_test$score(measures)\n\nregr.mse \n16.78818 \n\n\nPodemos ver la solución gráfica del modelo con el código siguiente:\n\n# Muestra de entrenamiento\np1 = autoplot(pred_train, type = \"xy\") + labs(title = \"Observados vs predichos\")\np2 = autoplot(pred_train, type = \"residual\") + labs(title = \"Observados vs residuos\")\np3 = autoplot(pred_train, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\n# Muestra de validación\np4 = autoplot(pred_test, type = \"xy\") + labs(title = \"Observados vs predichos\")\np5 = autoplot(pred_test, type = \"residual\") + labs(title = \"Observados vs residuos\")\np6 = autoplot(pred_test, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\nggarrange(p1,p2,p3, p4, p5, p6, nrow = 2, ncol = 3)\n\n\n\n\nFigura 6.22: Gráficos del modelo. Task Electricity”\n\n\n\n\nLa solución se parece bastante a la obtenida con el modelo lineal habitual. Para poder comparar ambas situaciones vamos a construir un nuevo modelo donde combinamos efectos lineales y efectos de suavizado.\n\n# Modelo de aprendizaje\nlearner = mlr3::lrn(\"regr.gam\")\n# Ecuación del modelo\nlearner$param_set$values$formula = PE ~ AP + \n  s(AT, k=10, m=2, bs = \"ps\") + \n  s(RH, k=10, m=2, bs = \"ps\") + \n  V\n# Graphlearner: Estructura del modelo y learner\ngr2 = pp_electricity %>>% learner\ngr2 = GraphLearner$new(gr2)\n# Entrenamiento del modelo\ngr2$train(tsk_train_electricity)\n# Resumen del modelo\nsummary(gr2$model$regr.gam$model)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nPE ~ AP + s(AT, k = 10, m = 2, bs = \"ps\") + s(RH, k = 10, m = 2, \n    bs = \"ps\") + V\n\nParametric coefficients:\n             Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) 454.36149    0.04919 9237.262  < 2e-16 ***\nAP            0.46619    0.06076    7.673 1.89e-14 ***\nV            -3.51974    0.10495  -33.538  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n        edf Ref.df    F p-value    \ns(AT) 6.512  6.997 2282  <2e-16 ***\ns(RH) 4.930  5.712  118  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.936   Deviance explained = 93.7%\nGCV = 18.556  Scale est. = 18.521    n = 7655\n\n\nDe nuevo todos los efectos del modelo (lineales y suavizados) resultan significativos con una variabilidad explicada del 93.7% y un GCV de 18.556. Aunque los resultados empeoran un poco respecto del modelo con todas las funciones de suavizado, este modelo podría ser más adecuado, ya que la ecuación del modelo es más simple. Veamos las métricas de evaluación:\n\n# Predicción muestra entrenamiento y validación\npred_train = gr2$predict(tsk_train_electricity)\npred_test = gr2$predict(tsk_test_electricity)\n# Scores para muestras de entrenamiento y validación\npred_train$score(measures)\n\nregr.mse \n18.48594 \n\npred_test$score(measures)\n\nregr.mse \n17.99236 \n\n\nComo era de esperar los MSE empeoran con respecto al modelo de suavizado completo. Para finalizar podemos ver las curvas de predicción:\n\n# Curvas de suavizado\npar(mfrow=c(2,2))\nplot(gr2$model$regr.gam$model)\n\n\n\n\nFigura 6.23: Curvas de predicción. Task Electricity”\n\n\n\n\nLas curvas son más simples y con una capacidad explicativa similar al del modelo más complejo. Optamos por este modelo como modelo de predicción.\nEstudiamos ahora la estabilidad de la solución del modelo mediante un proceso de validación cruzada.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_electricity, gr2, resamp, store_models=TRUE)\n\nINFO  [17:38:28.277] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:28.519] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:28.764] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:28.983] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:29.274] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:29.459] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:29.633] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:29.803] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:29.986] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:38:30.246] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\n\n# Scores individuales\nskim(rr$score()$regr.mse)\n\n\nData summary\n\n\nName\nrr\\(score()\\)regr.mse\n\n\nNumber of rows\n10\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n18.44\n1.37\n16.47\n17.42\n18.27\n19.18\n20.77\n▇▅▅▂▅\n\n\n\n\n\nLa solución obtenida es muy estable dado que la desviación típica es bastante pequeña en comparación con la media de los scores individuales. Para finalizar representamos la curva de aprendizaje para ver como cambia el valor de la métrica de aprendizaje tanto para la muestra de entrenamiento y validación.\n\n# Curva de aprendizaje\nplot_learningcurve(tsk_electricity, gr2, \"regr.mse\", ptr = seq(0.1, 0.9, 0.1), rpeats = 10)\n\nINFO  [17:38:31.179] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:31.465] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:31.743] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:32.018] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:32.363] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:32.670] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:32.966] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:33.312] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:33.574] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:38:33.832] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\nINFO  [17:38:34.307] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:34.609] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:34.956] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:35.264] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:35.570] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:35.860] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:36.148] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:36.472] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:36.762] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:38:37.032] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\nINFO  [17:38:37.607] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:37.951] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:38.294] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:38.595] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:38.874] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:39.193] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:39.469] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:39.807] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:40.125] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:38:40.410] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\nINFO  [17:38:40.968] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:41.258] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:41.594] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:41.899] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:42.192] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:42.520] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:42.832] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:43.116] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:43.486] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:38:43.805] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\nINFO  [17:38:44.219] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:44.571] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:44.872] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:45.175] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:45.537] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:45.832] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:46.128] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:46.573] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:47.128] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:38:47.491] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\nINFO  [17:38:48.015] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:48.340] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:48.708] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:49.062] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:49.453] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:49.847] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:50.276] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:50.643] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:51.026] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:38:51.340] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\nINFO  [17:38:51.855] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:52.177] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:52.499] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:52.876] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:53.201] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:53.587] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:53.914] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:54.237] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:54.619] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:38:54.937] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\nINFO  [17:38:55.456] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:38:55.792] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:38:56.183] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:38:56.535] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:38:56.917] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:38:57.295] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:38:58.235] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:38:58.836] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:38:59.975] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:39:00.458] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\nINFO  [17:39:00.936] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 1/10)\nINFO  [17:39:01.436] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 2/10)\nINFO  [17:39:01.797] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 3/10)\nINFO  [17:39:02.264] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 4/10)\nINFO  [17:39:02.745] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 5/10)\nINFO  [17:39:03.230] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 6/10)\nINFO  [17:39:04.367] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 7/10)\nINFO  [17:39:04.862] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 8/10)\nINFO  [17:39:05.315] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 9/10)\nINFO  [17:39:05.734] [mlr3] Applying learner 'scale.regr.gam' on task 'electricity' (iter 10/10)\n\n\n\n\n\nFigura 6.24: Curva de aprendizaje. Task Electricity”\n\n\n\n\nComo se aprecia en los gráficos el porcentaje más adecuado de la muestra de entrenamiento es del 80% (mejor combinación de MSE para entrenamiento y validación)."
  },
  {
    "objectID": "60_RegressionModels.html#sec-60.5",
    "href": "60_RegressionModels.html#sec-60.5",
    "title": "6  Modelos de Regresión",
    "section": "6.4 Ejercicios",
    "text": "6.4 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en modelos de regresión para el banco de datos Penguins4.2.6.\nAjustar un modelo de aprendizaje automático basado en modelos de regresión para el banco de datos Us economic time series4.2.7.\nAjustar un modelo de aprendizaje automático basado en modelos de regresión para el banco de datos QSAR4.2.8."
  },
  {
    "objectID": "70_LogisticModels.html#sec-70.1",
    "href": "70_LogisticModels.html#sec-70.1",
    "title": "7  Modelos de Regresión Logística",
    "section": "7.1 Bancos de datos",
    "text": "7.1 Bancos de datos\nA continuación presentamos los bancos de datos con los que trabajaremos en este tema. En concreto usaremos los bancos de datos Breast Cancer Wisconsin ([-sec-brestcancer]), e Iris (4.3.3) que presentamos en el capítulo 4.\nA continuación se muestra el código para la carga de los diferentes bancos de datos y la creación de la correspondiente task de regresión para cada uno de ellos.\nPara crear una tarea de clasificación utilizamos la función as_task_classif. En el caso de un target binario debemos identificar además la categoría de mayor interés identificada con el parámetro positive.\n\n7.1.1 Breast Cancer Wisconsin\nEn esta base de datos se recoge información sobre los cánceres de mama en la ciudad de Wisconsin. Las características de la base de datos se calculan a partir de una imagen digitalizada de un aspiración de aguja fina (FNA) de una masa mamaria. Describen las características de los núcleos celulares presentes en la imagen y el objetivo que se persigue es clasificar un tumor como benigno o maligno en función de las variables predictoras. Como en este caso estamos interesados en saber que predictoras influyen más en el carácter maligno del cáncer, utilizaremos esa categoría como la de interés.\n\n# Cargamos datos\nbreastcancer = read_rds(\"breastcancer.rds\")\n# Creación de task eliminado la columna que identifica os sujetos\ntsk_cancer = as_task_classif(breastcancer[,-1], target = \"diagnosis\", positive = \"M\")\n# información de la tarea\nprint(tsk_cancer)\n\n<TaskClassif:breastcancer[, -1]> (569 x 31)\n* Target: diagnosis\n* Properties: twoclass\n* Features (30):\n  - dbl (30): area_mean, area_se, area_worst, compactness_mean,\n    compactness_se, compactness_worst, concave_points_mean,\n    concave_points_se, concave_points_worst, concavity_mean,\n    concavity_se, concavity_worst, fractal_dimension_mean,\n    fractal_dimension_se, fractal_dimension_worst, perimeter_mean,\n    perimeter_se, perimeter_worst, radius_mean, radius_se,\n    radius_worst, smoothness_mean, smoothness_se, smoothness_worst,\n    symmetry_mean, symmetry_se, symmetry_worst, texture_mean,\n    texture_se, texture_worst\n\n\nTodas la variables son de tipo numérico. En primer lugar evaluamos la existencia de missings\n\n# Missings\ntsk_cancer$missings()\n\n              diagnosis               area_mean                 area_se \n                      0                       0                       0 \n             area_worst        compactness_mean          compactness_se \n                      0                       0                       0 \n      compactness_worst     concave_points_mean       concave_points_se \n                      0                       0                       0 \n   concave_points_worst          concavity_mean            concavity_se \n                      0                       0                       0 \n        concavity_worst  fractal_dimension_mean    fractal_dimension_se \n                      0                       0                       0 \nfractal_dimension_worst          perimeter_mean            perimeter_se \n                      0                       0                       0 \n        perimeter_worst             radius_mean               radius_se \n                      0                       0                       0 \n           radius_worst         smoothness_mean           smoothness_se \n                      0                       0                       0 \n       smoothness_worst           symmetry_mean             symmetry_se \n                      0                       0                       0 \n         symmetry_worst            texture_mean              texture_se \n                      0                       0                       0 \n          texture_worst \n                      0 \n\n\nNo hay valores perdidos en ninguna de las variables el conjunto de datos.\nComo el conjunto de predictoras es tan grande no resulta útil representar todos los datos de la tarea en un único gráfico. vamos a utilizar un filtro de las características para determinar a priori las más relevantes, y poder representarlas gráficamente. En este caso utilizamos el filtro anova que nos permite realizar el test F de comparación de medias de las predictoras numéricas con respecto al target categórico.\n\n# Construimos el objeto que define el filtro\nfilter = flt(\"anova\")\n# Aplicamos el filtro sobre los datos de entrenamiento\nfilter$calculate(tsk_cancer)\n# Vemos los resultados\nfilter\n\n<FilterAnova:anova>: ANOVA F-Test\nTask Types: classif\nProperties: -\nTask Properties: -\nPackages: stats\nFeature types: integer, numeric\n                   feature        score\n 1:   concave_points_worst 123.70573229\n 2:        perimeter_worst 118.23871904\n 3:    concave_points_mean 115.14867130\n 4:           radius_worst 115.07148679\n 5:         perimeter_mean 100.07385051\n---                                    \n26:   fractal_dimension_se   1.20015272\n27:          smoothness_se   0.95743784\n28: fractal_dimension_mean   0.11922252\n29:             texture_se   0.07400141\n30:            symmetry_se   0.05717782\n\n# o los representamos gráficamente\nautoplot(filter)\n\n\n\n\nFigura 7.1: Autoplot filter. Task Breast Cancer\n\n\n\n\nEn la tabla aparecen ordenados las variables con mayores diferencias entre las muestras clasificadas como benignas o malignas. A continuación vemos el gráfico para las cuatro variables más relevantes.\n\ng1 <- ggplot(tsk_cancer$data(), aes(x = concave_points_worst, y= diagnosis)) + geom_boxplot() \ng2 <- ggplot(tsk_cancer$data(), aes(x = perimeter_worst, y= diagnosis)) + geom_boxplot()\ng3 <- ggplot(tsk_cancer$data(), aes(x = concave_points_mean, y= diagnosis)) + geom_boxplot() \ng4 <- ggplot(tsk_cancer$data(), aes(x = radius_worst, y= diagnosis)) + geom_boxplot()\nggarrange(g1, g2, g3, g4, nrow = 2, ncol = 2)\n\n\n\n\nFigura 7.2: Gráficos de cajas predictoras más relevantes. Task Breast Cancer\n\n\n\n\nComo se puede ver en los cuatro gráficos hay diferencias entre las cajas correspondientes a cada grupo, indicando que todas ellas pueden ser utilizadas para clasificar el tipo de tumor como benigno o maligno. De hecho en todas ellas se aprecia el mismo efecto. Cuanto mayor es el valor de la predictora más seguros podemos estar de que el cáncer es maligno.\n\n\n7.1.2 Iris\nEl banco de datos iris ya los presentamos en temas anteriores y aquí solo se presenta el código para crear la tarea de clasificación correspondiente.\n\n# Cargamos datos\niris = read_rds(\"iris.rds\")\n# creamos la tarea\ntsk_iris = as_task_classif(iris, target = \"species\")\n# información de la tarea\nprint(tsk_iris)\n\n<TaskClassif:iris> (150 x 5)\n* Target: species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): petal_length, petal_width, sepal_length, sepal_width"
  },
  {
    "objectID": "70_LogisticModels.html#sec-70.2",
    "href": "70_LogisticModels.html#sec-70.2",
    "title": "7  Modelos de Regresión Logística",
    "section": "7.2 Modelos de Regresión Logística Binaria",
    "text": "7.2 Modelos de Regresión Logística Binaria\nA continuación se detallan los aspectos teóricos más importantes de este tipo de modelos.\n\n7.2.1 El modelo\nEn este tipo de modelos disponemos de una variable respuesta (\\(y\\)) de tipo cualitativo con dos posibles respuestas que se codifican habitualmente con 0-1, donde el 0 indica “fracaso” y el 1 indica “éxito”. Además disponemos de una matriz de variables predictoras de tipo numérico y/o categórico, a partir de las cuales podemos obtener la matriz \\(X\\). La definición de “éxito” o “fracaso” depende de cada problema específico.\nImaginemos que disponemos de \\(p\\) posibles predictoras de forma que el conjunto de muestras viene dado por:\n\\[\\{(y_i, x_{1i},...x_{pi})\\}_{i=1}^n \\tag{7.1}\\]\ndonde \\(x_{ji}\\) es el valor de la muestra \\(i\\) en la predictora \\(j\\) e \\(y_i\\) nos da el valor de la categoría de la muestra \\(i\\) (codificada como 0 o 1). En esta situación construimos el predictor lineal:\n\\[z = w_0 + w_1X_1+...+w_pX_p \\tag{7.2}\\]\ndonde cada \\(w_j\\) representa la pendiente o variación del predictor lineal con respecto a cada predictora, y \\(w_0\\) representa el sesgo del modelo. Dado que el valor del predictor lineal no necesariamente se encuentra restringido al intervalo \\([0, 1]\\), resulta necesario convertir dicho valor en una probabilidad para determinar la solución del modelo. Para realizar esta operación utilizamos la función logística:\n\\[\\phi(z) = \\frac{1}{1+e^{-z}}, \\tag{7.3}\\]\nque permite pasar cualquier valor del intervalo \\([-∞,∞]\\) al intervalo \\([0,1]\\), es decir, pasamos cualquier valor numérico a una probabilidad. En la figura siguiente podemos ver la representación gráfica de la función logística para diferentes valores de z:\n\nz = seq(-8, 8, 0.001)\ny = 1/(1+exp(-z))\nres = data.frame(z , y)\nggplot(res, aes(x = z, y = y)) + geom_line()\n\n\n\n\nFigura 7.3: Función logística\n\n\n\n\nLa función logit se define entonces como:\n\\[logit(P(y=1 | X) = z, \\tag{7.4}\\]\ndonde \\(P(y=1 | X)\\) es la probabilidad condicional de que una muestra concreta pertenezca a la clase 1 dadas sus predictoras \\(X\\). La función logit toma entradas en el rango \\([0, 1]\\) y las transforma en valores en todo el rango de números reales. En cambio, la función logística toma valores de entrada en todo el rango de números reales y los transforma en valores en el rango \\([0, 1]\\). En otras palabras, la función logística es la inversa de la función logit, y nos permite predecir la probabilidad condicional de que una determinada muestra pertenezca a la clase 1 (o a la clase 0). En realidad a partir de ambas expresiones podemos escribir:\n\\[P(y=1 | X) = \\frac{e^{w_0 + w_1X_1+...+w_pX_p}}{1+e^{w_0 + w_1X_1+...+w_pX_p}} \\tag{7.5}\\]\ny podemos relacionar las probabilidades condicionadas de ambas respuestas mediante el log-odds:\n\\[log(\\frac{P(y=1 | X)}{P(y=0 | X)}) = w_0 + w_1X_1+...+w_pX_p \\tag{7.6}\\]\nque podemos utilizar para representar el ratio entre la probabilidad de evento verdadero y la probabilidad de evento falso (odds ratio):\n\\[\\frac{P(y=1 | X)}{P(y=0 | X)} = exp(w_0 + w_1X_1+...+w_pX_p) \\tag{7.7}\\]\nLos principales elementos que hay que interpretar en un modelo de regresión logística son los siguientes coeficientes de los predictores:\n\n\\(w_0\\) es la ordenada en el origen o intercept. Se corresponde con el valor esperado del logaritmo de odds cuando todos los predictores son cero.\n\\(w_p\\) son los coeficientes de regresión parcial de cada predictor e indican el cambio promedio del logaritmo de odds al incrementar en una unidad la variable predictora, manteniéndose constantes el resto de variables. Esto equivale a decir que, por cada unidad que se incrementa la predictora, se multiplican los odds por \\(e^{w_p}\\), es decir, aumenta el riesgo en esa cantidad.\n\nDado que la relación entre la probabilidad condicional y las predictoras no es lineal, los coeficientes de regresión no se corresponden con el cambio en la probabilidad de la respuesta asociada con el incremento en una unidad de la predictora, sino con el cambio en el log-odds.\n\n\n7.2.2 Evaluación del modelo\nPara la evaluación del modelo se utilizan diferentes métricas obtenidas a partir de la clasificación con las probabilidades proporcionadas por el modelo de regresión logística, entre las que podemos destacar:\n\nEl porcentaje de clasificación correcta/incorrecta.\nLa matriz de confusión, que nos permite obtener todos los porcentajes de interés para la clasificación.\nLa curva ROC, que se utilizan a menudo para obtener una visión del resultado de un clasificador en términos de sus verdaderos frente a los falsos positivos. Suelen presentar la tasa de verdaderos positivos en el eje Y, y la tasa de falsos positivos en el eje X. Por tanto, la inclinación de la curva y el espacio entre la diagonal y la curva son importantes ya que cuanto más alejada esté la curva y más hacia arriba mejor será nuestro modelo.\nEl área bajo la curva (AUC), que nos proporciona el área bajo la curva ROC de forma que valores próximos a 1 indican un buen ajuste (clasificación perfecta), mientras que valores próximos a 0.5 indican un ajuste malo (clasificación incorrecta).\nEl score de Brier, que mide la precisión de las predicciones probabilísticas. Para situaciones unidimensionales su comportamiento es igual al del MSE.\n\n\n\n7.2.3 Validación del modelo\nLas técnicas de validación para este tipo de modelos son las mismas que las de los modelos de regresión lineal: validación cruzada y curva de aprendizaje."
  },
  {
    "objectID": "70_LogisticModels.html#sec-70.3",
    "href": "70_LogisticModels.html#sec-70.3",
    "title": "7  Modelos de Regresión Logística",
    "section": "7.3 Modelos de Regresión Logística Multinomial",
    "text": "7.3 Modelos de Regresión Logística Multinomial\nA continuación se presentan brevemente los conceptos teóricos más relevantes de los modelos de regresión logística para respuesta multinomial.\n\n7.3.1 El modelo\nImaginemos que disponemos de \\(p\\) posibles predictoras de forma que el conjunto de muestras viene dado por:\n\\[\\{(y_i, x_{1i},...x_{pi})\\}_{i=1}^n\\]\ndonde \\(x_{ji}\\) es el valor de la muestra \\(i\\) en la predictora \\(j\\) e \\(y_i\\) nos da el valor de la categoría de la muestra \\(i\\), codificada de 1 a k . En esta situación construimos el predictor lineal para una clase k de la respuesta:\n\\[z_k = w_{0k} + w_{1k}X_1+...+w_{pk}X_p\\]\ndonde cada \\(w_{jk}\\) representa la pendiente o variación del predictor lineal con respecto a cada predictora para la clase k, y \\(w_{0k}\\) representa el sesgo del modelo para la clase k.\nLa regresión logística multinomial clasifica usando una generalización de la función sigmoide, conocida como la función softmax, para calcular la probabilidad \\(P(y=k|x)\\). La función softmax toma un vector \\(z=[z_1,z_2,…,z_k]'\\) de k valores obtenidos a partir de los predictores lineales correspondientes, y los transforma en una distribución de probabilidades.\nPara un vector \\(z\\) de dimensionalidad \\(k\\) la función softmax se define a partir de los elementos individuales:\n\\[\\phi(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}, \\quad 1\\leq i\\leq k\\]\ncomo:\n\\[\\phi(z)=\\left[\\frac{e^{z_1}}{\\sum_{j=1}^k e^{z_j}},\\frac{e^{z_2}}{\\sum_{j=1}^k e^{z_j}},...,\\frac{e^{z_k}}{\\sum_{j=1}^k e^{z_j}} \\right],\\]\ndonde queda claro que todos los elementos están restringidos al intervalo \\([0,1]\\) y la suma de todas las componentes es 1. Tenemos una distribución de probabilidad para la respuesta en función de las predictoras consideradas.\nComo ocurría en el modelo de regresión logística binomial:\n\n\\(w_{0k}\\) es la ordenada en el origen o intercept. Se corresponde con el valor esperado del logaritmo de odds cuando todos los predictores son cero para la clase k.\n\\(w_{pk}\\) los coeficientes de regresión parcial de cada predictor que indican el cambio promedio del logaritmo de odds al incrementar en una unidad la variable predictora, manteniéndose constantes el resto de variables para una clase dada. Esto equivale a decir que, por cada unidad que se incrementa la predictora, se multiplican los odds por \\(e^{w_{pk}}\\).\n\nDado que la relación entre la probabilidad condicional y las predictoras no es lineal, los coeficientes de regresión no se corresponden con el cambio en la probabilidad de la respuesta asociada con el incremento en una unidad de la predictora, sino con el cambio en el log-odds.\nComo ocurría en los modelos lineales la magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor. Para poder determinar qué impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar las predictoras.\n\n\n7.3.2 Evaluación del modelo\nPara la evaluación del modelo se utilizan diferentes métricas obtenidas a partir de la clasificación con las probabilidades proporcionadas por el modelo de regresión logística multinomial, entre las que podemos destacar:\n\nEl porcentaje de clasificación correcta/incorrecta.\nLa matriz de confusión, que nos permite obtener todos los porcentajes de interés para la clasificación.\nEl score de Brier para clasificaciones múltiples que mide la precisión de las predicciones probabilísticas."
  },
  {
    "objectID": "70_LogisticModels.html#sec-70.4",
    "href": "70_LogisticModels.html#sec-70.4",
    "title": "7  Modelos de Regresión Logística",
    "section": "7.4 Regresión logística en mlr3",
    "text": "7.4 Regresión logística en mlr3\nPara realizar el proceso de aprendizaje de un modelo de regresión logística podemos usar dos learner en la librería mlr3:\n\nclassif.log_reg, que esta enfocado en la tarea de clasificación pero que nos permite predecir las probabilidades de cada categoría de la respuesta.\nclassif.multinom, que esta enfocado en la tarea de clasificación cuando hay más de dos categorías en la respuesta. En este caso se fija una categoría como referencia y se obtienen los modelos que proporcionan la probabilidad de clasificación para las otras dos. La probabilidad de la referencia se calcula como 1 menos las otras dos.\nregr.glm, que esta enfocado en al tarea de regresión para predecir las probabilidades de cada categoría (cuando la respuesta tiene dos o más categorías).\n\nUsar cualquiera de ellos implica que el proceso de evaluación y validación (scores) debe estar adaptado a ellos. Sin embargo, en la práctica ambos algoritmos utilizan la misma función y producen el miso resultado\n\n7.4.1 Datos Breast Cancer\nEn este caso utilizamos el learner classif.log_reg. Antes de comenzar definimos el preprocesado vinculado a este conjunto de datos (estandarización de predictoras numéricas), y preparamos la muestra de entrenamiento y validación.\nHerramienta de preprocesado:\n\n# preprocesado\npp_cancer = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n\nAntes de realizar la división de muestras es necesario saber si el reparto de las categorías maligno-benigno es equitativo en al respuesta. Para ello evaluamos la variable de interés:\n\ntable(tsk_cancer$data()[,\"diagnosis\"])\n\ndiagnosis\n  M   B \n212 357 \n\n\nDado que el reparto esta algo desequilibrado vamos a introducir un estrato de agrupación basado en la variable de interés. A continuación consideramos un reparto 80-20.\n\n# Generamos variable de estrato\ntsk_cancer$col_roles$stratum <- \"diagnosis\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_cancer, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_cancer = tsk_cancer$clone()$filter(splits$train)\ntsk_test_cancer  = tsk_cancer$clone()$filter(splits$test)\n\nVemos en primer lugar si el reparto realizado respeta el estrato establecido\n\ntable(tsk_train_cancer$data()[,\"diagnosis\"])\n\ndiagnosis\n  M   B \n170 286 \n\ntable(tsk_test_cancer$data()[,\"diagnosis\"])\n\ndiagnosis\n M  B \n42 71 \n\n\nEl ratio entre ambas categorías se mantiene en ambas muestras. Ahora podemos comenzar el proceso de aprendizaje asociado con este modelo.\n\n# Definimos learner para predecir la probabilidad\nlearner = lrn(\"classif.log_reg\", predict_type = \"prob\")\n# Graphlearner: Preprocesado y learner\ngr = pp_cancer %>>% learner\ngr = GraphLearner$new(gr)\n\nPodemos comenzar ahora con el entrenamiento del modelo y su interpretación:\n\n# Entrenamiento\ngr$train(tsk_train_cancer)\n# Resumen del modelo\nmodelo = gr$model$classif.log_reg$model\nsummary(modelo)\n\n\nCall:\nstats::glm(formula = task$formula(), family = \"binomial\", data = data, \n    model = FALSE)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(>|z|)\n(Intercept)               -122.67   16932.81  -0.007    0.994\narea_mean                 2341.84  209910.76   0.011    0.991\narea_se                   1486.08   69511.26   0.021    0.983\narea_worst               -4921.17  184349.14  -0.027    0.979\ncompactness_mean          -925.12   34344.47  -0.027    0.979\ncompactness_se             537.52   22005.83   0.024    0.981\ncompactness_worst         -130.88   34031.57  -0.004    0.997\nconcave_points_mean        538.98   34397.58   0.016    0.987\nconcave_points_se          347.88   16817.30   0.021    0.983\nconcave_points_worst       -32.41   29127.43  -0.001    0.999\nconcavity_mean             452.60   34364.21   0.013    0.989\nconcavity_se              -464.10   21009.29  -0.022    0.982\nconcavity_worst            461.56   29012.29   0.016    0.987\nfractal_dimension_mean     441.67   22878.54   0.019    0.985\nfractal_dimension_se      -803.44   35348.09  -0.023    0.982\nfractal_dimension_worst     36.81   22706.77   0.002    0.999\nperimeter_mean             577.41  125345.81   0.005    0.996\nperimeter_se              -628.13   31167.33  -0.020    0.984\nperimeter_worst            -53.22   85923.85  -0.001    1.000\nradius_mean              -3287.17  184474.29  -0.018    0.986\nradius_se                  158.98   40696.27   0.004    0.997\nradius_worst              5105.11  197081.97   0.026    0.979\nsmoothness_mean             77.36   12740.55   0.006    0.995\nsmoothness_se               17.27    7127.00   0.002    0.998\nsmoothness_worst           -53.56   13280.63  -0.004    0.997\nsymmetry_mean              -78.23    8201.80  -0.010    0.992\nsymmetry_se               -245.52   10115.26  -0.024    0.981\nsymmetry_worst             239.48   13681.56   0.018    0.986\ntexture_mean               223.12   12884.20   0.017    0.986\ntexture_se                 -25.41    6898.48  -0.004    0.997\ntexture_worst               33.00   10877.41   0.003    0.998\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6.0231e+02  on 455  degrees of freedom\nResidual deviance: 2.9881e-06  on 425  degrees of freedom\nAIC: 62\n\nNumber of Fisher Scoring iterations: 25\n\n\nEn el resumen del modelo podemos ver el valor del estadístico AIC (62) y los coeficientes asociados a cada una de las predictoras consideradas. Para su interpretación debemos atender a las siguientes indicaciones:\n\nlos coeficientes negativos reducen la probabilidad de ocurrencia de la categoría de interés (en este caso tumor maligno), mientras que los positivos aumentan la probabilidad de ocurrencia.\nCuanto mayor es el valor negativo o positivo asociado con el coeficiente mayor es la influencia de la predictora sobre la probabilidad de ocurrencia.\n\nEn este caso tenemos coeficientes muy grandes tanto en signo negativo como positivo pero todos ellos resultan no significativos, debido seguramente a un efecto de confusión entre las predictoras. Esto suele ocurrir cuando consideramos un número muy elevado de predictoras y estas están relacionadas entre si. Como veremos más adelante existen diferentes formas de solucionar este problemas entre las que se encuentran: i) trabajar con un número de predictoras reducido, 2) reducir la información contenida en las predictoras mediante un algoritmo de reducción de la dimensión, 3) platear un algoritmo de clasificación distinto, 4) introducir penalización en los coeficientes para obtener un modelo más adecuado. Por el momento, vamos a analizar con más detalle esta solución y dejamos para más adelante cada una de las posibles soluciones.\nEn el gráfico siguiente podemos apreciar mejor el efecto de las diferentes predictoras sobre la respuesta. Para ello vamos a definir una función que nos permite representar los coeficientes, reutilizando el código del tema anterior.\n\n# Función para representar los coeficientes\nplot_coef = function(mod)\n{\n  # mod: modelo utilizado\n  # Data frame con los coeficientes obtenidos y su codificación) positivo-negativo\n  coeficientes = na.omit(as.data.frame(mod$coefficients))\n  coeficientes = rownames_to_column(coeficientes)\n  colnames(coeficientes) = c(\"Coef\", \"Estimate\")\n  coeficientes$Value = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n  # Gráfico de coeficientes\n  ggplot(coeficientes, aes(Estimate, Coef, color = Value)) + \n    geom_point() + \n    geom_vline(xintercept = 0, linetype = 2, color = \"black\") +\n    theme(legend.position = \"none\")\n}\n\nRepresentamos la solución para nuestro modelo:\n\nplot_coef(modelo)\n\n\n\n\nFigura 7.4: Coeficientes del modelo de regresión logística. Task Breast Cancer\n\n\n\n\nPasamos a evaluar la capacidad predictiva de nuestro modelo a través de diferentes scores como son el porcentaje de clasificación correcto, el score de brier, las curvas ROC y el AUC. En primer lugar obtenemos los valores de la predicción tanto para la muestra de entrenamiento como de validación.\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_cancer)\npred_test = gr$predict(tsk_test_cancer)\n# Visualizamos las primeras predicciones de la muestra de validación\npred_test\n\n<PredictionClassif> for 113 observations:\n    row_ids truth response       prob.M       prob.B\n         12     M        M 1.000000e+00 2.220446e-16\n         21     B        B 2.220446e-16 1.000000e+00\n         26     M        M 1.000000e+00 2.220446e-16\n---                                                 \n        550     B        B 2.220446e-16 1.000000e+00\n        565     M        M 1.000000e+00 2.220446e-16\n        567     M        M 1.000000e+00 2.220446e-16\n\n\nEn la tabla anterior tenemos tanto la respuesta predicha según el modelo, como las probabilidades asociadas a cada una de las categorías. Podemos ver como la categoría que proporciona el modelo está asociada directamente con la probabilidad de ocurrencia de cada nivel del factor.\nEvaluamos la capacidad de clasificación del modelo con diferentes scores:\n\nclassif.acc (porcentaje de clasificación correcta),\nclassif.bacc (porcentaje de clasificación correcta ponderado para muestras no balanceadas),\nclassif.bbrier (score de Brier para clasificaciones binarias),\nclassif.auc (área bajo la curva ROC).\n\nComenzamos obteniendo la matriz de confusión para la muestra de validación:\n\n# matriz de confusión\npred_test$confusion\n\n        truth\nresponse  M  B\n       M 41  2\n       B  1 69\n\n\nPodemos ver el alto grado de clasificación correcta que proporciona el modelo con solo 3 errores sobre todas las muestras de validación. Representamos gráficamente la matriz de confusión con todos los porcentajes involucrados.\n\n# Cargamos la librería para representar la matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nCalculamos ahora los scores para ambas muestras.\n\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\", \"classif.bbrier\", \"classif.auc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n  1.000000e+00   1.000000e+00   2.970536e-16   1.000000e+00 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.97345133     0.97401073     0.02654867     0.97401073 \n\n\nPodemos ver que tenemos una clasificación perfecta en la muestra de entrenamiento, lo que puede ser debido a un problema de sobre estimación al considerar todas las predictoras sin eliminar las no relevantes. En cuanto a la muestra de validación tenemos valores similares con una pequeña disminución en el porcentaje de clasificación correcta.\nAntes de continuar nos planteamos la posibilidad de seleccionar el conjunto de predictoras que están relacionadas más directamente con la respuesta. Para ello utilizamos un procedimiento de selección de características que identifica el mejor conjunto de ellas con respecto a un score de validación mediante un proceso iterativo. En concreto utilizamos el procedimiento fselect() y utilizamos el selector fs con las opciones sequential y estrategia de búsqueda hacia adelante (sfs). Se pueden consultar todas las opciones en este enlace.\nA continuación se muestra el proceso de selección para este conjunto de datos donde utilizamos el porcentaje de clasificación correcta ponderado como criterio de selección. Para la validación utilizamos un objeto de remuestreo con división 80-20. El código se muestra a continuación:\n\nset.seed(145)\ninstance = fselect(\n  fselector = fs(\"sequential\", strategy = \"sfs\"),\n  task = tsk_cancer,\n  learner = gr,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measure = msr(\"classif.bacc\"),\n  term_evals = 10\n)\n\nINFO  [17:39:25.941] [bbotk] Starting to optimize 30 parameter(s) with '<FSelectorSequential>' and '<TerminatorEvals> [n_evals=10, k=0]'\nINFO  [17:39:25.982] [bbotk] Evaluating 30 configuration(s)\nINFO  [17:39:26.117] [mlr3] Running benchmark with 30 resampling iterations\nINFO  [17:39:26.174] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:26.306] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:26.501] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:26.648] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:26.774] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:26.930] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:27.057] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:27.200] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:27.396] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:27.552] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:27.679] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:27.806] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:27.949] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:28.087] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:28.229] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:28.360] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:28.579] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:28.721] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:28.847] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:28.991] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:29.117] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:29.256] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:29.388] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:29.519] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:29.636] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:29.744] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:29.868] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:29.980] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:30.105] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:30.216] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:30.349] [mlr3] Finished benchmark\nINFO  [17:39:30.733] [bbotk] Result of batch 1:\nINFO  [17:39:30.738] [bbotk]  area_mean area_se area_worst compactness_mean compactness_se compactness_worst\nINFO  [17:39:30.738] [bbotk]       TRUE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE    TRUE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE       TRUE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE             TRUE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE           TRUE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE              TRUE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.738] [bbotk]  area_mean area_se area_worst compactness_mean compactness_se compactness_worst\nINFO  [17:39:30.738] [bbotk]  concave_points_mean concave_points_se concave_points_worst concavity_mean\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                 TRUE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE              TRUE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                 TRUE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE           TRUE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.738] [bbotk]  concave_points_mean concave_points_se concave_points_worst concavity_mean\nINFO  [17:39:30.738] [bbotk]  concavity_se concavity_worst fractal_dimension_mean fractal_dimension_se\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]          TRUE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE            TRUE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                   TRUE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                 TRUE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.738] [bbotk]  concavity_se concavity_worst fractal_dimension_mean fractal_dimension_se\nINFO  [17:39:30.738] [bbotk]  fractal_dimension_worst perimeter_mean perimeter_se perimeter_worst\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                     TRUE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE           TRUE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE         TRUE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE            TRUE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]                    FALSE          FALSE        FALSE           FALSE\nINFO  [17:39:30.738] [bbotk]  fractal_dimension_worst perimeter_mean perimeter_se perimeter_worst\nINFO  [17:39:30.738] [bbotk]  radius_mean radius_se radius_worst smoothness_mean smoothness_se\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]         TRUE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE      TRUE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE         TRUE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE            TRUE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE          TRUE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.738] [bbotk]  radius_mean radius_se radius_worst smoothness_mean smoothness_se\nINFO  [17:39:30.738] [bbotk]  smoothness_worst symmetry_mean symmetry_se symmetry_worst texture_mean\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]              TRUE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE          TRUE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE        TRUE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE           TRUE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE         TRUE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.738] [bbotk]  smoothness_worst symmetry_mean symmetry_se symmetry_worst texture_mean\nINFO  [17:39:30.738] [bbotk]  texture_se texture_worst classif.bacc warnings errors runtime_learners\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9453387        0      0            0.117\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9502012        0      0            0.180\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9572435        0      0            0.131\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.8246144        0      0            0.113\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.5432596        0      0            0.143\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.8527834        0      0            0.113\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9220322        0      0            0.131\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.6130114        0      0            0.172\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9101274        0      0            0.127\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.8933602        0      0            0.114\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.5362173        0      0            0.114\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.8884977        0      0            0.129\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.5000000        0      0            0.124\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.5000000        0      0            0.130\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.6292757        0      0            0.116\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9263917        0      0            0.200\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.8673709        0      0            0.129\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9691482        0      0            0.114\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9263917        0      0            0.130\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.8765929        0      0            0.114\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.9572435        0      0            0.125\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.6562710        0      0            0.119\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.5000000        0      0            0.119\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.6514085        0      0            0.106\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.6839370        0      0            0.097\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.5000000        0      0            0.111\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.7174715        0      0            0.100\nINFO  [17:39:30.738] [bbotk]       FALSE         FALSE    0.6265929        0      0            0.113\nINFO  [17:39:30.738] [bbotk]        TRUE         FALSE    0.5000000        0      0            0.098\nINFO  [17:39:30.738] [bbotk]       FALSE          TRUE    0.7147887        0      0            0.114\nINFO  [17:39:30.738] [bbotk]  texture_se texture_worst classif.bacc warnings errors runtime_learners\nINFO  [17:39:30.738] [bbotk]                                 uhash\nINFO  [17:39:30.738] [bbotk]  172d71f8-d547-41a2-9546-3bc8c27b0fff\nINFO  [17:39:30.738] [bbotk]  11c1c280-58cf-4488-9726-c1e258e854ff\nINFO  [17:39:30.738] [bbotk]  3df4eea1-1040-42ae-b1dc-9be18ffbb31d\nINFO  [17:39:30.738] [bbotk]  919d17ec-df39-4bab-84f1-339f74206e96\nINFO  [17:39:30.738] [bbotk]  c714b6b2-4f77-45e9-b7ee-f08ddd783d03\nINFO  [17:39:30.738] [bbotk]  50ebbe93-bc9c-4a59-9220-247be38805b1\nINFO  [17:39:30.738] [bbotk]  75ea1203-6bce-44d0-a4cc-8835c428b7c4\nINFO  [17:39:30.738] [bbotk]  77058338-15d4-4887-bfc6-6984e596b7c8\nINFO  [17:39:30.738] [bbotk]  5a93e285-17df-4a6b-91e9-982dd2e81dbb\nINFO  [17:39:30.738] [bbotk]  2f61609b-8de0-4b79-85fd-5e4d5b340a40\nINFO  [17:39:30.738] [bbotk]  a2ba1fdf-35fc-46c9-8ce8-9ef72fe30173\nINFO  [17:39:30.738] [bbotk]  4ca53168-2aca-4af1-acda-f4b4700cc53b\nINFO  [17:39:30.738] [bbotk]  5b83c3d8-7e3b-49b7-80c0-607780ffa470\nINFO  [17:39:30.738] [bbotk]  def4c4cb-316a-4940-b4fd-33d0dbfa5e5f\nINFO  [17:39:30.738] [bbotk]  2e861a06-149e-421f-b61e-a710d983ec61\nINFO  [17:39:30.738] [bbotk]  15f3cc13-c9ea-463d-86c2-d532a543325f\nINFO  [17:39:30.738] [bbotk]  7ceda123-c038-4ab4-bb77-fe9b8b725ae6\nINFO  [17:39:30.738] [bbotk]  382e9178-7645-497b-889c-30580895e434\nINFO  [17:39:30.738] [bbotk]  3c7d70bb-e9a6-437a-9cd7-d3836c4d22e2\nINFO  [17:39:30.738] [bbotk]  8212052b-93b2-4454-86d3-4bdabecfb339\nINFO  [17:39:30.738] [bbotk]  5838d0ae-859f-4fb5-a918-939b002a9bff\nINFO  [17:39:30.738] [bbotk]  e09cc22b-c8ba-450c-b822-6d49d01bd67a\nINFO  [17:39:30.738] [bbotk]  d1f3a4a0-47fb-40a4-bd42-c70250c24925\nINFO  [17:39:30.738] [bbotk]  19110c99-4761-4bbe-befb-15601ce1b2a5\nINFO  [17:39:30.738] [bbotk]  7022e1b9-f585-4c34-99ec-15ab2bd74050\nINFO  [17:39:30.738] [bbotk]  5fed614b-0c95-49b5-8a64-17b8eccb9fc9\nINFO  [17:39:30.738] [bbotk]  64105392-93ff-4ee2-86d2-2b85799c7990\nINFO  [17:39:30.738] [bbotk]  b73b7331-b6f1-4bbe-975b-20cde9b3aabd\nINFO  [17:39:30.738] [bbotk]  7b10796f-8c02-4ec9-953c-4012f69a4665\nINFO  [17:39:30.738] [bbotk]  9f94d62f-781a-4a1e-b9c1-b8bcac273013\nINFO  [17:39:30.738] [bbotk]                                 uhash\nINFO  [17:39:30.746] [bbotk] Finished optimizing after 30 evaluation(s)\nINFO  [17:39:30.747] [bbotk] Result:\nINFO  [17:39:30.749] [bbotk]  area_mean area_se area_worst compactness_mean compactness_se compactness_worst\nINFO  [17:39:30.749] [bbotk]      FALSE   FALSE      FALSE            FALSE          FALSE             FALSE\nINFO  [17:39:30.749] [bbotk]  concave_points_mean concave_points_se concave_points_worst concavity_mean\nINFO  [17:39:30.749] [bbotk]                FALSE             FALSE                FALSE          FALSE\nINFO  [17:39:30.749] [bbotk]  concavity_se concavity_worst fractal_dimension_mean fractal_dimension_se\nINFO  [17:39:30.749] [bbotk]         FALSE           FALSE                  FALSE                FALSE\nINFO  [17:39:30.749] [bbotk]  fractal_dimension_worst perimeter_mean perimeter_se perimeter_worst\nINFO  [17:39:30.749] [bbotk]                    FALSE          FALSE        FALSE            TRUE\nINFO  [17:39:30.749] [bbotk]  radius_mean radius_se radius_worst smoothness_mean smoothness_se\nINFO  [17:39:30.749] [bbotk]        FALSE     FALSE        FALSE           FALSE         FALSE\nINFO  [17:39:30.749] [bbotk]  smoothness_worst symmetry_mean symmetry_se symmetry_worst texture_mean\nINFO  [17:39:30.749] [bbotk]             FALSE         FALSE       FALSE          FALSE        FALSE\nINFO  [17:39:30.749] [bbotk]  texture_se texture_worst        features classif.bacc\nINFO  [17:39:30.749] [bbotk]       FALSE         FALSE perimeter_worst    0.9691482\n\n\nPodemos ver las variables seleccionadas y el score asociado con:\n\n# Variables seleccionas\ninstance$result_feature_set\n\n[1] \"perimeter_worst\"\n\n# Score\ninstance$result_y\n\nclassif.bacc \n   0.9691482 \n\n\nEl proceso selecciona únicamente la variable perimeter_worst con un porcentaje de clasificación correcta ponderado del 96.91%. Ajustamos el nuevo modelo con esa única variable para lo que es necesario modificar la tarea. El código siguiente muestra todo ese proceso:\n\n# Creación de task seleccionando la predictora de interés\ntsk_cancer2 = as_task_classif(breastcancer[c(\"diagnosis\", \"perimeter_worst\")], target = \"diagnosis\", positive = \"M\")\n# Generamos variable de estrato\ntsk_cancer2$col_roles$stratum <- \"diagnosis\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_cancer2, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_cancer2 = tsk_cancer2$clone()$filter(splits$train)\ntsk_test_cancer2  = tsk_cancer2$clone()$filter(splits$test)\n# Graphlearner: Preprocesado y learner\nlearner = lrn(\"classif.log_reg\", predict_type = \"prob\")\ngr = pp_cancer %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento\ngr$train(tsk_train_cancer2)\n# Resumen del modelo\nmodelo = gr$model$classif.log_reg$model\nsummary(modelo)\n\n\nCall:\nstats::glm(formula = task$formula(), family = \"binomial\", data = data, \n    model = FALSE)\n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      -0.5693     0.1874  -3.038  0.00238 ** \nperimeter_worst   5.4145     0.5971   9.068  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 602.31  on 455  degrees of freedom\nResidual deviance: 187.07  on 454  degrees of freedom\nAIC: 191.07\n\nNumber of Fisher Scoring iterations: 7\n\n\nPodemos ver como el modelo resulta significativo para la predictora de interés, mostrando además u efecto positivo, es decir, cuanto mayor es el valor de la predictora mayor es la probabilidad de que el tumor sea maligno. De hecho si \\(p_i\\) es la probabilidad de que el tumor sea clasificado como maligno la ecuación del modelo viene dada por la expresión:\n\\[log\\left(\\frac{p_i}{1-p_i}\\right) = -0.5693 + 5.4145 * \\text{perimeter_worst}_{estandarizada}\\] donde \\(\\text{perimeter_worst}_{estandarizada}\\) es la variable estandarizada. Evaluamos ahora el modelo obteniendo los scores asociados a las muestras de entrenamiento y validación:\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_cancer2)\npred_test = gr$predict(tsk_test_cancer2)\n# Scores\npred_train$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.90789474     0.89555738     0.06304534     0.96980666 \n\npred_test$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.96460177     0.96210597     0.02695919     0.99362844 \n\n\nComparando los resultados con los del modelo con todas las predictoras tenemos unos resultados bastante sorprendentes, ya que con un única predictora alcanzamos un porcentaje de clasificación correcta del 90% para la muestra de entrenamiento y del 96% para la muestra de validación. Esto nos da un indicativo de que le modelo anterior estaba sobreajustado. Si embargo, reducir el conjunto de predictoras a una única puede resultar excesivo y más adelante veremos otro tipo de técnicas para no perder la información de todas las posibles predictoras. Pasamos a validar el modelo mediante un proceso de validación cruzada similar a los del tema anterior.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_cancer2, gr, resamp, store_models=TRUE)\n\nINFO  [17:39:31.153] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:31.290] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:31.431] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:31.560] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:31.674] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:31.837] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:31.952] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:32.079] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:32.197] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:32.324] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\n\n# Análisis de los valores obtenidos con los scores definidos anteriormente\nskim(rr$score(measures))\n\n\nData summary\n\n\nName\nrr$score(measures)\n\n\nNumber of rows\n10\n\n\nNumber of columns\n12\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nlist\n4\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntask_id\n0\n1\n47\n47\n0\n1\n0\n\n\nlearner_id\n0\n1\n21\n21\n0\n1\n0\n\n\nresampling_id\n0\n1\n2\n2\n0\n1\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\ntask\n0\n1\n1\n51\n51\n\n\nlearner\n0\n1\n10\n38\n38\n\n\nresampling\n0\n1\n1\n20\n20\n\n\nprediction\n0\n1\n10\n20\n20\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\niteration\n0\n1\n5.50\n3.03\n1.00\n3.25\n5.50\n7.75\n10.00\n▇▇▇▇▇\n\n\nclassif.acc\n0\n1\n0.92\n0.03\n0.86\n0.91\n0.93\n0.93\n0.95\n▂▂▂▇▆\n\n\nclassif.bacc\n0\n1\n0.91\n0.03\n0.86\n0.90\n0.91\n0.92\n0.95\n▅▁▇▇▅\n\n\nclassif.bbrier\n0\n1\n0.06\n0.02\n0.04\n0.04\n0.05\n0.07\n0.09\n▇▃▃▂▂\n\n\nclassif.auc\n0\n1\n0.98\n0.02\n0.95\n0.96\n0.98\n0.99\n0.99\n▃▃▁▂▇\n\n\n\n\n\nEn todos los scores considerados hay poca variabilidad mostrado que la solución propuesta es bastante estable. De hecho, podemos ver que el porcentaje de clasificación correcta promedio se sitúa en el 91%. Podemos representar gráficamente la curva ROC asociada a nuestra tarea de clasificación con el objeto de analizar la estabilidad de la solución.\n\nautoplot(rr, type = \"roc\")\n\n\n\n\nFigura 7.5: Estimación por remuestreo de la curva ROC . Task Breast Cancer\n\n\n\n\nLa curva se acerca al extremo superior alejándose de la diagonal indicando que el clasificador obtenido resulta muy adecuado. Para finalizar vamos a obtener la curva de aprendizaje asociada con este modelo. Para ello debemos cargar las funciones que definimos en el tema anterior.\n\n\n\n\nplot_learningcurve(tsk_cancer2, gr, \"classif.bacc\", ptr = seq(0.1, 0.9, 0.1), rpeats = 10)\n\nINFO  [17:39:33.545] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:33.710] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:33.891] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:34.085] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:34.663] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:34.841] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:35.047] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:35.241] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:35.451] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:35.648] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\nINFO  [17:39:35.925] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:36.119] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:36.305] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:36.530] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:36.723] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:36.904] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:37.097] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:37.276] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:37.525] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:37.681] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\nINFO  [17:39:37.945] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:38.105] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:38.283] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:38.452] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:38.626] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:38.787] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:38.958] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:39.114] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:39.289] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:39.455] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\nINFO  [17:39:39.709] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:39.864] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:40.035] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:40.195] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:40.373] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:40.531] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:40.709] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:40.865] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:41.035] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:41.198] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\nINFO  [17:39:41.463] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:41.632] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:41.788] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:41.966] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:42.125] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:42.658] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:42.818] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:42.978] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:43.138] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:43.310] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\nINFO  [17:39:43.562] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:43.737] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:43.894] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:44.051] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:44.214] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:44.399] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:44.573] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:44.757] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:44.923] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:45.090] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\nINFO  [17:39:45.350] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:45.516] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:45.674] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:45.852] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:46.011] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:46.174] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:46.354] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:46.516] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:46.676] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:46.852] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\nINFO  [17:39:47.097] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:47.262] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:47.497] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:47.688] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:47.847] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:48.024] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:48.181] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:48.344] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:48.528] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:48.689] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\nINFO  [17:39:48.957] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 1/10)\nINFO  [17:39:49.117] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 2/10)\nINFO  [17:39:49.275] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 3/10)\nINFO  [17:39:49.457] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 4/10)\nINFO  [17:39:49.618] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 5/10)\nINFO  [17:39:49.779] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 6/10)\nINFO  [17:39:49.957] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 7/10)\nINFO  [17:39:50.121] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 8/10)\nINFO  [17:39:50.282] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 9/10)\nINFO  [17:39:50.471] [mlr3] Applying learner 'scale.classif.log_reg' on task 'breastcancer[c(\"diagnosis\", \"perimeter_worst\")]' (iter 10/10)\n\n\n\n\n\nCurva de aprendizaje. Task Breast Cancer\n\n\n\n\nComo se puede ver el tamaño óptimo para la muestra de entrenamiento se podría situar entre el 60% y el 70%.\n\n\n7.4.2 Datos iris\nDefinimos el preprocesado vinculado a este conjunto de datos (estandarización de predictoras numéricas), y preparamos la muestra de entrenamiento y validación.\nHerramienta de preprocesado:\n\n# preprocesado\npp_iris = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n\nEstablecemos el estrato en función de la respuesta antes de realizar las divisiones de muestra de entrenamiento y validación.\n\n# Generamos variable de estrato\ntsk_iris$col_roles$stratum <- \"species\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_iris, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_iris = tsk_iris$clone()$filter(splits$train)\ntsk_test_iris  = tsk_iris$clone()$filter(splits$test)\n\nAhora podemos comenzar el proceso de aprendizaje asociado con este modelo.\n\n# Definimos learner para predecir la probabilidad\nlearner = lrn(\"classif.multinom\", predict_type = \"prob\")\n# Graphlearner: Preprocesado y learner\ngr = pp_iris %>>% learner\ngr = GraphLearner$new(gr)\n\nPodemos comenzar ahora con el entrenamiento del modelo y su interpretación:\n\n# Entrenamiento\ngr$train(tsk_train_iris)\n\n# weights:  18 (10 variable)\ninitial  value 131.833475 \niter  10 value 10.994243\niter  20 value 1.458573\niter  30 value 0.260241\niter  40 value 0.155686\niter  50 value 0.131700\niter  60 value 0.106444\niter  70 value 0.104300\niter  80 value 0.101794\niter  90 value 0.094495\niter 100 value 0.092939\nfinal  value 0.092939 \nstopped after 100 iterations\n\n# Resumen del modelo\nmodelo = gr$model$classif.multinom$model\nmodelo\n\nCall:\nnnet::multinom(formula = species ~ ., data = task$data())\n\nCoefficients:\n                (Intercept) petal_length petal_width sepal_length sepal_width\nIris-versicolor    61.34338      51.0178    44.87509    -21.33789   -3.784367\nIris-virginica   -121.31870     257.5752   137.67151    -46.55536  -23.152353\n\nResidual Deviance: 0.1858771 \nAIC: 20.18588 \n\n\nComo era de esperar los coeficientes asociados con petal_length y petal_width so los que presentan valores más grandes. Podemos ver además que cuanto mayores son esos valores más fácil es que clasifiquemos la muestra como Iris-virginica. Antes de proceder con un modelo más sencillo vamos a estudiar la capacidad de clasificación de este modelo. Para ello obtenemos las predicciones de la muestra de entrenamiento y validación:\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_iris)\npred_test = gr$predict(tsk_test_iris)\n# Visualizamos las primeras predicciones de la muestra de validación\npred_test\n\n<PredictionClassif> for 30 observations:\n    row_ids          truth       response prob.Iris-setosa prob.Iris-versicolor\n         15    Iris-setosa    Iris-setosa     1.000000e+00         1.830158e-35\n         16    Iris-setosa    Iris-setosa     1.000000e+00         2.482010e-27\n         18    Iris-setosa    Iris-setosa     1.000000e+00         1.065164e-20\n---                                                                            \n        147 Iris-virginica Iris-virginica     1.696644e-81         6.989337e-25\n        148 Iris-virginica Iris-virginica     7.623387e-84         1.510832e-26\n        150 Iris-virginica Iris-virginica     2.901361e-76         9.370432e-19\n    prob.Iris-virginica\n          7.954259e-319\n          1.048209e-292\n          1.090171e-268\n---                    \n           1.000000e+00\n           1.000000e+00\n           1.000000e+00\n\n\nEn la tabla anterior podemos ver la categoría en la que clasifica cada muestra el modelo y las probabilidades asociadas a cada uno de los niveles de la respuesta. Veamos la matriz de confusión para la muestra de validación:\n\n# matriz de confusión\npred_test$confusion\n\n                 truth\nresponse          Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa              10               0              0\n  Iris-versicolor           0               9              0\n  Iris-virginica            0               1             10\n\n\nPodemos ver el alto grado de clasificación correcta que proporciona el modelo con solo 1 error sobre todas las muestras de validación. Calculamos los scores para ambas muestras. En este caso utilizamos el porcentaje de clasificación correcta y el score de brier para problemas con múltiples categorías.\n\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.mbrier\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc classif.mbrier \n  1.000000e+00   3.351533e-05 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc classif.mbrier \n    0.96666667     0.06666667 \n\n\nObtenemos un porcentaje de clasificación correcta para la muestra de validación del 96.67% lo que demuestra que el modelo de aprendizaje propuesto funciona adecuadamente. Aunque podríamos plantear un proceso de selección de variables, con toda la información hasta ahora vamos a plantear un modelo con las variables de pétalo y estudiar su capacidad de clasificación.\n\n# Creación de task seleccionando la predictora de interés\ntsk_iris2 = as_task_classif(iris[c(\"species\", \"petal_length\", \"petal_width\")], target = \"species\")\n# Generamos variable de estrato\ntsk_iris2$col_roles$stratum <- \"species\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_iris2, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_iris2 = tsk_iris2$clone()$filter(splits$train)\ntsk_test_iris2  = tsk_iris2$clone()$filter(splits$test)\n# Graphlearner: Preprocesado y learner\nlearner = lrn(\"classif.multinom\", predict_type = \"prob\")\ngr = pp_iris %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento\ngr$train(tsk_train_iris2)\n\n# weights:  12 (6 variable)\ninitial  value 131.833475 \niter  10 value 8.469047\niter  20 value 7.352470\niter  30 value 7.290925\niter  40 value 7.282925\niter  50 value 7.273208\niter  60 value 7.271514\niter  70 value 7.271445\niter  80 value 7.271434\niter  90 value 7.271408\nfinal  value 7.271313 \nconverged\n\n# Resumen del modelo\nmodelo = gr$model$classif.multinom$model\nmodelo\n\nCall:\nnnet::multinom(formula = species ~ ., data = task$data())\n\nCoefficients:\n                (Intercept) petal_length petal_width\nIris-versicolor   16.813857     18.18722     6.66505\nIris-virginica     6.453382     26.24143    15.86914\n\nResidual Deviance: 14.54263 \nAIC: 26.54263 \n\n\nEvaluamos la capacidad de clasificación de este modelo\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_iris2)\npred_test = gr$predict(tsk_test_iris2)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.mbrier\"))\n# valoración\npred_train$score(measures)\n\n   classif.acc classif.mbrier \n     0.9666667      0.0412616 \n\npred_test$score(measures)\n\n   classif.acc classif.mbrier \n    0.93333333     0.08086887 \n\n\nPara este modelo tenemos un porcentaje de clasificación correcta del 93.3% para la muestra de validación frente al 96.67% del modelo anterior. Perdemos solo un 3% al quitar dos posibles variables predictoras lo que podemos considerar como aceptable si queremos un modelo que seleccione pocas predictoras. Podemos finalizar el análisis de este modelo con un estudio de remuestreo y la construcción de la curva de aprendizaje. Esta tarea se deja como ejercicio.\n\n\n7.4.3 Actualizando los modelos\nComo hicimos en el tema anterior, tratamos de mejorar el modelo del punto anterior introduciendo penalización sobre los coeficientes del modelo. En concreto utilizamos el proceso de optimización de \\(\\alpha\\) del tema anterior adaptado a los problemas de clasificación tratados en este punto. para hacer esto hacemos uso del learner classif.cv_glmnet y classif.glmnet indicando que estamos en un modelo logístico.\n\n7.4.3.1 Datos Breast Cancer\nDefinimos el proceso de aprendizaje mediante el algoritmo de validación cruzada para encontrar el valor óptimo de \\(\\alpha\\).\n\nset.seed(145)\n# Algoritmo de aprendizaje \nlearner = lrn(\"classif.cv_glmnet\", type.logistic = \"Newton\", standardize = FALSE,\n              alpha = to_tune(1e-10, 1))\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_cancer %>>% learner\ngr = GraphLearner$new(gr)\n# Definimos instancia de optimización\ninstance = ti(\n  task = tsk_cancer,\n  learner = gr,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measures = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 15)\n)\n# Tipo de optimizador\ntuner = tnr(\"random_search\")\n# Proceso de optimización\ntuner$optimize(instance)\n\nINFO  [17:39:52.447] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=15, k=0]'\nINFO  [17:39:52.465] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:52.521] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:52.529] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:53.413] [mlr3] Finished benchmark\nINFO  [17:39:53.456] [bbotk] Result of batch 1:\nINFO  [17:39:53.458] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:53.458] [bbotk]                0.9465745   0.9823009        0      0            0.805\nINFO  [17:39:53.458] [bbotk]                                 uhash\nINFO  [17:39:53.458] [bbotk]  d99cbf72-ed3d-4533-b202-35e00ae96ab7\nINFO  [17:39:53.463] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:53.518] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:53.547] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:53.895] [mlr3] Finished benchmark\nINFO  [17:39:53.926] [bbotk] Result of batch 2:\nINFO  [17:39:53.928] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:53.928] [bbotk]               0.08194529           1        0      0            0.337\nINFO  [17:39:53.928] [bbotk]                                 uhash\nINFO  [17:39:53.928] [bbotk]  fe01bc92-67c6-4390-a32e-ba3f6a05ab00\nINFO  [17:39:53.930] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:53.971] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:53.977] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:54.318] [mlr3] Finished benchmark\nINFO  [17:39:54.351] [bbotk] Result of batch 3:\nINFO  [17:39:54.353] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:54.353] [bbotk]               0.01887609           1        0      0            0.332\nINFO  [17:39:54.353] [bbotk]                                 uhash\nINFO  [17:39:54.353] [bbotk]  4de35f27-0491-49f0-90e2-e6b76ef2a343\nINFO  [17:39:54.356] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:54.400] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:54.406] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:54.945] [mlr3] Finished benchmark\nINFO  [17:39:54.975] [bbotk] Result of batch 4:\nINFO  [17:39:54.977] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:54.977] [bbotk]                0.6223626           1        0      0             0.53\nINFO  [17:39:54.977] [bbotk]                                 uhash\nINFO  [17:39:54.977] [bbotk]  d248cdbe-9b60-41e4-95da-7a0eeb50a94f\nINFO  [17:39:54.980] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:55.021] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:55.028] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:55.647] [mlr3] Finished benchmark\nINFO  [17:39:55.683] [bbotk] Result of batch 5:\nINFO  [17:39:55.686] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:55.686] [bbotk]                0.8763153   0.9734513        0      0            0.609\nINFO  [17:39:55.686] [bbotk]                                 uhash\nINFO  [17:39:55.686] [bbotk]  d9602110-6cfe-4b0c-a2d6-bd14754e8324\nINFO  [17:39:55.689] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:55.759] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:55.767] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:56.212] [mlr3] Finished benchmark\nINFO  [17:39:56.248] [bbotk] Result of batch 6:\nINFO  [17:39:56.253] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:56.253] [bbotk]                 0.296863           1        0      0            0.434\nINFO  [17:39:56.253] [bbotk]                                 uhash\nINFO  [17:39:56.253] [bbotk]  d5f8ab53-c434-4aaf-adf9-b2f81e9072ae\nINFO  [17:39:56.258] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:56.309] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:56.316] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:56.983] [mlr3] Finished benchmark\nINFO  [17:39:57.016] [bbotk] Result of batch 7:\nINFO  [17:39:57.018] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:57.018] [bbotk]                0.9421247   0.9734513        0      0            0.657\nINFO  [17:39:57.018] [bbotk]                                 uhash\nINFO  [17:39:57.018] [bbotk]  dca696eb-b618-4ae8-982f-b0597e2f8f2f\nINFO  [17:39:57.021] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:57.063] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:57.070] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:57.627] [mlr3] Finished benchmark\nINFO  [17:39:57.676] [bbotk] Result of batch 8:\nINFO  [17:39:57.677] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:57.677] [bbotk]                 0.734974           1        0      0            0.545\nINFO  [17:39:57.677] [bbotk]                                 uhash\nINFO  [17:39:57.677] [bbotk]  842ebb6b-3d95-421d-ae76-9a6a39694bf9\nINFO  [17:39:57.680] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:57.722] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:57.729] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:58.131] [mlr3] Finished benchmark\nINFO  [17:39:58.163] [bbotk] Result of batch 9:\nINFO  [17:39:58.165] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:58.165] [bbotk]                0.2421076           1        0      0            0.393\nINFO  [17:39:58.165] [bbotk]                                 uhash\nINFO  [17:39:58.165] [bbotk]  bdfa0935-04aa-412a-9f09-347316a7e7cf\nINFO  [17:39:58.168] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:58.210] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:58.216] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:58.793] [mlr3] Finished benchmark\nINFO  [17:39:58.825] [bbotk] Result of batch 10:\nINFO  [17:39:58.826] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:58.826] [bbotk]                0.7850792   0.9823009        0      0            0.568\nINFO  [17:39:58.826] [bbotk]                                 uhash\nINFO  [17:39:58.826] [bbotk]  1e1b4b82-74c1-4da5-9ad5-95b994306c6e\nINFO  [17:39:58.829] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:58.870] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:58.877] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:59.507] [mlr3] Finished benchmark\nINFO  [17:39:59.538] [bbotk] Result of batch 11:\nINFO  [17:39:59.540] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:59.540] [bbotk]                0.9245107   0.9823009        0      0            0.622\nINFO  [17:39:59.540] [bbotk]                                 uhash\nINFO  [17:39:59.540] [bbotk]  3b468fec-8426-4e24-aedf-67bcac28f4c8\nINFO  [17:39:59.543] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:39:59.585] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:39:59.591] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:39:59.948] [mlr3] Finished benchmark\nINFO  [17:39:59.979] [bbotk] Result of batch 12:\nINFO  [17:39:59.981] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:39:59.981] [bbotk]               0.07095276           1        0      0             0.35\nINFO  [17:39:59.981] [bbotk]                                 uhash\nINFO  [17:39:59.981] [bbotk]  d4032777-1f4c-44a8-8134-8837aeacf718\nINFO  [17:39:59.985] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:00.032] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:00.040] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:40:00.805] [mlr3] Finished benchmark\nINFO  [17:40:00.836] [bbotk] Result of batch 13:\nINFO  [17:40:00.838] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:00.838] [bbotk]                0.9846512   0.9823009        0      0            0.755\nINFO  [17:40:00.838] [bbotk]                                 uhash\nINFO  [17:40:00.838] [bbotk]  cb76e7c6-021b-4cd5-9892-f8b741ae4688\nINFO  [17:40:00.841] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:00.883] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:00.889] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:40:01.427] [mlr3] Finished benchmark\nINFO  [17:40:01.467] [bbotk] Result of batch 14:\nINFO  [17:40:01.469] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:01.469] [bbotk]                0.4820736           1        0      0            0.527\nINFO  [17:40:01.469] [bbotk]                                 uhash\nINFO  [17:40:01.469] [bbotk]  31967268-6855-44b9-82f0-cc50f4024592\nINFO  [17:40:01.472] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:01.516] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:01.522] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'breastcancer[, -1]' (iter 1/1)\nINFO  [17:40:01.939] [mlr3] Finished benchmark\nINFO  [17:40:01.971] [bbotk] Result of batch 15:\nINFO  [17:40:01.972] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:01.972] [bbotk]                0.3840564           1        0      0            0.408\nINFO  [17:40:01.972] [bbotk]                                 uhash\nINFO  [17:40:01.972] [bbotk]  714fdb87-3198-4705-94ec-2e099be74847\nINFO  [17:40:01.982] [bbotk] Finished optimizing after 15 evaluation(s)\nINFO  [17:40:01.983] [bbotk] Result:\nINFO  [17:40:01.985] [bbotk]  classif.cv_glmnet.alpha learner_param_vals  x_domain classif.acc\nINFO  [17:40:01.985] [bbotk]               0.08194529          <list[6]> <list[1]>           1\n\n\n   classif.cv_glmnet.alpha learner_param_vals  x_domain classif.acc\n1:              0.08194529          <list[6]> <list[1]>           1\n\n# Resultado\ninstance$result\n\n   classif.cv_glmnet.alpha learner_param_vals  x_domain classif.acc\n1:              0.08194529          <list[6]> <list[1]>           1\n\n\nEl proceso de optimización nos da un \\(\\alpha\\) de 0.081945293 con un porcentaje de clasificación correcta del 100%. Utilizamos este valor para obtener el modelo definitivo:\n\nset.seed(145)\n# Algoritmo de aprendizaje \nlearner = lrn(\"classif.glmnet\", type.logistic = \"Newton\", standardize = FALSE,\n              alpha = instance$result$classif.cv_glmnet.alpha)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_cancer %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_cancer)\n# Modelo obtenido para todos los lambda\nmodelo_AA = gr$model$classif.glmnet$model\n# Lambda óptimo\nlmin = modelo_AA$lambda[100]\n##############################################\nlearner = lrn(\"classif.glmnet\", type.logistic = \"Newton\", standardize = FALSE, \n              alpha = instance$result$classif.cv_glmnet.alpha, \n              lambda = lmin)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_cancer %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_cancer)\n# Modelo resultante\nmodelo_glmnet_AA = gr$model$classif.glmnet$model\nmodelo_glmnet_AA\n\n\nCall:  (if (cv) glmnet::cv.glmnet else glmnet::glmnet)(x = data, y = target,      family = \"binomial\", alpha = 0.0819452890181685, lambda = 0.000463893969288626,      standardize = FALSE, type.logistic = \"Newton\") \n\n  Df %Dev    Lambda\n1 29 93.1 0.0004639\n\n\nEl modelo considera 29 predictoras con un porcentaje de clasificación correcta del 95.77% que es salgo superior al el modelo sin penalización. Podemos apreciar el efecto de cada predictor en el gráfico siguiente.\n\n# Data frame con los coeficientes obtenidos y su codificación) positivo-negativo quitando intercept\ncoeficientes = as.data.frame(as.matrix(modelo_glmnet_AA$beta))\ncoeficientes = rownames_to_column(coeficientes)\ncolnames(coeficientes) = c(\"Coef\", \"Estimate\")\ncoeficientes$Value = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n# Gráfico de coeficientes\nggplot(coeficientes, aes(Estimate, Coef, color = Value)) + \n  geom_point() + \n  geom_vline(xintercept = 0, linetype = 2, color = \"black\") +\n  theme(legend.position = \"none\")\n\n\n\n\nCoeficientes del modelo actualizado. Task Breast Cancer\n\n\n\n\nApreciamos claramente los coeficientes que contribuyen al aumento de la probabilidad de clasificación como maligno (puntos en azul) frente a los que no (puntos en rojo).\n\n\n7.4.3.2 Datos iris\nDefinimos el proceso de aprendizaje mediante el algoritmo de validación cruzada para encontrar el valor óptimo de \\(\\alpha\\).\n\nset.seed(145)\n# Algoritmo de aprendizaje \nlearner = lrn(\"classif.cv_glmnet\", type.multinomial = \"ungrouped\", standardize = FALSE,\n              alpha = to_tune(1e-10, 1))\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_cancer %>>% learner\ngr = GraphLearner$new(gr)\n# Definimos instancia de optimización\ninstance = ti(\n  task = tsk_iris,\n  learner = gr,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measures = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 15)\n)\n# Tipo de optimizador\ntuner = tnr(\"random_search\")\n# Proceso de optimización\ntuner$optimize(instance)\n\nINFO  [17:40:03.530] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=15, k=0]'\nINFO  [17:40:03.538] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:03.582] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:03.589] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:03.955] [mlr3] Finished benchmark\nINFO  [17:40:03.992] [bbotk] Result of batch 1:\nINFO  [17:40:03.994] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:03.994] [bbotk]                0.7946393           1        0      0            0.342\nINFO  [17:40:03.994] [bbotk]                                 uhash\nINFO  [17:40:03.994] [bbotk]  961f9b26-3c93-43a5-8317-e5f0e2640b37\nINFO  [17:40:03.998] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:04.046] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:04.053] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:04.397] [mlr3] Finished benchmark\nINFO  [17:40:04.429] [bbotk] Result of batch 2:\nINFO  [17:40:04.431] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:04.431] [bbotk]                0.2701054           1        0      0            0.338\nINFO  [17:40:04.431] [bbotk]                                 uhash\nINFO  [17:40:04.431] [bbotk]  3f5445ad-eab1-4458-b38c-c180cd9220a4\nINFO  [17:40:04.434] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:04.524] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:04.532] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:04.902] [mlr3] Finished benchmark\nINFO  [17:40:04.943] [bbotk] Result of batch 3:\nINFO  [17:40:04.946] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:04.946] [bbotk]                0.3366187           1        0      0            0.362\nINFO  [17:40:04.946] [bbotk]                                 uhash\nINFO  [17:40:04.946] [bbotk]  ed3f0fcc-44df-4402-bce3-2bb63c6227d4\nINFO  [17:40:04.949] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:05.001] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:05.027] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:05.442] [mlr3] Finished benchmark\nINFO  [17:40:05.495] [bbotk] Result of batch 4:\nINFO  [17:40:05.500] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:05.500] [bbotk]                0.9559843   0.9666667        0      0            0.406\nINFO  [17:40:05.500] [bbotk]                                 uhash\nINFO  [17:40:05.500] [bbotk]  65c63ae0-63c5-4861-b545-a196579cfe97\nINFO  [17:40:05.505] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:05.562] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:05.569] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:05.965] [mlr3] Finished benchmark\nINFO  [17:40:06.000] [bbotk] Result of batch 5:\nINFO  [17:40:06.003] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:06.003] [bbotk]                0.1747555           1        0      0            0.388\nINFO  [17:40:06.003] [bbotk]                                 uhash\nINFO  [17:40:06.003] [bbotk]  1d0a5845-8a7e-4733-b5b9-ac8a66a352d1\nINFO  [17:40:06.006] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:06.056] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:06.064] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:06.474] [mlr3] Finished benchmark\nINFO  [17:40:06.512] [bbotk] Result of batch 6:\nINFO  [17:40:06.515] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:06.515] [bbotk]                0.4126345   0.9666667        0      0            0.401\nINFO  [17:40:06.515] [bbotk]                                 uhash\nINFO  [17:40:06.515] [bbotk]  115a8316-b276-4873-b35e-05a2c7e52a7c\nINFO  [17:40:06.518] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:06.566] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:06.572] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:06.925] [mlr3] Finished benchmark\nINFO  [17:40:06.956] [bbotk] Result of batch 7:\nINFO  [17:40:06.958] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:06.958] [bbotk]                0.1601827           1        0      0            0.345\nINFO  [17:40:06.958] [bbotk]                                 uhash\nINFO  [17:40:06.958] [bbotk]  0cfd99b6-88bc-4b66-a215-cf0a7db8d66f\nINFO  [17:40:06.961] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:07.003] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:07.010] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:07.390] [mlr3] Finished benchmark\nINFO  [17:40:07.421] [bbotk] Result of batch 8:\nINFO  [17:40:07.423] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:07.423] [bbotk]                0.8289039           1        0      0            0.373\nINFO  [17:40:07.423] [bbotk]                                 uhash\nINFO  [17:40:07.423] [bbotk]  9a6e0fd7-9253-4320-a03f-e489f059adb2\nINFO  [17:40:07.426] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:07.471] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:07.477] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:07.896] [mlr3] Finished benchmark\nINFO  [17:40:07.927] [bbotk] Result of batch 9:\nINFO  [17:40:07.929] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:07.929] [bbotk]                0.9111099   0.9666667        0      0            0.411\nINFO  [17:40:07.929] [bbotk]                                 uhash\nINFO  [17:40:07.929] [bbotk]  ae8991b8-af6e-4e67-ab7c-075f1f54b477\nINFO  [17:40:07.932] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:07.973] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:07.979] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:08.374] [mlr3] Finished benchmark\nINFO  [17:40:08.406] [bbotk] Result of batch 10:\nINFO  [17:40:08.408] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:08.408] [bbotk]                0.9065234   0.9666667        0      0            0.388\nINFO  [17:40:08.408] [bbotk]                                 uhash\nINFO  [17:40:08.408] [bbotk]  a0b33163-d3ec-4250-a5d9-4f28edb7d4ed\nINFO  [17:40:08.411] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:08.454] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:08.460] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:08.837] [mlr3] Finished benchmark\nINFO  [17:40:08.868] [bbotk] Result of batch 11:\nINFO  [17:40:08.870] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:08.870] [bbotk]                0.9017083           1        0      0            0.369\nINFO  [17:40:08.870] [bbotk]                                 uhash\nINFO  [17:40:08.870] [bbotk]  e45e8087-004f-4c13-8533-b616c06bc98f\nINFO  [17:40:08.873] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:08.914] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:08.920] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:09.286] [mlr3] Finished benchmark\nINFO  [17:40:09.318] [bbotk] Result of batch 12:\nINFO  [17:40:09.320] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:09.320] [bbotk]                0.4936158           1        0      0            0.358\nINFO  [17:40:09.320] [bbotk]                                 uhash\nINFO  [17:40:09.320] [bbotk]  e698c3e9-abb1-4724-92f5-03d2fb4aeabb\nINFO  [17:40:09.323] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:09.369] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:09.376] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:09.743] [mlr3] Finished benchmark\nINFO  [17:40:09.777] [bbotk] Result of batch 13:\nINFO  [17:40:09.778] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:09.778] [bbotk]                0.5996826           1        0      0            0.358\nINFO  [17:40:09.778] [bbotk]                                 uhash\nINFO  [17:40:09.778] [bbotk]  09881733-297e-40ea-ab9f-91d2b03f8952\nINFO  [17:40:09.781] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:09.824] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:09.831] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:10.200] [mlr3] Finished benchmark\nINFO  [17:40:10.231] [bbotk] Result of batch 14:\nINFO  [17:40:10.233] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:10.233] [bbotk]                0.8073887           1        0      0            0.361\nINFO  [17:40:10.233] [bbotk]                                 uhash\nINFO  [17:40:10.233] [bbotk]  23b19742-c5b0-47fb-8d54-07d6e0c3dd39\nINFO  [17:40:10.236] [bbotk] Evaluating 1 configuration(s)\nINFO  [17:40:10.279] [mlr3] Running benchmark with 1 resampling iterations\nINFO  [17:40:10.285] [mlr3] Applying learner 'scale.classif.cv_glmnet' on task 'iris' (iter 1/1)\nINFO  [17:40:10.653] [mlr3] Finished benchmark\nINFO  [17:40:10.685] [bbotk] Result of batch 15:\nINFO  [17:40:10.686] [bbotk]  classif.cv_glmnet.alpha classif.acc warnings errors runtime_learners\nINFO  [17:40:10.686] [bbotk]                0.6659255           1        0      0             0.36\nINFO  [17:40:10.686] [bbotk]                                 uhash\nINFO  [17:40:10.686] [bbotk]  1fdc1e53-d508-4f76-8b78-fffd0c87d1d3\nINFO  [17:40:10.694] [bbotk] Finished optimizing after 15 evaluation(s)\nINFO  [17:40:10.695] [bbotk] Result:\nINFO  [17:40:10.697] [bbotk]  classif.cv_glmnet.alpha learner_param_vals  x_domain classif.acc\nINFO  [17:40:10.697] [bbotk]                0.7946393          <list[6]> <list[1]>           1\n\n\n   classif.cv_glmnet.alpha learner_param_vals  x_domain classif.acc\n1:               0.7946393          <list[6]> <list[1]>           1\n\n# Resultado\ninstance$result\n\n   classif.cv_glmnet.alpha learner_param_vals  x_domain classif.acc\n1:               0.7946393          <list[6]> <list[1]>           1\n\n\nEl proceso de optimización nos da un \\(\\alpha\\) de 0.6604496 con un porcentaje de clasificación correcta del 87.5%. Utilizamos este valor para obtener el modelo definitivo:\n\nset.seed(145)\n# Algoritmo de aprendizaje \nlearner = lrn(\"classif.glmnet\",  type.multinomial = \"ungrouped\", standardize = FALSE,\n              alpha = instance$result$classif.cv_glmnet.alpha)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_iris %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_iris)\n# Modelo obtenido para todos los lambda\nmodelo_AA = gr$model$classif.glmnet$model\n# Lambda óptimo\nlmin = modelo_AA$lambda[100]\n##############################################\nlearner = lrn(\"classif.glmnet\",  type.multinomial = \"ungrouped\", standardize = FALSE,\n              alpha = instance$result$classif.cv_glmnet.alpha, \n              lambda = lmin)\n# Proceso de aprendizaje\n# Graphlearner: Estructura del modelo y learner\ngr =  pp_cancer %>>% learner\ngr = GraphLearner$new(gr)\n# Entrenamiento del modelo\ngr$train(tsk_train_iris)\n# Modelo resultante\nmodelo_glmnet_AA = gr$model$classif.glmnet$model\nmodelo_glmnet_AA\n\n\nCall:  (if (cv) glmnet::cv.glmnet else glmnet::glmnet)(x = data, y = target,      family = \"multinomial\", alpha = 0.794639304999309, lambda = 5.46223434838074e-05,      standardize = FALSE, type.multinomial = \"ungrouped\") \n\n  Df  %Dev    Lambda\n1  4 98.14 5.462e-05\n\n\nEl modelo considera 4 predictoras con un porcentaje de clasificación correcta del 97.79% similar al que teníamos con le modelo anterior. En este caso el estudio de los coeficientes es algo más complejo ya que tenemos un conjunto para cada nivel de la respuesta. Preparamos los coeficientes de cada nivel para su representación gráfica:\n\n# Data frame con los coeficientes obtenidos y su codificación) positivo-negativo quitando intercept\ncoeficientes = rbind(as.matrix(modelo_glmnet_AA$beta$`Iris-virginica`),\n      as.matrix(modelo_glmnet_AA$beta$`Iris-versicolor`),\n      as.matrix(modelo_glmnet_AA$beta$`Iris-setosa`))\nvariables = rownames(coeficientes)\n\ncoeficientes = as.data.frame(coeficientes)\nnames(coeficientes)[1] = \"Estimate\"\ncoeficientes$Variable = variables\ncoeficientes$Species = c(rep(\"Iris-virginica\", 4), rep(\"Iris-versicolor\", 4), rep(\"Iris-setosa\", 4))\ncoeficientes$Valor = ifelse(coeficientes$Estimate > 0, \"Positivo\", \"Negativo\")\n# Gráfico de coeficientes\nggplot(coeficientes, aes(Estimate, Variable, color = Valor)) + \n  geom_point(aes(shape = Species), size = 2.5) + \n  geom_vline(xintercept = 0, linetype = 2, color = \"black\") \n\n\n\n\nCoeficientes del modelo actualizado. Task Iris\n\n\n\n\nApreciamos claramente los coeficientes que contribuyen al aumento de la probabilidad de clasificación como maligno (puntos en azul) frente a los que no (puntos en rojo)."
  },
  {
    "objectID": "70_LogisticModels.html#sec-70.5",
    "href": "70_LogisticModels.html#sec-70.5",
    "title": "7  Modelos de Regresión Logística",
    "section": "7.5 Ejercicios",
    "text": "7.5 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en modelos de regresión logística para el banco de datos Mushroom4.3.4.\nAjustar un modelo de aprendizaje automático basado en modelos de regresión logística para el banco de datos Water potability4.3.7.\nAjustar un modelo de aprendizaje automático basado en modelos de regresión logística para el banco de datos Hepatitis4.3.9.\nAjustar un modelo de aprendizaje automático basado en modelos de regresión logística para el banco de datos Abalone4.3.1."
  },
  {
    "objectID": "80_SurvivalModels.html#sec-80.1",
    "href": "80_SurvivalModels.html#sec-80.1",
    "title": "8  Modelos de supervivencia",
    "section": "8.1 Aspectos teóricos de los modelos de supervivencia",
    "text": "8.1 Aspectos teóricos de los modelos de supervivencia\nAntes de presentar el modelos de supervivencia estableemos la notación y definiciones necesarias para el estudio de este tipo de modelos.\nSi \\(f(t)\\) denota a la función de densidad de probabilidad para la variable aleatoria T, tiempo de supervivencia, y \\(F(t)\\) a la correspondiente función de distribución, entonces se define la función de supervivencia \\(S(t)\\) como la probabilidad de sobrevivir al menos hasta el instante \\(t\\), esto es,\n\\[S(t) = P(T >t) = 1 - F(t)\\] Se define el riesgo instantáneo de morir o función hazard, \\(h(t)\\), como el cociente entre la función de densidad y la función de supervivencia, es decir:\n\\[h(t) = \\frac{f(t)}{S(t)}\\]\nDe hecho, \\(h(t)dt\\) o incremento de la función hazard representa la intensidad del proceso de ocurrencia del evento, o lo que es lo mismo, la probabilidad de que ocurra el evento en un intervalo pequeño de tiempo \\(dt\\) dado que el individuo no ha registrado el evento de interés hasta el instante \\(t\\).\nUna distribución para los tiempos de supervivencia ha de tener una función hazard con buenas propiedades; por ejemplo, es de esperar que la función hazard no decrezca con \\(t\\), esto es, a más tiempo transcurrido, mayor riesgo de de que ocurra el evento de interés. Teniendo esto en cuenta, se define la función hazard acumulada, \\(H(t)\\) como:\n\\[H(t) = - log S(t)\\]\nUna característica importante de la función de supervivencia es la denominada mediana de supervivencia que es el valor de \\(t_{0.5}\\) de forma que: \\[S(t_{0.5}) = 0.5\\]. Asociado con este valor se puede obtener un intervalo de confianza para la mediana de supervivencia."
  },
  {
    "objectID": "80_SurvivalModels.html#sec-80.2",
    "href": "80_SurvivalModels.html#sec-80.2",
    "title": "8  Modelos de supervivencia",
    "section": "8.2 Task de supervivencia",
    "text": "8.2 Task de supervivencia\nComo vimos en la introducción de esta sección, los algoritmos de supervivencia requieren dos objetivos para el entrenamiento, esto significa que el nuevo objeto TaskSurv espera dos variables objetivo. La forma más sencilla de crear una tarea de supervivencia es utilizar as_task_surv(), como en el siguiente fragmento de código. Hay que tener en cuenta Tenga en cuenta que esta tarea tiene más argumentos que as_task_regr() para reflejar múltiples tipos de objetivos y de censura, los argumentos de tiempo y evento esperan cadenas que representen nombres de columnas donde se almacenan las variables ‘tiempo’ y ‘evento’, el tipo se refiere al tipo de censura (actualmente solo censura derecha compatible, por lo que este es el valor predeterminado). as_task_surv() convierte las columnas de destino en un objeto Surv. En esta sección usaremos el conjunto de datos de rats como ejemplo. Este conjunto de datos busca predecir si un tratamiento farmacológico tuvo éxito en prevenir que 150 ratas desarrollen tumores. El conjunto de datos, según admite él mismo, no es perfecto y, en general, debe tratarse como datos “ficticios”, lo cual es bueno para ejemplos pero no para análisis del mundo real.\nVamos a ver como crear la tarea de supervivencia vinculado con ese banco de datos, y veremos las primeras muestras.\n\n# Tarea de supervivencia\ntsk_rats = as_task_surv(survival::rats, time = \"time\",\n  event = \"status\", type = \"right\", id = \"rats\")\n# Cabecera del banco de datos\ntsk_rats$head()\n\n   time status litter rx sex\n1:  101      0      1  1   f\n2:   49      1      1  0   f\n3:  104      0      1  0   f\n4:   91      0      2  1   m\n5:  104      0      2  0   m\n6:  102      0      2  0   m\n\n\nRepresentamos la tarea con un autoplot que nos proporciona la curva de Kaplan-Meier, que es un estimador no paramétrico de la probabilidad de supervivencia para la observación promedio.\n\nautoplot(tsk_rats)\n\n\n\n\nFigura 8.1: Autoplot curva supervivencia. Task Rats\n\n\n\n\nEn este gráfico el eje x representa la variable de tiempo y el eje y es la función de supervivencia, \\(S(T)\\), definida por \\(F(T)\\) donde \\(F\\) es la función de distribución acumulada. Las cruces en rojo indican los puntos donde se produce la censura.\nAdemás de crear tus propias tareas, puedes cargar cualquiera de las tareas incluidas con mlr3proba:\n\nas.data.table(mlr_tasks)[task_type == \"surv\"]\n\n            key                  label task_type nrow ncol properties lgl int\n1:         actg               ACTG 320      surv 1151   13              0   3\n2:         gbcs   German Breast Cancer      surv  686   10              0   4\n3:        grace             GRACE 1000      surv 1000    8              0   2\n4:         lung            Lung Cancer      surv  228   10              0   7\n5:         rats                   Rats      surv  300    5              0   2\n6: unemployment  Unemployment Duration      surv 3343    6              0   1\n7:         whas Worcester Heart Attack      surv  481   11              0   4\n   dbl chr fct ord pxc\n1:   4   0   4   0   0\n2:   4   0   0   0   0\n3:   4   0   0   0   0\n4:   0   0   1   0   0\n5:   0   0   1   0   0\n6:   2   0   1   0   0\n7:   3   0   2   0   0"
  },
  {
    "objectID": "80_SurvivalModels.html#sec-80.3",
    "href": "80_SurvivalModels.html#sec-80.3",
    "title": "8  Modelos de supervivencia",
    "section": "8.3 Modelo aprendizaje, predición y tipos de predicción",
    "text": "8.3 Modelo aprendizaje, predición y tipos de predicción\nLa interfaz para los objetos LearnerSurv y PredictionSurv es idéntica a otras configuraciones de modelos de aprendizaje. Esto significa que debemos utilizar la función lrn() para construir nuestro modelo de supervivencia. Con el código siguiente podemos ver todos los modelos de aprendizaje para supervivencia disponibles:\n\nsurvival_learners = mlr_learners$keys()[startsWith(mlr_learners$keys(),\"surv\")]\nsurvival_learners\n\n [1] \"surv.akritas\"        \"surv.aorsf\"          \"surv.blackboost\"    \n [4] \"surv.cforest\"        \"surv.coxboost\"       \"surv.coxph\"         \n [7] \"surv.coxtime\"        \"surv.ctree\"          \"surv.cv_coxboost\"   \n[10] \"surv.cv_glmnet\"      \"surv.deephit\"        \"surv.deepsurv\"      \n[13] \"surv.dnnsurv\"        \"surv.flexible\"       \"surv.gamboost\"      \n[16] \"surv.gbm\"            \"surv.glmboost\"       \"surv.glmnet\"        \n[19] \"surv.kaplan\"         \"surv.loghaz\"         \"surv.mboost\"        \n[22] \"surv.nelson\"         \"surv.obliqueRSF\"     \"surv.parametric\"    \n[25] \"surv.pchazard\"       \"surv.penalized\"      \"surv.priority_lasso\"\n[28] \"surv.ranger\"         \"surv.rfsrc\"          \"surv.rpart\"         \n[31] \"surv.svm\"            \"surv.xgboost\"       \n\n\nSe pueden consultar los detalles de todos ellos en este enlace.\nmlr3proba tiene una interfaz de predicción diferente a mlr3, ya que todos los tipos posibles de predicción (“’predict types”) se devuelven cuando es posible para todos los modelos de supervivencia; es decir, si un modelo puede calcular un tipo de predicción particular, se devolverá en PredictionSurv. El motivo de esta decisión de diseño es que todos estos tipos de predicción se pueden transformar entre sí y, por lo tanto, es computacionalmente más sencillo devolverlos todos a la vez en lugar de volver a ejecutar los modelos para cambiar el tipo de predicción. En el análisis de supervivencia, se pueden hacer las siguientes predicciones:\n\nresponse: tiempo de supervivencia previsto.\ndistr: distribución de supervivencia prevista, ya sea discreta o continua.\nlp: predictor lineal calculado como los coeficientes ajustados multiplicados por los datos de prueba.\ncrank: Clasificación de riesgo continua.\n\nAnalizaremos cada uno de estos tipos de predicción con más detalle y con ejemplos para hacerlos menos abstractos. Usaremos el modelo de aprendizaje lrn(\"surv.coxph\"), basado en un modelo de regresión de Cox de riesgos proporcionales, entrenado sobre el conjunto tsk(\"rats\") como ejemplo de ejecución. Para este modelo, se pueden calcular todos los tipos de predicción excepto la respuesta.\n\n# Definimos tarea\ntsk_rats = tsk(\"rats\")\n# División de muestras\nset.seed(123)\nsplit = partition(tsk_rats, ratio = 0.8)\n# Entrenamiento y predicción sobre la muestra de validación\nprediction_cph = lrn(\"surv.coxph\")$train(tsk_rats, split$train)$\n  predict(tsk_rats, split$test)\nprediction_cph\n\n<PredictionSurv> for 60 observations:\n    row_ids time status      crank         lp     distr\n          3  104  FALSE -0.3944479 -0.3944479 <list[1]>\n          4   91  FALSE -2.5079657 -2.5079657 <list[1]>\n         13  104  FALSE  0.3357268  0.3357268 <list[1]>\n---                                                    \n        211   94   TRUE  0.8704798  0.8704798 <list[1]>\n        241   72   TRUE  0.9515030  0.9515030 <list[1]>\n        297   79   TRUE  0.3995794  0.3995794 <list[1]>\n\n\nEn la tabla podemos ver los diferentes tipos de predicción disponibles para este banco de datos. Analizamos con un poco más de detalle cada uno de los tipos de predicción.\npredict_type = “response”\nContrariamente a la intuición para muchos, la predicción de respuesta de los tiempos de supervivencia previstos es el tipo de predicción menos común en el análisis de supervivencia. La razón probable de esto se debe a la presencia de censura. Rara vez observamos el tiempo de supervivencia real para muchas observaciones y, por lo tanto, es poco probable que algún modelo de supervivencia pueda hacer predicciones con confianza sobre los tiempos de supervivencia. Esto se ilustra en el código siguiente.\nEn el siguiente ejemplo, entrenamos y predecimos desde una SVM de supervivencia (lrn(\"surv.svm\")), tenga en cuenta que usamos type = \"regression\" para seleccionar el algoritmo que optimiza las predicciones del tiempo de supervivencia y se selecciona gamma.mu = 1e-3. arbitrariamente ya que este es un parámetro requerido (este parámetro generalmente debe ajustarse). Luego comparamos las predicciones del modelo con los datos reales.\n\nlibrary(survivalsvm)\n# Modelo de aprendizaje, entrenamiento y predicción\nprediction_svm = lrn(\"surv.svm\", type = \"regression\", gamma.mu = 1e-3)$\n  train(tsk_rats, split$train)$predict(tsk_rats, split$test)\n# Vemos los tres primeros casos\ndata.frame(pred = prediction_svm$response[1:3],\n  truth = prediction_svm$truth[1:3])\n\n      pred truth\n1 89.20804  104+\n2 89.02754   91+\n3 88.48202  104+\n\n\nComo se puede ver en el resultado, todas nuestras predicciones son menores que el tiempo real observado, lo que significa que sabemos que nuestro modelo subestimó la verdad. Sin embargo, debido a que cada uno de los valores verdaderos son tiempos censurados, no tenemos absolutamente ninguna manera de saber si estas predicciones son levemente malas o absolutamente terribles (es decir, los verdaderos tiempos de supervivencia podrían ser 105, 99, 92 o podrían ser 300, 1000, 200). Por lo tanto, sin una forma realista de evaluar estos modelos, las predicciones del tiempo de supervivencia rara vez son útiles.\npredict_type = “distr”\nA diferencia de la regresión, en la que las predicciones deterministas/puntuales son las más comunes, en el análisis de supervivencia las predicciones de distribución son mucho más comunes. Por lo tanto, encontraremos que la mayoría de los modelos de supervivencia en mlr3proba hacen predicciones de distribución de forma predeterminada. Estas predicciones se implementan utilizando el paquete distr6, que permite la visualización y evaluación de curvas de supervivencia (definidas como función de distribución acumulativa). A continuación extraemos las primeras tres predicciones de $distr de nuestro ejemplo en ejecución y calculamos la probabilidad de supervivencia en \\(t = 77\\)\n\nprediction_cph$distr[1:3]$survival(77)\n\n        [,1]      [,2]      [,3]\n77 0.9381592 0.9923175 0.8759141\n\n\nEl resultado indica que existe una probabilidad del 92,1%, 98,7% y 99,4% de que las tres primeras ratas predichas estén vivas en el momento 77, respectivamente.\npredict_type = “lp”\nlp, a menudo escrito como \\(\\eta\\) en escritura académica, es computacionalmente la predicción más simple y tiene un análogo natural en el modelado de regresión. Los lectores familiarizados con la regresión lineal sabrán que al ajustar un modelo de regresión lineal simple, \\(Y = X\\beta\\), estamos estimando los valores para \\(\\beta\\), y el predictor lineal estimado (lp) es entonces \\(X\\hat{\\beta}\\), dónde \\(\\hat{\\beta}\\) son nuestros coeficientes estimados. En los modelos de supervivencia simples, el predictor lineal es la misma cantidad (pero estimada de una manera un poco más complicada). Las implementaciones de aprendizaje en mlr3proba se centran principalmente en el aprendizaje automático y pocos de estos modelos tienen una forma lineal simple, lo que significa que lp no se puede calcular para la mayoría de ellos. En la práctica, cuando se utiliza para la predicción, lp es un indicador de una predicción de riesgo relativo/clasificación continua, que se analiza a continuación.\npredict_type = “crank”\nEl último tipo de predicción, crank, es el más común en el análisis de supervivencia y quizás también el más confuso. Los textos académicos a menudo se refieren a predicciones de “riesgo” en el análisis de supervivencia (de ahí que los modelos de supervivencia a menudo se conozcan como “modelos de predicción de riesgos”), sin definir qué significa “riesgo”. A menudo, el riesgo se define como \\(exp(\\eta)\\) ya que esta es una cantidad común que se encuentra en modelos lineales simples de supervivencia. Sin embargo, a veces el riesgo se define como \\(exp(-\\eta)\\), y a veces puede ser una cantidad arbitraria que no tiene una interpretación significativa. Para evitar esta confusión en mlr3proba, definimos el tipo de predicción crank, que significa clasificación continua. Esto se explica mejor con el ejemplo. Continuando con lo anterior, generamos las primeras tres predicciones crank.\n\nprediction_cph$crank[1:3]\n\n         1          2          3 \n-0.3944479 -2.5079657  0.3357268 \n\n\nMultiplicando los valores por -1, el resultado nos dice que la tercera rata tiene el riesgo más bajo de muerte (los valores más bajos representan un riesgo menor) y la segunda rata tiene el riesgo más alto. La distancia entre las predicciones también nos dice que la diferencia de riesgo entre la primera y la segunda rata es menor que la diferencia entre la segunda y la tercera. Los valores reales en sí mismos no tienen sentido y, por lo tanto, comparar valores de predicción crank entre muestras no tiene sentido.\nEl tipo de predicción crank es informativo y común en la práctica porque permite identificar observaciones con menor/mayor riesgo entre sí, lo cual es útil para la asignación de recursos, por ejemplo, qué paciente debe recibir un tratamiento costoso y ensayos clínicos, por ejemplo, son personas. en un grupo de tratamiento con menor riesgo de enfermedad X que las personas en el grupo de control.\n\n\n\n\n\n\nLa interpretación de “riesgo” para las predicciones de supervivencia difiere entre los paquetes de R y, a veces, incluso entre los modelos del mismo paquete. En mlr3proba hay una interpretación consistente de crank: los valores más altos representan un riesgo menor de que ocurra el evento y los valores más bajos representan un riesgo mayor."
  },
  {
    "objectID": "80_SurvivalModels.html#sec-80.4",
    "href": "80_SurvivalModels.html#sec-80.4",
    "title": "8  Modelos de supervivencia",
    "section": "8.4 Métricas de evaluación",
    "text": "8.4 Métricas de evaluación\nLos modelos de supervivencia en mlr3proba se evalúan con objetos MeasureSurv, que se construyen de la forma habitual con msr(). En general, las medidas de supervivencia se pueden agrupar en las siguientes:\n\nMedidas de discriminación: cuantificar si un modelo identifica correctamente si una observación tiene mayor riesgo que otra. Evaluar predicciones crank y/o lp.\nMedidas de calibración: cuantificar si la predicción promedio se acerca a la verdad (desafortunadamente, todas las definiciones de calibración son vagas en un contexto de supervivencia). Evaluar predicciones crank y/o distr.\nReglas de puntuación: cuantificar si las predicciones probabilísticas se aproximan a los valores verdaderos. Evaluar predicciones distr.\n\nPodemos ver las métricas más habituales en el código siguiente:\n\nas.data.table(mlr_measures)[\n  task_type == \"surv\", c(\"key\", \"predict_type\")]\n\n                   key predict_type\n 1:         surv.brier        distr\n 2:   surv.calib_alpha        distr\n 3:    surv.calib_beta           lp\n 4: surv.chambless_auc           lp\n 5:        surv.cindex        crank\n 6:        surv.dcalib        distr\n 7:          surv.graf        distr\n 8:      surv.hung_auc           lp\n 9:    surv.intlogloss        distr\n10:       surv.logloss        distr\n11:           surv.mae     response\n12:           surv.mse     response\n13:     surv.nagelk_r2           lp\n14:   surv.oquigley_r2           lp\n15:          surv.rcll        distr\n16:          surv.rmse     response\n17:        surv.schmid        distr\n18:      surv.song_auc           lp\n19:      surv.song_tnr           lp\n20:      surv.song_tpr           lp\n21:       surv.uno_auc           lp\n22:       surv.uno_tnr           lp\n23:       surv.uno_tpr           lp\n24:         surv.xu_r2           lp\n                   key predict_type\n\n\nNo existe un consenso en la literatura sobre cuáles son las “mejores” medidas de supervivencia a utilizar para evaluar los modelos. Las más habituales son RCLL (logloss censurado por la derecha) (msr(\"surv.rcll\")) para evaluar la calidad de las predicciones de distribución, índice de concordancia (msr(\"surv.cindex\")) para evaluar la discriminación de un modelo, y D-Calibración ( msr(\"surv.dcalib\")) para evaluar la calibración de un modelo.\nUsando estas medidas, ahora podemos evaluar nuestras predicciones del ejemplo anterior.\n\nprediction_cph$score(msrs(c(\"surv.rcll\", \"surv.cindex\", \"surv.dcalib\")))\n\n  surv.rcll surv.cindex surv.dcalib \n  3.7395949   0.8312808   0.2342570 \n\n\nEl rendimiento del modelo parece bueno ya que RCLL y DCalib son relativamente bajos y el índice C es superior a 0,5; sin embargo, es muy difícil determinar el rendimiento de cualquier modelo de supervivencia sin compararlo con algún modelo basal (generalmente el Kaplan-Meier)."
  },
  {
    "objectID": "80_SurvivalModels.html#sec-80.5",
    "href": "80_SurvivalModels.html#sec-80.5",
    "title": "8  Modelos de supervivencia",
    "section": "8.5 Composición",
    "text": "8.5 Composición\nEn toda la documentación de mlr3proba se habla de predicciones “nativas (native)” y “compuestas (composed)”. Se define una predicción “nativa” como la predicción realizada por un modelo sin ningún posprocesamiento, mientras que una predicción “compuesta” se devuelve después del posprocesamiento. A continuación se presentan los diferentes tipos de composición.\n\n8.5.1 Composición interna\nmlr3proba hace uso de la composición internamente para devolver una predicción “crank” para cada modelo de aprendizaje. Esto es para garantizar que podamos comparar de manera significativa todos los modelos según al menos un criterio. El paquete utiliza las siguientes reglas para crear predicciones “crank”:\n\nSi un modelo devuelve una predicción de risk``, entoncescrank = risk` (podemos multiplicar esto por -1 para garantizar la interpretación de “bajo valor y bajo riesgo”).\nPor otro lado, si un modelo devuelve una predicción resonse`` configuramoscrank = -response`.\nPor otro lado, si un modelo devuelve una predicción lp, entonces configuramos crank = lp (o rank = -lp si es necesario).\nPor otro lado, si un modelo devuelve una predicción distr, establecemos crank como la suma de la función de riesgo acumulativo (consultar R. Sonabend, Bender y Vollmer (2022) para una discusión completa de por qué se elige este método).\n\n\n\n8.5.2 Composición explícita y pipelines\nAl comienzo de esta sección, mencionamos que es posible transformar tipos de predicción entre sí. En mlr3proba esto es posible con pipelines de composición. Hay varios pipelines implementados en el paquete, pero dos en particular se centran en transformar los tipos de predicción:\n\npipeline_crankcompositor() – Transforma una predicción distr en crank\npipeline_distrcompositor() – Transforma una predicción lp en distr\n\nEn la práctica, el segundo pipeline es más común ya que usamos internamente una versión del primer pipeline cada vez que devolvemos predicciones de modelos de supervivencia (por lo tanto, solo usamos el primer pipeline para sobrescribir estas predicciones de clasificación), por lo que solo veremos el segundo pipeline.\nEn el siguiente ejemplo, cargamos el conjunto de datos de rats, eliminamos columnas de factores y luego dividimos los datos en entrenamiento y prueba. Construimos el pipeline distrcompositor en torno a un modelo de aprendizaje de GLMnet de supervivencia (lrn(\"surv.glmnet\")) que, de forma predeterminada, solo puede hacer predicciones para lp y crank. En el proceso, especificamos que estimaremos la distribución de referencia con un estimador de Kaplan-Meier (estimador = \"kaplan\") y que queremos asumir una forma de riesgos proporcionales para nuestra distribución estimada (form = \"ph\"). Luego entrenamos y predecimos de la forma habitual y en nuestro resultado ahora podemos ver una predicción de distribución.\n\n# Generamos task seleccionando las predictoras\ntsk_rats = tsk(\"rats\")$select(c(\"litter\", \"rx\"))\nsplit = partition(tsk_rats)\n# Modelo de aprendizaje\nlearner = lrn(\"surv.glmnet\")\n# Predicción sin distr\nlearner$train(tsk_rats, split$train)$predict(tsk_rats, split$test)\n\n<PredictionSurv> for 99 observations:\n    row_ids time status    crank.1       lp.1\n          1  101  FALSE 0.53989543 0.53989543\n          8  102  FALSE 0.03106737 0.03106737\n         15  104  FALSE 0.05177895 0.05177895\n---                                          \n        235   80   TRUE 1.34764710 1.34764710\n        247   73   TRUE 1.38907026 1.38907026\n        289  103   TRUE 1.53405133 1.53405133\n\n\nConstruimos ahora el graphlearner que nos permite obtener la predicción distr utilizando el pipeline seleccionado:\n\ngraph_learner = as_learner(ppl(\n  \"distrcompositor\",\n  learner = learner,\n  estimator = \"kaplan\",\n  form = \"ph\"\n))\n\n# now with distr\npredicciones = graph_learner$train(tsk_rats, split$train)$predict(tsk_rats, split$test)\npredicciones\n\n<PredictionSurv> for 99 observations:\n    row_ids time status    crank.1       lp.1     distr\n          1  101  FALSE 0.53989543 0.53989543 <list[1]>\n          8  102  FALSE 0.03106737 0.03106737 <list[1]>\n         15  104  FALSE 0.05177895 0.05177895 <list[1]>\n---                                                    \n        235   80   TRUE 1.34764710 1.34764710 <list[1]>\n        247   73   TRUE 1.38907026 1.38907026 <list[1]>\n        289  103   TRUE 1.53405133 1.53405133 <list[1]>\n\n\nAhora aparece una nueva columna con la predicción distr. Esta columna contiene las funciones de distribución asociadas a diferentes cantidades de interés para todas las muestras. En concreto en este caso tenemos las más destacadas son: survival (función de supervivencia), cumHazard (función hazard acumulada). En el código siguiente extraemos esas dos funciones para el instante de tiempo 100 y todas las muestras.\n\ndistribucion = data.frame(survival = t(predicciones$distr$survival(100)),\ncumhazard = t(predicciones$distr$cumHazard(100)))\ncolnames(distribucion) = c(\"Survival\" , \"CumHazard\")\ndistribucion\n\n    Survival CumHazard\n1  0.7824974 0.2452647\n2  0.8629029 0.1474531\n3  0.8602443 0.1505389\n4  0.7723645 0.2582987\n5  0.8588973 0.1521059\n6  0.8575384 0.1536893\n7  0.8575384 0.1536893\n8  0.8505630 0.1618568\n9  0.8491311 0.1635417\n10 0.8476868 0.1652441\n11 0.8462299 0.1669642\n12 0.8432782 0.1704583\n13 0.7464123 0.2924771\n14 0.8417832 0.1722327\n15 0.8402754 0.1740256\n16 0.7418576 0.2985979\n17 0.8387546 0.1758372\n18 0.8372207 0.1776675\n19 0.8356738 0.1795170\n20 0.8356738 0.1795170\n21 0.8341136 0.1813857\n22 0.7325479 0.3112265\n23 0.8309533 0.1851817\n24 0.8277393 0.1890571\n25 0.7229688 0.3243893\n26 0.8261119 0.1910251\n27 0.7180774 0.3311779\n28 0.8228159 0.1950227\n29 0.8228159 0.1950227\n30 0.7106123 0.3416283\n31 0.7080897 0.3451845\n32 0.8160572 0.2032709\n33 0.7029930 0.3524083\n34 0.8108396 0.2096851\n35 0.8036806 0.2185533\n36 0.8036806 0.2185533\n37 0.6846118 0.3789033\n38 0.8000132 0.2231271\n39 0.7944002 0.2301679\n40 0.7944002 0.2301679\n41 0.7924992 0.2325638\n42 0.7924992 0.2325638\n43 0.7905829 0.2349847\n44 0.6681830 0.4031932\n45 0.7886515 0.2374308\n46 0.7867047 0.2399024\n47 0.7847425 0.2423996\n48 0.6568855 0.4202455\n49 0.7807717 0.2474725\n50 0.6482322 0.4335063\n51 0.7746984 0.2552815\n52 0.7726425 0.2579389\n53 0.6423779 0.4425786\n54 0.7705707 0.2606239\n55 0.6364556 0.4518406\n56 0.7642595 0.2688479\n57 0.7556188 0.2802183\n58 0.6182847 0.4808062\n59 0.7512006 0.2860826\n60 0.7512006 0.2860826\n61 0.7467166 0.2920696\n62 0.6058390 0.5011410\n63 0.7444498 0.2951099\n64 0.7421663 0.2981819\n65 0.5995184 0.5116286\n66 0.7328656 0.3107929\n67 0.5866851 0.5332670\n68 0.7257135 0.3206000\n69 0.5736014 0.5558205\n70 0.7208607 0.3273093\n71 0.7184088 0.3307165\n72 0.7134538 0.3376376\n73 0.7058929 0.3482917\n74 0.7007664 0.3555806\n75 0.6981774 0.3592821\n76 0.6955711 0.3630221\n77 0.6955711 0.3630221\n78 0.5363957 0.6228832\n79 0.6929475 0.3668010\n80 0.5259557 0.6425383\n81 0.6849733 0.3783754\n82 0.6795708 0.3862939\n83 0.6768436 0.3903150\n84 0.5082994 0.6766847\n85 0.6685585 0.4026314\n86 0.8655151 0.1444305\n87 0.7618348 0.2720255\n88 0.8519825 0.1601893\n89 0.7531203 0.2835303\n90 0.7301785 0.3144663\n91 0.7253889 0.3210473\n92 0.8072892 0.2140733\n93 0.7726425 0.2579389\n94 0.7684830 0.2633369\n95 0.7599716 0.2744742\n96 0.7281144 0.3172970\n97 0.5768955 0.5500942\n98 0.5636288 0.5733594\n99 0.5153992 0.6628136\n\n\n\n\n8.5.3 Combinando todas las funciones\nFinalmente, pondremos en práctica todo lo anterior en un pequeño experimento de referencia. Primero cargamos tsk(\"grace\") (que solo tiene características numéricas) y tomamos muestras de 500 filas al azar. Luego seleccionamos RCLL, D-Calibration y C-index para evaluar las predicciones, configuramos el mismo pipeline que usamos en el experimento anterior y cargamos un estimador Cox PH y Kaplan-Meier. Realizamos nuestro experimento de validación cruzada con k =3 y agregamos los resultados.\nMucho del código que aquí se presenta lo estudiaremos más tarde pero por el momento nos sirve para comparar diferentes modelos de aprendizaje de supervivencia.\n\ntsk_grace = tsk(\"grace\")\ntsk_grace$filter(sample(tsk_grace$nrow, 500))\nmsr_txt = c(\"surv.rcll\", \"surv.cindex\", \"surv.dcalib\")\nmeasures = msrs(msr_txt)\n\ngraph_learner = as_learner(ppl(\n  \"distrcompositor\",\n  learner = lrn(\"surv.glmnet\"),\n  estimator = \"kaplan\",\n  form = \"ph\"\n))\ngraph_learner$id = \"Coxnet\"\nlearners = c(lrns(c(\"surv.coxph\", \"surv.kaplan\")), graph_learner)\n\nbmr = benchmark(benchmark_grid(tsk_grace, learners,\n  rsmp(\"cv\", folds = 3)))\n\nINFO  [17:40:28.525] [mlr3] Running benchmark with 9 resampling iterations\nINFO  [17:40:28.639] [mlr3] Applying learner 'surv.coxph' on task 'grace' (iter 1/3)\nINFO  [17:40:28.685] [mlr3] Applying learner 'surv.coxph' on task 'grace' (iter 2/3)\nINFO  [17:40:28.718] [mlr3] Applying learner 'surv.coxph' on task 'grace' (iter 3/3)\nINFO  [17:40:28.745] [mlr3] Applying learner 'surv.kaplan' on task 'grace' (iter 1/3)\nINFO  [17:40:28.772] [mlr3] Applying learner 'surv.kaplan' on task 'grace' (iter 2/3)\nINFO  [17:40:28.802] [mlr3] Applying learner 'surv.kaplan' on task 'grace' (iter 3/3)\nINFO  [17:40:28.830] [mlr3] Applying learner 'Coxnet' on task 'grace' (iter 1/3)\nINFO  [17:40:28.987] [mlr3] Applying learner 'Coxnet' on task 'grace' (iter 2/3)\nINFO  [17:40:29.111] [mlr3] Applying learner 'Coxnet' on task 'grace' (iter 3/3)\nINFO  [17:40:29.298] [mlr3] Finished benchmark\n\nbmr$aggregate(measures)[, c(\"learner_id\", ..msr_txt)]\n\n    learner_id surv.rcll surv.cindex surv.dcalib\n1:  surv.coxph  4.884953   0.8356144    5.214107\n2: surv.kaplan  5.123078   0.5000000    4.299182\n3:      Coxnet  4.876609   0.8365884    3.653854\n\n\nEn este pequeño experimento, Coxnet y Cox PH tienen la mejor discriminación, la línea de base de Kaplan-Meier tiene la mejor calibración y Coxnet y Cox PH tienen una precisión predictiva general similar (con el RCLL más bajo)."
  },
  {
    "objectID": "80_SurvivalModels.html#sec-80.6",
    "href": "80_SurvivalModels.html#sec-80.6",
    "title": "8  Modelos de supervivencia",
    "section": "8.6 Casos prácticos",
    "text": "8.6 Casos prácticos"
  },
  {
    "objectID": "80_SurvivalModels.html#sec-80.7",
    "href": "80_SurvivalModels.html#sec-80.7",
    "title": "8  Modelos de supervivencia",
    "section": "8.7 Ejercicios",
    "text": "8.7 Ejercicios"
  },
  {
    "objectID": "90_BayesianClassif.html#sec-90.1",
    "href": "90_BayesianClassif.html#sec-90.1",
    "title": "9  Modelos de clasificación Naïve Bayes",
    "section": "9.1 Tipos de clasificadores Naïve Bayes",
    "text": "9.1 Tipos de clasificadores Naïve Bayes\nDentro de los clasificadores Naïve Bayes encontramos tres tipos principales en función de las características de la variable respuesta y las predictoras. Dichos tipos son: Naïve Bayes Bernouilli, Naïve Bayes Multinomial, y Naïve Bayes Gaussiano.\n\n9.1.1 Naïve Bayes Bernouilli\nEl algoritmo Naïve Bayes Bernouilli se utiliza cuando tanto la respuesta como las predictoras tienen únicamente dos etiquetas o categorías, es decir, son variables de tipo binario. En esta situación si la variable \\(y\\) solo puede tomar los valores \\(\\{0,1\\}\\), la verosimilitud individual de cada predictora se expresa como:\n\\[P(x_i|y) = P(y=1)x_i+(1-P(y=1))(1-x_i)\\]\ncon la que podemos obtener de forma muy rápida la regla de clasificación para este algoritmo ya que esta viene dada por elegir la clase 1 si:\n\\[[P(y=1)^{q+1}] > [P(y=0)^{p-q+1}]\\]\ndonde \\(p\\) es el número de predictoras disponibles, \\(q\\) el número de predictoras que toman el valor 1 para la muestra que tratamos de clasificar, y \\(P(y=1)\\), \\(P(y=0)\\) se estiman a partir de los valores de la muestra de entrenamiento.\nPor lo tanto, este tipo de clasificador requiere que las muestras se representen como vectores de características de tipo binario. Es el algoritmo menos utilizado de los tres debido a sus restricciones de aplicación pero es el que proporciona una solución más rápida en este tipo de situaciones.\n\n\n9.1.2 Naïve Bayes Multinomial\nSe utiliza cuando la variable respuesta tiene dos o más etiquetas posibles, y las variables predictoras son de tipo categórico multinomial, es decir, con múltiples etiquetas en cada una de ellas. Es un algoritmo muy extendido que se ha utilizado en la clasificación de textos como por ejemplo la identificación de correo en Spam versus No Spam. Este algoritmo evalúa la probabilidad de cada etiqueta para una muestra determinada y devuelve la etiqueta con la mayor posibilidad.\nEn este caso si \\(\\theta_{y_l}=(\\theta_{y_l;1},...,\\theta_{y_l;p})\\) representa el vector de probabilidades para la clase \\(l\\) asociada con el conjunto de predictoras, podemos estimar dichos parámetros mediante la expresión:\n\\[\\hat{\\theta}_{y_l;j} = \\frac{N_{y_l;j} + \\alpha}{N_{y_l} + \\alpha n},\\]\ndonde \\(N_{y_l;j}\\) es el número de predictoras de la muestra con valor 1 para la clase l, \\(N_{y_l} = \\sum_{j=1}^n N_{y_l;j}\\), y \\(\\alpha\\) es el parámetro de suavizado que tiene en cuenta las clases que no están presentes en las muestras de aprendizaje y evitan las probabilidades nulas en los cálculos posteriores. El ajuste se denomina suavizado de Laplace cuando \\(\\alpha=1\\), mientras que se denomina suavizado de Lidstone cuando \\(\\alpha < 1\\).\n\n\n9.1.3 Naïve Bayes Gaussiano\nEs una variante del Naïve Bayes Multinomial donde las variables predictoras son todas de tipo numérico. Todas ellas se distribuyen mediante una distribución Normal Multivariante. Este algoritmo hace uso de las medias y desviaciones estándar de las predictoras para obtener la probabilidad de clasificación de cada etiqueta de la respuesta. En este caso las verosimilitudes necesarias para la regla de clasificación se obtienen a partir de la función de densidad de la distribución normal como:\n\\[P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}}exp\\left(-\\frac{(x_i-\\mu_y)^2}{2\\sigma_y^2}\\right),\\]\ndonde los parámetros \\(\\mu_y\\) y \\(\\sigma_y^2\\) se estiman por máxima verosimilitud a partir de la información contenida en la muestra de entrenamiento, es decir, las medias y varianzas muestrales de cada predictora cuando estamos en la clase \\(l\\)."
  },
  {
    "objectID": "90_BayesianClassif.html#sec-80.2",
    "href": "90_BayesianClassif.html#sec-80.2",
    "title": "9  Modelos de clasificación Naïve Bayes",
    "section": "9.2 Ventajas e inconvenientes de los clasificadores Naïve Bayes",
    "text": "9.2 Ventajas e inconvenientes de los clasificadores Naïve Bayes\nLos clasificadores Naïve Bayes tienden a funcionar especialmente bien en cualquiera de las siguientes situaciones:\n\nCuando las clases de la respuesta están bien separadas, es decir, la distribución de probabilidad posterior de las clases en función de las predictoras son diferentes.\nCuando disponemos de una gran cantidad de predictoras y la complejidad del modelo no es relevante.\n\nEstos dos puntos están relacionados ya que a medida que aumenta la dimensión de un conjunto de datos, es mucho menos probable que se descubran dos puntos cercanos entre sí. Esto significa que las agrupaciones en dimensiones altas tienden a estar más separadas que las agrupaciones en dimensiones bajas.\nEl clasificador Naïve Bayes tiene además las siguientes ventajas computacionales:\n\nEs extremadamente rápido tanto para el entrenamiento como para la predicción, y por tanto tiene un coste de cálculo muy bajo.\nProporciona una predicción probabilística directa.\nPuede trabajar eficazmente en un gran conjunto de datos.\nCuando se cumple el supuesto de independencia (algo que en la práctica es bastante difícil), un clasificador Naïve Bayes funciona mejor en comparación con otros modelos como la regresión logística.\n\nEntre las desventajas de este algoritmo podemos mencionar:\n\nLa hipótesis de la independencia condicional no siempre se cumple. En la mayoría de las situaciones, las variables predictoras muestran alguna forma de dependencia.\nEl problema de la probabilidad cero hace referencia a las situaciones en las que en la muestra de test tenemos valores de la respuesta que no estaban en la muestra de entrenamiento. Esto provoca automáticamente que la probabilidad de esa clase sea siempre cero. Por ese motivo hay que tener cuidado y analizar con detalle la muestra de entrenamiento para asegurar de que se dispone de valores de todas las clases de la respuesta."
  },
  {
    "objectID": "90_BayesianClassif.html#sec-90.3",
    "href": "90_BayesianClassif.html#sec-90.3",
    "title": "9  Modelos de clasificación Naïve Bayes",
    "section": "9.3 Bancos de datos",
    "text": "9.3 Bancos de datos\nPara mostrar el funcionamiento de los algoritmos de clasificación naïve Bayes utilizamos los mismos ejemplos del tema anterior, para poder comparar los resultados entre ambos algoritmos. Como ya hemos visto y trabajado con ambos bancos de datos, en este punto solo cargamos las tareas correspondientes.\n\n9.3.1 Breast Cancer Wisconsin\nEn esta base de datos se recoge información sobre los cánceres de mama en la ciudad de Wisconsin. Las características de la base de datos se calculan a partir de una imagen digitalizada de un aspiración de aguja fina (FNA) de una masa mamaria. Describen las características de los núcleos celulares presentes en la imagen y el objetivo que se persigue es clasificar un tumor como benigno o maligno en función de las variables predictoras. Como en este caso estamos interesados en saber que predictoras influyen más en el carácter maligno del cáncer, utilizaremos esa categoría como la de interés.\n\n# Cargamos datos\nbreastcancer = read_rds(\"breastcancer.rds\")\n# Creación de task eliminado la columna que identifica os sujetos\ntsk_cancer = as_task_classif(breastcancer[,-1], target = \"diagnosis\", positive = \"M\")\n\n\n\n9.3.2 Iris\nEl banco de datos iris ya los presentamos en temas anteriores y aquí solo se presenta el código para crear la tarea de clasificación correspondiente.\n\n# Cargamos datos\niris = read_rds(\"iris.rds\") \n# creamos la tarea\ntsk_iris = as_task_classif(iris, target = \"species\")"
  },
  {
    "objectID": "90_BayesianClassif.html#sec-90.4",
    "href": "90_BayesianClassif.html#sec-90.4",
    "title": "9  Modelos de clasificación Naïve Bayes",
    "section": "9.4 Clasificador naïve Bayes en mlr3",
    "text": "9.4 Clasificador naïve Bayes en mlr3\nPara realizar el proceso de aprendizaje de un modelo de clasificación de clasificación naïve Bayes debemos usar el learner classif.naive_bayes que permite obtener de forma automática el clasificador correspondiente a cada situación. Podemos cargar el clasificador con el código siguiente:\n\n# Cargamos learner\nlearner = lrn(\"classif.naive_bayes\")\n\nLos hiperparámetros de este algoritmo son:\n\n# Hiperparámetros para árboles de clasificación\nlearner$param_set$ids()\n\n[1] \"eps\"       \"laplace\"   \"threshold\"\n\n\ny se interpretan como:\n\neps: es un número para especificar un rango épsilon para aplicar el suavizado de Laplace, es decir, para reemplazar probabilidades cero o cercanas a cero por theshold.\nlaplace: Suavizado de Laplace de doble control que se específica con un valor positivo. El valor predeterminado (0) desactiva el suavizado de Laplace.\nthreshold: valor con el que se reemplazan probabilidades dentro del rango dado por eps.\n\nEn primer lugar obtenemos el clasificador naïve Bayes por defecto para cada banco de datos, lo que nos permitirá tener un modelo de base para comparación. El análisis de estos modelos es similar al de los problemas de clasificación de la regresión logística en cuanto a términos de evaluación y validación de la solución obtenida. A continuación mostramos los resultados obtenidos para las muestras de validación en los modelos de regresión lógistica para poder comparar los resultados.\n\n\n\n\n\n\n\n\n\nDatos\nModelo\n% Clasificación correcta\nScore de Brier\n\n\n\n\nCáncer\nclassif.log_reg\n97.34\n0.0265\n\n\nCáncer\nclassif.glmnet\n96.70\n\n\n\nIris\nclassif.multinom\n96.66\n0.0667\n\n\nIris\nclassif.glmnet\n85.50\n\n\n\n\n\n9.4.1 Datos Breast Cancer\nComenzamos nuestro análisis con el banco de datos breast cancer. Para ello debemos definir el grpahlearner asociado (prerprocesamiento y modelo):\n\n# Definimos learner para predecir la probabilidad\nlearner = lrn(\"classif.naive_bayes\", predict_type = \"prob\")\n# Preprocesado\npp_cancer = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Graphlearner\ngr = pp_cancer %>>% learner\ngr = GraphLearner$new(gr)\n\nPara poder entrenar el modelo consideramos la división de muestras (80-20) y estratificamos según la variable diagnosis dado que los niveles no están equilibrados (ver tema de regresión logística).\n\n# Generamos variable de estrato\ntsk_cancer$col_roles$stratum <- \"diagnosis\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_cancer, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_cancer = tsk_cancer$clone()$filter(splits$train)\ntsk_test_cancer  = tsk_cancer$clone()$filter(splits$test)\n\nPodemos comenzar ahora con el entrenamiento del modelo y su interpretación:\n\n# Entrenamiento\ngr$train(tsk_train_cancer)\n\nEl entrenamiento de este modelo proporciona dos resultados:\n\napriori: probabilidades a priori de cada una de los niveles del factor utilizados en el proceso de clasificación.\ntables: Una lista de tablas, una para cada variable predictiva. Para cada variable categórica, una tabla que proporciona, para cada nivel de atributo, las probabilidades condicionales dada la clase objetivo. Para cada variable numérica, una tabla que proporciona, para cada clase objetivo, la media y la desviación estándar de la (sub)variable.\n\nVeamos el resultado para este modelo donde todas las predictoras son de tipo numérico:\n\n# Tablas\nmodelo = gr$model$classif.naive_bayes$model$tables\nmodelo\n\n$area_mean\n   area_mean\ny         [,1]      [,2]\n  M  0.9136272 1.0262682\n  B -0.5430651 0.4220094\n\n$area_se\n   area_se\ny         [,1]     [,2]\n  M  0.7251339 1.330193\n  B -0.4310236 0.217210\n\n$area_worst\n   area_worst\ny         [,1]      [,2]\n  M  0.9436145 1.0483184\n  B -0.5608897 0.3130474\n\n$compactness_mean\n   compactness_mean\ny         [,1]      [,2]\n  M  0.7633525 1.0244019\n  B -0.4537410 0.6480995\n\n$compactness_se\n   compactness_se\ny         [,1]      [,2]\n  M  0.3551035 1.0293835\n  B -0.2110755 0.9209903\n\n$compactness_worst\n   compactness_worst\ny         [,1]      [,2]\n  M  0.7544960 1.0866349\n  B -0.4484766 0.5957466\n\n$concave_points_mean\n   concave_points_mean\ny         [,1]      [,2]\n  M  1.0005673 0.8713090\n  B -0.5947428 0.4406619\n\n$concave_points_se\n   concave_points_se\ny         [,1]      [,2]\n  M  0.4888882 0.8801630\n  B -0.2905979 0.9538363\n\n$concave_points_worst\n   concave_points_worst\ny         [,1]      [,2]\n  M  1.0196676 0.6939235\n  B -0.6060962 0.5675614\n\n$concavity_mean\n   concavity_mean\ny         [,1]      [,2]\n  M  0.8824299 0.9271187\n  B -0.5245213 0.5884118\n\n$concavity_se\n   concavity_se\ny         [,1]      [,2]\n  M  0.2891704 0.6909921\n  B -0.1718845 1.1107806\n\n$concavity_worst\n   concavity_worst\ny         [,1]      [,2]\n  M  0.8293056 0.8670979\n  B -0.4929439 0.7046776\n\n$fractal_dimension_mean\n   fractal_dimension_mean\ny           [,1]     [,2]\n  M  0.007241941 1.054936\n  B -0.004304650 0.967738\n\n$fractal_dimension_se\n   fractal_dimension_se\ny          [,1]      [,2]\n  M  0.09314092 0.7653912\n  B -0.05536349 1.1139378\n\n$fractal_dimension_worst\n   fractal_dimension_worst\ny         [,1]      [,2]\n  M  0.4250828 1.1807233\n  B -0.2526716 0.7732786\n\n$perimeter_mean\n   perimeter_mean\ny         [,1]      [,2]\n  M  0.9503260 0.8836812\n  B -0.5648791 0.5239512\n\n$perimeter_se\n   perimeter_se\ny         [,1]      [,2]\n  M  0.7203788 1.2528486\n  B -0.4281972 0.4149526\n\n$perimeter_worst\n   perimeter_worst\ny         [,1]      [,2]\n  M  1.0014746 0.8772335\n  B -0.5952821 0.4316392\n\n$radius_mean\n   radius_mean\ny         [,1]      [,2]\n  M  0.9325751 0.8930061\n  B -0.5543278 0.5445070\n\n$radius_se\n   radius_se\ny         [,1]      [,2]\n  M  0.7345592 1.2239759\n  B -0.4366261 0.4415538\n\n$radius_worst\n   radius_worst\ny         [,1]      [,2]\n  M  0.9956696 0.8820899\n  B -0.5918316 0.4384883\n\n$smoothness_mean\n   smoothness_mean\ny         [,1]      [,2]\n  M  0.4580643 0.8947329\n  B -0.2722760 0.9603276\n\n$smoothness_se\n   smoothness_se\ny          [,1]      [,2]\n  M -0.10941078 0.9633435\n  B  0.06503438 1.0172515\n\n$smoothness_worst\n   smoothness_worst\ny         [,1]      [,2]\n  M  0.5416746 0.9584092\n  B -0.3219744 0.8790675\n\n$symmetry_mean\n   symmetry_mean\ny         [,1]      [,2]\n  M  0.4395975 0.9661733\n  B -0.2612992 0.9269095\n\n$symmetry_se\n   symmetry_se\ny           [,1]      [,2]\n  M  0.004624306 1.2092047\n  B -0.002748713 0.8540645\n\n$symmetry_worst\n   symmetry_worst\ny         [,1]      [,2]\n  M  0.5812294 1.1792987\n  B -0.3454860 0.6712026\n\n$texture_mean\n   texture_mean\ny         [,1]      [,2]\n  M  0.5351311 0.8394401\n  B -0.3180849 0.9519942\n\n$texture_se\n   texture_se\ny          [,1]     [,2]\n  M -0.01687734 0.878520\n  B  0.01003198 1.067032\n\n$texture_worst\n   texture_worst\ny         [,1]      [,2]\n  M  0.6031754 0.8540254\n  B -0.3585308 0.9044240\n\n\nPara cada variable se presenta en la primera columna las medias y en la segunda las desviaciones estándar. Podemos ver los resultados para area_mean:\n\nmodelo$area_mean\n\n   area_mean\ny         [,1]      [,2]\n  M  0.9136272 1.0262682\n  B -0.5430651 0.4220094\n\n\nPodemos ver como los valores más altos de area_men están vinculados con los tumores clasificados como malignos, mientras que los valores más bajos están vinculados a los tumores benignos. Estudiamos ahora la clasificación proporcionada por el algoritmo. En primer lugar calculamos las predicciones tanto para la muestra de entrenamiento como la de validación.\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_cancer)\npred_test = gr$predict(tsk_test_cancer)\n# Visualizamos las primeras predicciones de la muestra de validación\npred_test\n\n<PredictionClassif> for 113 observations:\n    row_ids truth response       prob.M        prob.B\n         12     M        M 1.000000e+00  2.388608e-21\n         21     B        B 4.558006e-12  1.000000e+00\n         26     M        M 1.000000e+00  7.027807e-84\n---                                                  \n        550     B        B 2.575166e-14  1.000000e+00\n        565     M        M 1.000000e+00 2.574139e-138\n        567     M        M 1.000000e+00  1.758746e-10\n\n\nConsideramos ahora diferentes métricas de evaluación y obtenemos sus valores para las muestras de predicción:\n\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\", \"classif.bbrier\", \"classif.auc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.93201754     0.92194570     0.06090643     0.98679556 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n     0.9734513      0.9691482      0.0255175      0.9979879 \n\n\nEl porcentaje de clasificación correcta para la muestra de validación es del 97.34%, que es igual al mejor que obteníamos para el modelo de regresión logística. Tanto el score de brier como el AUC proporcionan valores que indican que el algoritmo utilizado proporciona una buena clasificación. Podemos ver la tabla de confusión para ver donde se concentran los errores de clasificación cometidos.\n\n# Muestra de validación\npred_test$confusion\n\n        truth\nresponse  M  B\n       M 40  1\n       B  2 70\n\n\nEn este caso tenemos dos muestras que originalmente correspondían a un tumor maligno pero que el modelo clasifica como benigno. Por contra, hay 1 que originalmente era benigno y que el modelo clasifica como maligno. Representamos gráficamente la matriz de confusión.\n\n# Cargamos la librería para representar la matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nProcedemos ahora con el estudio de validación de la solución mediante un análisis de validación cruzada con 10 folds.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_cancer, gr, resamp, store_models=TRUE)\n\nINFO  [17:40:45.324] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:40:45.547] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:40:45.772] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:40:46.473] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:40:46.673] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:40:46.834] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:40:46.979] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:40:47.146] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:40:47.293] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:40:47.459] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\n\n\n\n# Resumen Scores individuales\nscores = rr$score(measures)$classif.acc\nskim(scores)\n\n\nData summary\n\n\nName\nscores\n\n\nNumber of rows\n10\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n0.93\n0.04\n0.88\n0.89\n0.95\n0.96\n0.97\n▅▁▂▂▇\n\n\n\n\n\nPodemos ver como el promedio del porcentaje de clasificación correcta se sitúa en el 92.95% con una desviación del 3%, lo que indica que tenemos bastante precisión en la solución obtenida. Sin embargo, como la diferencia entre la mediana y la media es de casi un 2% la distribución de los resultados es algo asimétrica lo que puede indicar cierta dependencia de los resultados con respecto a la muestra de entrenamiento utilizada. Analizamos la curva de aprendizaje asociada cargando en primer lugar las funciones correspondientes.\n\n\n\n\nplot_learningcurve(tsk_cancer, gr, \"classif.acc\", ptr = seq(0.1, 0.9, 0.1), rpeats = 10)\n\nINFO  [17:40:48.062] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:40:48.601] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:40:49.106] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:40:49.579] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:40:50.106] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:40:50.610] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:40:51.109] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:40:51.586] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:40:52.050] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:40:52.486] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\nINFO  [17:40:53.050] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:40:53.526] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:40:53.997] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:40:54.438] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:40:54.877] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:40:55.324] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:40:55.761] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:40:56.252] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:40:56.695] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:40:57.127] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\nINFO  [17:40:57.642] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:40:58.082] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:40:58.565] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:40:59.025] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:40:59.485] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:40:59.916] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:41:00.366] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:41:00.801] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:41:01.236] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:41:01.678] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\nINFO  [17:41:02.216] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:41:02.666] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:41:03.132] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:41:03.574] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:41:04.007] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:41:04.459] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:41:04.909] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:41:05.353] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:41:05.792] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:41:06.238] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\nINFO  [17:41:06.782] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:41:07.223] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:41:07.684] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:41:08.127] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:41:08.920] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:41:09.345] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:41:09.773] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:41:10.206] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:41:10.646] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:41:11.065] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\nINFO  [17:41:11.572] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:41:12.013] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:41:12.449] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:41:12.881] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:41:13.321] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:41:13.760] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:41:14.177] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:41:14.615] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:41:15.046] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:41:15.483] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\nINFO  [17:41:15.994] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:41:16.435] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:41:16.871] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:41:17.292] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:41:17.735] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:41:18.167] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:41:18.638] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:41:19.069] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:41:19.538] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:41:19.974] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\nINFO  [17:41:20.481] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:41:20.914] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:41:21.375] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:41:21.809] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:41:22.255] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:41:22.698] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:41:23.121] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:41:23.565] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:41:23.999] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:41:24.443] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\nINFO  [17:41:24.957] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:41:25.403] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:41:25.845] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:41:26.269] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:41:26.713] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:41:27.146] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:41:27.583] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:41:28.017] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:41:28.459] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:41:28.964] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'breastcancer[, -1]' (iter 10/10)\n\n\n\n\n\nCurva dea rpendixaje modelo naïve Bayes. task Breast Cancer\n\n\n\n\nPodemos ver cierta irregularidad en ambas curvas indicando que en este caso no aprece existir una tamaño mejor que otro. De hecho, los valores se mueven entre el 92% y el 95% cualquiera que sea el tamaño de la muestra de entrenamiento.\n\n\n9.4.2 Datos iris\nVemos ahora el análisis para el banco de datos iris que recordemos tiene tres niveles en su variable respuesta. Comenzamos definiendo la estructura del algoritmo de aprendizaje.\n\n# Definimos learner para predecir la probabilidad\nlearner = lrn(\"classif.naive_bayes\", predict_type = \"prob\")\n# Preprocesado\npp_iris = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Graphlearner\ngr = pp_iris %>>% learner\ngr = GraphLearner$new(gr)\n\nDefinimos las muestras de entrenamiento y validación\n\n# Generamos variable de estrato\ntsk_iris$col_roles$stratum <- \"species\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_iris, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_iris = tsk_iris$clone()$filter(splits$train)\ntsk_test_iris  = tsk_iris$clone()$filter(splits$test)\n\nAhora podemos comenzar el proceso de aprendizaje asociado con este modelo.\n\n# Entrenamiento\ngr$train(tsk_train_iris)\n# Tablas\nmodelo = gr$model$classif.naive_bayes$model\nmodelo\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = x, y = y)\n\nA-priori probabilities:\ny\n    Iris-setosa Iris-versicolor  Iris-virginica \n      0.3333333       0.3333333       0.3333333 \n\nConditional probabilities:\n                 petal_length\ny                       [,1]       [,2]\n  Iris-setosa     -1.3021518 0.08463312\n  Iris-versicolor  0.2826886 0.22953367\n  Iris-virginica   1.0194633 0.32317365\n\n                 petal_width\ny                       [,1]      [,2]\n  Iris-setosa     -1.2515277 0.1432356\n  Iris-versicolor  0.1572487 0.2527098\n  Iris-virginica   1.0942790 0.3269682\n\n                 sepal_length\ny                       [,1]      [,2]\n  Iris-setosa     -1.0430969 0.3914919\n  Iris-versicolor  0.1402483 0.5633032\n  Iris-virginica   0.9028485 0.7800932\n\n                 sepal_width\ny                        [,1]      [,2]\n  Iris-setosa      0.76933531 0.8388127\n  Iris-versicolor -0.68141128 0.7906397\n  Iris-virginica  -0.08792404 0.7945786\n\n\nEstudiamos la capacidad explicativa del modelo propuesto. Calculamos las predicciones:\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_iris)\npred_test = gr$predict(tsk_test_iris)\n# Visualizamos las primeras predicciones de la muestra de validación\npred_test\n\n<PredictionClassif> for 30 observations:\n    row_ids          truth       response prob.Iris-setosa prob.Iris-versicolor\n         15    Iris-setosa    Iris-setosa     1.000000e+00         7.418733e-22\n         16    Iris-setosa    Iris-setosa     1.000000e+00         4.068547e-19\n         18    Iris-setosa    Iris-setosa     1.000000e+00         2.819040e-20\n---                                                                            \n        147 Iris-virginica Iris-virginica    1.038232e-171         3.935583e-02\n        148 Iris-virginica Iris-virginica    2.036225e-192         3.474621e-04\n        150 Iris-virginica Iris-virginica    2.942447e-170         6.339116e-02\n    prob.Iris-virginica\n           4.864661e-26\n           1.792012e-23\n           1.425085e-25\n---                    \n           9.606442e-01\n           9.996525e-01\n           9.366088e-01\n\n\nObtenemos la matriz de confusión para la muestra de validación:\n\n# matriz de confusión\npred_test$confusion\n\n                 truth\nresponse          Iris-setosa Iris-versicolor Iris-virginica\n  Iris-setosa              10               0              0\n  Iris-versicolor           0              10              2\n  Iris-virginica            0               0              8\n\n\nEl modelo clasifica correctamente todas las muestras de las dos primeras clases, pero no lo hace para la última clase. En ese caso dos muestras que originalmente eran Iris-virginica se clasifican como Iris versicolor. Vemos la solución gráfica:\n\n# Cargamos la librería para representar la matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nA continuación vemos los scores de clasificación para este problema:\n\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.mbrier\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc classif.mbrier \n    0.96666667     0.05841196 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc classif.mbrier \n    0.93333333     0.09138642 \n\n\nObtenemos un porcentaje de clasificación correcta para la muestra de validación del 93.33% lo que demuestra que el modelo de aprendizaje propuesto funciona adecuadamente. El resultado es consistente con el proporcionado por el modelo de regresión logística. El score de brier es bastante bajo y similar al del modelo de regresión logística. Procedemos ahora con el estudio de validación de la solución mediante un análisis de validación cruzada con 10 folds.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_iris, gr, resamp, store_models=TRUE)\n\nINFO  [17:41:30.659] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:30.803] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:31.016] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:31.169] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:31.276] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:31.386] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:31.511] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:31.616] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:31.731] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:31.891] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\n\n\n\n# Resumen Scores individuales\nscores = rr$score(measures)$classif.acc\nskim(scores)\n\n\nData summary\n\n\nName\nscores\n\n\nNumber of rows\n10\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n0.96\n0.06\n0.87\n0.93\n1\n1\n1\n▂▁▂▁▇\n\n\n\n\n\nPodemos ver como el promedio del porcentaje de clasificación correcta se sitúa en el 96% con una desviación del 5%, lo que indica que tenemos bastante precisión en la solución obtenida. Analizamos la curva de aprendizaje asociada cargando en primer lugar las funciones correspondientes.\n\nplot_learningcurve(tsk_iris, gr, \"classif.acc\", ptr = seq(0.1, 0.9, 0.1), rpeats = 10)\n\nINFO  [17:41:32.339] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:32.577] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:32.767] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:32.960] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:33.167] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:33.348] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:33.562] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:33.743] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:33.943] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:34.106] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\nINFO  [17:41:34.372] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:34.543] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:34.726] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:34.901] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:35.073] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:35.251] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:35.423] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:35.602] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:35.791] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:35.996] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\nINFO  [17:41:36.264] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:36.467] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:36.646] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:36.845] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:37.008] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:37.224] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:37.390] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:37.584] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:37.745] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:37.936] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\nINFO  [17:41:38.177] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:38.378] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:38.540] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:38.719] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:38.930] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:39.096] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:39.281] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:39.453] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:39.641] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:39.802] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\nINFO  [17:41:40.069] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:40.235] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:40.475] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:40.665] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:40.852] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:41.021] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:41.186] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:41.378] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:41.549] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:41.740] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\nINFO  [17:41:41.992] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:42.180] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:42.350] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:42.551] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:42.715] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:42.905] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:43.073] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:43.264] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:43.434] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:43.624] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\nINFO  [17:41:43.869] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:44.057] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:44.224] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:44.417] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:44.584] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:44.792] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:44.993] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:45.199] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:45.383] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:45.587] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\nINFO  [17:41:45.898] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:46.100] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:46.267] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:46.462] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:46.639] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:46.820] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:46.998] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:47.165] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:47.361] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:47.540] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\nINFO  [17:41:47.814] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 1/10)\nINFO  [17:41:48.003] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 2/10)\nINFO  [17:41:48.176] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 3/10)\nINFO  [17:41:48.360] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 4/10)\nINFO  [17:41:48.543] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 5/10)\nINFO  [17:41:49.092] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 6/10)\nINFO  [17:41:49.249] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 7/10)\nINFO  [17:41:49.414] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 8/10)\nINFO  [17:41:49.575] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 9/10)\nINFO  [17:41:49.735] [mlr3] Applying learner 'scale.classif.naive_bayes' on task 'iris' (iter 10/10)\n\n\n\n\n\nCurva de aprendizaje modelo naïve Bayes. Task Iris"
  },
  {
    "objectID": "90_BayesianClassif.html#sec-90.5",
    "href": "90_BayesianClassif.html#sec-90.5",
    "title": "9  Modelos de clasificación Naïve Bayes",
    "section": "9.5 Ejercicios",
    "text": "9.5 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en un modelo de clasificación naïve Bayes para el banco de datos Mushroom4.3.4.\nAjustar un modelo de aprendizaje automático basado en un modelo de clasificación naïve Bayes para el banco de datos Water potability4.3.7.\nAjustar un modelo de aprendizaje automático basado en un modelo de clasificación naïve Bayes para el banco de datos Hepatitis4.3.9.\nAjustar un modelo de aprendizaje automático basado en un modelo de clasificación naïve Bayes para el banco de datos Abalone4.3.1."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.1",
    "href": "100_kNNmodels.html#sec-100.1",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.1 Etapas del algorirmo kNN",
    "text": "10.1 Etapas del algorirmo kNN\nImaginemos que tenemos una matriz de predictoras \\(X\\) y un vector de respuestas \\(y\\), de tipo categórico para un problema de clasificación (o de tipo numérico para un problema de regresión), de forma que dividimos el conjunto de muestras en entrenamiento y validación:\n\\[X = (X_e, X_v); \\quad y = (y_e, y_v)\\]\nLa lógica de funcionamiento detrás del algoritmo k-NN es una de las más sencillas de todos los algoritmos de aprendizaje automático supervisados y se organiza a través de las etapas siguientes:\n\nEtapa 1. Fijar valor de k. Establecer el número de k vecinos que vamos a utilizar.\nEtapa 2. Obtención de distancias. Se establece una distancia \\(d\\), y se calcula la distancia de todos los elementos de la muestra de entrenamiento con respecto a cada uno de los elementos de la muestra de validación utilizando para ello las variables predictoras:\n\n\\[d_v= d(X_e,X_v).\\]\ndonde \\(d_v\\) es una matriz de dimensiones igual a muestras de entrenamiento por muestras de validación.\n\nEtapa 3. Identificación de k vecinos. Para cada una de las muestras del conjunto de validación se deben seleccionar los k vecinos más cercanos (basado en las distancias de la etapa 2) de la muestra de entrenamiento. Debemos tomar los k elementos con menor valor de distancia para cada una de las columnas de \\(d_v\\) e identificar dichas observaciones para seleccionar los correspondientes valores del vector \\(y_e\\).\nEtapa 4. Predicción. Para un problema de clasificación se establece la etiqueta mayoritaria de la variable respuesta dentro de los k vecinos seleccionados para cada una de las muestras de validación, y se asigna como respuesta dicho valor, es decir, se calcula la frecuencia de cada valor de la respuesta para el conjunto de k vecinos de \\(y_e\\) y se asigna como valor de predicción el mayoritario. En un problema de regresión para cada una de las muestras de validación se calcula la media de la variable respuesta del conjunto de k vecinos seleccionados dentro de \\(y_e\\), y se asigna dicho valor como predicción de la respuesta.\nEtapa 5. Validación. Una vez obtenida la predicción con los k vecinos debemos validar la solución comparándola con los valores reales observados \\(y_v\\). Para ello se utilizan las métricas habituales en función de que el problema sea de clasificación o regresión.\n\nEn la imagen siguiente se muestra un resumen de las etapas del k-NN para una tarea de clasificación usando los 4 vecinos más próximos."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.2",
    "href": "100_kNNmodels.html#sec-100.2",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.2 Métricas de distancia",
    "text": "10.2 Métricas de distancia\nPara determinar qué elementos de la muestra de entrenamiento están más cerca de cada uno de los puntos de la muestra de validación, es necesario calcular la distancia entre ellos, a partir de alguna función de distancia.\nSi bien hay varias medidas de distancia entre las que puede elegir, se presentan a continuación las más habituales. Cualquier otra función que cumpla con los requisitos para ser distancia puede ser utilizada dentro de este algoritmo.\nEn todos los casos consideramos que el conjunto de muestras de entrenamiento es \\(n_e\\), el conjunto de muestras de validación es \\(n_v\\), y el número de predictoras es \\(p\\).\n\nDistancia euclídea: esta es la medida de distancia más utilizada y está limitada a variables predictoras de tipo numérico. Se define la distancia como:\n\n\\[d(x_{e_i},x_{v_j}) = \\sqrt{\\sum_{l=1}^{p} (x_{v_l}-x_{e_l})^2}, \\quad i=1,...,n_e, \\quad j=1,...,n_v\\]\n\nDistancia Manhattan: esta es también otra métrica de distancia popular, que mide la distancia en términos del valor absoluto entre dos puntos. También se conoce como distancia de taxi o distancia de la ciudad, ya que comúnmente se visualiza con una cuadrícula, que ilustra cómo se puede navegar de una dirección a otra a través de las calles de la ciudad. Se define como:\n\n\\[d(x_{e_i},x_{v_j}) = \\sum_{l=1}^{p} |x_{v_l}-x_{e_l}|, \\quad i=1,...,n_e, \\quad j=1,...,n_v\\]\n\nDistancia Minkowski: esta medida de distancia es la forma generalizada de las métricas de distancia euclidiana y Manhattan. El parámetro, g, en la fórmula a continuación, permite la creación de otras métricas de distancia. La distancia euclidiana se representa mediante esta fórmula cuando g es igual a dos, y la distancia de Manhattan se denota con g igual a uno. Se define como:\n\n\\[d(x_{e_i},x_{v_j}) = \\left(\\sum_{l=1}^{p} |x_{v_l}-x_{e_l}| \\right)^{1/g}, \\quad i=1,...,n_e, \\quad j=1,...,n_v\\]\n\nDistancia Hamming: esta métrica se usa típicamente cuando las variables predictoras son de tipo booleano o de cadena, identificando los puntos donde los vectores no coinciden. Como resultado, también se la conoce como la métrica de superposición. Esto se puede representar con la siguiente fórmula:\n\n\\[d(x_{e_i},x_{v_j}) = \\left(\\sum_{l=1}^{p} |x_{v_l}-x_{e_l}| \\right), \\quad i=1,...,n_e, \\quad j=1,...,n_v, \\quad \\text{con}\\]\n\\[d = 0 \\quad \\text{ si } x_{v_l}=x_{e_l} \\quad \\text{ y } \\quad d = 1 \\quad \\text{ si } x_{v_l}\\neq x_{e_l}\\]\nSi en el conjunto de predictoras disponemos tanto de variables numéricas como variables booleanas o de tipo cadena podemos utilizar una métrica que combina la distancia euclídea y la Hamming en una sola. Esta distancia se define como:\n\\[d_M(x_{e_i},x_{v_j}) = \\sqrt{\\sum_{l=1}^{p} d(x_{v_l},x_{e_l})}\\]\ndonde \\(d\\) se corresponde con la distancia euclídea cuando la predictora es numérica, y con Hamming cuando la predictora es booleana o cadena. Sin embargo, hay que tener cuidado con esta métrica cuando entre las predictoras tenemos una variable ordinal con más de dos categorías ya que la distancia Hamming no respeta el orden y asigna la misma diferencia entre los distintos niveles de la variable independientemente de lo cerca o lejos que estén dichas categorías."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.3",
    "href": "100_kNNmodels.html#sec-100.3",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.3 Predictoras irrelevantes y problemas de escala",
    "text": "10.3 Predictoras irrelevantes y problemas de escala\nDos aspectos muy relevantes en la implementación del algoritmo k-NN son la utilización de variables predictoras que no aportan información suficiente para explicar el comportamiento de la respuesta, y los efectos de las escalas de medida en que cada una de las predictoras ha sido registrada, dado que la elección de los k-vecinos se realiza en términos de distancia y puede verse muy afectada por estas situaciones.\nEn cuanto a la primera dificultad, la magnitud del efecto causado por las predictoras irrelevantes depende de cuántas de ellas se utilicen para realizar el entrenamiento del modelo. En un conjunto con cientos de predictoras, de las cuales sólo una es irrelevante, no hay motivos de preocupación ya que es poco probable que un solo culpable distorsione el valor de la distancia de forma significativa. Pero las cosas pueden cambiar a medida que aumenta el porcentaje de predictoras irrelevantes. Si la gran mayoría de ellas no tiene capacidad para explicar el comportamiento de la respuesta, entonces independientemente de la distancia utilizada esta carecerá prácticamente de sentido y el rendimiento del algoritmo será pésimo.\nPara solucionar este problema se puede utilizar el siguiente procedimiento:\n\nRealizar un análisis previo del conjunto de predictoras para tratar de determinar un conjunto inicial mínimo de las que estén muy relacionadas con la respuesta que deseamos predecir.\nIr añadiendo poco a poco el resto de predictoras al conjunto anterior e ir valorando la capacidad del algoritmo hasta que esta sea estable o disminuya.\n\nPara explicar cómo influye el efecto de escala en el cálculo de las distancias consideramos el ejemplo siguiente. Tenemos tres variables predictoras de forma que la primera es de tipo booleano (verdadero o falso), la segunda es una variable numérica con valores en el rango [0, 1], y la tercera es una variable numérica con valores en el rango [0, 1000], de forma que queremos evaluar la distancia entre dos muestras con vectores de datos dados por \\(p_1= (V, 0.2, 254)\\) y \\(p_2= (F, 0.1, 194)\\). Utilizando la distancia \\(d_M\\) tenemos que:\n\\[d_M(p_1, p_2) = \\sqrt{(1-0)^2 + (0.2-0.1)^2 + (254-194)^2}\\]\nAl estudiar en detalle esta distancia observamos que la tercera predictora domina por completo el valor de la distancia, reduciendo las otras dos predictoras a una virtual insignificancia. No importa cómo modifiquemos sus valores dentro de sus rangos, la distancia apenas cambiará. Afortunadamente, la situación es fácil de rectificar. Si dividimos, en el conjunto de entrenamiento, todos los valores del tercer atributo por 1000, modificamos su escala de forma que, los impactos de las predictoras se volverán más equilibrados. Por tanto, la mejor forma de evitar los posibles problemas de escala es normalizar o estandarizar las variables para conseguir que todas ellas se encuentren en una misma escala y se afecte lo menos posible el comportamiento del algoritmo k-NN. Sin embargo, incluso en este caso es necesario estudiar con detalle la normalización o estandarización propuesta para evitar comportamientos erróneos del algoritmo k-NN."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.4",
    "href": "100_kNNmodels.html#sec-100.4",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.4 Seleccionando el número de vecinos",
    "text": "10.4 Seleccionando el número de vecinos\nUno de los aspectos más relevantes en la utilización del algoritmo k-NN es la selección de k. Para dicha elección se utiliza una estrategia de búsqueda basada en la capacidad del algoritmo de clasificar o predecir de forma más precisa.\nPara ello se establece un grid continuo de valores de k y se analiza la curva de evolución del porcentaje de clasificación erróneo (problema de clasificación) o del error cuadrático medio de predicción (problema de regresión). Una vez evaluado todo el grid de valores considerado se selecciona como posible valor de k aquel que proporciona el menor valor de la medida de eficiencia.\nEn la práctica se suele utilizar una búsqueda automática del valor de k sin necesidad de especificar el grid correspondiente lo que facilita la programación del proceso de aprendizaje."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.5",
    "href": "100_kNNmodels.html#sec-100.5",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.5 Ponderación de los vecinos más cercanos",
    "text": "10.5 Ponderación de los vecinos más cercanos\nComo ya se ha comentado anteriormente la elección del número de vecinos es un aspecto muy relevante ya que la predicción para una nueva observación depende del conjunto de vecinos utilizado. Por ejemplo, imaginemos un problema de clasificación de dos grupos (en valor positivo o negativo) donde queremos predecir la clase de un nuevo elemento a partir de cinco vecinos a distancias 10/6, 10/7, 10, 10/2, y 10/3, donde los dos primeros tienen etiqueta positiva y los tres últimos tienen etiqueta negativa. En esta situación, el algoritmo estándar asignaría la etiqueta negativa ya que la mayoría de vecinos tienen esa etiqueta, ignorando el hecho de que los vecinos más cercanos tienen la etiqueta positiva.\nPara solucionar este hecho se puede modificar el algoritmo para que las observaciones más cercanas tengan un mayor impacto sobre la predicción final del modelo de aprendizaje. Para ello se considera la introducción de pesos sobre cada uno de los vecinos y se obtiene la predicción a partir de la suma de dichos pesos para cada una de las categorías consideradas. En la situación anterior podemos considerar como peso el inverso de las distancias de forma que tendríamos pesos 0.6 y 0.7 para los vecinos positivos y 0.1, 0.2, y 0.3 para los negativos. De esta forma la suma de los pesos positivos es de 1.3 y la de los pesos negativos 0.5, de forma que asignaríamos la etiqueta positiva a la nueva observación.\nPara mantener cierta objetividad en la elección de pesos se suele usar el procedimiento que pasamos a desarrollar. Supongamos que los k vecinos para una nueva observación se encuentran a distancias \\(d_1,...,d_k\\), ordenadas de menor a mayor, de forma que el peso del i-ésimo vecino \\(w_i\\) se define como:\n\\[w_i = \\frac{d_k - d_i}{d_k - d_i} \\quad { si}  \\quad d_k = d_1 \\text{ y 1 si } \\quad d_k \\neq d_1\\]\nEvidentemente, los pesos así obtenidos irán de 0 para el vecino más lejano a 1 para el más cercano. Esto significa que el enfoque sólo tiene en cuenta a \\(k-1\\) vecinos. Por supuesto, esto sólo tiene sentido para \\(k >3\\). Si utilizáramos \\(k = 3\\), sólo participarían realmente dos vecinos, y el clasificador ponderado de 3-NN degeneraría en el clasificador de 1-NN.\nPodemos obtener la predicción a partir de la clase con mayor suma de los pesos obtenidos para cada una de las clases consideradas."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.6",
    "href": "100_kNNmodels.html#sec-100.6",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.6 Ventajas y desventajas de kNN",
    "text": "10.6 Ventajas y desventajas de kNN\nk-NN es uno de los algoritmos más simples de aprendizaje automático, y es muy implementado por los desarrolladores de sistemas basados en el aprendizaje, intuitivos e inteligentes para efectuar y tomar pequeñas decisiones solos. Sin embargo, como todos los algoritmos o modelos de aprendizaje automático dispone de ciertas ventajas respecto de otros modelos de clasificación pero también adolece de ciertas dificultades. Las ventajas de uso del algoritmo k-NN son:\n\nEs simple y fácil de aplicar.\nNo es necesario crear un modelo, configurar varios parámetros o formular hipótesis suplementarias.\nA medida que se agregan nuevas muestras de entrenamiento, el algoritmo se ajusta para tener en cuenta cualquier dato nuevo, ya que todos los datos de entrenamiento se almacenan en la memoria.\nEs polivalente. Puede ser utilizado para la clasificación o la regresión, aunque su uso más generalizado es en problemas de clasificación.\n\nEntre las desventajas podemos destacar:\n\nEl algoritmo se vuelve más lento a medida que el número de observaciones aumenta y las variables predictoras aumentan.\nOcupa más memoria y almacenamiento de datos en comparación con otros clasificadores. Esto puede ser costoso desde una perspectiva de tiempo y dinero.\nDebido al efecto de la dimensionalidad k-NN también es más propenso al sobreajuste. Si bien se aprovechan las técnicas de selección de características y reducción de dimensionalidad para evitar que esto ocurra, el valor de k también puede afectar el comportamiento del algoritmo. Los valores más bajos de k pueden sobreajustar los datos, mientras que los valores más altos de k tienden a “suavizar” los valores de predicción, ya que están promediando los valores en un área o vecindario más grande. Sin embargo, si el valor de k es demasiado alto, entonces puede ajustarse mal a los datos."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.7",
    "href": "100_kNNmodels.html#sec-100.7",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.7 Bancos de datos",
    "text": "10.7 Bancos de datos\nPara mostrar el funcionamiento del algoritmo kNN tanto en problemas de clasificación como de regresión vamos a utilizar los bancos de datos de temas anteriores. Como ya los hemos descrito y trabajado con ellos, en este punto únicamente creamos las tareas correspondientes.\n\n10.7.1 Breast Cancer Wisconsin\n\n# Cargamos datos\nbreastcancer = read_rds(\"breastcancer.rds\")\n# Creación de task eliminando la columna que identifica os sujetos\ntsk_cancer = as_task_classif(breastcancer[,-1], target = \"diagnosis\", positive = \"M\")\ntsk_cancer\n\n<TaskClassif:breastcancer[, -1]> (569 x 31)\n* Target: diagnosis\n* Properties: twoclass\n* Features (30):\n  - dbl (30): area_mean, area_se, area_worst, compactness_mean,\n    compactness_se, compactness_worst, concave_points_mean,\n    concave_points_se, concave_points_worst, concavity_mean,\n    concavity_se, concavity_worst, fractal_dimension_mean,\n    fractal_dimension_se, fractal_dimension_worst, perimeter_mean,\n    perimeter_se, perimeter_worst, radius_mean, radius_se,\n    radius_worst, smoothness_mean, smoothness_se, smoothness_worst,\n    symmetry_mean, symmetry_se, symmetry_worst, texture_mean,\n    texture_se, texture_worst\n\n\n\n\n10.7.2 Iris\n\n# Cargamos datos\niris = read_rds(\"iris.rds\")\n# Convertimos carácter en factor\niris$species = as.factor(iris$species)\n# creamos la tarea\ntsk_iris = as_task_classif(iris, target = \"species\")\ntsk_iris\n\n<TaskClassif:iris> (150 x 5)\n* Target: species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): petal_length, petal_width, sepal_length, sepal_width\n\n\n\n\n10.7.3 Electricity\n\n# Carga de datos\nelectricity = read_rds(\"electricity.rds\")\n# Creación de task\ntsk_electricity = as_task_regr(electricity, target = \"PE\")\n# información de la tarea\ntsk_electricity\n\n<TaskRegr:electricity> (9568 x 5)\n* Target: PE\n* Properties: -\n* Features (4):\n  - dbl (4): AP, AT, RH, V\n\n\n\n\n10.7.4 Diabetes\n\n# Carga de datos\ndiabetes = read_rds(\"diabetes.rds\")\n# Creación de task\ntsk_diabetes = as_task_regr(diabetes, target = \"Y\")\n# información de la tarea\ntsk_diabetes\n\n<TaskRegr:diabetes> (442 x 11)\n* Target: Y\n* Properties: -\n* Features (10):\n  - dbl (9): AGE, BMI, BP, S1, S2, S3, S4, S5, S6\n  - fct (1): SEX"
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.8",
    "href": "100_kNNmodels.html#sec-100.8",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.8 Algoritmos kNN en mlr3",
    "text": "10.8 Algoritmos kNN en mlr3\nPara realizar el proceso de aprendizaje de un modelo basado en el algoritmo kNN podemos utilizar dos funciones:\n\nclassif.kknn para tareas de clasificación.\nregr.kknn para tareas de regresión.\n\nPodemos cargar los algoritmos con el código siguiente:\n\n# Learner tarea de clasificación\nlknn_classif = lrn(\"classif.kknn\")\n# Learner tarea de regresión\nlknn_regr = lrn(\"regr.kknn\")\n\nLos hiperparámetros para ambos algoritmos son los mismos y los podemos consultar con este código:\n\n# Hiperparámetros para árboles de clasificación\nlknn_classif$param_set$ids()\n\n[1] \"k\"           \"distance\"    \"kernel\"      \"scale\"       \"ykernel\"    \n[6] \"store_model\"\n\n\ndonde cada uno de ellos se interpreta como:\n\nk: número de vecinos considerados. El valor por defecto es \\(k=7\\).\ndistance: parámetro \\(g\\) de la distancia de minkowski. Por defecto se utiliza el valor 2 que representa la distancia euclídea.\nkernel: kernel a utilizar para la ponderación de distancias. Las opciones posibles son “rectangular” (que es el estándar no ponderado), “triangular”, “epanechnikov” (o beta(2,2)), “biweight” (o beta(3,3)), “triweight” (o beta(4,4)), “cos”, “inv”, “gaussiano”, “rango” y “óptimo”.\nscale: valor lógico que indica si debemos escalar las variables para tener la misma desviación típica. Por defecto toma el valor “TRUE”.\nykernel: ancho de ventana de un kernel y, especialmente para predicción de clases ordinales.\nstore_model: para guardar los resultados del modelo."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.9",
    "href": "100_kNNmodels.html#sec-100.9",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.9 Nuestros primeros modelos",
    "text": "10.9 Nuestros primeros modelos\nPara comenzar nuestros análisis utilizaremos los parámetros por defecto del algoritmo, y más tarde plantearemos una búsqueda automática para su elección óptima.\n\n10.9.1 Modelos por defecto\nModelos por defecto para cada uno de los bancos de datos donde en principio consideramos todas las posibles predictoras.\n\n10.9.1.1 Datos Breast Cancer\nComenzamos nuestro análisis con el banco de datos breast cancer. Para ello debemos definir el grpahlearner asociado (prerprocesamiento y modelo). En este caso estandarizamos las predictoras con lo que no es necesario escalarlas en el algoritmo:\n\n# Definimos learner para predecir la probabilidad\nlknn_classif = lrn(\"classif.kknn\", scale = FALSE, predict_type = \"prob\")\n# Preprocesado\npp_cancer = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Graphlearner\ngr = pp_cancer %>>% lknn_classif\ngr = GraphLearner$new(gr)\n\nPara poder entrenar el modelo consideramos la división de muestras (80-20) y estratificamos según la variable diagnosis dado que los niveles no están equilibrados.\n\n# Generamos variable de estrato\ntsk_cancer$col_roles$stratum <- \"diagnosis\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_cancer, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_cancer = tsk_cancer$clone()$filter(splits$train)\ntsk_test_cancer  = tsk_cancer$clone()$filter(splits$test)\n\nPodemos comenzar ahora con el entrenamiento del modelo:\n\n# Entrenamiento\ngr$train(tsk_train_cancer)\n\nPara estudiar la bondad de la clasificación debemos obtener en primer lugar la predicción tanto para la muestra de entrenamiento como la de validación. A continuación estableces los scores de clasificación y obtenemos dichos valores.\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_cancer)\npred_test = gr$predict(tsk_test_cancer)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\", \"classif.bbrier\", \"classif.auc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.98245614     0.97647059     0.01129958     0.99987659 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.99115044     0.99295775     0.01739496     0.99849095 \n\n\nEl algoritmo proporciona porcentajes de clasificación correcta muy elevados para ambas muestras. Además tanto el score de Brier (valor bajo) como el valor de AUC (próximo a 1) indican que el modelo parece adecuado. De hecho estos resultados son mejores que los otros modelos de clasificación que hemos visto en temas anteriores. Veamos que ocurre con la matriz de confusión:\n\n# Cargamos la librería para representar la matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nTan sólo se observa un error (0.9% respecto del total de muestras) en la clasificación de las muestras de validación, indicando que el modelo propuesto parece funcionar bien. De todas formas como el número de predictoras es muy grande, más adelante veremos que ocurre cuando no las consideramos a todas ellas. Recordemos que considerar predictoras poco relevantes puede llevar a conclusiones erróneas con nuestros modelos kNN.\nEl estudio de validación lo posponemos hasta encontrar el mejor modelo basado en kNN que veremos en el punto siguiente.\n\n\n10.9.1.2 Datos Iris\nVemos ahora el análisis para el banco de datos iris. El modelo de aprendizaje es el miso que en el punto anterior y solo resulta necesario modificar el graphlearner.\n\n# Preprocesado\npp_iris = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Graphlearner\ngr = pp_iris %>>% lknn_classif\ngr = GraphLearner$new(gr)\n\nDefinimos las muestras de entrenamiento y validación.\n\n# Generamos variable de estrato\ntsk_iris$col_roles$stratum <- \"species\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_iris, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_iris = tsk_iris$clone()$filter(splits$train)\ntsk_test_iris  = tsk_iris$clone()$filter(splits$test)\n\nEntrenamos el modelo y evaluamos la calidad de la clasificación:\n\n# Entrenamiento\ngr$train(tsk_train_iris)\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_iris)\npred_test = gr$predict(tsk_test_iris)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.mbrier\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc classif.mbrier \n    0.98333333     0.02790482 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc classif.mbrier \n     0.9333333      0.1473825 \n\n# Matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nLos porcentaje de clasificación correcta vuelven a ser muy elevados en ambos conjuntos, mientras que el score de brier para la muestra de validación es bastante más grande que el de la muestra de test. Los resultados son similares a los de los moldeos de clasificación anteriores. En cuanto a la matriz de confusión podemos ver que tenemos un 6.6% de error concentrado en las especies iris-virginica e iris-versicolor. Esto puede indicar que al algoritmo le cuesta más diferenciar estas especies en términos de las predictoras consideradas.\n\n\n10.9.1.3 Datos Electricity\nRecordemos que estamos en un problema de regresión y debemos cambiar nuestro algoritmo de aprendizaje para resolver esta tarea:\n\n# Algoritmo de aprendizaje\nlknn_regr = lrn(\"regr.kknn\", scale = FALSE)\n# Preprocesado\npp_electricity = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Graphlearner\ngr = pp_electricity %>>% lknn_regr\ngr = GraphLearner$new(gr)\n\nRealizamos la división de muestras:\n\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_electricity, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_electricity = tsk_electricity$clone()$filter(splits$train)\ntsk_test_electricity  = tsk_electricity$clone()$filter(splits$test)\n\nAhora procedemos con el entrenamiento del modelo y valoramos su capacidad explicativa a partir de las predicciones de la muestra de entrenamiento y validación.\n\n# Entrenamiento del modelo\ngr$train(tsk_train_electricity)\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_electricity)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_electricity)\n# Scores de validación\nmeasures = msrs(c(\"regr.rsq\", \"regr.mse\", \"regr.smape\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\n   regr.rsq    regr.mse  regr.smape \n0.978454115 6.280417213 0.003899406 \n\npred_test$score(measures)\n\n    regr.rsq     regr.mse   regr.smape \n 0.951522672 14.072844253  0.005988903 \n\n\nEl \\(R^2\\) para ambas muestras es bastante elevado lo que parece indicar una buena capacidad explicativa de las predictoras sobre la respuesta. Por otro lado, el MSE de validación es casi el doble que el de entrenamiento debido en gran parte a la gran cantidad de muestras con las que estamos trabajando. El SMAPE está muy próximo a 0 indicando un buen ajuste. De hecho tenemos porcentajes de error del 0.3% y 0.5% respectivamente para ambos conjuntos de muestras.\nPodemos ver la solución gráfica mediante el código siguiente:\n\n# Muestra de entrenamiento\np1 = autoplot(pred_train, type = \"xy\") + labs(title = \"Observados vs predichos\")\np2 = autoplot(pred_train, type = \"residual\") + labs(title = \"Observados vs residuos\")\np3 = autoplot(pred_train, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\n# Muestra de validación\np4 = autoplot(pred_test, type = \"xy\") + labs(title = \"Observados vs predichos\")\np5 = autoplot(pred_test, type = \"residual\") + labs(title = \"Observados vs residuos\")\np6 = autoplot(pred_test, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\nggarrange(p1,p2,p3, p4, p5, p6, nrow = 2, ncol = 3)\n\n\n\n\nFigura 10.1: Gráficos del modelo para muestras de entrenamiento y validación. Task Electricity.\n\n\n\n\nLa solución obtenida se parece bastante a la que pudimos ver con los modelos de regresión lineal.\n\n\n10.9.1.4 Datos Diabetes\nEste es el primer conjunto de datos donde en el conjunto de predictoras tenemos variables numéricas y factores. En este caso el algoritmo se adapta a esta situación para evaluar las distancias y seleccionar los vecinos. En primer lugar definimos el algoritmo de aprendizaje y seleccionamos las muestras de entrenamiento y validación. Vamos a establecer un estrato por sexo para repartir las observaciones en ambas muestras.\n\n# Algoritmo de aprendizaje\nlknn_regr = lrn(\"regr.kknn\", scale = FALSE)\n# Preprocesado\npp_diabetes = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) \n# Graphlearner\ngr = pp_diabetes %>>% lknn_regr\ngr = GraphLearner$new(gr)\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\n# Generamos variable de estrato\ntsk_diabetes$col_roles$stratum <- \"SEX\"\nsplits = mlr3::partition(tsk_diabetes, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_diabetes = tsk_diabetes$clone()$filter(splits$train)\ntsk_test_diabetes  = tsk_diabetes$clone()$filter(splits$test)\n\nAhora procedemos con el entrenamiento del modelo y valoramos su capacidad explicativa a partir de las predicciones de la muestra de entrenamiento y validación.\n\n# Entrenamiento del modelo\ngr$train(tsk_train_diabetes)\n# Predicción de la muestra de entrenamiento\npred_train = gr$predict(tsk_train_diabetes)\n# Predicción de la muestra de validación\npred_test = gr$predict(tsk_test_diabetes)\n# Scores de validación\nmeasures = msrs(c(\"regr.rsq\", \"regr.mse\", \"regr.smape\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\n    regr.rsq     regr.mse   regr.smape \n   0.7616309 1353.1423579    0.2176852 \n\npred_test$score(measures)\n\n    regr.rsq     regr.mse   regr.smape \n   0.3670029 4378.6019286    0.3555997 \n\n\nComo se aprecia en los resultados en modelo propuesto no parece funcionar muy bien. Tengamos en cuenta que en este caso no hemos considerado posibles interacciones entre predictores numéricas y el factor, como si hicimos en los modelos lineales. En este caso estamos explorando únicamente el funcionamiento de este algoritmo.\nPodemos ver la solución gráfica:\n\n# Muestra de entrenamiento\np1 = autoplot(pred_train, type = \"xy\") + labs(title = \"Observados vs predichos\")\np2 = autoplot(pred_train, type = \"residual\") + labs(title = \"Observados vs residuos\")\np3 = autoplot(pred_train, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\n# Muestra de validación\np4 = autoplot(pred_test, type = \"xy\") + labs(title = \"Observados vs predichos\")\np5 = autoplot(pred_test, type = \"residual\") + labs(title = \"Observados vs residuos\")\np6 = autoplot(pred_test, type = \"histogram\") + labs(title = \"Histograma residuos\")\n\nggarrange(p1,p2,p3, p4, p5, p6, nrow = 2, ncol = 3)\n\n\n\n\nFigura 10.2: Gráficos del modelo para muestras de entrenamiento y validación. Task Electricity.\n\n\n\n\nSe aprecia claramente el mal comportamiento de los residuos del modelo para la muestra de validación. Es evidente que este modelo necesita mejoras.\n\n\n\n10.9.2 Seleccionando predictoras\nPara seleccionar las variables de interés vamos a considerar un selector de predictoras como vimos en temas anteriores.\n\n10.9.2.1 Datos Breast Cancer\nDefinimos el selector para este banco de datos reproduciendo el que ya vimos en el tema de regresión logística.\n\n# Graphlearner\ngr = pp_cancer %>>% lknn_classif\ngr = GraphLearner$new(gr)\n\nset.seed(321)\n# Algoritmo de selección\ninstance = fselect(\n  fselector = fs(\"sequential\", strategy = \"sfs\"),\n  task = tsk_cancer,\n  learner = gr,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measure = msr(\"classif.bacc\"),\n  term_evals = 10\n)\n\nVeamos el resultado de este proceso\n\n# Variables seleccionadas\ninstance$result_feature_set\n\n[1] \"concave_points_mean\"\n\n# Score\ninstance$result_y\n\nclassif.bacc \n   0.9502012 \n\n\nNos quedamos únicamente con la variable concave_points_mean con un porcentaje de clasificación correcta corregido bastante alto. Hemos reducido el número de variables considerablemente pero no hemos perdido un porcentaje de clasificación significativo.\n\n\n10.9.2.2 Datos Iris\nDefinimos el selector para este banco de datos reproduciendo el que ya vimos en el tema de regresión logística.\n\n# Graphlearner\ngr = pp_iris %>>% lknn_classif\ngr = GraphLearner$new(gr)\n\nset.seed(321)\n# Algoritmo de selección\ninstance = fselect(\n  fselector = fs(\"sequential\", strategy = \"sfs\"),\n  task = tsk_iris,\n  learner = gr,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measure = msr(\"classif.bacc\"),\n  term_evals = 10\n)\n\nVeamos el resultado\n\n# Variables seleccionadas\ninstance$result_feature_set\n\n[1] \"petal_length\" \"petal_width\" \n\n# Score\ninstance$result_y\n\nclassif.bacc \n   0.9333333 \n\n\nEn este caso se selecciona petal_length y petal_width con un porcentaje de clasificación correcta del 93.3%. Hemos reducido a la mitad las predictoras pero la pérdida de clasificación es inferior al 10%.\n\n\n10.9.2.3 Datos Electricity\nDefinimos el selector para este banco de datos reproduciendo pero cambiamos el score utilizado para valorar el modelo. En este caso utilizamos el SMAPE.\n\n# Graphlearner\ngr = pp_electricity %>>% lknn_regr\ngr = GraphLearner$new(gr)\n\nset.seed(321)\n# Algoritmo de selección\ninstance = fselect(\n  fselector = fs(\"sequential\", strategy = \"sfs\"),\n  task = tsk_electricity,\n  learner = gr,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measure = msr(\"regr.smape\"),\n  term_evals = 10\n)\n\ncuyo resultado es:\n\n# Variables seleccionadas\ninstance$result_feature_set\n\n[1] \"AP\" \"AT\" \"RH\" \"V\" \n\n# Score\ninstance$result_y\n\n regr.smape \n0.005919884 \n\n\nEn este caso no se prescinde de ninguna predictora.\n\n\n10.9.2.4 Datos Diabetes\nDefinimos el selector de forma similar al banco de datos anterior.\n\n# Graphlearner\ngr = pp_diabetes %>>% lknn_regr\ngr = GraphLearner$new(gr)\n\nset.seed(321)\n# Algoritmo de selección\ninstance = fselect(\n  fselector = fs(\"sequential\", strategy = \"sfs\"),\n  task = tsk_diabetes,\n  learner = gr,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  measure = msr(\"regr.smape\"),\n  term_evals = 10\n)\n\nEl resultado obtenido es:\n\n# Variables seleccionadas\ninstance$result_feature_set\n\n[1] \"S5\"\n\n# Score\ninstance$result_y\n\nregr.smape \n 0.3605362 \n\n\nEn este caso nos quedamos únicamente con la variable S5 pero el SMAPE sigue siendo bastante malo.\n\n\n\n10.9.3 Seleccionando k\nEn este punto utilizamos los modelos con todas las predictoras para realizar la selección óptima del número de vecinos en cada uno de los ejemplos. Para este proceso utilizamos un proceso de tuning similar al que presentamos en el apartado de búsqueda del \\(\\alpha\\) del tema 6. Las librerías necesarias ya las hemos cargado en la configuración inicial del cuaderno.\n\n10.9.3.1 Datos breast Cancer\nConfiguramos el proceso de optimización considerando el intervalo \\([3, 50]\\) como posibles valores de k. A continuación se muestra los diferentes pasos en el proceso de optimización. En primer lugar definimos el learner correspondiente:\n\n# Algoritmo de aprendizaje\nlknn_classif = lrn(\"classif.kknn\", scale = FALSE, predict_type = \"prob\", \n              k = to_tune(3, 50))\n# Proceso de aprendizaje\ngr =  pp_cancer %>>% lknn_classif\ngr = GraphLearner$new(gr)\n\nAhora configuramos el proceso de optimización utilizando la función tune() donde debemos establecer el terminador del proceso, el procedimiento de remuestreo, y el score para la tarea definida. En este caso utilizamos grid_search fijando una resolution = 5 para establecer la resolución del grid de búsqueda.\n\n# Fijamos semilla para reproducibilidad\nset.seed(123)\n# Definimos instancia\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 5),\n  task = tsk_cancer,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measure = msr(\"classif.bacc\")\n)\n\nPodemos ver el resultado del proceso de optimización:\n\ninstance$result\n\n   classif.kknn.k learner_param_vals  x_domain classif.bacc\n1:             26          <list[5]> <list[1]>    0.9573356\n\n\nEl algoritmo de búsqueda selecciona el valor de k = 26 con un porcentaje de clasificación correcta corregido del 95.73%. Podemos utilizar este valor para realizar el proceso de validación del modelo. Consideramos validación cruzada con 10 folds.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Nuevo algoritmo de aprendizaje con k óptimo\nlknn_classif = lrn(\"classif.kknn\", scale = FALSE, predict_type = \"prob\", \n              k = instance$result$classif.kknn.k)\n# Proceso de aprendizaje\ngr =  pp_cancer %>>% lknn_classif\ngr = GraphLearner$new(gr)\nmeasure = msr(\"classif.bacc\")\n# Remuestreo\nrr = resample(tsk_cancer, gr, resamp, store_models=TRUE)\n\nINFO  [17:42:29.738] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 1/10)\nINFO  [17:42:30.027] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 2/10)\nINFO  [17:42:30.187] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 3/10)\nINFO  [17:42:30.354] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 4/10)\nINFO  [17:42:30.546] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 5/10)\nINFO  [17:42:30.812] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 6/10)\nINFO  [17:42:30.967] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 7/10)\nINFO  [17:42:31.130] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 8/10)\nINFO  [17:42:31.390] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 9/10)\nINFO  [17:42:31.643] [mlr3] Applying learner 'scale.classif.kknn' on task 'breastcancer[, -1]' (iter 10/10)\n\n# Análisis de los valores obtenidos con los scores definidos anteriormente\nskim(rr$score(measure)[,\"classif.bacc\"])\n\n\nData summary\n\n\nName\nrr$score(measure)[, “clas…\n\n\nNumber of rows\n10\n\n\nNumber of columns\n1\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nclassif.bacc\n0\n1\n0.96\n0.03\n0.93\n0.94\n0.97\n0.98\n1\n▇▅▁▅▇\n\n\n\n\n\nEl porcentaje de clasificación correcta ponderado se sitúa en el 96.44% con una desviación típica del 2.7%, lo que demuestra una gran estabilidad del clasificador.\n\n\n10.9.3.2 Datos Iris\nVamos a configurar el optimizador de forma similar al ejemplo anterior.\n\n# Algoritmo de aprendizaje\nlknn_classif = lrn(\"classif.kknn\", scale = FALSE, predict_type = \"prob\", \n              k = to_tune(3, 50))\n# Proceso de aprendizaje\ngr =  pp_iris %>>% lknn_classif\ngr = GraphLearner$new(gr)\n\nOptimizamos el modelo\n\n# Fijamos semilla para reproducibilidad\nset.seed(123)\n# Definimos instancia\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 5),\n  task = tsk_iris,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 10),\n  measure = msr(\"classif.bacc\")\n)\n\nPodemos ver el resultado del proceso de optimización:\n\ninstance$result\n\n   classif.kknn.k learner_param_vals  x_domain classif.bacc\n1:             14          <list[5]> <list[1]>    0.9666667\n\n\nLa solución en este caso selecciona k = 14 vecinos con un porcentaje de clasificación correcta del 96.67%.\n\n\n10.9.3.3 Datos Electricity\nEl procedimiento en este caso es similar al de los dos anteriores salvo por el hecho que debemos cambiar la métrica de evaluación al tratarse de una tarea de regresión. Veamos el código necesario:\n\n# Algoritmo de aprendizaje\nlknn_regr = lrn(\"regr.kknn\", scale = FALSE,  \n              k = to_tune(3, 50))\n# Proceso de aprendizaje\ngr =  pp_electricity %>>% lknn_regr\ngr = GraphLearner$new(gr)\n\nOptimizamos el modelo aumentando la resolución debido a la gran cantidad de datos.\n\n# Fijamos semilla para reproducibilidad\nset.seed(123)\n# Definimos instancia\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 10),\n  task = tsk_electricity,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 10),\n  measure = msr(\"regr.smape\")\n)\n\nVeamos la solución óptima\n\ninstance$result\n\n   regr.kknn.k learner_param_vals  x_domain  regr.smape\n1:           8          <list[5]> <list[1]> 0.005697392\n\n\nLa solución se corresponde con un valor de k = 8 que proporciona un SMAPE del 0.0057, es decir un porcentaje de error del 0.57%.\n\n\n10.9.3.4 Datos Diabetes\nEl procedimiento en este caso es similar al del apartado anterior:\n\n# Algoritmo de aprendizaje\nlknn_regr = lrn(\"regr.kknn\", scale = FALSE,  \n              k = to_tune(3, 50))\n# Proceso de aprendizaje\ngr =  pp_diabetes %>>% lknn_regr\ngr = GraphLearner$new(gr)\n\nOptimizamos el modelo aumentando la resolución debido a la gran cantidad de datos.\n\n# Fijamos semilla para reproducibilidad\nset.seed(123)\n# Definimos instancia\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 10),\n  task = tsk_diabetes,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 10),\n  measure = msr(\"regr.smape\")\n)\n\nVeamos la solución óptima\n\ninstance$result\n\n   regr.kknn.k learner_param_vals  x_domain regr.smape\n1:          13          <list[5]> <list[1]>  0.3255004\n\n\nEn este caso el número de vecinos seleccionado es k = 13 con un SMAPE del 32.5%.\n\n\n\n10.9.4 Conclusiones finales sobre kNN\nEs evidente que para finalizar con el proceso de encontrar la solución óptima a cada tarea es necesario realizar una selección de variables a partir de la solución obtenida en este punto, o hacer una combinación de ambas tarea para encontrar en el mejor modelo. En este caso el impacto de este proceso será pequeño porque vemos pocas diferencias, pero en otras situaciones este proceso puede tener un gran impacto en la solución final. Lo que no está claro es cual es el orden para proceder: i) selección de predictoras y búsqueda de k o ii) búsqueda de k y selección de predictoras, por lo que en muchas ocasiones hay que hacer una combinación de ambos para encontrar la mejor solución.\nUna forma de evitar este proceso es considerar una solución inicial basada en la búsqueda del k óptimo, y compararla con la solución de otros algoritmos mediante un proceso de benchmarking para evitar tener que realizar un proceso refinado de búsqueda. En el punto siguiente utilizamos los algoritmos de clasificación estudiados para responder a la tarea de breast cancer."
  },
  {
    "objectID": "100_kNNmodels.html#sec-100.10",
    "href": "100_kNNmodels.html#sec-100.10",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.10 Combinación de soluciones",
    "text": "10.10 Combinación de soluciones\nEn este punto vemos obtener diferentes soluciones de forma única. En concreto utilizamos las soluciones de regresión logística, naïve Bayes y kNN para encontrar una solución única para la tarea de clasificación de breast cancer. Más adelante estudiaremos los detalles teóricos para la combinación de las soluciones de los algoritmos para obtener una única solución mediante la combinación de los resultados individuales.\nPara utilizar la función benchmark() primero llamamos a benchmark_grid(), que construye un diseño exhaustivo para describir todas las combinaciones de algoritmos, tareas y remuestreos que se utilizarán en un experimento de referencia, y crea instancias de las estrategias de remuestreo. En primer lugar definimos los tres algoritmos de aprendizaje que utilizaremos. En el de regresión logística usamos un modelo penalizado ridge\n\n# Learner regresión logística\nlrn1 = lrn(\"classif.glmnet\", type.logistic = \"Newton\", standardize = FALSE,\n              alpha = 0)\ngr1 = as_learner(pp_cancer %>>% lrn1)\n# Learner naïve Bayes\nlrn2 = lrn(\"classif.naive_bayes\", predict_type = \"prob\")\ngr2 = as_learner(pp_cancer %>>% lrn2)\n# Learner kNN\nlrn3 = lrn(\"classif.kknn\", scale = FALSE, k = 26, predict_type = \"prob\")\ngr3 = as_learner(pp_cancer %>>% lrn3)\n\nA continuación definimos el objeto de remuestreo:\n\n# Remuestreo\nremuestreo = rsmp(\"cv\", folds = 10)\n\nYa estamos en condiciones de definir el grid de modelos y podemos proceder con la combinación de soluciones:\n\n# Grid\ndesign = benchmark_grid(tsk_cancer, list(gr1, gr2, gr3), remuestreo)\n# Combinación de soluciones\nbmr = benchmark(design)\n\nPodemos estudiar ahora las soluciones individuales de cada modelo-remuestreo, y la solución agregada para cada modelo. Para el análisis individual optamos por una solución gráfica que nos muestra la distribución de los valores de remuestreo para cada uno de los modelos de aprendizaje.\n\n# Solución gráfica\nautoplot(bmr, measure = msr(\"classif.bacc\"))\n\n\n\n\nDiagrama de cajas modelo aprendizaje - remuestro del bacc\n\n\n\n\nPodemos ver que el modelo basado regresión logística penalizada es el que proporciona mejores valores del porcentaje de clasificación correcta ponderado, similares a los del algoritmo kNN, pero superiores a los de naïve Bayes. Por último vemos el valor agregado para cada algoritmo:\n\n# Solución agregada\nbmr$aggregate(msr(\"classif.bacc\"))\n\n   nr            task_id                learner_id resampling_id iters\n1:  1 breastcancer[, -1]      scale.classif.glmnet            cv    10\n2:  2 breastcancer[, -1] scale.classif.naive_bayes            cv    10\n3:  3 breastcancer[, -1]        scale.classif.kknn            cv    10\n   classif.bacc\n1:    0.9632900\n2:    0.9192605\n3:    0.9642821\nHidden columns: resample_result\n\n\nEl resultado obtenido es el que se vislumbraba en el gráfico anterior, mostrando que el mejor modelo tiene una capacidad de clasificación correcta del 96.4%, mientras que el peor se queda en el 92.4%."
  },
  {
    "objectID": "100_kNNmodels.html#ejercicios",
    "href": "100_kNNmodels.html#ejercicios",
    "title": "10  Modelo de los k vecinos más cercanos (kNN)",
    "section": "10.11 Ejercicios",
    "text": "10.11 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en un modelo kNN para el banco de datos Mushroom4.3.4.\nAjustar un modelo de aprendizaje automático basado en un modelo kNN para el banco de datos Water potability4.3.7.\nAjustar un modelo de aprendizaje automático basado en un modelo kNN para el banco de datos Hepatitis4.3.9.\nAjustar un modelo de aprendizaje automático basado en un modelo kNN para el banco de datos Abalone4.3.1.\nAjustar un modelo de aprendizaje automático basado en un modelo kNN para el banco de datos Penguins4.2.6.\nAjustar un modelo de aprendizaje automático basado en un modelo kNN para el banco de datos Us economic time series4.2.7.\nAjustar un modelo de aprendizaje automático basado en un modelo kNN para el banco de datos QSAR4.2.8."
  },
  {
    "objectID": "110_SVMmodels.html#sec-110.1",
    "href": "110_SVMmodels.html#sec-110.1",
    "title": "11  Máquinas de Vector Soporte (SVM)",
    "section": "11.1 Clasificadores de vector soporte",
    "text": "11.1 Clasificadores de vector soporte\nEn un espacio p-dimensional, un hiperplano se define como un subespacio plano y afín de dimensiones \\(𝑝−1\\) . El término afín significa que el subespacio no tiene por qué pasar por el origen. En un espacio de dos dimensiones, el hiperplano es un subespacio de una dimensión, es decir, una recta. En un espacio tridimensional, un hiperplano es un subespacio de dos dimensiones, un plano convencional. Para dimensiones \\(𝑝>3\\) no es intuitivo visualizar un hiperplano, pero el concepto de subespacio con \\(𝑝−1\\) dimensiones se mantiene.\nPara mostrar el uso de los hiperplanos tomamos un ejemplo muy sencillo de un problema de clasificación con dos clases en dos dimensiones cuyos puntos vienen dados por:\n\n\n\n\n\n\n11.1.1 Casos separables linealmente\nSi la distribución de las observaciones es tal que se pueden separar linealmente de forma perfecta en las dos clases, entonces, la definición matemática de un hiperplano es bastante simple. En el caso de dos dimensiones, el hiperplano se describe acorde a la ecuación de una recta:\n\\[\\beta_0+\\beta_1𝑥_1+\\beta_2𝑥_2=0\\]\nDados los parámetros \\(\\beta_0\\) , \\(\\beta_1\\) y \\(\\beta_2\\), todos los pares de valores \\(𝐱=(𝑥_1,𝑥_2)\\) para los que se cumple la igualdad son puntos del hiperplano. Esta ecuación puede generalizarse para p-dimensiones:\n\\[\\beta_0+\\beta_1𝑥_1+\\beta_2𝑥_2 +...+\\beta_px_p=0\\]\ny de igual manera, todos los puntos definidos por el vector \\((𝐱=𝑥_1,𝑥_2,...,𝑥_𝑝)\\) que cumplen la ecuación pertenecen al hiperplano.\nCuando \\(𝐱\\) no satisface la ecuación:\n\\[\\beta_0+\\beta_1𝑥_1+\\beta_2𝑥_2 +...+\\beta_px_p < 0\\]\no bien\n\\[\\beta_0+\\beta_1𝑥_1+\\beta_2𝑥_2 +...+\\beta_px_p > 0\\]\nel punto \\(𝐱\\) cae a un lado o al otro del hiperplano. Así pues, se puede entender que un hiperplano divide un espacio p-dimensional en dos mitades. Para saber en qué lado del hiperplano se encuentra un determinado punto \\(𝐱\\), solo hay que calcular el signo de la ecuación.\nLa definición de hiperplano para casos perfectamente separables linealmente resulta en un número infinito de posibles hiperplanos, lo que hace necesario un método que permita seleccionar uno de ellos como clasificador óptimo. En este problema podemos considerar diferentes hiperplanos (en este caso rectas) que nos permiten clasificar la muestra de datos de forma adecuada como podemos ver en el gráfico siguiente:\n\n\n\n\n\nTenemos tres rectas y posibles soluciones al problema planteado. La solución a este problema consiste en seleccionar como clasificador óptimo el hiperplano que se encuentra más alejado de todas las observaciones de entrenamiento. A este se le conoce como maximal margin hyperplane o hiperplano óptimo de separación. Para identificarlo, se tiene que calcular la distancia perpendicular de cada observación a un determinado hiperplano. La menor de estas distancias (conocida como margen) determina cuán alejado está el hiperplano de las observaciones de entrenamiento. Así pues, el maximal margin hyperplane se define como el hiperplano que consigue un mayor margen, es decir, que la distancia mínima entre el hiperplano y las observaciones es lo más grande posible. Aunque esta idea suena razonable, no es posible aplicarla, ya que habría infinitos hiperplanos contra los que medir las distancias. En la imagen siguiente se muestra la solución gráfica del algoritmo SVM para este problema:\n\n\n\n\n\nLa imagen anterior muestra el maximal margin hyperplane, formado por el hiperplano (línea negra continua y su margen, las dos líneas discontinuas). Las tres observaciones equidistantes respecto al maximal margin hyperplane que se encuentran a lo largo de las líneas discontinuas se les conoce como vectores de soporte, ya que son vectores en un espacio p-dimensional y soportan (definen) el maximal margin hyperplane. Cualquier modificación en estas observaciones (vectores soporte) conlleva cambios en el maximal margin hyperplane. Sin embargo, modificaciones en observaciones que no son vector soporte no tienen impacto alguno en el hiperplano.\n\n\n11.1.2 Casos no separables linealmente\nEl maximal margin hyperplane descrito en el apartado anterior es una forma muy simple y natural de clasificación siempre y cuando exista un hiperplano de separación. En la gran mayoría de casos reales, los datos no se pueden separar linealmente de forma perfecta, por lo que no existe un hiperplano de separación y no puede obtenerse un maximal margin hyperplane. Para el siguiente ejemplo se emplea un set de datos publicado en el libro Elements of Statistical Learning que contiene observaciones simuladas con una función no lineal en un espacio de dos dimensiones (2 predictores). El objetivo es entrenar un modelo SVM capaz de clasificar las observaciones.\n Para solucionar estas situaciones, se puede extender el concepto de maximal margin hyperplane para obtener un hiperplano que “casi” separe las clases, pero permitiendo que se cometan unos pocos errores. A este tipo de hiperplano se le conoce como Support Vector Classifier o Soft Margin.\n\n\n11.1.3 Soft margin SVM\nEl Maximal Margin Classifier descrito en la sección anterior tiene poca aplicación práctica, ya que rara vez se encuentran casos en los que las clases sean perfecta y linealmente separables. De hecho, incluso cumpliéndose estas condiciones ideales, en las que exista un hiperplano capaz de separar perfectamente las observaciones en dos clases, esta aproximación sigue presentando dos inconvenientes:\n\nDado que el hiperplano tiene que separar perfectamente las observaciones, es muy sensible a variaciones en los datos. Incluir una nueva observación puede suponer cambios muy grandes en el hiperplano de separación (poca robustez).\nQue el maximal margin hyperplane se ajuste perfectamente a las observaciones de entrenamiento para separarlas todas correctamente suele conllevar problemas de overfitting.\n\nPor estas razones, es preferible crear un clasificador basado en un hiperplano que, aunque no separe perfectamente las dos clases, sea más robusto y tenga mayor capacidad predictiva al aplicarlo a nuevas observaciones (menos problemas de overfitting). Esto es exactamente lo que consiguen los clasificadores de vector soporte, también conocidos como soft margin classifiers o Support Vector Classifiers. Para lograrlo, en lugar de buscar el margen de clasificación más ancho posible que consigue que las observaciones estén en el lado correcto del margen; se permite que ciertas observaciones estén en el lado incorrecto del margen o incluso del hiperplano.\nLa identificación del hiperplano que clasifique correctamente la mayoría de las observaciones a excepción de unas pocas, es un problema de optimización convexa. Si bien la demostración matemática queda fuera del objetivo de esta introducción, es importante mencionar que el proceso incluye un hiperparámetro llamado 𝐶. 𝐶 controla el número y severidad de las violaciones del margen (y del hiperplano) que se toleran en el proceso de ajuste. Si 𝐶=∞ , no se permite ninguna violación del margen y por lo tanto, el resultado es equivalente al Maximal Margin Classifier (teniendo en cuenta que esta solución solo es posible si las clases son perfectamente separables). Cuando más se aproxima 𝐶 a cero, menos se penalizan los errores y más observaciones pueden estar en el lado incorrecto del margen o incluso del hiperplano. 𝐶 es, a fin de cuentas, el hiperparámetro encargado de controlar el balance entre sesgo y varianza del modelo. En la práctica, su valor óptimo se identifica mediante validación cruzada.\nEl proceso de optimización tiene la peculiaridad de que solo las observaciones que se encuentran justo en el margen o que lo violan influyen sobre el hiperplano. A estas observaciones se les conoce como vectores soporte y son las que definen el clasificador obtenido. Esta es la razón por la que el parámetro 𝐶 controla el balance entre sesgo y varianza. Cuando el valor de 𝐶 es pequeño, el margen es más ancho, y más observaciones violan el margen, convirtiéndose en vectores soporte. El hiperplano está, por lo tanto, sustentado por más observaciones, lo que aumenta el sesgo pero reduce la varianza. Cuando mayor es el valor de 𝐶, menor el margen, menos observaciones son vectores soporte y el clasificador resultante tiene menor sesgo pero mayor varianza.\nOtra propiedad importante que deriva de que el hiperplano dependa únicamente de una pequeña proporción de observaciones (vectores soporte), es su robustez frente a observaciones muy alejadas del hiperplano.\n\n\n11.1.4 Límites de separación no lineales\nEl Support Vector Classifier descrito en el apartado anterior consigue buenos resultados cuando el límite de separación entre clases es aproximadamente lineal. Si no lo es, su capacidad decae drásticamente. Una estrategia para enfrentarse a escenarios en los que la separación de los grupos es de tipo no lineal consiste en expandir las dimensiones del espacio original.\nEl hecho de que los grupos no sean linealmente separables en el espacio original no significa que no lo sean en un espacio de mayores dimensiones. Las imágenes siguientes muestran dos grupos cuya separación en dos dimensiones no es lineal, pero sí lo es al añadir una tercera dimensión.\n\n\n\n\n\nEl método de Máquinas Vector Soporte (SVM) se puede considerar como una extensión del Support Vector Classifier obtenida al aumentar la dimensión de los datos. Los límites de separación lineales generados en el espacio aumentado se convierten en límites de separación no lineales al proyectarlos en el espacio original. Este algoritmo se estudiará con detalle en el cuaderno siguiente, ya que por el momento nos centramos en la aplicación del Support Vector Classifier en problemas de clasificación y regresión."
  },
  {
    "objectID": "110_SVMmodels.html#sec-110.2",
    "href": "110_SVMmodels.html#sec-110.2",
    "title": "11  Máquinas de Vector Soporte (SVM)",
    "section": "11.2 Máquinas de vector soporte",
    "text": "11.2 Máquinas de vector soporte\nComo hemos visto en la última imagen es necesario expandir las dimensiones del problema en cuestión para poder obtener clasificadores basados en hiperplanos lineales. A continuación, estudiamos los aspectos teóricos correspondientes a las máquinas de vector de soporte en este tipo de situaciones.\nLa pregunta que nos queda por responder es ¿cómo aumentamos la dimensión del espacio y cual es la dimensión correcta que debemos utilizar? La dimensión de un conjunto de datos puede transformarse combinando o modificando cualquiera de sus dimensiones. Por ejemplo, se puede transformar un espacio de dos dimensiones en uno de tres aplicando la siguiente función:\n\\[𝑓(𝑥_1,𝑥_2)=(𝑥_1^2,2\\sqrt{x_1 x_2},𝑥_2^2)\\]\nEsta es solo una de las infinitas transformaciones posibles, ¿cómo saber cuál es la adecuada? Es aquí donde el concepto de kernel entra en juego. Un kernel (K) es una función que devuelve el resultado del producto escalar entre dos vectores realizado en un nuevo espacio dimensional distinto al espacio original en el que se encuentran los vectores. Aunque no se ha entrado en detalle en las fórmulas matemáticas empleadas para resolver el problema de optimización, esta contiene un producto escalar. Si se sustituye este producto escalar por un kernel, se obtienen directamente los vectores soporte (y el hiperplano) en la dimensión correspondiente al kernel. A esto se le suele conocer como kernel trick porque, con solo una ligera modificación del problema original, se puede obtener el resultado para cualquier dimensión. A continuación se muestran los más utilizados. De ahora en adelante:\n\\[<x_i, x_j> = x_i^t x_j\\]\nrepresenta el producto escalar entre \\(x_i\\) y \\(x_j\\).\n\n11.2.1 Kernel lineal\nEl kernel viene definido por la expresión:\n\\[k(x_i, x_j) = x^t_i x_j,\\]\nque es simplemente el producto escalar del vector de características. Si se emplea un Kernel lineal, el clasificador que obtenemos es idéntico al que obteníamos en el cuaderno anterior sin el aumento de dimensiones.\n\n\n11.2.2 Kernel polinómico\nEl kernel viene definido por la expresión:\n\\[k(x_i, x_j) = (\\gamma x^t_i x_j + \\tau)^d.\\]\nCuando se emplea \\(𝑑=1\\) y \\(\\tau = 0\\), el resultado es el mismo que el de un kernel lineal. Si \\(𝑑>1\\) , se generan límites de decisión no lineales, aumentando la no linealidad a medida que aumenta \\(𝑑\\). No suele ser recomendable emplear valores de \\(𝑑\\) mayores a 5 por problemas de sobreajuste. El valor de \\(\\gamma\\) controla el comportamiento del kernel.\n\n\n11.2.3 Kernel Gaussiano (RBF)\nEl kernel viene definido por la expresión:\n\\[k(x_i, x_j) = exp(-\\gamma||x-x^t||^2), \\quad \\gamma >0.\\]\nEl valor de \\(\\gamma\\) controla el comportamiento del kernel, cuando es muy pequeño, el modelo final es equivalente al obtenido con un kernel lineal. A medida que aumenta su valor, también lo hace la flexibilidad del modelo.\n\n\n11.2.4 Kernel sigmoidal\nEl kernel viene definido por la expresión:\n\\[k(x_i, x_j) = tanh(\\gamma x^t_i x_j + \\tau).\\]\nLos kernels descritos son solo unos pocos de los muchos que existen. Cada uno tiene una serie de hiperparámetros cuyo valor óptimo puede encontrarse mediante validación cruzada. No puede decirse que haya un kernel que supere al resto, depende en gran medida de la naturaleza del problema que se esté tratando. Ahora bien, tal como indican los autores de A Practical Guide to Support Vector Classification, es muy recomendable probar el kernel RBF. Este kernel tiene dos ventajas: que solo tiene dos hiperparámetros que optimizar (𝛾y la penalización 𝐶 común a todos los SVM) y que su flexibilidad puede ir desde un clasificador lineal a uno muy complejo.\n\n\n11.2.5 SVM en problemas de regresión\nLas máquinas de vectores soporte (SVM) son bien conocidas en problemas de clasificación. Sin embargo, el uso de SVM en regresión no está tan bien documentado. Este tipo de modelos se conoce como regresión de vectores de soporte (SVR).\nEn la mayoría de los modelos de regresión lineal, el objetivo es minimizar la suma de errores al cuadrado. Tomemos como ejemplo los mínimos cuadrados ordinarios (MCO). La función objetivo para MCO con un predictor (característica) es la siguiente:\n\\[\\underset{\\beta}{min} \\sum_{i=1}^n (y_i-\\beta x_i)^2.\\]\nLasso, Ridge y ElasticNet son extensiones de esta sencilla ecuación, con un parámetro de penalización adicional que pretende minimizar la complejidad y/o reducir el número de características utilizadas en el modelo final. En cualquier caso, el objetivo -como ocurre con muchos modelos- es reducir el error del conjunto de pruebas.\nSin embargo, ¿qué ocurre si sólo nos preocupa reducir el error hasta cierto punto? ¿Y si no nos importa lo grandes que sean nuestros errores, siempre que estén dentro de un rango aceptable?\nSVR nos da la flexibilidad de definir cuánto error es aceptable en nuestro modelo y encontrar una línea adecuada (o hiperplano en dimensiones superiores) para ajustarse a los datos.\nEn contraste con OLS, la función objetivo de SVR se encarga de minimizar los coeficientes - más específicamente, la norma l2 del vector de coeficientes - no el error al cuadrado. El término de error se maneja en las restricciones, donde se establece el error absoluto menor o igual a un margen especificado, llamado el error máximo, \\(\\epsilon\\). Podemos ajustar epsilon para obtener la precisión deseada de nuestro modelo. Nuestra nueva función objetivo y las restricciones son las siguientes:\n\\[\\text{Función: }\\underset{\\beta}{min} \\quad \\frac{1}{2} ||\\mathbf{\\beta}||^2 \\]\n\\[\\text{Restricción: } |y_i-\\beta x_i| \\leq \\epsilon\\]  Este algoritmo no está exento de problemas ya que aunque se resuelve la función objetivo algunos de los puntos siguen quedando fuera de los márgenes establecidos. Como tal, tenemos que tener en cuenta la posibilidad de errores que sean mayores que \\(\\epsilon\\). Esto se hace introduciendo variables de holgura.\nEl concepto de variables de holgura es sencillo: para cualquier valor que quede fuera de \\(\\epsilon\\), podemos denotar su desviación del margen como \\(\\xi\\). Sabemos que estas desviaciones pueden existir, pero aun así nos gustaría minimizarlas en la medida de lo posible. Por lo tanto, podemos añadir estas desviaciones a la función objetivo:\n\\[\\text{Función: }\\underset{\\beta}{min} \\quad \\frac{1}{2} ||\\mathbf{\\beta}||^2 + C \\sum_{i=1}^n |\\xi|\\]\n\\[\\text{Restricción: } |y_i-\\beta x_i| \\leq \\epsilon + |\\xi|\\]  Ahora tenemos un hiperparámetro adicional, C, que podemos ajustar. A medida que C aumenta, nuestra tolerancia para los puntos fuera de ϵ también aumenta. A medida que C se acerca a 0, la tolerancia se aproxima a 0 y la ecuación colapsa en la simplificada (aunque a veces inviable).\nAntes de comenzar con la implementación de las SVM en mlr3 vamos a cargar las librarías necesarias:\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(cvms)\nlibrary(kknn)\ntheme_set(theme_sjplot2())\n\n# Paquetes AA\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3tuningspaces)"
  },
  {
    "objectID": "110_SVMmodels.html#sec-110.3",
    "href": "110_SVMmodels.html#sec-110.3",
    "title": "11  Máquinas de Vector Soporte (SVM)",
    "section": "11.3 Máquinas de vector soporte en mlr3",
    "text": "11.3 Máquinas de vector soporte en mlr3\nPara implementar las máquinas de vector soporte en el paquete mlr3 disponemos de dos funciones:\n\nregr.svm para tareas de regresión.\nclassif.svm para tareas de clasificación\n\nque utilizan como base las funciones definidas en la librería e1071.\nPodemos cargar los algoritmos con el código siguiente:\n\n# Learner tarea de clasificación\nlsvm_classif = lrn(\"classif.svm\")\n# Learner tarea de regresión\nlsvm_regr = lrn(\"regr.svm\")\n\nEn este caso los hiperparámetros de ambos algoritmos no son los mismos aunque coinciden en la mayoría. A continuación se muestran todos ellos:\n\n# Hiperparámetros para SVM clasificación\nlsvm_classif$param_set$ids()\n\n [1] \"cachesize\"       \"class.weights\"   \"coef0\"           \"cost\"           \n [5] \"cross\"           \"decision.values\" \"degree\"          \"epsilon\"        \n [9] \"fitted\"          \"gamma\"           \"kernel\"          \"nu\"             \n[13] \"scale\"           \"shrinking\"       \"tolerance\"       \"type\"           \n\n# Hiperparámetros para SVM regresión\nlsvm_regr$param_set$ids()\n\n [1] \"cachesize\" \"coef0\"     \"cost\"      \"cross\"     \"degree\"    \"epsilon\"  \n [7] \"fitted\"    \"gamma\"     \"kernel\"    \"nu\"        \"scale\"     \"shrinking\"\n[13] \"tolerance\" \"type\"     \n\n\nLos parámetros más relevantes son:\n\nscale: valor lógico que indica si debemos estandarizar las variables.\nkernel: que indica el kernel a utilizar (linear, polynomial, radial o rbf, y sigmoid). Por defecto se usa el radial.\ndegree: parámetro \\(d\\) del kernel polinómico. Por defecto se utiliza el valor 3.\ngamma: parámetro \\(\\gamma\\) de todos los kernel salvo el lineal. Por defecto toma el valor \\(1/muestras\\).\ncoef0: parámetro \\(\\tau\\) de los kernel polinomial y sigmoidal. Por defecto toma el valor 0.\ncost: parámetro \\(C\\) que representa el coste establecido por la violación del contraste. Por defecto toma el valor 1.\ntolerance: tolerancia para la finalización del algoritmo. Valor por defecto igual a 0.001.\nepsilon: epsilon en la función de pérdida. Por defecto toma el valor 0.1"
  },
  {
    "objectID": "110_SVMmodels.html#sec-110.4",
    "href": "110_SVMmodels.html#sec-110.4",
    "title": "11  Máquinas de Vector Soporte (SVM)",
    "section": "11.4 Bancos de datos",
    "text": "11.4 Bancos de datos\nPara ejemplificar el uso de estos algoritmos vamos a introducir los bancos de datos stroke y penguins. En este caso vamos a modificar el objetivo del banco de datos penguins, ya que cambiamos a una tarea de clasificación donde estamos interesados en determinar el sexo del sujeto en función del resto de predictoras. Vamos a cargar los datos y prepararlos para el análisis definiendo las tareas correspondientes y el código de preprocesado. Como estos algoritmos no permiten trabajar directamente con predictoras de tipo factor es necesario hacer una codificación en el preprocesamiento.\n\n11.4.1 Stroke\nSegún la Organización Mundial de la Salud (OMS), el ictus es la segunda causa de muerte en el mundo, responsable de aproximadamente el 11% del total de fallecimientos. El banco de datos Stroke se utiliza para predecir si es probable que un paciente sufra un ictus en función de los parámetros de entrada como el sexo, la edad, diversas enfermedades y estatus de fumador. Cada fila de los datos proporciona información relevante sobre el paciente. El objetivo se encuentra en la variable stroke que puede tomar dos valores posibles. Hay valores perdidos en la variable bmi.\n\n# Leemos datos\nstroke = read_rds(\"stroke.rds\")\n# creamos la tarea\ntsk_stroke = as_task_classif(stroke, target = \"stroke\")\n# información de la tarea\nprint(tsk_stroke)\n\n<TaskClassif:stroke> (5110 x 12)\n* Target: stroke\n* Properties: twoclass\n* Features (11):\n  - fct (7): Residence_type, ever_married, gender, heart_disease,\n    hypertension, smoking_status, work_type\n  - dbl (4): age, avg_glucose_level, bmi, id\n\n\nRepresentamos la información contenida en la tarea\n\nautoplot(tsk_stroke, type =\"duo\")\n\n\n\n\nRepresentación gráfica tarea stroke\n\n\n\n\nGeneramos ahora el código para el preprocesado de los datos. En este caso tenemos imputación, estandarización y codificación.\n\npp_stroke = \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) %>>%\n   po(\"encode\", param_vals = list(method = \"one-hot\"))\n\nPor último creamos la división de muestras:\n\n# Generamos variable de estrato\ntsk_stroke$col_roles$stratum <- \"stroke\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_stroke, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_stroke = tsk_stroke$clone()$filter(splits$train)\ntsk_test_stroke  = tsk_stroke$clone()$filter(splits$test)\n\n\n\n11.4.2 Penguins\nEl banco de datos ya ha sido descrito en detalle en temas anteriores, salvo por el hecho de que cambiamos a una tarea de clasificación. En este caso tenemos valores perdidos en la respuesta y debemos eliminar dichas muestras.\n\n# Leemos datos\npenguins = read_rds(\"penguins.rds\")\n# Seleccionamos observaciones missing\nids = which(is.na(penguins$sex) == TRUE)\ndata_penguins = penguins[-ids,]\n# creamos la tarea\ntsk_penguins = as_task_classif(data_penguins, target = \"sex\")\n# información de la tarea\nprint(tsk_penguins)\n\n<TaskClassif:data_penguins> (333 x 9)\n* Target: sex\n* Properties: twoclass\n* Features (8):\n  - dbl (6): Id, bill_depth_mm, bill_length_mm, body_mass_g,\n    flipper_length_mm, year\n  - fct (2): island, species\n\n\nRepresentamos la información contenida en la tarea\n\nautoplot(tsk_penguins, type =\"duo\")\n\n\n\n\nRepresentación gráfica tarea penguins\n\n\n\n\nGeneramos ahora el código para el preprocesado de los datos. En este caso tenemos estandarización y codificación.\n\npp_penguins = \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"encode\", param_vals = list(method = \"one-hot\"))\n\nPor último creamos la división de muestras:\n\n# Generamos variable de estrato\ntsk_penguins$col_roles$stratum <- \"sex\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_penguins, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_penguins = tsk_penguins$clone()$filter(splits$train)\ntsk_test_penguins  = tsk_penguins$clone()$filter(splits$test)"
  },
  {
    "objectID": "110_SVMmodels.html#sec-110.5",
    "href": "110_SVMmodels.html#sec-110.5",
    "title": "11  Máquinas de Vector Soporte (SVM)",
    "section": "11.5 Nuestros primeros modelos",
    "text": "11.5 Nuestros primeros modelos\nEn primer lugar consideramos modelos básicos de SVM para los dos bancos de datos presentados. Trabajaremos con las opciones por defecto para poder comparar los resultados con el modelo optimizado que veremos posteriormente. También compararemos los resultados con otros modelos de clasificación de los estudiados hasta ahora.\n\n11.5.1 Stroke\nEn primer lugar generamos el graphlearner correspondiente a este banco de datos y entrenamos el algoritmo. Como ya estandarizamos los datos ponemos como false al parámetro scale.\n\n# Definimos learner para predecir la probabilidad\nlsvm_classif = lrn(\"classif.svm\", scale = FALSE, predict_type = \"prob\")\n# Graphlearner\ngr = pp_stroke %>>% lsvm_classif\ngr = GraphLearner$new(gr)\n# Entrenamiento\ngr$train(tsk_train_stroke)\n\nPodemos estudiar el funcionamiento del algoritmo una vez obtenidas las predicciones tanto para la muestra de entrenamiento y validación. En este caso consideramos diferentes métricas para valorar la clasificación obtenida.\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_stroke)\npred_test = gr$predict(tsk_test_stroke)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\", \"classif.bbrier\", \"classif.auc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.95181018     0.50502513     0.04156817     0.92280766 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.95107632     0.50000000     0.04521452     0.66539095 \n\n\nCentrándonos en la muestra de validación, el porcentaje de clasificación correcta parece indicar que el modelo propuesto es muy adecuado, sin embargo el porcentaje de clasificación correcta ponderado nos indica que solo el 50% de los casos (ponderarlos por su peso en el banco de datos) ha sido clasificado correctamente. Esto ocurre en ocasiones cuando una de las categorías de la respuesta está muy desequilibrada con respecto a la otra. De hecho, un efecto similar ocurre con le valor de AUC. Podemos estudiar con más detalle este efecto mediante la matriz de confusión.\n\n# Cargamos la librería para representar la matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nEn la tabla se aprecia claramente el efecto comentado anteriormente. Ninguna de las observaciones originales clasificadas con ictus es clasifica de forma correcta por el modelo (lo que provoca u porcentaje de clasificación correcta ponderado del 50%). De hecho, podemos ver que le porcentaje de clasificación correcta del 95.1% se corresponde únicamente con las observaciones originales que no habían padecido ictus. Este porcentaje es así de alto por la diferencia de muestras en ambos grupos (972 para los que no han sufrido ictus, frente a los 50 que si lo han sufrido). Podemos concluir que el modelo no resulta efectivo ya que no es capaz de clasificar ninguna observación del grupo de los que han sufrido ictus.\nEn los puntos siguientes vernos si podemos modificar el algoritmo para mejorar nuestros resultados. De momento vamos a compara los resultados con otros algoritmos de clasificación como el clasificador Bayes y el algoritmo kNN con sus opciones por defecto. Para ello establecemos los modelos de aprendizaje y evaluamos el porcentaje de clasificación correcta ponderada en ambas situaciones.\n\n# Learner naïve Bayes\nlrn_nb = lrn(\"classif.naive_bayes\", predict_type = \"prob\")\ngr_nb = as_learner(pp_stroke %>>% lrn_nb)\n# Learner kNN\nlrn_knn = lrn(\"classif.kknn\", scale = FALSE, predict_type = \"prob\")\ngr_knn = as_learner(pp_stroke %>>% lrn_knn)\n# Definimos modelo de remuestreo\nremuestreo = rsmp(\"cv\", folds = 10)\n# Grid de comparación de modelos\ndesign = benchmark_grid(tsk_stroke, list(gr, gr_nb, gr_knn), remuestreo)\n# Combinación de soluciones\nbmr = benchmark(design)\n\nINFO  [17:43:59.963] [mlr3] Running benchmark with 30 resampling iterations\nINFO  [17:44:00.049] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 1/10)\nINFO  [17:44:04.430] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 2/10)\nINFO  [17:44:07.807] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 3/10)\nINFO  [17:44:11.167] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 4/10)\nINFO  [17:44:14.495] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 5/10)\nINFO  [17:44:17.437] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 6/10)\nINFO  [17:44:20.242] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 7/10)\nINFO  [17:44:22.961] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 8/10)\nINFO  [17:44:26.258] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 9/10)\nINFO  [17:44:29.278] [mlr3] Applying learner 'scale.imputemedian.encode.classif.svm' on task 'stroke' (iter 10/10)\nINFO  [17:44:32.043] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 1/10)\nINFO  [17:44:32.580] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 2/10)\nINFO  [17:44:33.130] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 3/10)\nINFO  [17:44:33.665] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 4/10)\nINFO  [17:44:34.190] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 5/10)\nINFO  [17:44:34.713] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 6/10)\nINFO  [17:44:35.242] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 7/10)\nINFO  [17:44:35.777] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 8/10)\nINFO  [17:44:36.319] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 9/10)\nINFO  [17:44:36.857] [mlr3] Applying learner 'scale.imputemedian.encode.classif.naive_bayes' on task 'stroke' (iter 10/10)\nINFO  [17:44:37.442] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 1/10)\nINFO  [17:44:37.850] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 2/10)\nINFO  [17:44:38.329] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 3/10)\nINFO  [17:44:38.768] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 4/10)\nINFO  [17:44:39.172] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 5/10)\nINFO  [17:44:39.582] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 6/10)\nINFO  [17:44:40.050] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 7/10)\nINFO  [17:44:40.531] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 8/10)\nINFO  [17:44:40.995] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 9/10)\nINFO  [17:44:41.480] [mlr3] Applying learner 'scale.imputemedian.encode.classif.kknn' on task 'stroke' (iter 10/10)\nINFO  [17:44:41.956] [mlr3] Finished benchmark\n\n# Solución agregada del criterio de validación\nbmr$aggregate(msr(\"classif.bacc\"))\n\n   nr task_id                                    learner_id resampling_id iters\n1:  1  stroke         scale.imputemedian.encode.classif.svm            cv    10\n2:  2  stroke scale.imputemedian.encode.classif.naive_bayes            cv    10\n3:  3  stroke        scale.imputemedian.encode.classif.kknn            cv    10\n   classif.bacc\n1:    0.5016916\n2:    0.6755137\n3:    0.5056038\nHidden columns: resample_result\n\n\nAunque con le clasificador naïve bayes alcanzamos un 66% de clasificación correcta, también es cierto que con el kNN por defecto alcanzamos un valor similar que con SVM. Esto refuerza el hecho de que es necesario mejorar nuestro modelo, si es posible, ya que tenemos otro algoritmo con le que alcanzamos resultados mucho mejores.\n\n\n11.5.2 Penguins\nAnalizamos ahora el banco de datos penguins comenzando por definir el graphlearner asociado.\n\n# Definimos learner para predecir la probabilidad\nlsvm_classif = lrn(\"classif.svm\", scale = FALSE, predict_type = \"prob\")\n# Graphlearner\ngr = pp_penguins %>>% lsvm_classif\ngr = GraphLearner$new(gr)\n# Entrenamiento\ngr$train(tsk_train_penguins)\n\nAnalizamos ahora la validez el modelo:\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr$predict(tsk_train_penguins)\npred_test = gr$predict(tsk_test_penguins)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\", \"classif.bbrier\", \"classif.auc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.92481203     0.92480778     0.05423681     0.97840344 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.92537313     0.92557932     0.04704015     0.98752228 \n\n\nEn este caso tanto el porcentaje de clasificación correcta como el ponderado son muy similares y superiores al 90% indicando que el modelo funciona relativamente bien para clasificar las posibles respuestas en función de las predictoras consideradas. tanto el score de brier como e AUC proporcionan resultados satisfactorios. Veamos por último la matriz de confusión:\n\n# Cargamos la librería para representar la matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nEn este caso tenemos un 7.5% de porcentaje de error de clasificación par la muestra de validación, pero no tenemos el efecto inadecuado del modelo como en el banco de datos stroke.\nComo en el caso anterior vamos a comparar los resultados de este algoritmo con otros utilizados anteriormente.\n\n# Learner naïve Bayes\nlrn_nb = lrn(\"classif.naive_bayes\", predict_type = \"prob\")\ngr_nb = as_learner(pp_penguins %>>% lrn_nb)\n# Learner kNN\nlrn_knn = lrn(\"classif.kknn\", scale = FALSE, predict_type = \"prob\")\ngr_knn = as_learner(pp_penguins %>>% lrn_knn)\n# Definimos modelo de remuestreo\nremuestreo = rsmp(\"cv\", folds = 10)\n# Grid de comparación de modelos\ndesign = benchmark_grid(tsk_penguins, list(gr, gr_nb, gr_knn), remuestreo)\n# Combinación de soluciones\nbmr = benchmark(design)\n\nINFO  [17:44:43.708] [mlr3] Running benchmark with 30 resampling iterations\nINFO  [17:44:43.714] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 1/10)\nINFO  [17:44:43.975] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 2/10)\nINFO  [17:44:44.232] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 3/10)\nINFO  [17:44:44.510] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 4/10)\nINFO  [17:44:44.750] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 5/10)\nINFO  [17:44:44.949] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 6/10)\nINFO  [17:44:45.163] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 7/10)\nINFO  [17:44:45.486] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 8/10)\nINFO  [17:44:45.737] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 9/10)\nINFO  [17:44:45.962] [mlr3] Applying learner 'scale.encode.classif.svm' on task 'data_penguins' (iter 10/10)\nINFO  [17:44:46.297] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 1/10)\nINFO  [17:44:46.591] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 2/10)\nINFO  [17:44:46.797] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 3/10)\nINFO  [17:44:47.053] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 4/10)\nINFO  [17:44:47.263] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 5/10)\nINFO  [17:44:47.971] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 6/10)\nINFO  [17:44:48.236] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 7/10)\nINFO  [17:44:48.580] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 8/10)\nINFO  [17:44:48.909] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 9/10)\nINFO  [17:44:49.254] [mlr3] Applying learner 'scale.encode.classif.naive_bayes' on task 'data_penguins' (iter 10/10)\nINFO  [17:44:49.500] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 1/10)\nINFO  [17:44:49.690] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 2/10)\nINFO  [17:44:49.868] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 3/10)\nINFO  [17:44:50.106] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 4/10)\nINFO  [17:44:50.304] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 5/10)\nINFO  [17:44:50.484] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 6/10)\nINFO  [17:44:50.892] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 7/10)\nINFO  [17:44:51.271] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 8/10)\nINFO  [17:44:51.501] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 9/10)\nINFO  [17:44:51.739] [mlr3] Applying learner 'scale.encode.classif.kknn' on task 'data_penguins' (iter 10/10)\nINFO  [17:44:52.026] [mlr3] Finished benchmark\n\n# Solución agregada del criterio de validación\nbmr$aggregate(msr(\"classif.bacc\"))\n\n   nr       task_id                       learner_id resampling_id iters\n1:  1 data_penguins         scale.encode.classif.svm            cv    10\n2:  2 data_penguins scale.encode.classif.naive_bayes            cv    10\n3:  3 data_penguins        scale.encode.classif.kknn            cv    10\n   classif.bacc\n1:    0.9130515\n2:    0.6965074\n3:    0.8979779\nHidden columns: resample_result\n\n\nEn este caso el resultado obtenido con el algoritmo SVM es superior a los otros dos algoritmos. Con el kNN hay una diferencia de un 2%, pero con respecto a Naïve Bayes hay una mejora del 22% en el porcentaje de clasificación correcta. El algoritmo funciona bien y tan solo nos queda afinarlo un poco para tratar de mejorar si es posible dicho porcentaje."
  },
  {
    "objectID": "110_SVMmodels.html#sec-110.6",
    "href": "110_SVMmodels.html#sec-110.6",
    "title": "11  Máquinas de Vector Soporte (SVM)",
    "section": "11.6 Optimizando los modelos",
    "text": "11.6 Optimizando los modelos\nPara el proceso de optimización de los modelos vamos a considerar únicamente los parámetros cost y gamma, ya que fijaremos el kernel de tipo radial. Para favorecer la convergencia del algoritmo de optimización consideramos la transformación de los parámetros numéricos mediante la función logaritmo, ya que de esta forma se reduce el espacio de búsqueda. Además indicamos type = \"C-classification\" para indicar al proceso de optimización que estamos en un problema de clasificación.\n\n11.6.1 Stroke\nRecuperamos y adaptamos el código que ya estudiamos en el tema 6.\n\n# Algoritmo de aprendizaje definiendo el espacio de búsqueda\nlsvm_classif = lrn(\"classif.svm\", scale = FALSE, predict_type = \"prob\",\n                    kernel = \"radial\",\n                    gamma = to_tune(1e-04, 10000, logscale = TRUE),\n                    cost = to_tune(1e-04, 10000, logscale = TRUE),\n                    type = \"C-classification\")\n# Proceso de aprendizaje\ngr =  pp_stroke %>>% lsvm_classif\ngr = GraphLearner$new(gr)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_stroke,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.bacc\"),\n  term_evals = 15\n)\n\nVeamos los resultados obtenidos:\n\n# Veamos si converge el algoritmo\ninstance$is_terminated\n\n[1] TRUE\n\n# Resultados del proceso de optimización\ninstance$result_learner_param_vals[c(\"classif.svm.gamma\", \"classif.svm.cost\")]\n\n$classif.svm.gamma\n[1] 0.06846151\n\n$classif.svm.cost\n[1] 0.8649166\n\n# Valor de la métrica para resultado óptimo\ninstance$result_y\n\nclassif.bacc \n   0.5018971 \n\n\nEl proceso de optimización ha finalizado encontrando los valores óptimos de gamma y cost, pero sin embargo el porcentaje de clasificación ponderado sigue siendo del 50%. Esto muestra que este algoritmo no resulta válido para resolver este problema de clasificación.\n\n\n11.6.2 Penguins\nProcedemos ahora con el banco de datos penguins. Tomamos las mismas opciones de configuración que en el problema anterior.\n\n# Algoritmo de aprendizaje definiendo el espacio de búsqueda\nlsvm_classif = lrn(\"classif.svm\", scale = FALSE, predict_type = \"prob\",\n                    kernel = \"radial\",\n                    gamma = to_tune(1e-04, 10000, logscale = TRUE),\n                    cost = to_tune(1e-04, 10000, logscale = TRUE),\n                    type = \"C-classification\")\n# Proceso de aprendizaje\ngr =  pp_penguins %>>% lsvm_classif\ngr = GraphLearner$new(gr)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_penguins,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 10),\n  measures = msr(\"classif.bacc\"),\n  term_evals = 20\n)\n\nVeamos los resultados obtenidos:\n\n# Veamos si converge el algoritmo\ninstance$is_terminated\n\n[1] TRUE\n\n# Resultados del proceso de optimización\ninstance$result_learner_param_vals[c(\"classif.svm.gamma\", \"classif.svm.cost\")]\n\n$classif.svm.gamma\n[1] 0.000240813\n\n$classif.svm.cost\n[1] 2721.482\n\n# Valor de la métrica para resultado óptimo\ninstance$result_y\n\nclassif.bacc \n   0.9224265 \n\n\nDe nuevo el porcentaje de clasificación ponderado para la configuración óptima de hiperparámetros es del 92% (similar a la configuración por defecto), lo que demuestra que el algoritmo es bastante robusto frente a cambios en dichos valores y resulta adecuado para este problema."
  },
  {
    "objectID": "110_SVMmodels.html#sec-110.7",
    "href": "110_SVMmodels.html#sec-110.7",
    "title": "11  Máquinas de Vector Soporte (SVM)",
    "section": "11.7 SVM para tareas de regresión",
    "text": "11.7 SVM para tareas de regresión\nAunque el algoritmo SVM se puede utilizar para resolver tareas de regresión no es los habitual. Si se desea utilizar este algoritmo con búsqueda óptima tan solo hay que modificar el parámetro type con el valor eps-regression."
  },
  {
    "objectID": "110_SVMmodels.html#sec-110.8",
    "href": "110_SVMmodels.html#sec-110.8",
    "title": "11  Máquinas de Vector Soporte (SVM)",
    "section": "11.8 Ejercicios",
    "text": "11.8 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en un modelo SVM para el banco de datos Mushroom4.3.4.\nAjustar un modelo de aprendizaje automático basado en un modelo SVM para el banco de datos Water potability4.3.7.\nAjustar un modelo de aprendizaje automático basado en un modelo SVM para el banco de datos Hepatitis4.3.9.\nAjustar un modelo de aprendizaje automático basado en un modelo SVM para el banco de datos Abalone4.3.1."
  },
  {
    "objectID": "120_DTmodels.html#sec-120.1",
    "href": "120_DTmodels.html#sec-120.1",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.1 Árboles de decisión en tareas de clasificación",
    "text": "12.1 Árboles de decisión en tareas de clasificación\nPara construir un árbol de clasificación, se emplea un método de división binaria recursiva en cuyo proceso es necesario tener en cuenta que:\n\nTodo el conjunto de datos se considera como parte del nodo raíz al comienzo del proceso de división.\nDado que se producen divisiones de tipo binario a cada paso del algoritmo se prefieren los valores de características categóricas en el proceso de construcción. Si los valores son continuos, deben discretizarse antes de construir el modelo.\nLas muestras se distribuyen recursivamente en función de las reglas de decisión establecidas con respecto a la predictora considerada en cada paso del algoritmo.\nEl proceso de división se debe realizar mediante un enfoque estadístico estableciendo una función de pérdida o ganancia que es necesario optimizar.\n\nAntes de presentar el algoritmo completo para la creación del árbol de decisión se presenta el enfoque estadístico para la construcción de las reglas de decisión óptimas.\n\n12.1.1 Medidas de impureza\nA la hora de establecer los criterios estadísticos a tener en cuenta en la construcción del árbol de decisión existen varias alternativas, todas ellas con el objetivo de encontrar nodos lo más puros/homogéneos posibles. Antes de presentar las medidas más habituales introducimos el concepto de entropía que está directamente relacionado con la construcción del árbol.\nLa incertidumbre en nuestro conjunto de datos o la medida del desorden se llama entropía. Su valor describe el grado de aleatoriedad de un nodo concreto, de forma que, cuanto mayor es la entropía, mayor será la aleatoriedad en el conjunto de datos, y por tanto menos influencia tiene la predictora considerada en la división del árbol. La fórmula general de la entropía en un conjunto de datos categóricos con k clases viene dada por:\n\\[H = \\sum_{i=1}^{k} -p_ilog(p_i)\\]\ndonde \\(p_i\\) es la proporción de observaciones de la categoría \\(i\\) en el conjunto de datos considerado.\nLos métodos de medidas más empleadas son:\n\nRatio de error de clasificación. Se define como la proporción de observaciones que no pertenecen a la clase mayoritaria del nodo, definida como:\n\n\\[E_m = 1- \\underset{k}{max} \\text{ } \\hat{p}_{mk}\\]\n\ndonde \\(\\hat{p}_{mk}\\) representa la proporción de observaciones del nodo \\(𝑚\\) que pertenecen a la clase \\(𝑘\\). A pesar de la sencillez de esta medida, no es suficientemente sensible para crear buenos árboles, por lo que, en la práctica, no suele emplearse.\n\n\nGanancia de información. La ganancia de información ayuda a determinar el orden en que las predictoras consideradas deben ser utilizadas para dividir un nodo o no. Es simplemente una medida de los cambios en la entropía tras la segmentación de un conjunto de datos basado en una predictora específica. Calcula cuánta información nos proporciona una característica sobre una clase. En función del valor de la ganancia de información, dividimos los nodos y construimos un árbol de decisión. El nodo/atributo con mayor ganancia de información se divide primero en una estructura de árbol, que siempre maximiza el valor de la ganancia de información. La expresión para dicha medida viene dada por:\n\n\\[D = - \\sum_{k=1}^{K} \\hat{p}_{mk} log(\\hat{p}_{mk}).\\]\n\nLos conocidos como algortimos C4.5 y C5.0 utilizan este criterio para la construcción del árbol.\n\n\nÍndice de Gini. El índice de Gini, también conocido como impureza de Gini o coeficiente de Gini, mide la probabilidad de que un nuevo valor de una variable aleatoria se clasifique incorrectamente si se clasificara al azar utilizando la distribución de etiquetas de clase del conjunto de datos. Técnicamente cuantifica la varianza total en el conjunto de las \\(𝐾\\) clases del nodo \\(𝑚\\), es decir, mide la pureza del nodo mediante la expresión:\n\n\\[G_m = \\sum_{k=1}^{K} \\hat{p}_{mk} (1-\\hat{p}_{mk}).\\]\n\nCuando \\(\\hat{p}_{mk}\\) es cercano a 0 o a 1 (el nodo contiene mayoritariamente observaciones de una sola clase), el término correspondiente es muy pequeño. Como consecuencia, cuanto mayor sea la pureza del nodo, menor el valor del índice Gini.\n\n\nEl algoritmo CART (Classification and Regression Tree) utiliza este criterio para la construcción del árbol.\n\n\nJi-cuadrado. Esta aproximación consiste en identificar si existe una diferencia significativa entre los nodos hijos y el nodo parental, es decir, si hay evidencias de que la división consigue una mejora. Para ello, se aplica un test estadístico ji-cuadrado de bondad de ajuste empleando como distribución esperada \\(𝐻_0\\) la frecuencia de cada clase en el nodo parental. Cuanto mayor el estadístico \\(𝜒^2\\) , mayor es la evidencia estadística de que existe una diferencia. Los árboles generados con este criterio de división reciben el nombre de CHAID (Chi-square Automatic Interaction Detector).\n\nIndependientemente de la medida empleada como criterio de selección de las divisiones, el proceso de construcción del árbol siempre es el mismo:\n\nPara cada posible división se calcula el valor de la medida considerada en cada uno de los dos nodos resultantes.\nSe suman los dos valores, ponderando cada uno por la fracción de observaciones que contiene cada nodo. Este paso es muy importante, ya que no es lo mismo dos nodos puros con 2 observaciones, que dos nodos puros con 100 observaciones. Si consideramos como \\(n_A\\) y \\(n_B\\) el número de observaciones en los nodos A y B resultantes de la división con \\(n=n_A+n_B\\), y por \\(p_A\\) y \\(p_B\\) las medidas de pureza calculadas para cada uno de ellos el criterio de división se basa en:\n\n\\[\\frac{n_a}{n}p_A + \\frac{n_b}{n}p_B\\]\n\nLa división con menor o mayor valor (dependiendo de la medida empleada) se selecciona como punto de corte óptimo.\n\n\n\n12.1.2 El algortmo CART\nEl algoritmo CART es uno de los más extendidos en la construcción de árboles de decisión. Este funciona dividiendo primero el conjunto de entrenamiento por características \\(k\\) y umbrales \\(t_k\\). Más concretamente, de entre todos los pares \\((k, t_k)\\) se eligen los que producen los subconjuntos más puros ponderados por su tamaño.\nLa función de pérdida en la que se basa el funcionamiento del algoritmo viene dada por:\n\\[J(k, t_k)=\\frac{n_a}{n}G_A + \\frac{n_B}{n}G_B\\]\ndonde \\(G_A\\) y \\(G_B\\) son respectivamente las medidas de impureza asociadas con cada uno de los nodos resultantes, que en este caso es el índice de Gini.\nUna vez que el algoritmo CART divide con éxito los datos de entrenamiento iniciales en dos subconjuntos, hace lo mismo con ambos subconjuntos. El algoritmo se detiene cuando no puede encontrar una división que reduzca la impureza.\nAl algoritmo CART no le importa si su división actual conduce a una hoja óptima en la parte inferior. Sólo le importa encontrar la mejor división posible en la hoja actual. En este sentido, no necesariamente da lugar a una solución óptima. Por desgracia, se sabe que encontrar el árbol óptimo es un problema NP-Completo con una complejidad de \\(O(exp(n))\\).\n\n\n12.1.3 Tratamiento de sobreajuste\nEl proceso de construcción de árboles tiende a reducir rápidamente el error de entrenamiento, es decir, el modelo se ajusta muy bien a las observaciones empleadas como entrenamiento. Como consecuencia, se genera un sobreajuste que reduce su capacidad predictiva al aplicarlo a nuevos datos. La razón de este comportamiento radica en la facilidad con la que los árboles se ramifican adquiriendo estructuras complejas. De hecho, si no se limitan las divisiones, todo árbol termina ajustándose perfectamente a las observaciones de entrenamiento creando un nodo terminal por observación. Las dos estrategias más habituales para prevenir este problema es limitar el tamaño del árbol (parada temprana) y el proceso de podado (pruning).\nParada temprana\nEl tamaño final que adquiere un árbol puede controlarse mediante reglas de parada que detengan la división de los nodos dependiendo de si se cumplen o no determinadas condiciones. El nombre de estas condiciones puede variar dependiendo del software o librería empleada, pero suelen estar presentes en todos ellos.\n\nObservaciones mínimas para división: define el número mínimo de observaciones que debe tener un nodo para poder ser dividido. Cuanto mayor el valor, menos flexible es el modelo.\nObservaciones mínimas de nodo terminal: define el número mínimo de observaciones que deben tener los nodos terminales. Su efecto es muy similar al de observaciones mínimas para división.\nProfundidad máxima del árbol: define la profundidad máxima del árbol, entendiendo por profundidad máxima el número de divisiones de la rama más larga (en sentido descendente) del árbol. Cuanto menor el valor, menos flexible es el modelo.\nNúmero máximo de nodos terminales: define el número máximo de nodos terminales que puede tener el árbol. Una vez alcanzado el límite, se detienen las divisiones. Su efecto es similar al de controlar la profundidad máxima del árbol.\nReducción mínima de error: define la reducción mínima de error que tiene que conseguir una división para que se lleve a cabo.\n\nTodos estos parámetros son lo que se conoce como hiperparámetros porque no se aprenden durante el entrenamiento del modelo. Su valor tiene que ser especificado por el usuario en base a su conocimiento del problema y mediante el uso de estrategias de validación.\nPodado del árbol\nLa estrategia de controlar el tamaño del árbol mediante reglas de parada tiene un inconveniente, el árbol crece seleccionando la mejor división en cada momento. Al evaluar las divisiones sin tener en cuenta las que vendrán después, nunca se elige la opción que resulta en el mejor árbol final, a no ser que también sea la que genera en ese momento la mejor división. A este tipo de estrategias se les conoce como greedy.\nUna alternativa no greedy que consigue evitar el sobreajuste consiste en generar los árboles más grandes posibles, sin establecer condiciones de parada más allá de las necesarias por las limitaciones computacionales, y después podarlos (pruning), mantener la estructura que consigue un test error bajo. La selección del sub-árbol óptimo puede hacerse mediante validación cruzada, sin embargo, dado que los árboles crecen lo máximo posible (tienen muchos nodos terminales) no suele ser viable estimar el test error de todas las posibles sub-estructuras que se pueden generar. En su lugar, se recurre a la “poda de complejidad de costes” o “poda del eslabón más débil”.\nLa poda de complejidad por costes es un método de penalización de tipo “coste” mas “penalización”, similar al empleado en Ridge Regression o Lasso. En este caso, se busca el sub-árbol \\(𝑇\\) que minimiza la ecuación:\n\\[\\text{coste} + \\alpha|T|\\]\ndonde \\(|T|\\) es el número de nodos terminales del árbol. El término de penalización, evalúa los modelos en función del número de nodos terminales (a mayor número, mayor penalización). El grado de penalización se determina mediante el parámetro de ajuste \\(\\alpha\\). Cuando \\(\\alpha=0\\), la penalización es nula y el árbol resultante es equivalente al árbol original. A medida que se incrementa dicho parámetro, la penalización es mayor y, como consecuencia, los árboles resultantes son de menor tamaño. El valor óptimo de \\(\\alpha\\) puede identificarse mediante validación cruzada.\n\n\n12.1.4 Predicción y evaluación del modelo\nTras la creación de un árbol, las observaciones de entrenamiento quedan agrupadas en los nodos terminales. Para predecir una nueva observación se recorre el árbol en función del valor de sus predictores hasta llegar a uno de los nodos terminales. En el caso de clasificación, suele emplearse la moda de la variable respuesta de los elementos que aparecen en el nodo terminal como valor de predicción. Lo habitual además es acompañar dicho valor con el porcentaje de cada clase en el nodo terminal, lo que aporta información sobre la confianza de la predicción, en caso de que los porcentajes de las diferentes clases se encuentren muy próximos."
  },
  {
    "objectID": "120_DTmodels.html#sec-120.2",
    "href": "120_DTmodels.html#sec-120.2",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.2 Árboles de decisión en tareas de regresión",
    "text": "12.2 Árboles de decisión en tareas de regresión\nLos árboles de regresión son el subtipo de árboles de predicción que se aplica cuando la variable respuesta es continua. En términos generales, en el entrenamiento de un árbol de regresión, las observaciones se van distribuyendo por bifurcaciones (nodos) generando la estructura del árbol hasta alcanzar un nodo terminal.\nEl proceso de entrenamiento de un árbol de decisión para problemas de regresión es similar al del proceso de clasificación donde se produce una división sucesiva del espacio de los predictores generando regiones no solapantes (nodos terminales) \\(𝑅_1\\) , \\(𝑅_2\\) , \\(𝑅_3\\) , …, \\(𝑅_𝑗\\) . Aunque, desde el punto de vista teórico las regiones podrían tener cualquier forma, si se limitan a regiones rectangulares (de múltiples dimensiones), se simplifica en gran medida el proceso de construcción y se facilita la interpretación.\n\n12.2.1 Medidas de impureza\nEn los árboles de regresión, el criterio empleado con más frecuencia para identificar las divisiones es la suma de cuadrados residual (SCE). El objetivo es encontrar las \\(𝐽\\) regiones \\((𝑅_1 ,..., 𝑅_𝑗)\\) que minimizan la suma de cuadrados del error total:\n\\[SCE = \\sum_{j=1}^J \\sum_{i \\in R_j} (𝑦_𝑖−\\hat{y}_{R_j})^2,\\]\ndonde \\(\\hat{y}_{R_j}\\) es la media de la variable respuesta en la región \\(𝑅_𝑗\\) . Una descripción menos técnica equivale a decir que se busca una distribución de regiones tal que, el sumatorio de las desviaciones al cuadrado entre las observaciones y la media de la región a la que pertenecen sea lo menor posible.\nDesafortunadamente, no es posible considerar todas las posibles particiones del espacio de los predictores. Por esta razón, se recurre a lo que se conoce como división binaria recursiva. Esta solución sigue la misma idea que la selección de predictores stepwise (backward o fordward) en regresión lineal múltiple, no evalúa todas las posibles regiones pero, alcanza un buen balance computación-resultado.\n\n\n12.2.2 Algoritmo CART para problemas de regresión\nEL algoritmo CART para problemas de regresión se basa en el método de división binaria recursiva cuyo objetivo es encontrar, en cada iteración, el predictor \\(𝑋_𝑗\\) y el punto de corte (umbral) \\(𝑠\\) tal que, si se distribuyen las observaciones en las regiones \\({𝑋|𝑋𝑗<𝑠}\\) y \\({𝑋|𝑋𝑗≥𝑠}\\) , se consigue la mayor reducción posible en la suma de cuadrados de los residuos (SCE). El algoritmo seguido es:\n\nEl proceso se inicia en lo más alto del árbol, donde todas las observaciones pertenecen a la misma región.\nSe identifican todos los posibles puntos de corte \\(𝑠\\) para cada uno de los predictores \\((𝑋_1, 𝑋_2 ,..., 𝑋_𝑝)\\). En el caso de predictores cualitativos, los posibles puntos de corte son cada uno de sus niveles. Para predictores continuos, se ordenan de menor a mayor sus valores, empleándose el punto intermedio entre cada par de valores como punto de corte.\nSe calcula la SCR total que se consigue con cada posible división identificada en el paso 2:\n\n\\[\\sum_{i: x_i \\in R_1(j,s)} (y_i-\\hat{y}_{R_1})^2 +\\sum_{i: x_i \\in R_2(j,s)} (y_i-\\hat{y}_{R_2})^2\\]\n\ndonde el primer término es la SCE de la región 1 y el segundo término es la SCE de la región 2, siendo cada una de las regiones el resultado de separar las observaciones acorde al predictor \\(𝑗\\) y valor \\(𝑠\\).\n\n\nSe selecciona el predictor \\(𝑋_𝑗\\) y el punto de corte \\(s\\) que resulta en la menor SCE total, es decir, que da lugar a las divisiones más homogéneas posibles. Si existen dos o más divisiones que consiguen la misma mejora, la elección entre ellas es aleatoria.\nSe repiten de forma iterativa los pasos 1 a 4 para cada una de las regiones que se han creado en la iteración anterior hasta que se alcanza alguna norma de parada. Algunas de las más empleadas son: alcanzar una profundidad máxima, que ninguna región contenga menos de n observaciones, que el árbol tenga un máximo de nodos terminales o que la incorporación del más nodos no reduzca el error en al menos un % mínimo.\n\nPara mejorar el funcionamiento de este algoritmo se suelen incorporar estrategias para evitar evaluar todos los posibles puntos de corte. Por ejemplo, para predictores continuos, primero se crea un histograma que agrupa los valores y luego se evalúan los puntos de corte de cada región del histograma.\n\n\n12.2.3 Tratamiento del sobreajuste\nEl tratamiento del sobreajuste utiliza los mismos procedimientos que en el caso de los árboles de decisión para clasificación. Leer el cuaderno anterior para conocer todos los detalles.\n\n\n12.2.4 Predicción y evaluación del modelo\nTras la creación de un árbol, las observaciones de entrenamiento quedan agrupadas en los nodos terminales. Para predecir una nueva observación, se recorre el árbol en función de los valores que tienen sus predictores hasta llegar a uno de los nodos terminales. En el caso de regresión, el valor predicho suele ser la media de la variable respuesta de las observaciones de entrenamiento que están en ese mismo nodo. Si bien la media es el valor más empleado, se puede utilizar cualquier otro (mediana, cuantil…).\nSin embargo, la predicción de un árbol de decisión para regresión puede verse como una variante de vecinos cercanos en la que, solo las observaciones que forman parte del mismo nodo terminal que la observación predicha, tienen influencia. Siguiendo esta aproximación, la predicción del árbol se define como la media ponderada de todas las observaciones de entrenamiento, donde el peso de cada observación depende únicamente de si forma parte o no del mismo nodo terminal.\nImaginemos que tenemos un árbol con cuatro nodos terminales con observaciones y valores de la respuesta para la muestra de entrenamiento:\n\nnodo 1: 1 (10), 3 (24), 7 (16)\nnodo 2: 4 (8), 10 (14)\nnodo 3: 2 (18), 3 (24), 5 (2), 9 (20)\nnodo 4: 6 (9), 8 (10)\n\nde forma que realizamos la predicción de una nueva observación y esta cae en el nodo tres. La predicción viene dada entonces por la media ponderada del número de observaciones:\n\\[\\hat{\\mu} = 0.25*18 + 0.25*24 + 0.25*2 + 0.25*20 = 16\\]"
  },
  {
    "objectID": "120_DTmodels.html#sec-120.3",
    "href": "120_DTmodels.html#sec-120.3",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.3 Ventajas y desventajas de los árboles de decisión",
    "text": "12.3 Ventajas y desventajas de los árboles de decisión\nEntre las ventajas y desventajas del uso de árboles de decisión podemos considerar:\nVentajas\n\nLos árboles son fáciles de interpretar aún cuando las relaciones entre predictores son complejas.\nLos modelos basados en un solo árbol (no es el caso de random forest, boosting) se pueden representar gráficamente aún cuando el número de predictores es mayor de 3.\nLos árboles pueden, en teoría, manejar tanto predictores numéricos como categóricos sin tener que crear variables dummy o one-hot-encoding. En la práctica, esto depende de la implementación del algoritmo que tenga cada librería.\nAl tratarse de métodos no paramétricos, no es necesario que se cumpla ningún tipo de distribución específica.\nPor lo general, requieren mucha menos limpieza y preprocesado de los datos en comparación con otros métodos de aprendizaje estadístico (por ejemplo, no requieren estandarización).\nNo se ven muy influenciados por observaciones anómalas.\nSi para alguna observación el valor de un predictor no está disponible, a pesar de no poder llegar a ningún nodo terminal, se puede conseguir una predicción empleando todas las observaciones que pertenecen al último nodo alcanzado. La precisión de la predicción se verá reducida pero al menos podrá obtenerse.\nSon muy útiles en la exploración de datos, ya que permiten identificar de forma rápida y eficiente las variables (predictores) más importantes.\nSon capaces de seleccionar predictores de forma automática.\n\nDesventajas\n\nLa capacidad predictiva de los modelos basados en un único árbol es bastante inferior a la conseguida con otros modelos. Esto es debido a su tendencia al sobreajuste y a la alta varianza. Sin embargo, existen técnicas más complejas que, haciendo uso de la combinación de múltiples árboles (bagging, random forest, boosting), consiguen mejorar en gran medida este problema.\nSon sensibles a datos de entrenamiento desbalanceados (una de las clases domina sobre las demás).\nCuando tratan con predictores continuos, pierden parte de su información al categorizarlos en el momento de la división de los nodos.\nLos predictores continuos tienen mayor probabilidad de contener, solo por azar, algún punto de corte óptimo, por lo que suelen verse favorecidos en la creación de los árboles.\nNo son capaces de extrapolar fuera del rango valores observados para los predictores en los datos de entrenamiento."
  },
  {
    "objectID": "120_DTmodels.html#sec-120.4",
    "href": "120_DTmodels.html#sec-120.4",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.4 Árboles de decisión en mlr3",
    "text": "12.4 Árboles de decisión en mlr3\nAntes de comenzar con la implementación de los DT en mlr3 vamos a cargar las librerías necesarias:\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(cvms)\nlibrary(kknn)\ntheme_set(theme_sjplot2())\n\n# Paquetes AA\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3tuningspaces)\n\nPara implementar los árboles de decisión en el paquete mlr3 disponemos de varias funciones tanto para las tareas de clasificación como de regresión, pero nosotros nos centraremos en los algoritmos más básicos:\n\nclassif.rpart para la obtención de árboles de decisión en tareas de clasificación.\nregr.rpart para la obtención de árboles de decisión en tareas de regresión.\n\nque utilizan como base las funciones definidas en la librería rpart.\nPodemos cargar los algoritmos con su hiperparámetros por defecto con el código siguiente:\n\n# Learner tarea de clasificación\nldt_classif = lrn(\"classif.rpart\", keep_model = TRUE)\n# Learner tarea de regresión\nldt_regr = lrn(\"regr.rpart\", keep_model = TRUE)\n\nEn este caso los hiperparámetros de ambos algoritmos son los mismos. A continuación se muestran todos ellos:\n\n# Hiperparámetros para DT clasificación\nldt_classif$param_set$ids()\n\n [1] \"cp\"             \"keep_model\"     \"maxcompete\"     \"maxdepth\"      \n [5] \"maxsurrogate\"   \"minbucket\"      \"minsplit\"       \"surrogatestyle\"\n [9] \"usesurrogate\"   \"xval\"          \n\n\nLos parámetros más relevantes son:\n\ncp: parámetro de complejidad. Cualquier división que no disminuya la falta general de ajuste en un factor de cp no es tenida en cuenta. Por ejemplo, con la división de anova, esto significa que el R cuadrado general debe aumentar en cp en cada paso. El papel principal de este parámetro es para ahorrar tiempo de cálculo eliminando divisiones que obviamente no valen la pena. Básicamente, el usuario informa al programa que cualquier división que no mejora el ajuste por cp probablemente será eliminado mediante validación cruzada, y que por lo tanto el programa no necesita perseguirlo. La opción por defecto es 0.01 y puede tomar cualquier valor en el rango \\([0, 1]\\).\nmaxcompete: el número de divisiones de competidores retenidas en la salida. Es útil saber no solo qué división se eligió, pero qué variable quedó en segundo, tercer lugar, etc. La opción por defecto es 4 con valores en el intervalo \\([0, \\infty]\\).\nmaxdepth: Establece la profundidad máxima de cualquier nodo del árbol final, con el nodo raíz contado como profundidad 0. Los valores superiores a 30 rpart darán resultados sin sentido en máquinas de 32 bits. Por defecto se utiliza el valor 30 con un rango de valores posibles en el intervalo \\([1, 30]\\).\nmaxsurrogate: número de divisiones sustitutas retenidas en la salida. Si se establece en cero, el tiempo de cálculo se reducirá, ya que aproximadamente la mitad del tiempo de cálculo (aparte del de configuración) se utiliza en la búsqueda de divisiones sustitutas. Por defecto se utiliza el valor de 5 con un rango de valores posibles en el intervalo \\([0, \\infty]\\).\nminbucket: el número mínimo de observaciones en cualquier nodo terminal. Si solo se especifica uno de minbucket o minsplit, el código establece minsplit en minbucket*3 o minbucket en minsplit/3, según corresponda. No tiene valor por defecto pero el rango de valores posibles en el intervalo \\([1, \\infty]\\).\nminsplit: el número mínimo de observaciones que deben existir en un nodo para que se intente una división. El valor por defecto es 20 con una rango de valores posibles en el intervalo \\([1, \\infty]\\).\nsurrogatestyle: controla la selección de la mejor sustituto. Si se establece en 0 (predeterminado), el programa usa el número total de clasificaciones correctas para una posible variable sustituta; si se establece en 1, usa el porcentaje correcto, calculado sobre los valores no faltantes del sustituto. La primera opción penaliza más severamente las covariables con una gran cantidad de valores faltantes. El valor por defecto es 0 con valores posibles en el rango \\([0, 1]\\).\nusesurrogate: cómo utilizar sustitutos en el proceso de división. 0 significa solo visualización; una observación con un valor faltante para la regla de división principal no se envía más abajo en el árbol. 1 significa utilizar sustitutos, en orden, para dividir a los sujetos a los que les falta la variable principal; si faltan todos los sustitutos, la observación no se divide. Para el valor 2, si faltan todos los sustitutos, envíe la observación en la dirección mayoritaria. Un valor de 0 corresponde a la acción del árbol, y 2 a las recomendaciones de Breiman y otros (1984). El valor por defecto es 2 con valores en el intervalo \\([0, 2]\\).\nxval: número de validaciones cruzadas. El valor por defecto es 10 con valores en el rango \\([0, \\infty]\\)."
  },
  {
    "objectID": "120_DTmodels.html#sec-120.5",
    "href": "120_DTmodels.html#sec-120.5",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.5 Bancos de datos",
    "text": "12.5 Bancos de datos\nPara ejemplificar el uso de estos algoritmos vamos a utilizar los bancos de datos stroke, penguins para tareas de clasificación, y electricity para tareas de regresión. Dado que los algoritmos DT permiten trabajar con factores, y predictores sin escalar no resulta necesario realizar dicha tarea de preprocesamiento. Lo que si resulta necesaria es imputar los valores perdidos como veremos en el punto siguiente.\n\n12.5.1 Stroke\nCargamos los datos y definimos la tarea correspondiente\n\n# Leemos datos\nstroke = read_rds(\"stroke.rds\")\n# Eliminamos la variable id\nstroke = stroke %>% dplyr::select(-id)\n# creamos la tarea\ntsk_stroke = as_task_classif(stroke, target = \"stroke\")\n\nCreamos ahora la división de muestras:\n\n# Generamos variable de estrato\ntsk_stroke$col_roles$stratum <- \"stroke\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(432)\n# Creamos la partición\nsplits = mlr3::partition(tsk_stroke, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_stroke = tsk_stroke$clone()$filter(splits$train)\ntsk_test_stroke  = tsk_stroke$clone()$filter(splits$test)\n\n\n\n12.5.2 Penguins\nEl banco de datos ya ha sido descrito en detalle en temas anteriores pero en este caso vamos a utilizar el que se encuentra disponible en la librería mlr3. Definimos la tarea.\n\n# creamos la tarea\ntsk_penguins = tsk(\"penguins\")\n\nCreamos ahora las muestras de entrenamiento y validación:\n\n# Generamos variable de estrato\ntsk_penguins$col_roles$stratum <- \"sex\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(432)\n# Creamos la partición\nsplits = mlr3::partition(tsk_penguins, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_penguins = tsk_penguins$clone()$filter(splits$train)\ntsk_test_penguins  = tsk_penguins$clone()$filter(splits$test)\n\n\n\n12.5.3 Electricity\nCargamos los datos y generamos las muestras de entrenamiento y validación.\n\n# Carga de datos\nelectricity = read_rds(\"electricity.rds\")\n# Creación de task\ntsk_electricity = as_task_regr(electricity, target = \"PE\")\n\nAhora la división de muestras:\n\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(432)\n# Creamos la partición\nsplits = mlr3::partition(tsk_electricity, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_electricity = tsk_electricity$clone()$filter(splits$train)\ntsk_test_electricity  = tsk_electricity$clone()$filter(splits$test)"
  },
  {
    "objectID": "120_DTmodels.html#sec-120.6",
    "href": "120_DTmodels.html#sec-120.6",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.6 Nuestros primeros modelos",
    "text": "12.6 Nuestros primeros modelos\nEn primer lugar consideramos modelos básicos de DT para los dos bancos de datos presentados. Trabajaremos con las opciones por defecto para poder comparar los resultados con el modelo optimizado que veremos posteriormente. También compararemos los resultados con otros modelos de clasificación de los estudiados hasta ahora.\n\n12.6.1 Stroke\nEn primer lugar generamos el algoritmo DT para este banco de datos y entrenamos el modelo. Dado que los datos preprocesados se generan a través de un graphlearner no podemos representar directamente la solución del árbol de decisión. Después de ver la solución habitual veremos como completar los datos para tener que definir únicamente un learner y poder representar la solución mediante la función autoplot.\n\n# Preprocesamiento\npp_stroke =  po(\"imputemedian\", affect_columns = selector_type(\"numeric\"))\n# Modelo de aprendizaje combinando preprocesado y algoritmo\nldt_classif_stroke = as_learner(pp_stroke %>>% lrn(\"classif.rpart\", keep_model = TRUE, predict_type = \"prob\"))\n# Entrenamiento del modelo\nldt_classif_stroke$train(tsk_train_stroke)\n\nPodemos ver el funcionamiento del algoritmo obteniendo el árbol de clasificación proporcionado en la fase de entrenamiento.\n\nldt_classif_stroke$model$classif.rpart$model\n\nn= 4088 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 4088 199 No (0.95132094 0.04867906) *\n\n\nComo se puede ver el algoritmo no es capaz de realizar ninguna división. Esto se puede deber a dos motivos: i) este algoritmo no funciona bien para este banco de datos, ii) hay que modificar los hiperparámetros del modelo porque resultan muy restrictivos. En el segundo caso podemos buscar una solución óptima y ver que tipo de árbol de decisión resulta.\n\n\n12.6.2 Penguins\nProcedemos directamente con el modelo ya que los datos han sido preparados. Al cargar directamente el learner podemos utilizar la función autoplot para representar la solución.\n\n# Modelo de aprendizaje \nldt_classif_penguins = lrn(\"classif.rpart\", keep_model = TRUE)\n# Entrenamiento del modelo\nldt_classif_penguins$train(tsk_train_penguins)\n\nVemos el árbol obtenido:\n\nldt_classif_penguins$model\n\nn= 274 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 274 153 Adelie (0.44160584 0.19708029 0.36131387)  \n  2) flipper_length< 207.5 170  51 Adelie (0.70000000 0.30000000 0.00000000)  \n    4) bill_length< 43.35 120   4 Adelie (0.96666667 0.03333333 0.00000000) *\n    5) bill_length>=43.35 50   3 Chinstrap (0.06000000 0.94000000 0.00000000) *\n  3) flipper_length>=207.5 104   5 Gentoo (0.01923077 0.02884615 0.95192308) *\n\n\nAunque seguramente resultará más fácil mediante un gráfico:\n\nautoplot(ldt_classif_penguins)\n\n\n\n\nEl modelo proporciona tres nodos terminales con 120, 50, y 104 casos respectivamente. Cada uno de los nodos terminales viene determinado principalmente por una especie en particular. El árbol selecciona en primer lugar como predictor la variable flipper_length con valor de corte 207.5, y posteriormente los menores a ese valor se subdividen de acuerdo a bill_length con valor de corte 43.35. De esta forma podemos establecer que:\n\nLa especie Adelie se caracteriza principalmente por flipper_length menor a 207.5 y bill_length menor a 43.35.\nLa especie Chinstrap se caracteriza principalmente por flipper_length menor a 207.5 y bill_length mayo o igual a 43.35.\nLa especie Gentoo se caracteriza principalmente por flipper_length mayor o igual a 207.5.\n\nSin embargo, la clasificación no es perfecta porque podemos ver que los nodos terminales combinan resultados de diferentes especies. Podemos ver los porcentajes de cada especie en cada nodo terminal con:\n\nldt_classif_penguins$model\n\nn= 274 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 274 153 Adelie (0.44160584 0.19708029 0.36131387)  \n  2) flipper_length< 207.5 170  51 Adelie (0.70000000 0.30000000 0.00000000)  \n    4) bill_length< 43.35 120   4 Adelie (0.96666667 0.03333333 0.00000000) *\n    5) bill_length>=43.35 50   3 Chinstrap (0.06000000 0.94000000 0.00000000) *\n  3) flipper_length>=207.5 104   5 Gentoo (0.01923077 0.02884615 0.95192308) *\n\n\ndonde podemos ver que el nodo terminal identificado como 4) tiene un 96.67% de muestras de Adelie, un 3.33% Chinstrap y un 0% de Gentoo. Los * indican los nodos terminales del árbol obtenido. Antes de valorar la clasificación obtenida vamos a estudiar la relevancia de cada predictora en la construcción del árbol. Para ello utilizamos el método importance().\n\nldt_classif_penguins$importance()\n\nflipper_length    bill_length     bill_depth      body_mass         island \n     102.07243       99.79389       77.64481       71.39752       58.01049 \n\n\nLa tabla proporciona el orden de importancia de las variables en la construcción del árbol de decisión. De las cinco predictoras disponibles la solución solo considera dos de ellas. Podemos identificar las predictoras seleccionadas con el código:\n\nldt_classif_penguins$selected_features()\n\n[1] \"flipper_length\" \"bill_length\"   \n\n\nAhora podemos estudiar la capacidad explicativa del modelo. Como no he os solicitado predecir la probabilidad no podemos obtener los scores asociados (bbrier y auc):\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = ldt_classif_penguins$predict(tsk_train_penguins)\npred_test = ldt_classif_penguins$predict(tsk_test_penguins)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\", \"classif.bbrier\", \"classif.auc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n     0.9562044      0.9430160            NaN            NaN \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n     0.9285714      0.9149616            NaN            NaN \n\n\nEl algoritmo se comporta bastante bien ya que alcanzamos un porcentaje de clasificación correcta del 91.5% (similar al de modelos anteriores para estos datos). Podemos estudiar la matriz de confusión:\n\n# Cargamos la librería para representar la matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nPodemos ver que los errores de clasificación en cada especie son a lo sumo de dos ejemplares, lo que indica que con tan solo esas dos predictoras somos capaces de construir un modelo con una gran capacidad de clasificación/predicción.\n\n\n12.6.3 Electricity\nPara finalizar este apartado de modelos iniciales vamos a finalizar con el primer árbol de decisión para un modelo de regresión. En este caso no tenemos valore perdidos por lo que podemos implementar directamente el algoritmo de aprendizaje.\n\n# Modelo de aprendizaje \nldt_regr_electricity = lrn(\"regr.rpart\", keep_model = TRUE)\n# Entrenamiento del modelo\nldt_regr_electricity$train(tsk_train_electricity)\n\nVeamos la solución gráfica del árbol:\n\nautoplot(ldt_regr_electricity)\n\n\n\n\nPara este conjunto de datos el árbol de decisión obtenido es bastante más complejo con 7 nodos terminales. Lo que puede resultar más curioso es que se utiliza la misma variable en diferentes niveles del árbol. El algoritmo determina en función de las subdivisiones que va realizando si una variable ya utilizada debe utilizarse de nuevo con unos puntos de corte distintos aunque siempre consistentes con lo decidido en las ramas superiores). De hecho, en los diagramas de caja podemos observar que el nodo terminal con mayores valores de PE se corresponde con la regla de decisión AT menor que 8.725, que es una combinación de los resultados de las divisiones anteriores. Por otro lado, el nodo terminal con menores valores de PE se corresponde con la regla de decisión AT mayor o igual a 23.055 y V mayor o igual a 66. Podemos ver la relevancia de las predictoras con:\n\nldt_regr_electricity$importance()\n\n       AT         V        AP        RH \n1968642.4 1401070.5  620869.4  442538.7 \n\n\nA partir de dichos valores podemos valorar la importancia relativa en términos de porcentaje mediante el código:\n\nimportancia = ldt_regr_electricity$importance()\n100*importancia/sum(importancia)\n\n       AT         V        AP        RH \n44.407595 31.604609 14.005243  9.982554 \n\n\nPodemos ver que AT tiene una importancia relativa del 44.40%. Además podemos conocer el valor medio estimado de la respuesta para cada nodo sin más que ver el modelo ajustado.\n\nldt_regr_electricity$model\n\nn= 7655 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 7655 2222770.00 454.3762  \n   2) AT>=18.225 4398  320007.40 441.9419  \n     4) AT>=23.055 2959  113057.40 437.9514  \n       8) V>=66.21 1759   38622.84 434.6881 *\n       9) V< 66.21 1200   28243.12 442.7350 *\n     5) AT< 23.055 1439   62944.99 450.1473 *\n   3) AT< 18.225 3257  304572.80 471.1666  \n     6) AT>=11.905 1812   71524.42 464.7302  \n      12) AT>=15.545 656   18707.28 459.8600 *\n      13) AT< 15.545 1156   28428.50 467.4939 *\n     7) AT< 11.905 1445   63851.74 479.2377  \n      14) AT>=8.725 832   21797.93 475.8558 *\n      15) AT< 8.725 613   19622.58 483.8278 *\n\n\nEn la última columna aparecen los valores estimados de la respuesta PE dentro de cada nodo del árbol. podemos estudiar ahora las precisiones de las estimaciones con el análisis de las predicciones tanto para la muestra de entrenamiento como la de validación.\n\n# Predicción de la muestra de entrenamiento\npred_train = ldt_regr_electricity$predict(tsk_train_electricity)\n# Predicción de la muestra de validación\npred_test = ldt_regr_electricity$predict(tsk_test_electricity)\n# Scores de validación\nmeasures = msrs(c(\"regr.rsq\", \"regr.mse\", \"regr.smape\"))\n# Valores de validación entrenamiento y validación\npred_train$score(measures)\n\n    regr.rsq     regr.mse   regr.smape \n 0.901758942 28.526091659  0.009117687 \n\npred_test$score(measures)\n\n    regr.rsq     regr.mse   regr.smape \n 0.902015151 28.884458772  0.009251825 \n\n\nLos resultados son muy buenos, incluso mejorando los que vimos para este mismo conjunto de datos con otros algoritmos. La ventaja principal es que la selección de predictoras se hace automáticamente con la construcción del árbol. Por último analizamos los gráficos de predicción (más concretamente el de valores observados versus valores predichos) para comprender de forma más precisa el proceso de predicción en los modelos de árboles. Para ello realizamos los gráficos siguientes.\n\n# Muestra de entrenamiento\np1 = autoplot(pred_train, type = \"xy\") + labs(title = \"Observados vs predichos\")\n\n# Muestra de validación\np2 = autoplot(pred_test, type = \"xy\") + labs(title = \"Observados vs predichos\")\n\nggarrange(p1, p2, ncol = 2)\n\n\n\n\nGráficos del modelo para muestras de entrenamiento y validación. Task Electricity.\n\n\n\n\nComo se puede ver tanto el gráfico de la muestra de entrenamiento como de validación la predicción no representa una nube de untos como en modelos anteriores de regresión, sino que únicamente se predice un valor por cada nodo terminal presenten el modelo. En este caso hay 7 nodos terminales y por eso solo tenemos 7 valores predichos, o que provoca que aparezcan 7 columnas de predicción y no una nube de puntos. Este efecto en la predicción es debida a que no tenemos una ecuación que nos proporcione valores, sino únicamente nodos donde todas las observaciones alojadas en él se les asigna el mismo valor de predicción, que en este caso es el valor medio de la respuesta para todas las observaciones en ese nodo.\nEn el punto siguiente tratamos de mejorar los modelos obtenidos en este bloque mediante un búsqueda óptima de los hiperparámetros del modelo."
  },
  {
    "objectID": "120_DTmodels.html#sec-120.7",
    "href": "120_DTmodels.html#sec-120.7",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.7 Optimizando los modelos",
    "text": "12.7 Optimizando los modelos\nPara el proceso de optimización de los modelos vamos a considerar únicamente los parámetros cp, minsplit y maxdepth. Aunque este algoritmo permite configurar muchos hiperparámetros nos centramos en estos para buscar una mejora sobre los modelos básicos aunque esta sea mínimo. No buscamos mejorar en exceso el modelo sino ver como funciona la selección de hiperparámetros en este modelo de aprendizaje.\n\n12.7.1 Stroke\nA continuación se muestra el código de optimización para el banco de datos Stroke. Recordemos que con las opciones por defecto no resulta posible obtener ningún árbol de decisión. Para el parámetro cp utilizaremos la escala logaritmo para la búsqueda del óptimo. Para la profundidad máxima del árbol consideramos el intervalo \\([3, 8]\\) para evitar tener un árbol demasiado pequeño o demasiado grande. Por último, consideramos que el tamaño mínimo de los nodos terminales debe estar en el intervalo \\([5, 50]\\). Consideramos una evaluación de 30 iteraciones debido al tamaño de la base de datos.\n\nldt_classif_stroke = lrn(\"classif.rpart\", keep_model = TRUE, predict_type = \"prob\",\n                         cp = to_tune(1e-04, 1, logscale = TRUE),\n                         maxdepth = to_tune(3, 8),\n                         minsplit = to_tune(5, 50)\n                         )\ngr_stroke =  pp_stroke %>>% ldt_classif_stroke\ngr_stroke = GraphLearner$new(gr_stroke)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_stroke,\n  learner = gr_stroke,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.bacc\"),\n  term_evals = 30\n)\n\nVeamos los resultados obtenidos:\n\n# Veamos si converge el algoritmo\ninstance$is_terminated\n\n[1] TRUE\n\n# Resultados del proceso de optimización\ninstance$result_learner_param_vals\n\n$imputemedian.affect_columns\nselector_type(\"numeric\")\n\n$classif.rpart.keep_model\n[1] TRUE\n\n$classif.rpart.xval\n[1] 0\n\n$classif.rpart.cp\n[1] 0.0003562633\n\n$classif.rpart.maxdepth\n[1] 6\n\n$classif.rpart.minsplit\n[1] 10\n\n# Valor de la métrica para resultado óptimo\ninstance$result_y\n\nclassif.bacc \n   0.5099052 \n\n\nEl proceso de optimización ha finalizado encontrando los valores óptimos de cp igual a 0.0003562633, minsplit igual 10, y maxdepth igual a 6. El porcentaje de clasificación correcta ponderada es del 50.9% mejorando los resultados del modelo por defecto, pero no mucho los de otros modelos de clasificación vistos anteriormente. El modelo sigue siendo bastante pobre en términos predictivos. Utilizamos los valores obtenidos para establecer un nuevo árbol de decisión:\n\n# Nuevo modelo\nldt_classif_stroke = lrn(\"classif.rpart\", keep_model = TRUE, predict_type = \"prob\",\n                         cp = instance$result_x_domain$classif.rpart.cp,\n                         maxdepth = instance$result_x_domain$classif.rpart.maxdepth,\n                         minsplit = instance$result_x_domain$classif.rpart.minsplit\n                         )\ngr_stroke =  pp_stroke %>>% ldt_classif_stroke\ngr_stroke = GraphLearner$new(gr_stroke)\n# Entrenamiento del modelo\ngr_stroke$train(tsk_train_stroke)\n\nPara representar la solución utilizamos la librería rpart.plot:\n\nmodelo = gr_stroke$model$classif.rpart$model\nlibrary(rpart.plot)\nrpart.plot(modelo, type = 5)\n\n\n\n\nEn el caso de árboles de clasificación binaria la información presentada en los nodos terminales es:\n\nLa categoría predicha para ese nodo\nLa probabilidad predicha de la clase de interés (en este caso Yes).\nEl porcentaje de observaciones en el nodo.\n\nDado que los sujetos que sufren un ictus (249) es muy bajo en comparación con los que no (4861) es lógico que los porcentaje de observaciones en los nodos terminales identificados con Yes sean muy bajos. Podemos ver las reglas de clasificación:\n\nrpart.rules(modelo)\n\n stroke                                                                                                                                                                                                    \n   0.00 when age is 57 to 68 & avg_glucose_level is 110 to 121                       & smoking_status is                              never smoked                                                         \n   0.01 when age <  57                                                                                                                                                                                     \n   0.04 when age >=       68 & avg_glucose_level <  251        & ever_married is  No                                                                                      & hypertension is  No & bmi >= 29\n   0.05 when age is 57 to 68 & avg_glucose_level <  110                                                                                                                                                    \n   0.09 when age is 57 to 68 & avg_glucose_level >=        121                                                                                     & heart_disease is  No                                  \n   0.11 when age >=       68 & avg_glucose_level is  94 to 251 & ever_married is  No                                                                                      & hypertension is  No & bmi <  29\n   0.13 when age >=       68 & avg_glucose_level <  166        & ever_married is Yes                                                                                                                       \n   0.15 when age is 57 to 68 & avg_glucose_level >=        121                       & smoking_status is formerly smoked or never smoked or smokes & heart_disease is Yes                                  \n   0.17 when age >=       68 & avg_glucose_level is 200 to 251 & ever_married is Yes                                                                                                                       \n   0.33 when age >=       68 & avg_glucose_level <  251        & ever_married is  No & smoking_status is      formerly smoked or smokes or Unknown                        & hypertension is Yes            \n   0.35 when age is 57 to 68 & avg_glucose_level is 110 to 121                       & smoking_status is      formerly smoked or smokes or Unknown & heart_disease is  No                                  \n   0.37 when age >=       68 & avg_glucose_level is 166 to 199 & ever_married is Yes                                                                                                                       \n   0.55 when age >=       68 & avg_glucose_level <   94        & ever_married is  No                                                                                      & hypertension is  No & bmi <  29\n   0.62 when age >=       68 & avg_glucose_level >=        251                                                                                                                                             \n   0.73 when age >=       68 & avg_glucose_level <  251        & ever_married is  No & smoking_status is                              never smoked                        & hypertension is Yes            \n   1.00 when age is 57 to 68 & avg_glucose_level >=        121                       & smoking_status is                                   Unknown & heart_disease is Yes                                  \n   1.00 when age is 57 to 68 & avg_glucose_level is 110 to 121                       & smoking_status is      formerly smoked or smokes or Unknown & heart_disease is Yes                                  \n   1.00 when age >=       68 & avg_glucose_level is 199 to 200 & ever_married is Yes                                                                                                                       \n\n\nLas reglas viene ordenadas de menor a mayor porcentaje de respuesta de sujetos que ha sufrido un ictus dentro de cada nodo terminal. Los tres últimos contienen sólo sujetos con ictus y podemos ver sus indicadores de clasificación con detalle. Analizamos ahora la importancia de las predictoras en este modelo:\n\nimportancia = gr_stroke$model$classif.rpart$model$variable.importance\n100*importancia/sum(importancia)\n\n              age avg_glucose_level    smoking_status     heart_disease \n       40.8966541        24.6690537         9.8401508         8.0895283 \n              bmi      hypertension      ever_married    Residence_type \n        6.3282122         4.9963743         4.1564304         0.5119157 \n        work_type \n        0.5116804 \n\n\nEn la tabla anterior observamos que age y avg_glucose_level son las predictoras más importantes, seguidas de smoking_status y heart_disease. Estas variables marcan el perfil de los sujetos con mayor probabilidad de ictus, Podemos buscar en las reglas de clasificación para encontrar los pintos de corte de cada una de ellas. Para ver el comportamiento del modelo obtenemos la matriz de confusión:\n\n# Predicción de la muestra de entrenamiento y validacion\npred_train = gr_stroke$predict(tsk_train_stroke)\npred_test = gr_stroke$predict(tsk_test_stroke)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\", \"classif.bbrier\", \"classif.auc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.95572407     0.57145008     0.03784596     0.83016458 \n\n# Muestra de validación\npred_test$score(measures)\n\n   classif.acc   classif.bacc classif.bbrier    classif.auc \n    0.94227006     0.50485597     0.04845108     0.78057613 \n\n# Matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nPodemos ver que en este caso la matriz de confusión si reparte observaciones entre todas las combinaciones, aunque el mayor error se comete de nuevo al clasificar casi todas observaciones originales con ictus como sanas.\nEl proceso de optimización nos ha permitido construir un árbol de decisión pero su poder de clasificación real para distinguir individuos sanos de enfermos es muy bajo. Para analizar la estabilidad de la solución planteamos una análisis de validación cruzada y el análisis de la curva de aprendizaje.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_stroke, gr_stroke, resamp, store_models=TRUE)\n\nINFO  [17:54:41.051] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:41.192] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:41.325] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:41.463] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:41.651] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:41.817] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:41.979] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:54:42.129] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:54:42.277] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:54:42.466] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\n\n\n\nmeasure = msr(\"classif.bacc\")\n# Resumen Scores individuales\nscores = rr$score(measure)\nskim(scores)\n\n\nData summary\n\n\nName\nscores\n\n\nNumber of rows\n10\n\n\nNumber of columns\n9\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nlist\n4\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntask_id\n0\n1\n6\n6\n0\n1\n0\n\n\nlearner_id\n0\n1\n26\n26\n0\n1\n0\n\n\nresampling_id\n0\n1\n2\n2\n0\n1\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\ntask\n0\n1\n1\n51\n51\n\n\nlearner\n0\n1\n10\n38\n38\n\n\nresampling\n0\n1\n1\n20\n20\n\n\nprediction\n0\n1\n10\n20\n20\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\niteration\n0\n1\n5.50\n3.03\n1.00\n3.25\n5.50\n7.75\n10.00\n▇▇▇▇▇\n\n\nclassif.bacc\n0\n1\n0.52\n0.02\n0.49\n0.50\n0.51\n0.52\n0.56\n▇▇▅▂▂\n\n\n\n\n\nEl valor estimado del porcentaje de clasificación ponderado se sitúa en el 51.59% con una desviación del 1.89%. Para finalizar analizamos la curva de aprendizaje:\n\n\n\n\nptr = seq(0.1, 0.9, 0.1)\nlcurve = learningcurve(tsk_stroke, gr_stroke, \"classif.bacc\", ptr = ptr, rpeats = 10)\n\nINFO  [17:54:43.150] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:43.352] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:43.586] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:43.823] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:44.007] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:44.210] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:44.430] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:54:44.677] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:54:44.881] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:54:45.073] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\nINFO  [17:54:45.406] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:45.611] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:45.804] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:46.017] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:46.228] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:46.424] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:46.693] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:54:46.908] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:54:47.142] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:54:47.364] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\nINFO  [17:54:47.721] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:47.920] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:48.506] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:48.684] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:48.868] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:49.060] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:49.247] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:54:49.661] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:54:49.903] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:54:50.128] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\nINFO  [17:54:50.419] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:50.606] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:50.799] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:50.981] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:51.166] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:51.362] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:51.571] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:54:51.766] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:54:51.951] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:54:52.133] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\nINFO  [17:54:52.430] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:52.623] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:52.806] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:52.990] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:53.189] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:53.371] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:53.575] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:54:53.760] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:54:53.954] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:54:54.140] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\nINFO  [17:54:54.433] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:54.629] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:54.815] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:55.018] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:55.207] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:55.402] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:55.595] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:54:55.793] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:54:55.984] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:54:56.181] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\nINFO  [17:54:56.506] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:56.707] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:56.895] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:57.097] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:57.305] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:57.505] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:57.760] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:54:57.948] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:54:58.190] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:54:58.376] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\nINFO  [17:54:58.705] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:54:58.898] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:54:59.107] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:54:59.295] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:54:59.515] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:54:59.703] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:54:59.913] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:55:00.110] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:55:00.321] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:55:00.518] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\nINFO  [17:55:00.824] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 1/10)\nINFO  [17:55:01.031] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 2/10)\nINFO  [17:55:01.233] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 3/10)\nINFO  [17:55:01.437] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 4/10)\nINFO  [17:55:01.652] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 5/10)\nINFO  [17:55:01.901] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 6/10)\nINFO  [17:55:02.102] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 7/10)\nINFO  [17:55:02.317] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 8/10)\nINFO  [17:55:02.524] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 9/10)\nINFO  [17:55:02.740] [mlr3] Applying learner 'imputemedian.classif.rpart' on task 'stroke' (iter 10/10)\n\n# Gráfico\nggplot(lcurve, aes(ptr, BACC, color = Sample)) + \n    geom_line() +\n    labs(x =\"Proporción tamaño muestra entrenamiento\", y = \"BACC\",color = \"Muestra\") +\n    scale_color_hue(labels = c(\"Validación\", \"Entrenamiento\")) +\n    scale_x_continuous(breaks=ptr)\n\n\n\n\nCurva de aprendizaje\n\n\n\n\nEl porcentaje de clasificación correcta ponderado para la muestra de validación se mantiene bastante estable cuando variamos el tamaño de la muestra de entrenamiento.\n\n\n12.7.2 Penguins\nPlanteamos ahora el proceso de optimización del árbol de decisión para el banco de datos Penguins. Recordemos que el porcentaje de clasificación correcta ponderado es del 94.3%. Para este proceso consideramos una configuración de hiperparámetros similar al del ejemplo anterior.\n\nldt_classif_penguins = lrn(\"classif.rpart\", keep_model = TRUE, predict_type = \"prob\",\n                         cp = to_tune(1e-04, 1, logscale = TRUE),\n                         maxdepth = to_tune(3, 8),\n                         minsplit = to_tune(5, 50)\n                         )\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_penguins,\n  learner = ldt_classif_penguins,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.bacc\"),\n  term_evals = 30\n)\n\nVeamos los resultados obtenidos:\n\n# Veamos si converge el algoritmo\ninstance$is_terminated\n\n[1] TRUE\n\n# Resultados del proceso de optimización\ninstance$result_learner_param_vals\n\n$xval\n[1] 0\n\n$keep_model\n[1] TRUE\n\n$cp\n[1] 0.006139712\n\n$maxdepth\n[1] 7\n\n$minsplit\n[1] 6\n\n# Valor de la métrica para resultado óptimo\ninstance$result_y\n\nclassif.bacc \n   0.9625707 \n\n\nA simple vista ya podemos ver que hemos mejorado un 2% (alcanzamos el 96.6%) nuestro porcentaje de clasificación. Utilizamos los valore obtenidos para generar el nuevo árbol de decisión.\n\n# Nuevo modelo\nldt_classif_penguins = lrn(\"classif.rpart\", keep_model = TRUE, predict_type = \"prob\",\n                         cp = instance$result_x_domain$cp,\n                         maxdepth = instance$result_x_domain$maxdepth,\n                         minsplit = instance$result_x_domain$minsplit\n                         )\n# Entrenamiento del modelo\nldt_classif_penguins$train(tsk_train_penguins)\n\nRepresentamos la solución:\n\nmodelo = ldt_classif_penguins$model\nrpart.plot(modelo, type = 5)\n\n\n\n\nEn este caso la información proporcionada en los nodos terminales es:\n\nClase mayoritaria del nodo terminal, es decir, predicción proporciona por el modelo para esa rama del árbol.\nLos porcentajes de cada una de las clases en ese nodo terminal.\nPorcentaje de observaciones en ese nodo respecto del total de muestras.\n\nPodemos ver claramente cuales son los nodos más relevantes mirando los porcentaje de observaciones y extraer las reglas de clasificación correspondientes (recordemos que las variables están estandarizadas:\n\nEl nodo clasificado con Adelie que contiene el 41% de las muestras se caracterizan por flipper_length menor a 0.47 y bill_length menor a -0.31.\nEl nodo clasificado con Chinstrap que contiene el 18% de las muestras se caracterizan por flipper_length menor a 0.47 y bill_length mayor o igual a -0.13, e island igual a Dream.\nEl nodo clasificado con Gentoo que contiene el 36% de las muestras se caracterizan por flipper_length mayor o igual a 0.47 e island igual a Biscoe.\n\nLas reglas de clasificación completas se encuentran en la tabla siguiente:\nrpart.rules(modelo)\nspecies Adel Chin Gent\nAdelie [ .99 .01 .00] when flipper_length < 208 & bill_length < 42\nAdelie [1.00 .00 .00] when flipper_length < 208 & bill_length is 42 to 43 & bill_depth >= 17\nAdelie [1.00 .00 .00] when flipper_length < 208 & bill_length >= 43 & island is Biscoe or Torgersen Chinstrap [ .40 .60 .00] when flipper_length >= 208 & bill_depth >= 18\nChinstrap [ .02 .98 .00] when flipper_length < 208 & bill_length >= 43 & island is Dream Chinstrap [ .00 1.00 .00] when flipper_length < 208 & bill_length is 42 to 43 & bill_depth < 17\nGentoo [ .00 .00 1.00] when flipper_length >= 208 & bill_depth < 18\nPara finalizar analizamos la matriz de confusión asociada con el nuevo modelo.\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = ldt_classif_penguins$predict(tsk_train_penguins)\npred_test = ldt_classif_penguins$predict(tsk_test_penguins)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n classif.acc classif.bacc \n   0.9854015    0.9855627 \n\n# Muestra de validación\npred_test$score(measures)\n\n classif.acc classif.bacc \n   0.9714286    0.9733333 \n\n# Matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nTodos los errores de clasificación (2.9%) están asociados con la especie Gentoo original, ya que el modelo obtenido clasifica esas muestras como pertenecientes a la especie Adelie.\nSe puede finalizar el análisis mediante el estudio de validación y la construcción de la curva de aprendizaje.\n\n\n12.7.3 Electricity\nFinalizamos con la optimización para el banco de datos Electricity. Recordemos que el sMAPE para la muestra de validación obtenido era del 0.00925. Empezamos definiendo la configuración del proceso de optimización cambiando algunos de los parámetros dado que el número de predictoras en este caso es muy bajo y no podemos considerar árboles muy profundos.\n\nldt_regr_electricity = lrn(\"regr.rpart\", keep_model = TRUE,\n                         cp = to_tune(1e-03, 1, logscale = TRUE),\n                         maxdepth = to_tune(2, 4),\n                         minsplit = to_tune(5, 50)\n                         )\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_electricity,\n  learner = ldt_regr_electricity,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"regr.smape\"),\n  term_evals = 30\n)\n\nVeamos los resultados obtenidos:\n\n# Veamos si converge el algoritmo\ninstance$is_terminated\n\n[1] TRUE\n\n# Resultados del proceso de optimización\ninstance$result_learner_param_vals\n\n$xval\n[1] 0\n\n$keep_model\n[1] TRUE\n\n$cp\n[1] 0.001742492\n\n$maxdepth\n[1] 4\n\n$minsplit\n[1] 6\n\n# Valor de la métrica para resultado óptimo\ninstance$result_y\n\n regr.smape \n0.008232575 \n\n\nEl modelo optimizado reduce el sMAPE hasta el valor 0.0082. Analizamos ahora e árbol obtenido con dichos valores.\n\n# Nuevo modelo\nldt_regr_electricity = lrn(\"regr.rpart\", keep_model = TRUE, \n                         cp = instance$result_x_domain$cp,\n                         maxdepth = instance$result_x_domain$maxdepth,\n                         minsplit = instance$result_x_domain$minsplit\n                         )\n# Entrenamiento del modelo\nldt_regr_electricity$train(tsk_train_electricity)\n\nRepresentamos la solución:\n\nmodelo = ldt_regr_electricity$model\nrpart.plot(modelo, type = 5)\n\n\n\n\nEn este caso la información de los nodos terminales es:\n\nPredicción de la respuesta en el nodo terminal.\nPorcentaje de muestras en el nodo terminal con respecto del total de muestras.\n\nPodemos ver además que las ramas del árbol hacen uso recursivo de las mismas predictores con diferentes scores de división.\nPara finalizar realizamos un estudio de validación cruzada de la solución obtenida.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_electricity, ldt_regr_electricity, resamp, store_models=TRUE)\n\nINFO  [17:55:18.036] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:18.068] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:18.105] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:18.133] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:18.161] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:18.193] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:18.232] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:18.275] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:18.302] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:18.333] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\n\n\nAnalizamos los resultados obtenidos:\n\nmeasure = msr(\"regr.smape\")\n# Resumen Scores individuales\nscores = rr$score(measure)\nskim(scores)\n\n\nData summary\n\n\nName\nscores\n\n\nNumber of rows\n10\n\n\nNumber of columns\n9\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nlist\n4\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntask_id\n0\n1\n11\n11\n0\n1\n0\n\n\nlearner_id\n0\n1\n10\n10\n0\n1\n0\n\n\nresampling_id\n0\n1\n2\n2\n0\n1\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\ntask\n0\n1\n1\n48\n48\n\n\nlearner\n0\n1\n10\n38\n38\n\n\nresampling\n0\n1\n1\n20\n20\n\n\nprediction\n0\n1\n10\n19\n19\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\niteration\n0\n1\n5.50\n3.03\n1.00\n3.25\n5.50\n7.75\n10.00\n▇▇▇▇▇\n\n\nregr.smape\n0\n1\n0.01\n0.00\n0.01\n0.01\n0.01\n0.01\n0.01\n▅▇▇▁▅\n\n\n\n\n\nEl valor estimado del sMAPE se sitúa en el 0.0082 con una desviación del 0.0001. Para finalizar analizamos la curva de aprendizaje:\n\nptr = seq(0.1, 0.9, 0.1)\nlcurve = learningcurve(tsk_electricity, ldt_regr_electricity, \"regr.smape\", ptr = ptr, rpeats = 10)\n\nINFO  [17:55:18.703] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:18.731] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:18.763] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:18.802] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:18.830] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:18.858] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:18.884] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:18.913] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:18.940] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:18.967] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\nINFO  [17:55:19.098] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:19.129] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:19.162] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:19.191] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:19.218] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:19.246] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:19.279] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:19.308] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:19.335] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:19.362] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\nINFO  [17:55:19.486] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:19.515] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:19.544] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:19.572] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:19.601] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:19.628] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:19.657] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:19.685] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:19.718] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:19.756] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\nINFO  [17:55:19.911] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:19.944] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:19.977] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:20.010] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:20.042] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:20.120] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:20.153] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:20.185] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:20.219] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:20.254] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\nINFO  [17:55:20.371] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:20.407] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:20.445] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:20.479] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:20.512] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:20.563] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:20.598] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:20.631] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:20.664] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:20.698] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\nINFO  [17:55:20.835] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:20.866] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:20.898] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:20.945] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:20.978] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:21.010] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:21.042] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:21.073] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:21.104] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:21.135] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\nINFO  [17:55:21.267] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:21.299] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:21.331] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:21.363] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:21.397] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:21.434] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:21.467] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:21.506] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:21.555] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:21.587] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\nINFO  [17:55:21.704] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:21.750] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:21.795] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:21.828] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:21.863] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:21.899] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:21.933] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:21.988] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:22.022] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:22.056] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\nINFO  [17:55:22.180] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 1/10)\nINFO  [17:55:22.214] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 2/10)\nINFO  [17:55:22.249] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 3/10)\nINFO  [17:55:22.283] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 4/10)\nINFO  [17:55:22.334] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 5/10)\nINFO  [17:55:22.371] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 6/10)\nINFO  [17:55:22.410] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 7/10)\nINFO  [17:55:22.447] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 8/10)\nINFO  [17:55:22.481] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 9/10)\nINFO  [17:55:22.515] [mlr3] Applying learner 'regr.rpart' on task 'electricity' (iter 10/10)\n\n# Gráfico\nggplot(lcurve, aes(ptr, BACC, color = Sample)) + \n    geom_line() +\n    labs(x =\"Proporción tamaño muestra entrenamiento\", y = \"sMAPE\",color = \"Muestra\") +\n    scale_color_hue(labels = c(\"Validación\", \"Entrenamiento\")) +\n    scale_x_continuous(breaks=ptr)\n\n\n\n\nCurva de aprendizaje\n\n\n\n\nSe aprecia como el sMAPE disminuye cuando aumentamos el tamaño de la muestra de entrenamiento. El valor óptimo se sitúa en un tamaño del 60%."
  },
  {
    "objectID": "120_DTmodels.html#otros-modelos-de-áboles-en-mlr3",
    "href": "120_DTmodels.html#otros-modelos-de-áboles-en-mlr3",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.8 Otros modelos de áboles en mlr3",
    "text": "12.8 Otros modelos de áboles en mlr3\nEn mlr3 existen otro algoritmos para la obtención de árboles de decisión:\n\nclassif.C50, para construir árboles de decisión en problemas de clasificación utilizando el criterio de ganancia de información.\nclassif.ctree, para construir árboles de decisión en problemas de clasificación donde se utiliza un test de comparación para encontrar la división de cada rama.\nregr.ctree, para construir árboles de decisión en problemas de regresión donde se utiliza un test de comparación para encontrar la división de cada rama."
  },
  {
    "objectID": "120_DTmodels.html#sec-120.8",
    "href": "120_DTmodels.html#sec-120.8",
    "title": "12  Árboles de decisiónn (DT)",
    "section": "12.9 Ejercicios",
    "text": "12.9 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en un modelo de árbol de decisión para el banco de datos Mushroom4.3.4.\nAjustar un modelo de aprendizaje automático basado en un modelo de árbol de decisión para el banco de datos Water potability4.3.7.\nAjustar un modelo de aprendizaje automático basado en un modelo de árbol de decisión para el banco de datos Hepatitis4.3.9.\nAjustar un modelo de aprendizaje automático basado en un modelo de árbol de decisión para el banco de datos Abalone4.3.1.\nAjustar un modelo de aprendizaje automático basado en un modelo de árbol de decisión para el banco de datos Us economic time series4.2.7.\nAjustar un modelo de aprendizaje automático basado en un modelo de árbol de decisión para el banco de datos QSAR4.2.8."
  },
  {
    "objectID": "130_Ensemblemodels.html#sec-130.1",
    "href": "130_Ensemblemodels.html#sec-130.1",
    "title": "13  Modelos de conjunto (Ensemble models)",
    "section": "13.1 Modelos básicos Bagging",
    "text": "13.1 Modelos básicos Bagging\nEl término bagging es el diminutivo de bootstrap agregation (bootstrap agregativo), y hace referencia al empleo del muestreo repetido con reposición bootstrapping con el fin de reducir la varianza de algunos modelos de aprendizaje estadístico haciendo uso del famoso resultado estadístico conocido como teorema central del límite.\nDicho teorema nos dice que si disponemos de n variables aleatorias independientes con varianza \\(\\sigma^2\\) para cada una de ellas, entonces la varianza de la media de todas ellas es \\(\\sigma^2/n\\). En otras palabras, promediando un conjunto de observaciones se reduce la varianza.\nBasándose en esta idea, una forma de reducir la varianza y aumentar la precisión de un método predictivo es obtener múltiples muestras de la población, ajustar un modelo distinto con cada una de ellas, y hacer la media (la moda en el caso de variables cualitativas) de las predicciones resultantes. Como en la práctica no se suele tener acceso a múltiples muestras, se puede simular el proceso recurriendo a la técnica de bootstrap, que genera pseudo-muestras a partir del conjunto de datos disponible mediante muestreo con reemplazamiento. Se ajusta entonces los diferentes modelos individuales propuestos a cada una de las pseudo-muestras y se agregan los resultados obtenidos en función de la variable objetivo considerada.\nLos dos algoritmos de bagging más utilizados son el de voto por mayoría o bagging por lotes.\n\n13.1.1 Voto por mayoría\nEl método de voto por mayoría es el más habitual dentro de los modelos de conjunto por agregación. Supongamos que ajustamos \\(L\\) modelos de aprendizaje distintos sobre toda la muestra de entrenamiento para resolver un problema de clasificación o regresión. Obtenemos ahora la predicción para la muestra de test ( de tamaño \\(m\\)) para cada uno de los \\(L\\) modelos mediante:\n\nlos valores predichos \\(\\hat{y}_1,\\hat{y}_2,..., \\hat{y}_m\\) si estamos en un problema de regresión, o\nlos valores de clasificación predichos \\(\\hat{c}_1,\\hat{c}_2,..., \\hat{c}_m\\) si nos enfrentamos a un problema de clasificación donde la variable respuesta puede tomar \\(k\\) valores distintos.\n\nPara la construcción de la predicción para el modelo de conjunto se procede obteniendo:\n\nla media de los valores predichos en un problema de regresión\n\n\\[\\hat{y}_{ensemble} = \\frac{\\sum_{i=1}^m \\hat{y}_i}{m},\\]\n\nla moda de los valores de clasificación en un problema de clasificación\n\n\\[\\hat{c}_{ensemble} = \\text{moda}\\{\\hat{c}_1,\\hat{c}_2,..., \\hat{c}_m\\}.\\]\nEn el problema de clasificación si los \\(L\\) modelos considerados pueden estimar las probabilidades de cada clase tendremos una matriz estimada de probabilidades de dimensiones \\(Lxk\\),\n\\[\\begin{equation}\n\\begin{pmatrix}\np_{1,1} & p_{1,2} &...& p_{1,k}\\\\\np_{2,1} & p_{2,2} &...& p_{2,k}\\\\\n... & ... &...& ...\\\\\np_{L-1,1} & p_{L-1,2} &...& p_{L-1,k}\\\\\np_{L,1} & p_{L,2} &...& p_{L,k}\n\\end{pmatrix}\n\\end{equation}\\]\nEn esta situación tomamos como clase resultante la que proporcione un mayor valor promedio (por columnas) de las probabilidades obtenidas. Este procedimiento suele dar mejores resultados que el voto por mayoría estándar ya que nos permite tener en cuenta la variabilidad de las predicciones.\n\n\n13.1.2 Bagging por lotes\nEn el bagging por lotes en lugar de entrenar diferentes algoritmos sobre el conjunto completo de datos de entrenamiento y promediar sus resultados como hace el voto por mayoría, este método entrena un único o múltiples clasificadores/regresores en diferentes subconjuntos o submuestras de los datos de entrenamiento y agrega los resultados en todos los subconjuntos de forma similar al voto por mayoría.\nEste método de bagging por lotes resulta de gran utilidad para reducir el problema de sobreajuste cuando trabajamos con modelos complejos como los proporcionados por los árboles de decisión. Estos modelos basados en árboles son los que veremos más tarde bajo el nombre de bosques aleatorios.\nDada la naturaleza de construcción de los estimadores de bagging por lotes donde usamos submuestras con reemplazamiento resulta posible estimar el error de test sin necesidad de recurrir a métodos de validación cruzada. Las submuestras que son utilizadas sobre cada modelo de aprendizaje se conocen como “in of bag” mientras que las que no son utilizadas se conocen como “out of bag”. Estas últimas son utilizadas como si fuera una muestra de validación para valorar el error cometido dentro de cada uno de los modelos de aprendizaje que conforman el modelo conjunto. Definimos así el OOB error asociado con cada uno de los modelos individuales a partir de esa muestra. Dos limitaciones de este proceso son:\n\nEl Out-of-Bag Error no es adecuado cuando las observaciones tienen una relación temporal (series temporales). Como la selección de las observaciones que participan en cada entrenamiento es aleatoria, no respetan el orden temporal y se estaría introduciendo información a futuro.\nEl preprocesado de los datos de entrenamiento se hace de forma conjunta, por lo que las observaciones out-of-bag pueden sufrir “data leakage”, es decir utilizan información de la muestra de entrenamiento para el ajuste del modelo. De ser así, las estimaciones del OOB-error son demasiado optimistas.\n\nEn el método de bagging por lotes resulta necesario también establecer ciertos hiperparámetros asociados con la forma de selección de las submuestras, el proceso de reemplazamiento y los correspondientes al modelo de aprendizaje de cada submuestra. En la descripción de cada uno de esos algoritmos que veremos más adelante se presentarán las características propias de cada uno de ellos. Por el momento incluiremos los correspondientes a la selección de submuestras."
  },
  {
    "objectID": "130_Ensemblemodels.html#sec-130.2",
    "href": "130_Ensemblemodels.html#sec-130.2",
    "title": "13  Modelos de conjunto (Ensemble models)",
    "section": "13.2 Modelos básicos Bagging en mlr3",
    "text": "13.2 Modelos básicos Bagging en mlr3\nAunque la librería mlr3 nos permite programar manualmente un algoritmo de bagging, lo habitual es utilizar los pipelines pre-construidos para bagging que están disponibles con ppl(\"bagging\"). Para configurar estos pipelines debemos añadir como parámetros:\n\nEl learner o graphlearner que utilizaremos en el proceso de bagging.\nEl número de iteraciones o repeticiones que usaremos durante el proceso de bagging, identificado con el parámetro iterations.\nLa proporción de muestras en el conjunto de entrenamiento, identificado con el parámetro frac.\nEl PipeOp que utilizamos para promediar los resultados, identificado con el parámetro averager. Las opciones por defecto son:\n\npo(\"classifavg\", collect_multiplicity = TRUE)) para el voto por mayoría (problemas de clasificación) guardando las predicciones individuales de cada modelo.\npo(\"regravg\", collect_multiplicity = TRUE)) para el valor promedio de todos los modleos (problemas de regresión) guardando las predicciones individuales de cada modelo.\n\n\nEl resultado de esta función es un graphlearner con los métodos asociados habituales.\nAntes de presentar los bancos de datos que utilizaremos para mostrar el uso de los modelos de bagging más básicos, cargamos todas las librerías necesarias.\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(cvms)\nlibrary(kknn)\nlibrary(rpart.plot)\ntheme_set(theme_sjplot2())\n\n# Paquetes AA\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3tuningspaces)\n\n\n13.2.1 Bancos de datos\nPara ejemplificar el uso de los modelos de bagging básicos vamos a utilizar tres bancos de datos: Stroke, Water Potability, y Housing in California que se pueden consultar en el tema 4. De los tres con el único con el que no hemos trabajado hasta ahora es Water Potability. A continuación se muestra el código necesario para la carga de cada uno de esos bancos de datos, y la creación de la tarea correspondiente. Los dos primeros corresponden a problemas de clasificación mientras que el último se corresponde con un problema de regresión.\n\n13.2.1.1 Stroke\nEl código para este banco de datos es:\n\n# Leemos datos\nstroke = read_rds(\"stroke.rds\")\n# Eliminamos la variable id\nstroke = stroke %>% dplyr::select(-id)\n# creamos la tarea\ntsk_stroke = as_task_classif(stroke, target = \"stroke\")\n# Generamos variable de estrato\ntsk_stroke$col_roles$stratum <- \"stroke\"\n\n\n\n13.2.1.2 Water Potability\nEl agua potable es el derecho humano más básico y un factor importante para la salud. El conjunto de datos Water potability, tiene por objetivo estudiar la potabilidad del agua utilizando varias propiedades químicas debido a su importancia como cuestión de salud y desarrollo a nivel nacional, regional y local. En algunas regiones, se ha demostrado que las inversiones en abastecimiento de agua y saneamiento pueden producir un beneficio económico neto, ya que la reducción de los efectos adversos para la salud y los costes de la atención sanitaria superan los costes de las intervenciones. EL target de interés potability indica si la muestra de agua es potable o no en función de: pH, Hardness, Solids, Chloramines, Sulfate, Conductivity, Organic_carbon, Trihalomethanes, y Turbidity. Este banco de datos contiene valores perdidos en diferentes variables.\n\n# Leemos datos\nwaterpot = read_rds(\"waterpot.rds\")\n# creamos la tarea\ntsk_water = as_task_classif(waterpot, target = \"Potability\")\n# Generamos variable de estrato\ntsk_water$col_roles$stratum <- \"Potability\"\n\nValoramos la presencia de missings\n\ntsk_water$missings()\n\n     Potability     Chloramines    Conductivity        Hardness  Organic_carbon \n              0               0               0               0               0 \n         Solids         Sulfate Trihalomethanes       Turbidity              ph \n              0             781             162               0             491 \n\n\nAparecen valores perdidos en tres de las posibles predictoras. Representamos los datos:\n\nautoplot(tsk_water, type = \"pairs\")\n\n\n\n\n\n\n13.2.1.3 Housing in California\nCargamos los datos correspondientes:\n\n# Carga de datos\nhousingCA = read_rds(\"housingCA.rds\")\n# Creación de task\ntsk_housing = as_task_regr(housingCA, target = \"median_house_value\")\n\n\n\n\n13.2.2 Modelos\nElaboramos nuestros primeros modelos de bagging para los diferentes bancos de datos. Utilizamos el voto por mayoría y el promedio de predicciones para resolver las tareas de clasificación y regresión.\n\n13.2.2.1 Stroke\nPara el proceso de bagging hemos de definir en primer caso el algoritmo de aprendizaje asociado. Utilizamos como base los modelos de árboles de decisión vistos en el tema anterior.\n\n# Preprocesamiento\npp_stroke =  po(\"imputemedian\", affect_columns = selector_type(\"numeric\"))\n# Modelo de aprendizaje combinando preprocesado y algoritmo\ndt_classif_stroke = as_learner(pp_stroke %>>% \n                                  lrn(\"classif.rpart\", keep_model = TRUE, predict_type = \"prob\", \n                                      cp = 0.0003562633,\n                                      minsplit = 10, \n                                      maxdepth = 6))\ndt_classif_stroke$id = \"TreeDecision\"\n\nDefinimos ahora el grpahlearner asociado con el proceso de bagging utilizando la función ppl y comparamos con el resultado de un único árbol de decisión. Realizamos un proceso de validación cruzada con cinco repeticiones para obtener resultados más estables:\n\nset.seed(345)\n# Graphleaner para bagging\nbgg_stroke = ppl(\"bagging\", dt_classif_stroke,\n  iterations = 10, frac = 0.8, averager = po(\"classifavg\", collect_multiplicity = TRUE))\nbgg_stroke = as_learner(bgg_stroke)\nbgg_stroke$id = \"Bagging\"\n# Comparación de modelos simple y ponderado\nlearners = c(dt_classif_stroke, bgg_stroke)\nbmr = benchmark(benchmark_grid(tsk_stroke, learners,\n  rsmp(\"cv\", folds = 5)))\n\nINFO  [17:55:56.328] [mlr3] Running benchmark with 10 resampling iterations\nINFO  [17:55:56.461] [mlr3] Applying learner 'TreeDecision' on task 'stroke' (iter 1/5)\nINFO  [17:55:56.670] [mlr3] Applying learner 'TreeDecision' on task 'stroke' (iter 2/5)\nINFO  [17:55:56.866] [mlr3] Applying learner 'TreeDecision' on task 'stroke' (iter 3/5)\nINFO  [17:55:57.052] [mlr3] Applying learner 'TreeDecision' on task 'stroke' (iter 4/5)\nINFO  [17:55:57.193] [mlr3] Applying learner 'TreeDecision' on task 'stroke' (iter 5/5)\nINFO  [17:55:57.362] [mlr3] Applying learner 'Bagging' on task 'stroke' (iter 1/5)\nINFO  [17:55:59.388] [mlr3] Applying learner 'Bagging' on task 'stroke' (iter 2/5)\nINFO  [17:56:01.161] [mlr3] Applying learner 'Bagging' on task 'stroke' (iter 3/5)\nINFO  [17:56:02.705] [mlr3] Applying learner 'Bagging' on task 'stroke' (iter 4/5)\nINFO  [17:56:04.235] [mlr3] Applying learner 'Bagging' on task 'stroke' (iter 5/5)\nINFO  [17:56:05.830] [mlr3] Finished benchmark\n\n# Resultados individuales\nbmr$score(msr(\"classif.bacc\"))\n\n    nr task_id   learner_id resampling_id iteration classif.bacc\n 1:  1  stroke TreeDecision            cv         1    0.4989723\n 2:  1  stroke TreeDecision            cv         2    0.5069136\n 3:  1  stroke TreeDecision            cv         3    0.5158848\n 4:  1  stroke TreeDecision            cv         4    0.5448560\n 5:  1  stroke TreeDecision            cv         5    0.5091753\n 6:  2  stroke      Bagging            cv         1    0.4994861\n 7:  2  stroke      Bagging            cv         2    0.5079424\n 8:  2  stroke      Bagging            cv         3    0.5000000\n 9:  2  stroke      Bagging            cv         4    0.4994856\n10:  2  stroke      Bagging            cv         5    0.4994856\nHidden columns: uhash, task, learner, resampling, prediction\n\n# Resultados agregados\nbmr$aggregate(msr(\"classif.bacc\"))\n\n   nr task_id   learner_id resampling_id iters classif.bacc\n1:  1  stroke TreeDecision            cv     5    0.5151604\n2:  2  stroke      Bagging            cv     5    0.5012799\nHidden columns: resample_result\n\n\nComo se puede ver en los resultados obtenidos el modelo de conjunto proporciona casi los mismo resultados que el modelo individual. En este caso la ponderación no mejora ya que el modelo individual también era bastante malo. Veamos gráficamente al comparación entre lo scores del modelo basal y del ponderado.\n\nautoplot(bmr, measure = msr(\"classif.bacc\"))\n\n\n\n\nPodemos obtener los resultados del modelo bagging de forma similar a otros modelos de aprendizaje automático:\n\n# Entrenamos el modelo\nbgg_stroke$train(tsk_stroke)\n# Construimos la predicción\npred = bgg_stroke$predict(tsk_stroke)\npred\n\n<PredictionClassif> for 5110 observations:\n    row_ids truth response   prob.No    prob.Yes\n          1   Yes       No 0.7080569 0.291943082\n          2   Yes       No 0.8683200 0.131680027\n          3   Yes       No 0.8566943 0.143305654\n---                                             \n       5108    No       No 0.9924404 0.007559553\n       5109    No       No 0.9187222 0.081277750\n       5110    No       No 0.9924404 0.007559553\n\n\nEvaluamos la matriz de confusión:\n\ncm = confusion_matrix(pred$truth, pred$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\n\n\n13.2.2.2 Water Potability\nEn este caso vamos a utilizar como modelo base de aprendizaje un modelo de regresión logística donde estamos interesados en conocer la probabilidad de que una muestra se clasifique como agua potable en función de las predictoras consideradas. Dado que todas las predictoras son numéricas (y relacionadas entre si como hemos visto en la interpretación gráfica de los datos) y algunas contienen valores perdidos consideramos un modelo penalizado (en este caso elastic net con peso igual a 0.5) y el correspondiente preprocesado. En primer lugar generamos la tarea de nuevo identificando la categoría de interés:\n\n# creamos la tarea\ntsk_water = as_task_classif(waterpot, target = \"Potability\", positive = \"1\")\n# Generamos variable de estrato\ntsk_water$col_roles$stratum <- \"Potability\"\n\n\n# Preprocesado\npp_water = po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) %>>% \n  po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) \n# Definimos learner basal\nlearner = lrn(\"classif.cv_glmnet\", type.logistic = \"Newton\", standardize = FALSE,\n              alpha = 0.5, predict_type = \"prob\")\n# Graphlearner: Preprocesado y learner\nlogm_classif_water = as_learner(pp_water %>>% learner)\nlogm_classif_water$id = \"LogisticReg\"\n\nConstruíos el modelo de bagging y vemos los resultados obtenidos:\n\nset.seed(345)\n# Graphleaner para bagging\nbgg_water = ppl(\"bagging\", logm_classif_water,\n  iterations = 10, frac = 0.8, averager = po(\"classifavg\", collect_multiplicity = TRUE))\nbgg_water = as_learner(bgg_water)\nbgg_water$id = \"Bagging\"\n# Comparación de modelos simple y ponderado\nlearners = c(logm_classif_water, bgg_water)\nbmr = benchmark(benchmark_grid(tsk_water, learners,\n  rsmp(\"cv\", folds = 5)))\n\nINFO  [17:56:09.782] [mlr3] Running benchmark with 10 resampling iterations\nINFO  [17:56:09.788] [mlr3] Applying learner 'LogisticReg' on task 'waterpot' (iter 1/5)\nINFO  [17:56:10.247] [mlr3] Applying learner 'LogisticReg' on task 'waterpot' (iter 2/5)\nINFO  [17:56:10.644] [mlr3] Applying learner 'LogisticReg' on task 'waterpot' (iter 3/5)\nINFO  [17:56:11.007] [mlr3] Applying learner 'LogisticReg' on task 'waterpot' (iter 4/5)\nINFO  [17:56:11.369] [mlr3] Applying learner 'LogisticReg' on task 'waterpot' (iter 5/5)\nINFO  [17:56:11.740] [mlr3] Applying learner 'Bagging' on task 'waterpot' (iter 1/5)\nINFO  [17:56:15.812] [mlr3] Applying learner 'Bagging' on task 'waterpot' (iter 2/5)\nINFO  [17:56:19.856] [mlr3] Applying learner 'Bagging' on task 'waterpot' (iter 3/5)\nINFO  [17:56:23.724] [mlr3] Applying learner 'Bagging' on task 'waterpot' (iter 4/5)\nINFO  [17:56:27.592] [mlr3] Applying learner 'Bagging' on task 'waterpot' (iter 5/5)\nINFO  [17:56:31.640] [mlr3] Finished benchmark\n\n# Resultados individuales\nbmr$score(msr(\"classif.bacc\"))\n\n    nr  task_id  learner_id resampling_id iteration classif.bacc\n 1:  1 waterpot LogisticReg            cv         1          0.5\n 2:  1 waterpot LogisticReg            cv         2          0.5\n 3:  1 waterpot LogisticReg            cv         3          0.5\n 4:  1 waterpot LogisticReg            cv         4          0.5\n 5:  1 waterpot LogisticReg            cv         5          0.5\n 6:  2 waterpot     Bagging            cv         1          0.5\n 7:  2 waterpot     Bagging            cv         2          0.5\n 8:  2 waterpot     Bagging            cv         3          0.5\n 9:  2 waterpot     Bagging            cv         4          0.5\n10:  2 waterpot     Bagging            cv         5          0.5\nHidden columns: uhash, task, learner, resampling, prediction\n\n# Resultados agregados\nbmr$aggregate(msr(\"classif.bacc\"))\n\n   nr  task_id  learner_id resampling_id iters classif.bacc\n1:  1 waterpot LogisticReg            cv     5          0.5\n2:  2 waterpot     Bagging            cv     5          0.5\nHidden columns: resample_result\n\n\nPodemos ver que el comportamiento con ambas modelizaciones nos proporciona el mismo resultado. La solución es bastante estable aunque bastante deficiente. Analizamos con detalle el modelo bagging que hemos construido:\n\n# Entrenamos el modelo\nbgg_water$train(tsk_water)\n# Construimos la predicción\npred = bgg_water$predict(tsk_water)\n# matriz de confusión\ncm = confusion_matrix(pred$truth, pred$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nSe ve claramente el mal funcionamiento del modelo ya que clasifica todas las muestras como no potable, a pesar de que el número de muestras que originalmente eran potables es bastante elevado.\n\n\n13.2.2.3 Housing in California\nEn este caso nos enfrentamos a un problema de regresión, pero en lugar de utilizar un modelo lineal como modelo basal vamos a utilizar un árbol de decisión con las configuraciones por defecto. En primer lugar configuramos el modelo basal.\n\n# Preprocesamiento\npp_housing = \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) \n# Modelo de aprendizaje combinando preprocesado y algoritmo\ndt_regr_housing = as_learner(pp_housing %>>% \n                                  lrn(\"regr.rpart\", keep_model = TRUE))\ndt_regr_housing$id = \"TreeDecision\"\n\nEstablecemos ahora el modelo bagging\n\nset.seed(345)\n# Graphleaner para bagging\nbgg_housing = ppl(\"bagging\", dt_regr_housing,\n  iterations = 10, frac = 0.8, averager = po(\"regravg\", collect_multiplicity = TRUE))\nbgg_housing = as_learner(bgg_housing)\nbgg_housing$id = \"Bagging\"\n# Comparación de modelos simple y ponderado\nlearners = c(dt_regr_housing, bgg_housing)\nbmr = benchmark(benchmark_grid(tsk_housing, learners,\n  rsmp(\"cv\", folds = 5)))\n\nINFO  [17:56:37.895] [mlr3] Running benchmark with 10 resampling iterations\nINFO  [17:56:37.902] [mlr3] Applying learner 'TreeDecision' on task 'housingCA' (iter 1/5)\nINFO  [17:56:38.256] [mlr3] Applying learner 'TreeDecision' on task 'housingCA' (iter 2/5)\nINFO  [17:56:38.631] [mlr3] Applying learner 'TreeDecision' on task 'housingCA' (iter 3/5)\nINFO  [17:56:39.014] [mlr3] Applying learner 'TreeDecision' on task 'housingCA' (iter 4/5)\nINFO  [17:56:39.456] [mlr3] Applying learner 'TreeDecision' on task 'housingCA' (iter 5/5)\nINFO  [17:56:39.800] [mlr3] Applying learner 'Bagging' on task 'housingCA' (iter 1/5)\nINFO  [17:56:43.281] [mlr3] Applying learner 'Bagging' on task 'housingCA' (iter 2/5)\nINFO  [17:56:46.854] [mlr3] Applying learner 'Bagging' on task 'housingCA' (iter 3/5)\nINFO  [17:56:50.040] [mlr3] Applying learner 'Bagging' on task 'housingCA' (iter 4/5)\nINFO  [17:56:53.265] [mlr3] Applying learner 'Bagging' on task 'housingCA' (iter 5/5)\nINFO  [17:56:56.952] [mlr3] Finished benchmark\n\n# Resultados individuales\nbmr$score(msr(\"regr.smape\"))\n\n    nr   task_id   learner_id resampling_id iteration regr.smape\n 1:  1 housingCA TreeDecision            cv         1  0.2744759\n 2:  1 housingCA TreeDecision            cv         2  0.2768960\n 3:  1 housingCA TreeDecision            cv         3  0.2748795\n 4:  1 housingCA TreeDecision            cv         4  0.2739820\n 5:  1 housingCA TreeDecision            cv         5  0.2774013\n 6:  2 housingCA      Bagging            cv         1  0.2690379\n 7:  2 housingCA      Bagging            cv         2  0.2753205\n 8:  2 housingCA      Bagging            cv         3  0.2705382\n 9:  2 housingCA      Bagging            cv         4  0.2717273\n10:  2 housingCA      Bagging            cv         5  0.2735402\nHidden columns: uhash, task, learner, resampling, prediction\n\n# Resultados agregados\nbmr$aggregate(msr(\"regr.smape\"))\n\n   nr   task_id   learner_id resampling_id iters regr.smape\n1:  1 housingCA TreeDecision            cv     5  0.2755269\n2:  2 housingCA      Bagging            cv     5  0.2720328\nHidden columns: resample_result\n\n\nAunque los resultados son muy similares para ambas modelizaciones, podemos ver que el sMAPE es algo inferior para el modelo bagging que para el modelo basal. Mejoramos, aunque muy levemente, la capacidad explicativa de nuestro modelo. En este caso también podríamos evaluar la capacidad del modelo mediante la representación gráfica de los valores observados frente a los predichos por el modelo.\n\n# Entrenamiento\nbgg_housing$train(tsk_housing)\n# Predicción\npred = bgg_housing$predict(tsk_housing)\n# Gráfico\nautoplot(pred, type = \"xy\") + labs(title = \"Observados vs predichos\")\n\n\n\n\nClaramente la nube de puntos es muy dispersa y el modelo no es capaz de ajustar correctamente.\nLos modelos de bagging tienden a producir soluciones muy similares a los de los modelos de base utilizados, dado que siempre se utilizan todas las observaciones y todas las predictoras en cada iteración de la ponderación. Sin embargo, se pueden construir otro tipo de modelos de bagging en los que en cada iteración se puede usar un conjunto de entrenamiento diferente y un conjunto de predictoras, en lugar de todas ellas. Estos son los modelos de bosques aleatorios que pasamos a describir a continuación."
  },
  {
    "objectID": "130_Ensemblemodels.html#sec-130.3",
    "href": "130_Ensemblemodels.html#sec-130.3",
    "title": "13  Modelos de conjunto (Ensemble models)",
    "section": "13.3 Bosques aleatorios (Random Forests)",
    "text": "13.3 Bosques aleatorios (Random Forests)\nUn modelo de bosque aleatorio está formado por un conjunto de árboles de decisión individuales, cada uno entrenado con una muestra ligeramente distinta de los datos de entrenamiento generada mediante bootstrapping. La predicción de una nueva observación se obtiene agregando las predicciones de todos los árboles individuales que forman el modelo.\nMuchos métodos predictivos generan modelos globales en los que disponemos de una única ecuación o modelo de predicción. Sin embargo, en situaciones complejas con múltiples predictores, que interaccionan entre ellos de forma compleja y no lineal, es muy difícil encontrar un modelo predictivo lo suficientemente preciso. Como ya hemos visto anteriormente los árboles de decisión nos permiten obtener un modelo con el que podemos manejar de forma sencilla relaciones complejas entre las posibles predictoras\nAhora, como ya hemos visto, la utilización de los árboles de decisión no está exenta de dificultades y por ese motivo se introduce aquí el algoritmo de bosque aleatorio que es un método de conjunto bagging que nos permite mejorar la capacidad predictiva de los árboles de decisión individuales.\nEntre las ventajas del uso de este tipo de algoritmo podemos destacar:\n\nSon capaces de seleccionar predictores de forma automática.\nPueden aplicarse a problemas de regresión y clasificación.\nAl tratarse de métodos no paramétricos no es necesario que se cumpla ningún tipo de distribución específica.\nPor lo general, requieren mucha menos limpieza y preprocesado de los datos en comparación a otros métodos de aprendizaje estadístico (por ejemplo, no requieren estandarización).\nNo se ven muy influenciados por outliers.\nSon muy útiles en la exploración de datos, permiten identificar de forma rápida y eficiente las variables (predictores) más importantes.\nGracias al Out-of-Bag Error puede estimarse su error de validación sin necesidad de recurrir a estrategias computacionalmente costosas como la validación cruzada.\n\nEntre las desventajas podemos destacar:\n\nAl combinar múltiples árboles, se pierde la interpretabilidad que tienen los modelos basados en un único árbol.\nCuando tratan con predictores continuos pierden parte de su información al categorizarlas en el momento de la división de los nodos.\nPor la forma de construcción de los árboles de decisión los predictores continuos o predictores cualitativos con muchos niveles tienen mayor probabilidad de contener, solo por azar, algún punto de corte óptimo, por lo que suelen verse favorecidos en la creación de los árboles.\nNo son capaces de extrapolar fuera del rango de los predictores observados en los datos de entrenamiento.\n\n\n13.3.1 Algortimo Bosques aleatorios\nAntes de presentar el algoritmo específico del bosque aleatorio es necesario conocer como funciona el proceso de bagging para un único árbol de decisión. Dicho algoritmo se organiza en tres pasos:\n\nGenerar 𝐵 pseudo-training sets mediante bootstrapping a partir de la muestra de entrenamiento original.\nEntrenar un árbol con cada una de las 𝐵 muestras del paso 1. Cada árbol se crea sin apenas restricciones y no se somete a pruning, por lo que tiene varianza alta pero poco sesgo. En la mayoría de casos, la única regla de parada es el número mínimo de observaciones que deben tener los nodos terminales. El valor óptimo de este hiperparámetro puede obtenerse comparando el out of bag error o mediante validación cruzada.\nPara cada una de la muestras de validación, se obtiene la predicción en cada uno de los 𝐵 árboles. El valor final de la predicción se obtiene como la media de las 𝐵 predicciones en el caso de variables cuantitativas y como la clase predicha más frecuente (moda) para variables cualitativas.\n\nEn el algoritmo descrito, el número de árboles creados no es un hiperparámetro crítico en cuanto a que, por mucho que se incremente el número, no se aumenta el riesgo de overfitting. Alcanzado un determinado número de árboles, la reducción del test error se estabiliza. A pesar de ello, cada árbol ocupa memoria, por lo que no conviene almacenar más de los necesarios.\nEl algoritmo de Random Forest es una modificación del proceso de bagging anterior que consigue mejorar los resultados gracias a que considera árboles lo más independientes posibles.\nSupóngase un conjunto de datos en el que hay un predictor muy influyente, junto con otros moderadamente influyentes. En este escenario, todos o casi todos los árboles creados en el proceso de bagging estarán dominados por el mismo predictor y serán muy parecidos entre ellos. Como consecuencia de la alta correlación entre los árboles, el proceso de bagging apenas conseguirá disminuir la varianza y, por lo tanto, tampoco mejorar el modelo. Random forest evita este problema haciendo una selección aleatoria de \\(𝑚\\) predictores antes de evaluar cada división. De esta forma, un promedio de \\((𝑝−𝑚)/𝑝\\) divisiones no contemplará el predictor influyente, permitiendo que otros predictores puedan ser seleccionados. Añadiendo este paso extra se consigue descorrelacionar los árboles todavía más, con lo que su agregación consigue una mayor reducción de la varianza. Algunas recomendaciones para la selección de \\(m\\) son:\n\nLa raíz cuadrada del número total de predictores para problemas de clasificación:\n\n\\[m \\approx \\sqrt{p}\\]\n\nUn tercio del número de predictores para problemas de regresión:\n\n\\[m \\approx p/3\\]\n\nSi los predictores están muy correlacionados, valores pequeños de \\(𝑚\\) consiguen mejores resultados.\n\n\n\n13.3.2 Predicción mediante bosque aleatorio\nPara realizar la predicción de un bosque aleatorio utilizamos el principio de bagging, de forma que, una vez determinamos el nodo terminal al que es asignada la observación a predecir en cada uno de los árboles, utilizamos las observaciones contenidas en dicho nodo terminal para la predicción individual de cada uno de ellos. Si estamos en un modelo de regresión obtenemos la media de todas las observaciones del nodo terminal en cada árbol, mientras que si estamos en un problema de clasificación actuamos mediante el voto por mayoría.\nUna vez obtenemos las predicciones individuales la predicción conjunta se obtiene a partir de la media o de la categoría más frecuente de todas ellas en función de que estemos en un problema de predicción o clasificación.\nSin embargo, en los problemas de regresión la predicción de un árbol de regresión puede verse como una variante de vecinos cercanos en la que, solo las observaciones que forman parte del mismo nodo terminal que la observación predicha tienen influencia. Siguiendo esta aproximación, la predicción del árbol se define como la media ponderada de todas las observaciones de entrenamiento, donde el peso de cada observación depende únicamente de si forma parte o no del mismo nodo terminal, es decir, definimos los pesos del árbol j como un vector de \\(n\\) componentes donde cada una de las componentes toma el valor \\(w_j = 1/n_j\\) si la observación pertenece al nodo terminal \\(j\\) con \\(n_j\\) observaciones, y 0 en otro caso. Para el bosque aleatorio esto equivale a la media ponderada de todas las observaciones, empleando como pesos la media de los vectores de pesos de los \\(M\\) árboles considerados, es decir,\n\\[\\hat{w}=\\frac{\\sum_{i=1}^{M}  w_i}{M}\\]\n\\[y_{pred} = \\sum_{i=1}^n \\hat{w}_i y_i\\]\n\n\n13.3.3 Importancia de los predictores\nSi bien es cierto que el bosque aleatorio consigue mejorar la capacidad predictiva en comparación a los modelos basados en un único árbol, esto tiene un coste asociado, la interpretabilidad del modelo se reduce. Al tratarse de una combinación de múltiples árboles, no es posible obtener una representación gráfica sencilla del modelo y no es inmediato identificar de forma visual que predictores son más importantes. Sin embargo, se han desarrollado nuevas estrategias para cuantificar la importancia de los predictores que hacen de los modelos de bosque aleatorio una herramienta muy potente, no solo para predecir, sino también para el análisis exploratorio. Dos de estas medidas son: importancia por permutación e impureza de nodos.\n\n13.3.3.1 Importancia por permutación\nIdentifica la influencia que tiene cada predictor sobre una determinada métrica de evaluación del modelo (estimada por out-of-bag error o validación cruzada). El valor asociado con cada predictor se obtiene de la siguiente forma:\n\nCrear el conjunto de árboles que forman el modelo.\nCalcular una determinada métrica de error (mse, classification error, …). Este es el valor de referencia (\\(𝑒𝑟𝑟_0\\)).\nPara cada predictor \\(𝑗\\):\n\n\nPermutar en todos los árboles del modelo los valores del predictor \\(𝑗\\) manteniendo el resto constante.\nRecalcular la métrica tras la permutación, llámese (\\(𝑒𝑟𝑟_𝑗\\)).\nCalcular el incremento en la métrica debido a la permutación del predictor \\(𝑗\\)\n\n\\[\\%I_𝑗=100*\\frac{err_j-err_0}{err_0}\\]\nSi el predictor permutado estaba contribuyendo al modelo, es de esperar que el modelo aumente su error, ya que se pierde la información que proporcionaba esa variable. El porcentaje en que se incrementa el error debido a la permutación del predictor \\(𝑗\\) puede interpretarse como la influencia que tiene \\(𝑗\\) sobre el modelo. Algo que suele llevar a confusiones es el hecho de que este incremento puede resultar negativo. Si la variable no contribuye al modelo, es posible que, al reorganizarla aleatoriamente, solo por azar, se consiga mejorar ligeramente el modelo, por lo que \\((𝑒𝑟𝑟_𝑗−𝑒𝑟𝑟_0)\\) es negativo. A modo general, se puede considerar que estas variables tienen una importancia próxima a cero.\nAunque esta estrategia suele ser la más recomendada, cabe tomar algunas precauciones en su interpretación. Lo que cuantifican es la influencia que tienen los predictores sobre el modelo, no su relación con la variable respuesta. ¿Por qué es esto tan importante? Supóngase un escenario en el que se emplea esta estrategia con la finalidad de identificar qué predictores están relacionados con el peso de una persona, y que dos de los predictores son: el índice de masa corporal (IMC) y la altura. Como IMC y altura están muy correlacionados entre sí (la información que aportan es redundante), cuando se permute uno de ellos, el impacto en el modelo será mínimo, ya que el otro aporta la misma información. Como resultado, estos predictores aparecerán como poco influyentes aun cuando realmente están muy relacionados con la variable respuesta. Una forma de evitar problemas de este tipo es, siempre que se excluyan predictores de un modelo, comprobar el impacto que tiene en su capacidad predictiva.\n\n\n13.3.3.2 Incremento de la pureza de los nodos\nCuantifica el incremento total en la pureza de los nodos debido a divisiones en las que participa el predictor (promedio de todos los árboles). La forma de calcularlo es la siguiente: en cada división de los árboles, se registra el descenso conseguido en la medida empleada como criterio de división (índice Gini, MSE, entropía, …). Para cada uno de los predictores, se calcula el descenso medio conseguido en el conjunto de árboles que forman el conjunto. Cuanto mayor sea este valor medio, mayor la contribución del predictor en el modelo.\n\n\n\n13.3.4 Hiperparámetros relevantes en el bosque aleatorio\nDel conjunto de hiperparámetros que se pueden modificar en el bosque aleatorio los dos más interesantes son el número de árboles considerados y el número máximo de predictoras usadas en la construcción de cada árbol.\nLo habitual es proceder de forma individual estudiando la influencia de cada uno de los hiperparámetros respecto de la capacidad predictiva del modelo utilizando el out of bag score (aunque se puede configurar el algoritmo para utilizar otra). Esas curvas de influencia nos permiten determinar la evolución del error del modelo con respecto a ese hiperparámetro y obtener así el conjunto óptimo de valores.\nSin embargo, aunque el análisis individual de los hiperparámetros es útil para entender su impacto en el modelo e identificar rangos de interés, la búsqueda final no debe hacerse de forma secuencial, ya que cada hiperparámetro interacciona con los demás. Es preferible recurrir a grid search o random search para analizar varias combinaciones de hiperparámetros. Los dos métodos más habituales son el grid search basado en el out of bag o el grid search basado en validación cruzada.\n\n\n13.3.5 Codificación de predictoras cualitativas\nLos modelos basados en árboles de decisión, entre ellos Random Forest, son capaces de utilizar predictores categóricos en su forma natural sin necesidad de convertirlos en variables dummy mediante one hot encoding. Sin embargo, en la práctica, depende de la implementación que tenga la librería o software utilizado. Esto tiene impacto directo en la estructura de los árboles generados y, en consecuencia, en los resultados predictivos del modelo y en la importancia calculada para los predictores.\nEntre las dificultades más relevantes al utilizar one hot encoding se pueden destacar:\n\nEl entrenamiento de los modelos es más costoso cuando se aplica one hot encoding debido al aumento de dimensionalidad al crear las nuevas variables dummy, obligando a que el algoritmo tenga que analizar muchos más puntos de división.\nAl convertir una variable categórica en múltiples variables dummy su importancia queda diluida, dificultando que el modelo pueda aprender de ella y perdiendo así capacidad predictiva. Este efecto es mayor cuantos más niveles tiene la variable original.\nAl diluir la importancia de los predictores categóricos, estos tienen menos probabilidad de ser seleccionados por el modelo, lo que desvirtúa las métricas que miden la importancia de los predictores.\n\nPor el momento, en Scikit-Learn es necesario hacer one hot encoding para convertir las variables categóricas en variables dummy si deseamos usar random forest. La implementación de H2O sí permite utilizar directamente variables categóricas."
  },
  {
    "objectID": "130_Ensemblemodels.html#sec-130.4",
    "href": "130_Ensemblemodels.html#sec-130.4",
    "title": "13  Modelos de conjunto (Ensemble models)",
    "section": "13.4 Bosque aleatorio en mlr3",
    "text": "13.4 Bosque aleatorio en mlr3\nLos algoritmos básicos de bosques aleatorios que podemos encontrar en mlr3:\n\nregr.ranger para abordar tareas de regresión.\nclassif.ranger para abordar tareas de clasificación.\n\nEstos algoritmos son una implementación rápida de bosques aleatorios o partición recursiva, particularmente adecuada para datos de alta dimensión.\nOtros algoritmos disponibles en la librería mlr3extralearners son:\n\nclassif.randomForest y regr.randomForest, para tareas de clasificación y regresión.\nclassif.rfsrc y regr.rfsrc, utilizando programación en paralelo, y que se pueden utilizar tanto en problemas de clasificación, regresión, supervivencia, y otros más.\nclassif.cforest y regr.cforest, que son algoritmos para la partición recursiva basada en modelos que produce un árbol con modelos ajustados asociados con cada nodo terminal.\n\nEn este tema nosotros nos centramos en los modelos básicos de bosques aleatorios cuyos hiperparámetros más relevantes son:\n\nimportance: Modo de importancia utilizado en la construcción del bosque aleatorio “none”, “impurity”, “impurity_corrected”, “permutation”. La medida de ‘impureza’ es el índice de Gini para la clasificación, la varianza de las respuestas para la regresión.\nmax_depth: profundidad máxima del árbol. Un valor de 0 corresponde a un árbol sin limitaciones.\nmin.node.size: Número de observaciones mínimo en los nodos terminales.\nmtry: Número de variables a considerar en la división de cada nodo. El valor predeterminado es la raíz cuadrada (redondeada hacia abajo) de las variables numéricas. Alternativamente, una función de un solo argumento devuelve un número entero, dado el número de variables independientes.\nmtry.ratio: Proporción de variables a considerar en la división de cada nodo que toma valores en el intervalo \\([0,1]\\).\nsplitrule: regla de división utilizada. Para tareas de clasificación se considera gini (valor por defecto), extratrees, y hellinger. Para tareas de regresión se considera variance (valor por defecto), extratrees, max-stat, y beta.\nsample.fraction: Fracción de observaciones para la muestra. El valor predeterminado es 1 para muestreo con reemplazo.\nnum.trees: número de árboles considerados en la construcción del bosque aleatorio. El valor por defecto es 500.\noob.error: condición lógica que indica se se debe calcular el error de predicción oob.\n\nEn los puntos siguientes analizamos los modelos básicos de bosques aleatorios para cada uno de nuestros problemas, y finalizaremos con la optimización de parámetros para alcanzar el mejor modelo posible. Se deja para el lector la modelización con otros algoritmos de bosques aleatorios.\n\n13.4.1 Modelos de bosques aleatorios\nComo en el caso de árboles aleatorios no resulta necesario estandarizar las variables numéricas pero si es necesario imputar los valores perdidos en la tarea de preprocesamiento. Sin embargo, para que los resultados sean comparables con los obtenidos en el punto anterior vamos a realizar todo el preprocesamiento.\n\n13.4.1.1 Breast Cancer Wisconsin\nDefinimos el algoritmo de aprendizaje asociado así como las tareas de preprocesamiento.\n\n# Preprocesamiento\npp_stroke =  po(\"imputemedian\", affect_columns = selector_type(\"numeric\"))\n# Modelo de aprendizaje combinando preprocesado y algoritmo\nrf_classif_stroke = as_learner(pp_stroke %>>% \n                                  lrn(\"classif.ranger\", importance = \"impurity\"))\nrf_classif_stroke$id = \"RandomForest\"\n\nComenzamos con le entrenamiento del modelo definiendo en primer lugar las muestras de entrenamiento y validación:\n\n# División de muestras\nset.seed(432)\n# Creamos la partición\nsplits = mlr3::partition(tsk_stroke, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_stroke = tsk_stroke$clone()$filter(splits$train)\ntsk_test_stroke  = tsk_stroke$clone()$filter(splits$test)\n# Entrenamiento del modelo\nrf_classif_stroke$train(tsk_train_stroke)\n# modelo construido\nmodelo = rf_classif_stroke$model$classif.ranger$model\n\nAnalizamos ahora los resultados del modelo obtenido. Comenzamos con los resultados generales del modelo:\n\n# Características del modelo\nmodelo\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L, importance = \"impurity\") \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      4088 \nNumber of independent variables:  10 \nMtry:                             3 \nTarget node size:                 1 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error:             4.94 % \n\n\nPodemos ver que el modelo obtenido tiene un 4.94% de error de predicción. Estudiamos ahora la contribución de cada predictora en el modelo obtenido partir de la importancia de cada una de ellas.\n\n# Importancia de las predictoras (ordenada)\nsort(modelo$variable.importance, decreasing = TRUE)\n\navg_glucose_level               age               bmi    smoking_status \n        96.466818         81.110334         79.201270         24.443464 \n        work_type    Residence_type            gender      hypertension \n        16.776643         12.009533         11.287812          9.087196 \n    heart_disease      ever_married \n         8.994403          7.157174 \n\n\nLas tres predictoras con una mayor contribución en la construcción del bosque aleatorio son avg_glucose_level, age, y bmi, muy por encima del resto de predictoras. En muchas situaciones prácticas se puede utilizar el resultado de la importancia para construir un nuevo modelo de aprendizaje basado únicamente en dichas predictoras. En este caso nos podríamos plantear un modelo de regresión logística o un árbol de decisión que solo contemplara dichas variables.\nPor el momento nos centramos en ampliar el aprendizaje sobre nuestro modelo analizando las predicciones para la muestra de entrenamiento y validación.\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = rf_classif_stroke$predict(tsk_train_stroke)\npred_test = rf_classif_stroke$predict(tsk_test_stroke)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n classif.acc classif.bacc \n   0.9985323    0.9849246 \n\n# Muestra de validación\npred_test$score(measures)\n\n classif.acc classif.bacc \n   0.9500978    0.4994856 \n\n\nDe nuevo el porcentaje de clasificación correcta ponderado muestra valores muy bajos en comparación con el no ponderado. Este comportamiento es similar al visto en modelos anteriores. Analizamos la tabla de confusión de la muestra de validación:\n\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nSe puede ver claramente que le modelo no proporciona una solución adecuada ya que no es capaz de clasificar correctamente ninguna de las muestras originales correspondientes a sujetos que han sufrido un ictus. En el proceso de optimización trataremos de mejorar los resultados de este algoritmo.\nPara comenzar el proceso de optimización nos centramos en los hiperparámetros: mtry.ratio, num.trees, y sample.fraction. Establecemos el proceso de optimización como en otras ocasiones aumentando el número de iteraciones a 50 debido al alto número de predictoras involucradas. Necesitamos recorrer el espacio de búsqueda (sin mucho coste computacional) para tratar de acercarnos al óptimo.\n\nrf_classif_stroke = lrn(\"classif.ranger\", importance = \"impurity\",\n                         mtry.ratio = to_tune(1e-02, 1, logscale = TRUE),\n                         num.trees = to_tune(100, 1000),\n                         sample.fraction = to_tune(1e-01, 1, logscale = TRUE)\n                         )\ngr_stroke =  pp_stroke %>>% rf_classif_stroke\ngr_stroke = GraphLearner$new(gr_stroke)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_stroke,\n  learner = gr_stroke,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.bacc\"),\n  term_evals = 50\n)\n\nPodemos ver los resultados obtenidos con\n\ninstance$result_y\n\nclassif.bacc \n   0.5033988 \n\ninstance$result_x_domain\n\n$classif.ranger.mtry.ratio\n[1] 0.8179929\n\n$classif.ranger.num.trees\n[1] 222\n\n$classif.ranger.sample.fraction\n[1] 0.2600791\n\n\ndonde observamos los valores óptimos de los hiperparámetros y el porcentaje de clasificación alcanzado del 50.3%. La solución óptima es muy similar a la opción por defecto. No hemos mejorado prácticamente nada respecto de la solución inicial.\n\n\n13.4.1.2 Water Potability\nComo en otras situaciones empezamos por el modelo de aprendizaje por defecto.\n\n# Preprocesado\npp_water = po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) %>>% \n  po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) \n# Modelo de aprendizaje combinando preprocesado y algoritmo\nrf_classif_water = as_learner(pp_water %>>% \n                                  lrn(\"classif.ranger\", importance = \"impurity\"))\nrf_classif_water$id = \"RandomForest\"\n\nComenzamos con le entrenamiento del modelo definiendo en primer lugar las muestras de entrenamiento y validación:\n\n# División de muestras\nset.seed(432)\n# Creamos la partición\nsplits = mlr3::partition(tsk_water, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_water = tsk_water$clone()$filter(splits$train)\ntsk_test_water  = tsk_water$clone()$filter(splits$test)\n# Entrenamiento del modelo\nrf_classif_water$train(tsk_train_water)\n# modelo construido\nmodelo = rf_classif_water$model$classif.ranger$model\n\nAnalizamos ahora los resultados del modelo obtenido. Comenzamos con los resultados generales del modelo:\n\n# Características del modelo\nmodelo\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L, importance = \"impurity\") \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      2620 \nNumber of independent variables:  9 \nMtry:                             3 \nTarget node size:                 1 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error:             33.44 % \n\n\nPodemos ver que el modelo obtenido tiene un 33.44% de error de predicción. Estudiamos ahora la contribución de cada predictora en el modelo obtenido partir de la importancia de cada una de ellas.\n\n# Importancia de las predictoras (ordenada)\nsort(modelo$variable.importance, decreasing = TRUE)\n\n             ph         Sulfate        Hardness     Chloramines          Solids \n       158.1590        150.7318        149.1244        145.9815        144.7664 \n   Conductivity  Organic_carbon Trihalomethanes       Turbidity \n       129.0813        126.2509        122.9520        119.2023 \n\n\nEn este caso no hay muchas diferencias entre las contribuciones de las predictoras, lo que no nos permite destacar una predictora o un grupo de ellas sobre el resto. Finalizamos con el análisis de predicción:\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = rf_classif_water$predict(tsk_train_water)\npred_test = rf_classif_water$predict(tsk_test_water)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n classif.acc classif.bacc \n           1            1 \n\n# Muestra de validación\npred_test$score(measures)\n\n classif.acc classif.bacc \n   0.6768293    0.6175781 \n\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nEn este caso el porcentaje de clasificación correcta ponderada se sitúa en el 61.75%. El mayor error de clasificación se produce al predecir las muestras clasificadas originalmente como potables, ya que el 25.5% de ellas son clasificadas por el modelo como no potables. Sin embargo, si hemos mejorado los resultados del modelo bagging planteado anteriormente.\nVeamos que ocurre al intentar optimizar los hiperparámetros del modelo. En este caso reducimos la búsqueda de mtry dado que tenemos menos predictoras, y reducimos le intervalo de búsqueda de num.trees. Además aumentamos el número de evaluaciones del algoritmo de búsqueda ya que utilizamos grid_search y deseamos una búsqueda fina.\n\nrf_classif_water = lrn(\"classif.ranger\", importance = \"impurity\",\n                         mtry.ratio = to_tune(1e-02, 1, logscale = TRUE),\n                         num.trees = to_tune(100, 2000),\n                         sample.fraction = to_tune(1e-01, 1, logscale = TRUE)\n                         )\ngr_water =  as_learner(pp_water %>>% rf_classif_water)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"grid_search\"),\n  task = tsk_water,\n  learner = gr_water,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.bacc\"),\n  term_evals = 50\n)\n\nPodemos ver los resultados obtenidos con:\n\ninstance$result_y\n\nclassif.bacc \n   0.6182979 \n\ninstance$result_x_domain\n\n$classif.ranger.mtry.ratio\n[1] 0.5994843\n\n$classif.ranger.num.trees\n[1] 2000\n\n$classif.ranger.sample.fraction\n[1] 1\n\n\nLa solución óptima alcanza un porcentaje de clasificación correcta ponderada del 61.82%, prácticamente igual al del modelo sin optimización. Sin modificar más hiperparámetros la solución obtenida en ambas situaciones es muy similar.\n\n\n13.4.1.3 Housing in California\nEn este caso nos enfrentamos a un problema de regresión. En primer lugar configuramos el modelo por defecto\n\n# Preprocesamiento\npp_housing = \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) \n# Modelo de aprendizaje combinando preprocesado y algoritmo\nrf_regr_housing = as_learner(pp_housing %>>% \n                                  lrn(\"regr.ranger\", importance = \"impurity\"))\nrf_regr_housing$id = \"RandomForest\"\n\nComenzamos con le entrenamiento del modelo definiendo en primer lugar las muestras de entrenamiento y validación:\n\n# División de muestras\nset.seed(432)\n# Creamos la partición\nsplits = mlr3::partition(tsk_housing, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_housing = tsk_housing$clone()$filter(splits$train)\ntsk_test_housing  = tsk_housing$clone()$filter(splits$test)\n# Entrenamiento del modelo\nrf_regr_housing$train(tsk_train_housing)\n# modelo construido\nmodelo = rf_regr_housing$model$regr.ranger$model\n\nAnalizamos ahora los resultados del modelo obtenido. Comenzamos con los resultados generales del modelo:\n\n# Características del modelo\nmodelo\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, num.threads = 1L, importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      16511 \nNumber of independent variables:  9 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       2399285409 \nR squared (OOB):                  0.8211186 \n\n\nEn este caso podemos ver que le \\(R^2\\) obtenido con este modelo se sitúa en el 82.11%, lo que no es un valor fantástico pero si bastante alto dado el gran número de predictoras y muestras consideradas. Podemos evaluar la importancia de las predictoras\n\n# Importancia de las predictoras (ordenada)\nsort(modelo$variable.importance, decreasing = TRUE)\n\n     median_income          longitude           latitude    ocean_proximity \n      8.751658e+13       2.907831e+13       2.750543e+13       2.563794e+13 \nhousing_median_age         population        total_rooms     total_bedrooms \n      1.125149e+13       1.113980e+13       1.009497e+13       7.433084e+12 \n        households \n      7.271314e+12 \n\n\nTodas contribuyen aunque en mayor medida el precio de la vivienda viene determinando por median_income, longitude. latitude, y ocean_proximity. Veamos el sMAPE que obtenemos con este modelo:\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = rf_regr_housing$predict(tsk_train_housing)\npred_test = rf_regr_housing$predict(tsk_test_housing)\n# scores de validación\nmeasures = msr(\"regr.smape\")\n# Muestra de entrenamiento\npred_train$score(measures)\n\nregr.smape \n0.07453686 \n\n# Muestra de validación\npred_test$score(measures)\n\nregr.smape \n 0.1619824 \n\n\nPodemos ver como los valores son mejores que los obtenidos con el modelo de bagging inicial, demostrando que el modelo random forest consigue mejorar la predicción del target. Para finalizar exploramos la optimización de hiperparámetros con un esquema similar al del ejemplo anterior.\n\nrf_regr_housing = lrn(\"regr.ranger\", importance = \"impurity\",\n                         mtry.ratio = to_tune(1e-02, 1, logscale = TRUE),\n                         num.trees = to_tune(100, 2000),\n                         sample.fraction = to_tune(1e-01, 1, logscale = TRUE)\n                         )\ngr_housing =  as_learner(pp_housing %>>% rf_regr_housing)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"grid_search\"),\n  task = tsk_housing,\n  learner = gr_housing,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"regr.smape\"),\n  term_evals = 50\n)\n\nPodemos ver los resultados obtenidos con:\n\ninstance$result_y\n\nregr.smape \n 0.1622815 \n\ninstance$result_x_domain\n\n$regr.ranger.mtry.ratio\n[1] 0.5994843\n\n$regr.ranger.num.trees\n[1] 2000\n\n$regr.ranger.sample.fraction\n[1] 1\n\n\nA pesar de la búsqueda que hemos hecho (sobre todo computacionalmente hablando) el smape es prácticamente idéntico al obtenido con el modelo sin optimización. Es posible que alguna combinación pueda alcanzar un valor más bajo, pero el tiempo computacional puede ser excesivo sino se usa más de un procesador."
  },
  {
    "objectID": "130_Ensemblemodels.html#sec-130.5",
    "href": "130_Ensemblemodels.html#sec-130.5",
    "title": "13  Modelos de conjunto (Ensemble models)",
    "section": "13.5 Ejercicios",
    "text": "13.5 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos Mushroom4.3.4.\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos Hepatitis4.3.9.\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos Abalone4.3.1.\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos Us economic time series4.2.7.\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos QSAR4.2.8."
  },
  {
    "objectID": "140_Boostingmodels.html#sec-140.1",
    "href": "140_Boostingmodels.html#sec-140.1",
    "title": "14  Modelos Boosting",
    "section": "14.1 Descenso del gradiente",
    "text": "14.1 Descenso del gradiente\nEl término boosting proviene del principal algoritmo que se utiliza como base para el ajuste de los diferentes modelos secuenciales que conforman este tipo de solución. Dicho algoritmo es el conocido como algoritmo de descenso del gradiente (gradient descent algorithm) y es muy utilizado en los algoritmos de optimización usados para minimizar funciones de coste o pérdida. Su aplicación se encuentra muy extendida en diferentes algoritmos de aprendizaje automático.\nDada una función de pérdida \\(f\\) que depende del parámetro \\(\\theta\\) , y fijada una tasa de aprendizaje \\(\\lambda\\), el algoritmo del descenso del gradiente se estructura de la forma siguiente:\n\nFijar un valor inicial \\(\\theta_0\\) para el parámetro de interés.\nObtener la derivada parcial de la función con respecto a \\(\\theta\\), y evaluarla en \\(\\theta_0\\) (gradiente descendiente):\n\n\\[\\bigtriangledown f_{\\theta_0} = \\left[\\frac{\\partial f}{\\partial \\theta} \\right ]_{\\theta = \\theta_0}\\]\n\nCalcular un nuevo valor del parámetro mediante el “descenso” del valor anterior a partir del gradiente obtenido y la tasa de aprendizaje:\n\n\\[\\theta_1 = \\theta_0 - \\lambda *\\bigtriangledown f_{\\theta_0}\\]\n\nComprobar si el cambio en la actualización del parámetro es inferior a un fijado previamente (llamada criterio de parada), de forma que si es afirmativo el algoritmo se detiene, y en caso contrario se actualiza el gradiente y se pasa a un nuevo valor de \\(\\theta\\).\n\nEste algoritmo se puede generalizar a situaciones más generales con múltiples parámetros. Sin embargo, la mayor dificultad estriba en que no tenemos asegurado alcanzar el mínimo absoluto de la función \\(f\\), ya que cuando el algoritmo encuentra un mínimo relativo resulta imposible salir de dicho punto.\nPor su modo de implementación, el descenso del gradiente puede realizarse de tres modos diferentes para la función de pérdida establecida en cada uno de los algoritmos de aprendizaje automático:\n\nDescenso de gradiente por lotes. Este es un tipo de descenso de gradiente que procesa todas las muestras de entrenamiento en cada iteración del descenso de gradiente. Si el número de muestras de entrenamiento es grande, el descenso de gradiente por lotes es computacionalmente muy caro.\nDescenso de gradiente estocástico. Este es un tipo de descenso de gradiente que procesa una muestra de entrenamiento por iteración. Por lo tanto, los parámetros se actualizan incluso después de cada iteración. Por lo tanto, es bastante más rápido que el descenso de gradiente por lotes. Pero de nuevo, cuando el número de muestras de entrenamiento es grande, incluso entonces se procesa sólo una muestra que puede ser una sobrecarga adicional para el sistema ya que el número de iteraciones será bastante grande.\nDescenso de gradiente en mini lotes. Este es un tipo de descenso de gradiente que funciona más rápido que el descenso de gradiente por lotes y el descenso de gradiente estocástico. Aquí se procesan \\(b\\) muestras por iteración donde \\(b\\) es inferior al tamaño de la muestra de entrenamiento. Así, aunque el número de muestras de entrenamiento sea grande, se procesa en lotes de tamaño \\(b\\) de una sola vez. Por lo tanto, funciona para los ejemplos de entrenamiento más grandes y también con un menor número de iteraciones."
  },
  {
    "objectID": "140_Boostingmodels.html#sec-140.2",
    "href": "140_Boostingmodels.html#sec-140.2",
    "title": "14  Modelos Boosting",
    "section": "14.2 Algorirmos Boosting",
    "text": "14.2 Algorirmos Boosting\nA continuación se muestran los algoritmos principales que hacen uso del descenso de gradiente para los modelos de conjunto. Casi todos ellos utilizan como modelo base los árboles de decisión. Como en el caso de los algoritmos de bagging mostraremos los aspectos teóricos de cada uno de ellos aplicados a un problema de clasificación, aunque se pueden generalizar a los problemas de regresión de forma inmediata. Para finalizar con cada algoritmo se presenta la función en mlr3 que nos permite el ajuste de dicho modelo.\n\n14.2.1 AdaBoost\nFue el primer algoritmo en hacer uso del boosting en los algoritmos de aprendizaje automático. Si estamos interesados en un problema de clasificación con dos grupos posibles necesitamos como punto de partida:\n\nUn weak learner que sea capaz de predecir la variable respuesta con un porcentaje de acierto ligeramente superior a lo esperado por azar.\nCodificar las dos clases de la variable respuesta como +1 y -1.\nUn peso inicial e igual para todas las observaciones que forman el set de entrenamiento.\n\nUna vez que estos tres puntos se han establecido, se inicia un proceso iterativo. En la primera iteración, se ajusta el weak learner empleando los datos de entrenamiento y los pesos iniciales (todos iguales). Con el weak learner ajustado y almacenado, se predicen las observaciones de entrenamiento y se identifican aquellas bien y mal clasificadas. Con esta información:\n\nSe actualizan los pesos de las observaciones, disminuyendo el de las que están bien clasificadas y aumentando el de las mal clasificadas.\nSe asigna un peso total al weak learner, proporcional al total de aciertos. Cuantos más aciertos consiga el weak learner, mayor su influencia en el conjunto del ensemble.\n\nEn la siguiente iteración, se llama de nuevo al weak learner y se vuelve a ajustar, esta vez, empleando los pesos actualizados en la iteración anterior. El nuevo weak learner se almacena, obteniendo así un nuevo modelo para el conjunto. Este proceso se repite \\(M\\) veces, generando un total de \\(M\\) weak learners. Para clasificar nuevas observaciones, se obtiene la predicción de cada uno de los weak learners que forman el conjunto y se agregan sus resultados, ponderando el peso de cada uno acorde al peso que se le ha asignado en el ajuste. El objetivo detrás de esta estrategia es que cada nuevo weak learner se centra en predecir correctamente las observaciones que los anteriores no han sido capaces. A continuación se muestra la estructura del algoritmo.\nConsideramos \\(y\\) la variable respuesta, \\(X\\) el conjunto de variables predictoras, \\(N\\) número de muestras de entrenamiento, \\(M\\) número de iteraciones de aprendizaje, \\(G_m\\) weak learner en la iteración \\(m\\), \\(w_i\\) peso de la observación \\(i\\), y \\(\\alpha_m\\) el peso del weak learner \\(m\\), de forma que el algoritmo viene dado por:\n\nInicializamos los pesos de las observaciones\n\n\\[w_i = \\frac{1}{N}, \\quad i=1,...,N\\]\n\nPara \\(m=1\\) hasta \\(M\\):\n\n\nAjustar el weak learner \\(G_m\\) utilizando las muestras de entrenamiento y los pesos \\(w_i\\), para obtener la predicción \\(\\hat{y}_i\\) de cada \\(y_i\\)\nCalcular el error del weak learner como:\n\n\\[err_m = \\frac{\\sum_{i=1}^N w_i I(y_i \\neq \\hat{y}_i)}{\\sum_{i=1}^N w_i}\\]\n\nCalcular el peso asignado al weak learner \\(G_m\\):\n\n\\[\\alpha_m = log\\left(\\frac{1-err_m}{err_m}\\right)\\]\n\nActualizar los pesos de las observaciones:\n\n\\[w_i = w_i exp[\\alpha_m I(y_i \\neq \\hat{y}_i)], \\quad i=1,...,N\\]\n\nConstrucción del strong learner agregando todos los weak learner obtenidos en el proceso iterativo ponderándolos por su peso:\n\n\\[G(x) = sign\\left[\\sum_{m=1}^M \\alpha_m G_m(X)\\right]\\]\n\n\n14.2.2 Gradient Boosting\nEl gradient boosting o refuerzo del gradiente es uno de los algoritmos de aprendizaje automático más populares. Es lo suficientemente potente como para encontrar cualquier relación no lineal entre el objetivo del modelo y las variables predictoras, y tiene una gran facilidad de uso ya que nos permite trabajar con valores perdidos, valores atípicos y valores categóricos de alta cardinalidad sin ningún tratamiento especial. De forma habitual este tipo de algoritmos toman como weak learner los árboles de decisión lo que provoca que muchas veces este algoritmo se conoce como gradient boosting tree. Se basa en un proceso de boosting donde la actualización en cada iteración se realiza mediante el algoritmo del descenso del gradiente.\nDe forma sencilla el gradient boosting ajusta un primer weak learner \\(f_1\\) con el que se predice la variable respuesta \\(y\\), obteniéndose los errores \\(y−f_1(x)\\). A continuación, se ajusta un nuevo modelo \\(𝑓_2\\), que intenta predecir los residuos del modelo anterior, en otras palabras, trata de corregir los errores que ha hecho el modelo \\(𝑓_1\\):\n\\[f_1(x) \\sim y,\\qquad f_2(x) \\sim y-f_1(x)\\]\nEn la siguiente iteración, se calculan los residuos de los dos modelos de forma conjunta \\(𝑦−𝑓_1(𝑥)−𝑓_2(𝑥)\\), los errores cometidos por \\(𝑓_1\\) y que \\(𝑓_2\\) no ha sido capaz de corregir, y se ajusta un tercer modelo \\(𝑓_3\\) para tratar de corregirlos:\n\\[f_3(x) \\sim y-f_1(x)-f_2(x)\\]\nEste proceso se repite \\(M\\) veces, de forma que cada nuevo modelo minimiza los errores del anterior, y construimos el strong learner como:\n\\[y \\sim f_1(x) + f_2(x) + ... + f_M(x)\\]\nDado que el objetivo de Gradient Boosting es ir minimizando los residuos iteración a iteración, es susceptible de sobreajuste. Una forma de evitar este problema es emplear la tasa de aprendizaje (\\(\\lambda\\)) sobre cada weak learner en el proceso de boosting, de forma que el predictor final viene dado por:\n\\[F(x) = \\lambda f_1(x) + \\lambda f_2(x) + ... + \\lambda f_M(x)\\] ### Gradient Boosting en mlr3 {#sec-140.2.4}\n\n\n14.2.3 XGBoost\nEl algoritmo más famoso que utiliza como base el gradient boosting es el extreme gradient boosting (XGBosst) que estudiamos a continuación. Todos los aspectos técnicos de este algoritmo se pueden consultar aquí.\nAntes de comentar las diferencias existentes entre gradient boosting y extreme gradient boosting veamos cuales son su puntos en común:\n\nAlgoritmos basados en árboles: tanto XGBoost como Gradient Boosting utilizan árboles de decisión como estimadores base.\nObjetivo de predicción: los árboles se construyen utilizando los residuos, no las etiquetas de clase reales. Por lo tanto, a pesar de que nos centramos en problemas de clasificación, los estimadores base de estos algoritmos son árboles de regresión y no árboles de clasificación. Esto se debe a que los residuos son continuos y no discretos. Al mismo tiempo, sin embargo, algunas de las fórmulas que se presentan a continuación son únicas para la clasificación, así que no podemos asumir su aplicación exactamente igual a los problemas de regresión.\nProfundidad del árbol: ambos algoritmos permiten controlar el tamaño máximo de los árboles para minimizar el riesgo de sobreajuste de los datos.\nMétodos de conjunto: similares a Random Forest o AdaBoost, estos algoritmos construyen muchos árboles en el proceso. Al final, la predicción final se basa en todos los árboles.\nTasa de aprendizaje: el valor de cada árbol se escala por la tasa de aprendizaje. Esto permite que el algoritmo tenga una mejora más gradual y constante en cada paso.\n\nA continuación se muestra una imagen resumen del funcionamiento de XGBoost:\n Las diferencias entre ambos algoritmos se basan en la construcción de los diferentes árboles de decisión secuenciales.\nGradient Boosting utiliza un método estándar para construir árboles de regresión, en el que se utiliza una métrica típica como el MSE (error cuadrático medio) u otra similar para determinar la mejor división del árbol. El algoritmo calcula el MSE para cada una de las posibles divisiones de nodos y luego elige la que tenga el menor MSE como la que se utilizará en el árbol.\nPor el contrario, XGBoost utiliza su propio método de construcción de árboles en el que la puntuación de similitud y la ganancia determinan las mejores divisiones de nodos. La puntuación de similitud o similarity score(SS) se define como:\n\\[SS = \\frac{\\left(\\sum_{i=1}^n r_i \\right)^2}{\\sum_{i=1}^n  \\left[pp_i(1-pp_i)\\right] + \\lambda}\\]\ndonde:\n\n\\(r_i\\) son los residuos o diferencia entre el valor observado actual y el valor predicho.\n\\(pp_i\\) es la probabilidad previa o probabilidad de un evento calculada en un paso anterior. Se supone que la probabilidad inicial es de 0.5 para cada observación, que se utiliza para construir el primer árbol. Para cualquier árbol posterior, la probabilidad anterior se recalcula basándose en la predicción inicial y en las predicciones de todos los árboles anteriores.\n\\(\\lambda\\) es un parámetro de regularización. Su aumento reduce desproporcionadamente la influencia de las hojas pequeñas (las que tienen pocas observaciones) mientras que sólo tiene un impacto menor en las hojas más grandes (las que tienen muchas observaciones).\n\nUna vez que tenemos la puntuación de similitud para cada hoja, calculamos la ganancia (gain) utilizando la siguiente fórmula:\n\\[Gain = SS_{i} + SS_{d} - SS_{r},\\]\ndonde \\(SS_i\\), \\(SS_d\\), y \\(SS_r\\) son las puntuaciones de similitud de la división de la rama izquierda, de la división de la rama derecha, y el nodo raíz del que parten ambas ramas respectivamente. La división del nodo con la mayor ganancia se elige como la mejor división del árbol.\nLa introducción de \\(\\lambda\\) en el proceso de evaluación de SS es la principal diferencia con gradient boosting ya que en este último el SS se calcula sin añadir ese término de regularización.\nAdemás de utilizar su propia manera de construir y podar árboles, XGBoost también tiene varias optimizaciones incorporadas para hacer el entrenamiento más rápido cuando se trabaja con conjuntos de datos enormes. He aquí algunas de las principales:\n\nAlgoritmo Greedy aproximado - utiliza cuantiles ponderados cuando busca la mejor división de nodos en lugar de evaluar cada división posible.\nAprendizaje paralelo: puede dividir los datos en conjuntos de datos más pequeños para ejecutar procesos en paralelo.\nSparsity-Aware Split Finding - cuando tiene algunos datos perdidos, calcula Gain poniendo las observaciones con valores perdidos en la hoja izquierda. A continuación, hace lo mismo colocándolas en la hoja de la derecha y elige el escenario que produce una mayor ganancia.\nAcceso consciente del efectivo - XGBoost utiliza la memoria caché de la CPU para almacenar los gradientes y así poder calcular las puntuaciones de similitud más rápidamente.\n\n\n\n14.2.4 LightGBM\nLightGBM, abreviatura de light gradient-boosting machine, es un algoritmo que toma como base el gradient boosting, y que fue desarrollado originalmente por Microsoft. Utiliza como modelo de partida los árboles de decisión y se utiliza para la clasificación y otras tareas de aprendizaje automático cuando el conjunto de la muestra de entrenamiento es muy grande. Sus puntos fuertes son la mejora en el rendimiento y la escalabilidad. LightGBM amplía el algoritmo de gradient boosting añadiendo un tipo de selección automática de predictoras centrándose en la evolución del algoritmo hacia las ramas de árbol de decisión con mayores gradientes. Esto puede dar lugar a una gran aceleración del entrenamiento y a una mejora del rendimiento predictivo.\nLightGMB posee muchas de las ventajas de XGBoost como la optimización dispersa, el entrenamiento paralelo, las funciones de pérdida múltiples, la regularización, el bagging y la detención temprana. Una de las principales diferencias entre ambos algoritmos es la construcción de los árboles. LightGBM no construye un árbol por niveles, fila por fila, como hacen la mayoría de las implementaciones, sino que lo hace por hojas. Además, LightGBM no utiliza el algoritmo de aprendizaje de árbol de decisión basado en la ordenación, que busca el mejor punto de división en valores de características ordenados, como hacen XGBoost u otras implementaciones. En su lugar, LightGBM implementa un algoritmo de aprendizaje de árbol de decisión basado en un histograma altamente optimizado, que ofrece grandes ventajas tanto en eficiencia como en consumo de memoria. El algoritmo LightGBM utiliza dos técnicas novedosas llamadas Gradient-Based One-Side Sampling (GOSS) y Exclusive Feature Bundling (EFB) que permiten que el algoritmo se ejecute más rápidamente manteniendo un alto nivel de precisión.\nEl muestreo unilateral basado en el gradiente, o GOSS por sus siglas en inglés, es una modificación del método gradient boosting que centra la atención en aquellas muestras de entrenamiento que dan lugar a un gradiente mayor, lo que a su vez acelera el aprendizaje y reduce la complejidad computacional del método.\nLa agrupación de rasgos exclusivos, o EFB por sus siglas en inglés, es un método para agrupar rasgos dispersos (en su mayoría nulos) mutuamente excluyentes, como los niveles de variables categóricas que han sido codificadas mediante hot-encoding. Como tal, es un tipo de selección automática de características.\nJuntos, estos dos modificaciones dentro del algoritmo de gradiente boosting pueden acelerar el tiempo de entrenamiento del algoritmo hasta 20 veces."
  },
  {
    "objectID": "140_Boostingmodels.html#sec-140.3",
    "href": "140_Boostingmodels.html#sec-140.3",
    "title": "14  Modelos Boosting",
    "section": "14.3 Algortimos Boosting en mlr3",
    "text": "14.3 Algortimos Boosting en mlr3\nA continuación se muestran las funciones principales para la obtención de los algoritmos de boosting presentados.\n\n14.3.1 AdaBoost\nPor el momento este algoritmo solo se encuentra disponible para tareas de clasificación mediante la función classif.AdaBoostM1 del paquete mlr3extralearners. Para su uso es necesario tener instalada la librería RWeka. Los hiperparámetros más relevantes para este modelo son:\n\nP: Porcentaje de peso masa en el que basar el entrenamiento. Por defecto toma el valor 100.\nQ: Si se usa remuestreo para el boosting. Por defecto el valor es False\nS: Semilla aleatoria. Por defecto toma el valor 1.\nI: Número de iteraciones. Por defecto se establece el valor 10.\nW: Tipo de weak learner utiliza como modelo de base. Por defecto se usan árboles de decisión.\n\n\n\n14.3.2 Gradient Boosting\nLos algoritmos principales para gradient boosting en mlr3 son classif.gbm para las tareas de clasificación, y regr.gbm para tareas de regresión, que se encuentran disponibles en el paquete mlr3extralearners. Para poder utilizarlos es necesario tener instalada la librería gbm.\nLos parámetros más relevantes para ambas funciones son:\n\ndistribution: cadena de caracteres que especifica el nombre de la distribución a utilizar o una lista con un nombre de componente que especifica la distribución y cualquier parámetro adicional necesario. Para tareas de clasificación las opciones disponibles son bernoulli (target con respuestas 0-1), adaboost (utiliza la función de pérdida exponencial de Adaboost para variables 0-1), huberized (función de pérdida de huber para variables 0-1), multinomial (para respuestas tipo factor.). Por defecto se utiliza el valor bernouilli. Para tareas de regresión las opciones disponibles son gaussian (donde se utiliza la función de pérdida cuadrática), laplace (función de pérdida del valor absoluto), poisson (para respuestas que son conteos), y tdist (para usar la función de pérdida basada en la distribución t). la opción por defecto es gaussian.\nn.tress: número de árboles de decisión utilizados como weak learner. Por defecto es valor es 100.\ninteraction.depth: Número entero que especifica la profundidad máxima de cada árbol. El valor por defecto es 1.\nn.minobsinnode: Número mínimo de observaciones en los nodos terminales en los árboles de decisión. El valor por defecto es 10.\nshrinkage: Tasa de aprendizaje. Por defecto toma el valor 0.001.\nbag.fraction: fracción de las observaciones del conjunto de entrenamiento seleccionadas aleatoriamente para proponer el siguiente árbol de la expansión. Valor por defecto igual a 0.5\ntrain.fraction: Las primeras observaciones de train.fraction*nrows(data) se utilizan para ajustar el gbm y el resto se utiliza para calcular estimaciones fuera de muestra de la función de pérdida. El valor por defecto es 1.\ncv.folds: Número de validaciones cruzadas consideradas. Por defecto se toma el valor 0.\nn.cores: Número de procesadores utilizados. Por defecto se toma el valor 1.\n\n\n\n14.3.3 XGBoost\nLos algoritmos principales para XGBoost (extrem gradient boosting) en mlr3 son classif.xgboost para las tareas de clasificación, y regr.xgboost para tareas de regresión, que se encuentran disponibles en el paquete mlr3extralearners. Para poder utilizarlos es necesario tener instalada la librería xgboost. La mayor dificultad con estas dos funciones es la gran cantidad de hiperparámetros disponibles para su ajuste. Los más interesantes son:\n\neta: que controla la tasa de aprendizaje, y que por defecto es igual a 0.3.\ngamma: reducción mínima de pérdida requerida para realizar una partición adicional en un nodo de hoja del árbol. Cuanto mayor, más conservador será el algoritmo. El valor por defecto es 0.\nlambda: parámetro de regularización. El valor por defecto es 1.\n\n\n\n14.3.4 LightGBM\nLos algoritmos principales para LightGBM en mlr3 son classif.lightgbm para las tareas de clasificación, y regr.lightgbm para tareas de regresión, que se encuentran disponibles en el paquete mlr3extralearners. Para poder utilizarlos es necesario tener instalada la librería lightgbm. Este algoritmo tiene una cantidad inmensa de hiperparámetros que nos describiremos. Se puede consultar este enlace. Tal vez uno de los más relevantes es early_stopping que os permite indicar si debemos realizar parada temprana para evitar sobre ajuste.\n\n\n\n\n\n\nDada la gran cantidad de hiperparámetros que involucran la mayoría de estos modelos en las aplicaciones que presentamos a continuación utilizaremos solo las opciones por defecto, sin búsqueda del óptimo."
  },
  {
    "objectID": "140_Boostingmodels.html#sec-140.4",
    "href": "140_Boostingmodels.html#sec-140.4",
    "title": "14  Modelos Boosting",
    "section": "14.4 Aplicaciones",
    "text": "14.4 Aplicaciones\nEn este apartado vamos a utilizar los bancos de datos del tema anterior para ejemplificar el uso de los algoritmos de boosting. Antes de presentar los bancos de datos de nuevo, cargamos todas las librerías necesarias así como las necesarias para los diferentes algoritmos de boosting.\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(cvms)\nlibrary(kknn)\nlibrary(rpart.plot)\ntheme_set(theme_sjplot2())\n\n# Paquetes AA\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3tuningspaces)\nlibrary(gbm)\nlibrary(RWeka)\nlibrary(xgboost)\nlibrary(lightgbm)\n\n\n14.4.1 Bancos de datos\nPara ejemplificar el uso de los modelos de bagging básicos vamos a utilizar tres bancos de datos: Stroke, Water Potability, y Housing in California que se pueden consultar en el tema 4. De los tres con el único con el que no hemos trabajado hasta ahora es Water Potability. A continuación se muestra el código necesario para la carga de cada uno de esos bancos de datos, y la creación de la tarea correspondiente. Los dos primeros corresponden a problemas de clasificación mientras que el último se corresponde con un problema de regresión.\n\n14.4.1.1 Stroke\nEl código para este banco de datos aparece a continuación. Para poder ejecutar todos los modelos debemos convertir la respuesta en variable 1-0.\n\n# Leemos datos\nstroke = read_rds(\"stroke.rds\")\n# Eliminamos la variable id\nstroke = stroke %>% dplyr::select(-id)\n# creamos la tarea\ntsk_stroke = as_task_classif(stroke, target = \"stroke\", positive =\"Yes\")\n# Generamos variable de estrato\ntsk_stroke$col_roles$stratum <- \"stroke\"\n\n\n\n14.4.1.2 Water Potability\nEl código para crear la tares es:\n\n# Leemos datos\nwaterpot = read_rds(\"waterpot.rds\")\n# creamos la tarea\ntsk_water = as_task_classif(waterpot, target = \"Potability\", positive=\"1\")\n# Generamos variable de estrato\ntsk_water$col_roles$stratum <- \"Potability\"\n\n\n\n14.4.1.3 Housing in California\nCargamos los datos correspondientes:\n\n# Carga de datos\nhousingCA = read_rds(\"housingCA.rds\")\n# Creación de task\ntsk_housing = as_task_regr(housingCA, target = \"median_house_value\")\n\n\n\n\n14.4.2 Modelos\nPuesto que los modelos a considerar quedan todos englobados dentro de los algoritmos de boosting, vamos a diseñar un análisis conjunto sobre todos ellos haciendo uso de las funciones benchmark() y benchmark_grid(). Para cada banco de datos estableceos los diferentes modelos de aprendizaje y seleccionamos el que mejor funciona en cada caso. Para ello hemos de tener en cuenta que no todos ellos se pueden utilizar. Por ejemplo xgboost no permite predictoras de tipo factor con lo que una forma de abordar esa situación es codificando los factores para tener solo variables numéricas. En nuestro caso vamos a considerar los algoritmos de AdaBoost, XGBoost, y lightGBM para problemas de clasificación, y GBM, XGBoost, y lightGBM para problemas de regresión.\n\n14.4.2.1 Stroke\nAl tratarse de un problema de clasificación podemos considerar los cuatro algoritmos presentados. A continuación se detalla el código para poder implantarlos. En cada uno de ellos se consideran las tareas de preprocesamiento correspondientes. En este caso consideramos todas las tareas para poder comparar con los resultado que proporcionarían otro tipo de modelos. En este caso el modelo classif.gbm no se puede ajustar, aunque se muestra el código correspondiente.\n\n# Preprocesamiento\npp_stroke =  \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) %>>%\n   po(\"encode\", param_vals = list(method = \"one-hot\"))\n \n\n# Modelo de aprendizaje AdaBoost\n# ==============================\nlrn1 = lrn(\"classif.AdaBoostM1\")\nstroke_adaboost = as_learner(pp_stroke %>>% lrn1)\n\n# Modelo de aprendizaje XGBoost\n# =======================================\nlrn2 = lrn(\"classif.xgboost\")\nstroke_xgboost = as_learner(pp_stroke %>>% lrn2)\n\n# Modelo de aprendizaje lightGBM\n# =======================================\nlrn3 = lrn(\"classif.lightgbm\")\nstroke_lightgbm = as_learner(pp_stroke %>>% lrn3)\n\nDefinimos el proceso de remuestreo necesario para la combinación de modelos y el proceso de combinación de todos los modelos definidos:\n\nset.seed(321)\n# Remuestreo\nremuestreo = rsmp(\"cv\", folds = 10)\n# Grid\ndesign = benchmark_grid(tsk_stroke, \n                        list(stroke_adaboost, stroke_xgboost, stroke_lightgbm), \n                        remuestreo)\n# Combinación de soluciones\nbmr = benchmark(design)\n\nPodemos ver ahora los resultados obtenidos con cada algoritmo:\n\nautoplot(bmr, measure = msr(\"classif.bacc\"))\n\n\n\n\nTanto el modelo xgboost como lightgbm son os que proporcionan mejores resultados del porcentaje de clasificación ponderado. En cualquier caso los resultados no mejoran significativamente los resultados obtenidos hasta ahora para este banco de datos. Vamos a intentar optimizar el modelo xgboost utilizando el espacio de búsqueda definido el la librería mlr3tuningspaces pero integrándolo manualmente. Para ello utilizamos los valores de búsqueda definidos en https://mlr3tuningspaces.mlr-org.com/reference/mlr_tuning_spaces_default.html.\n\nboost_classif_stroke = lrn(\"classif.xgboost\", \n                         eta = to_tune(1e-04, 1, logscale = TRUE),\n                         nrounds = to_tune(1,5000),\n                         max_depth = to_tune(1,20),\n                         colsample_bytree = to_tune(0.1,1),\n                         colsample_bylevel = to_tune(0.1,1),\n                         lambda = to_tune(1e-03, 1000, logscale = TRUE),\n                         alpha = to_tune(1e-03, 1000, logscale = TRUE),\n                         subsample = to_tune(0.1, 1)  \n                         )\ngr_stroke =  pp_stroke %>>% boost_classif_stroke\ngr_stroke = GraphLearner$new(gr_stroke)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_stroke,\n  learner = gr_stroke,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.bacc\"),\n  term_evals = 50\n)\n\nPodemos ver el resultado del proceso de optimización con:\n\ninstance$result$classif.bacc\n\n[1] 0.538524\n\n\nEl porcentaje de clasificación correcta ha alcanzado el 53.85 que es el valor más alto obtenido hasta ahora para este banco de datos. Podemos analizar con más detalle este modelo:\n\n# Modelo de aprendizaje\nboost_classif_stroke = lrn(\"classif.xgboost\", \n                         eta = instance$result_x_domain$classif.xgboost.eta,\n                         nrounds = instance$result_x_domain$classif.xgboost.nrounds,\n                         max_depth = instance$result_x_domain$classif.xgboost.max_depth,\n                         colsample_bytree = instance$result_x_domain$classif.xgboost.colsample_bytree,\n                         colsample_bylevel = instance$result_x_domain$classif.xgboost.colsample_bylevel,\n                         lambda = instance$result_x_domain$classif.xgboost.lambda,\n                         alpha = instance$result_x_domain$classif.xgboost.alpha,\n                         subsample = instance$result_x_domain$classif.xgboost.subsample  \n                         )\ngr_stroke =  pp_stroke %>>% boost_classif_stroke\ngr_stroke = GraphLearner$new(gr_stroke)\n\n# División de muestras\nset.seed(432)\nsplits = mlr3::partition(tsk_stroke, ratio = 0.8)\ntsk_train_stroke = tsk_stroke$clone()$filter(splits$train)\ntsk_test_stroke  = tsk_stroke$clone()$filter(splits$test)\n\n# Entrenamiento del modelo\ngr_stroke$train(tsk_train_stroke)\n\nObtenemos ahora las predicciones del modelo y la matriz de confusión\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr_stroke$predict(tsk_train_stroke)\npred_test = gr_stroke$predict(tsk_test_stroke)\n# scores de validación\nmeasures = msr(\"classif.bacc\")\n# Muestra de entrenamiento\npred_train$score(measures)\n\nclassif.bacc \n           1 \n\n# Muestra de validación\npred_test$score(measures)\n\nclassif.bacc \n   0.5355967 \n\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nEl porcentaje de clasificación sobre la muestra de validación es del 53.55%. El resultado de la matriz de confusión es algo superior al de otros modelos propuestos anteriormente. Además, en este caso si clasificamos algunas muestras con stroke dado que originalmente provenían de ese grupo.\nPara finalizar el estudio de este modelo es necesario estudiar la validez de la solución y valorar la curva de aprendizaje correspondiente:\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_stroke, gr_stroke, resamp, store_models=TRUE)\n\nINFO  [19:08:37.264] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:08:42.025] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:08:46.841] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:08:51.936] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:09:03.915] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:09:09.840] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:09:21.554] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:09:34.937] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:09:40.236] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:09:47.936] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\n\n# Análisis de los valores obtenidos con los scores definidos anteriormente\nskim(rr$score(measures))\n\n\nData summary\n\n\nName\nrr$score(measures)\n\n\nNumber of rows\n10\n\n\nNumber of columns\n9\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nlist\n4\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntask_id\n0\n1\n6\n6\n0\n1\n0\n\n\nlearner_id\n0\n1\n41\n41\n0\n1\n0\n\n\nresampling_id\n0\n1\n2\n2\n0\n1\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\ntask\n0\n1\n1\n51\n51\n\n\nlearner\n0\n1\n10\n38\n38\n\n\nresampling\n0\n1\n1\n20\n20\n\n\nprediction\n0\n1\n10\n20\n20\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\niteration\n0\n1\n5.50\n3.03\n1.00\n3.25\n5.50\n7.75\n10.00\n▇▇▇▇▇\n\n\nclassif.bacc\n0\n1\n0.54\n0.04\n0.49\n0.50\n0.54\n0.57\n0.59\n▇▁▅▇▅\n\n\n\n\n\nEl valor medio del porcentaje de clasificación correcta ponderada se sitúa en el 53.9% con una desviación típica del 3%. Aunque los resultados son bastante estables el rango de valores abarca modelos con peores porcentajes que los vistos anteriormente. Por último representamos la curva de aprendizaje.\n\n\n\n\nptr = seq(0.1, 0.9, 0.1)\nlcurve = learningcurve(tsk_stroke, gr_stroke, \"classif.bacc\", ptr = ptr, rpeats = 10)\n\nINFO  [19:10:02.517] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:10:06.520] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:10:11.111] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:10:14.044] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:10:17.295] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:10:21.570] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:10:25.363] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:10:29.173] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:10:32.756] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:10:36.830] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\nINFO  [19:10:40.241] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:10:44.231] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:10:45.990] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:10:48.295] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:10:50.205] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:10:52.326] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:10:55.498] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:10:57.162] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:10:58.791] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:11:01.501] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\nINFO  [19:11:05.980] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:11:09.602] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:11:12.419] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:11:15.525] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:11:19.907] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:11:22.300] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:11:24.858] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:11:27.456] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:11:31.377] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:11:33.878] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\nINFO  [19:11:36.696] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:11:39.359] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:11:43.774] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:11:46.315] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:11:49.478] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:11:52.399] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:11:56.280] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:11:58.589] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:12:01.713] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:12:04.442] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\nINFO  [19:12:07.970] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:12:11.615] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:12:14.710] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:12:20.094] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:12:23.193] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:12:26.874] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:12:31.014] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:12:34.476] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:12:37.945] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:12:42.615] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\nINFO  [19:12:45.889] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:12:49.512] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:12:53.333] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:12:57.256] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:13:01.205] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:13:05.336] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:13:09.628] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:13:14.792] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:13:21.104] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:13:26.186] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\nINFO  [19:13:30.403] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:13:34.506] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:13:38.610] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:13:47.761] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:13:52.279] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:13:56.066] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:14:00.345] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:14:08.131] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:14:11.814] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:14:17.195] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\nINFO  [19:14:27.234] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:14:35.052] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:14:39.212] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:14:43.194] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:14:47.171] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:14:56.587] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:15:04.561] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:15:09.318] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:15:14.120] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:15:18.632] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\nINFO  [19:15:23.783] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 1/10)\nINFO  [19:15:31.140] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 2/10)\nINFO  [19:15:35.790] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 3/10)\nINFO  [19:15:40.333] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 4/10)\nINFO  [19:15:44.705] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 5/10)\nINFO  [19:15:49.072] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 6/10)\nINFO  [19:15:53.436] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 7/10)\nINFO  [19:15:58.127] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 8/10)\nINFO  [19:16:03.254] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 9/10)\nINFO  [19:16:08.562] [mlr3] Applying learner 'scale.imputemedian.encode.classif.xgboost' on task 'stroke' (iter 10/10)\n\ndatos = lcurve[lcurve$Sample==\"sco_test\",]\nggplot(datos, aes(ptr, MSR)) + \n    geom_line() +\n    labs(x =\"Proporción tamaño muestra entrenamiento\", y = \"% Clasificación correcta\") +\n    scale_x_continuous(breaks=ptr)\n\n\n\n\nPodemos ver que el mejor valor para el porcentaje de clasificación correcta en la muestra de validación se alcanza para un tamaño de muestra de entrenamiento del 70%.\n\n\n14.4.2.2 Water Potability\nComo en el caso anterior comenzamos definiendo todos los modelos. En este caso todas las predictoras son de tipo numérico.\n\n# Preprocesado\npp_water = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n  po(\"imputemedian\", affect_columns = selector_type(\"numeric\"))\n# Modelo de aprendizaje AdaBoost\n# ==============================\nlrn1 = lrn(\"classif.AdaBoostM1\")\nwater_adaboost = as_learner(pp_water %>>% lrn1)\n\n# Modelo de aprendizaje XGBoost\n# =======================================\nlrn2 = lrn(\"classif.xgboost\")\nwater_xgboost = as_learner(pp_water %>>% lrn2)\n\n# Modelo de aprendizaje lightGBM\n# =======================================\nlrn3 = lrn(\"classif.lightgbm\")\nwater_lightgbm = as_learner(pp_water %>>% lrn3)\n\nDefinimos el proceso de remuestreo necesario para la combinación de modelos y el proceso de combinación de todos los modelos definidos:\n\nset.seed(321)\n# Remuestreo\nremuestreo = rsmp(\"cv\", folds = 10)\n# Grid\ndesign = benchmark_grid(tsk_water, \n                        list(water_adaboost, water_xgboost, water_lightgbm), \n                        remuestreo)\n# Combinación de soluciones\nbmr = benchmark(design)\n\nPodemos ver ahora los resultados obtenidos con cada algoritmo:\n\nautoplot(bmr, measure = msr(\"classif.bacc\"))\n\n\n\n\nPara este conjunto de datos el algoritmo lightgbm es claramente mejor que los otros con un porcentaje medio de clasificación ponderada por encima del 60%. Este resultado es mucho mejor que los obtenidos hasta ahora para este conjunto de datos. Analizamos los resultados de este modelo introduciendo el proceso de optimización para la tasa de aprendizaje:\n\nboost_classif_water = lrn(\"classif.lightgbm\", \n                         learning_rate = to_tune(1e-04, 10, logscale = TRUE)\n                         )\ngr_water =  as_learner(pp_water %>>% boost_classif_water)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_water,\n  learner = gr_water,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.bacc\"),\n  term_evals = 50\n)\n\nPodemos ver el resultado del proceso de optimización con:\n\ninstance$result$classif.bacc\n\n[1] 0.6140859\n\n\nEl porcentaje de clasificación correcta ha alcanzado el 61.405 que es el valor más alto obtenido hasta ahora para este banco de datos. Podemos analizar con más detalle este modelo:\n\n# Modelo de aprendizaje\nboost_classif_water = lrn(\"classif.lightgbm\", \n                         learning_rate = instance$result_x_domain$classif.lightgbm.learning_rate\n                         )\ngr_water =  as_learner(pp_stroke %>>% boost_classif_water)\n\n# División de muestras\nset.seed(432)\nsplits = mlr3::partition(tsk_water, ratio = 0.8)\ntsk_train_water = tsk_water$clone()$filter(splits$train)\ntsk_test_water  = tsk_water$clone()$filter(splits$test)\n\n# Entrenamiento del modelo\ngr_water$train(tsk_train_water)\n\nObtenemos ahora las predicciones del modelo y la matriz de confusión\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr_water$predict(tsk_train_water)\npred_test = gr_water$predict(tsk_test_water)\n# scores de validación\nmeasures = msr(\"classif.bacc\")\n# Muestra de entrenamiento\npred_train$score(measures)\n\nclassif.bacc \n   0.9166761 \n\n# Muestra de validación\npred_test$score(measures)\n\nclassif.bacc \n   0.6135156 \n\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nEl porcentaje de clasificación sobre la muestra de validación es del 61.35%. El resultado de la matriz de confusión es superior al de otros modelos propuestos anteriormente. En este caso el modelo ya es capaz de detectar muestras como potables, lo que no ocurría con otros modelos. De hecho, el 15.4% de las muestras que eran potables son clasificadas como tales con el modelo propuesto. Sin embargo, aún tenemos un 23.6% de muestras que originalmente eran potables y que nuestro modelo no es capaz de clasificar correctamente.\nEstudiamos la validez de la solución y la curva de aprendizaje.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_water, gr_water, resamp, store_models=TRUE)\n\nINFO  [19:18:28.577] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:18:30.695] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:18:32.160] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:18:33.258] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:18:34.127] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:18:35.009] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:18:36.240] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:18:37.919] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:18:38.828] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:18:40.857] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\n\n# Análisis de los valores obtenidos con los scores definidos anteriormente\nskim(rr$score(measures))\n\n\nData summary\n\n\nName\nrr$score(measures)\n\n\nNumber of rows\n10\n\n\nNumber of columns\n9\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nlist\n4\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntask_id\n0\n1\n8\n8\n0\n1\n0\n\n\nlearner_id\n0\n1\n42\n42\n0\n1\n0\n\n\nresampling_id\n0\n1\n2\n2\n0\n1\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\ntask\n0\n1\n1\n51\n51\n\n\nlearner\n0\n1\n10\n38\n38\n\n\nresampling\n0\n1\n1\n20\n20\n\n\nprediction\n0\n1\n10\n20\n20\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\niteration\n0\n1\n5.50\n3.03\n1.00\n3.25\n5.50\n7.75\n10.00\n▇▇▇▇▇\n\n\nclassif.bacc\n0\n1\n0.61\n0.03\n0.56\n0.59\n0.61\n0.63\n0.66\n▅▂▇▇▂\n\n\n\n\n\nEl valor medio del porcentaje de clasificación correcta ponderada se sitúa en el 60.8% con una desviación típica del 3%. En este caso el valor mínimo se sitúa en el 55.8% mostrando un mejor comportamiento que el resto de modelos utilizados sobre este banco de datos. Por último representamos la curva de aprendizaje.\n\nptr = seq(0.1, 0.9, 0.1)\nlcurve = learningcurve(tsk_water, gr_water, \"classif.bacc\", ptr = ptr, rpeats = 10)\n\nINFO  [19:18:50.205] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:18:53.729] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:18:57.212] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:19:00.490] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:19:02.180] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:19:04.111] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:19:05.781] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:19:07.169] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:19:08.080] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:19:09.028] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\nINFO  [19:19:11.207] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:19:13.795] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:19:15.688] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:19:16.607] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:19:17.563] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:19:18.460] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:19:19.479] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:19:20.379] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:19:21.499] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:19:22.539] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\nINFO  [19:19:23.884] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:19:24.946] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:19:27.549] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:19:28.799] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:19:29.836] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:19:30.912] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:19:32.615] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:19:33.765] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:19:34.856] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:19:36.453] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\nINFO  [19:19:37.996] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:19:39.202] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:19:39.846] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:19:40.780] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:19:41.547] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:19:42.305] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:19:43.297] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:19:44.080] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:19:44.758] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:19:45.697] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\nINFO  [19:19:48.039] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:19:49.478] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:19:50.229] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:19:51.153] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:19:52.007] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:19:53.099] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:19:53.785] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:19:54.650] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:19:55.561] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:19:56.325] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\nINFO  [19:19:58.710] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:19:59.763] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:20:00.668] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:20:01.748] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:20:02.399] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:20:03.782] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:20:04.463] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:20:05.272] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:20:05.912] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:20:06.914] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\nINFO  [19:20:08.731] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:20:11.067] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:20:12.609] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:20:13.928] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:20:14.665] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:20:15.361] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:20:16.078] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:20:16.704] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:20:17.428] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:20:18.124] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\nINFO  [19:20:19.175] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:20:20.619] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:20:21.470] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:20:22.572] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:20:23.400] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:20:24.522] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:20:25.600] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:20:26.765] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:20:27.767] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:20:28.719] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\nINFO  [19:20:30.420] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 1/10)\nINFO  [19:20:32.778] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 2/10)\nINFO  [19:20:33.846] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 3/10)\nINFO  [19:20:35.450] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 4/10)\nINFO  [19:20:36.411] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 5/10)\nINFO  [19:20:37.126] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 6/10)\nINFO  [19:20:37.870] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 7/10)\nINFO  [19:20:38.647] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 8/10)\nINFO  [19:20:39.317] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 9/10)\nINFO  [19:20:40.349] [mlr3] Applying learner 'scale.imputemedian.encode.classif.lightgbm' on task 'waterpot' (iter 10/10)\n\ndatos = lcurve[lcurve$Sample==\"sco_test\",]\nggplot(datos, aes(ptr, MSR)) + \n    geom_line() +\n    labs(x =\"Proporción tamaño muestra entrenamiento\", y = \"% Clasificación correcta\") +\n    scale_x_continuous(breaks=ptr)\n\n\n\n\nEn este caso el porcentaje de clasificación correcta no deja de aumentar conforme aumenta la muestra de tamaño de entrenamiento. Esto puede significar un problema de sobreajuste ya que cuanto más aumentamos mejor resultado tenemos. Como en el banco de datos anterior podríamos utilizar un porcentaje del 70% o 80%.\n\n\n14.4.2.3 Housing in California\nVeamos ahora el análisis del modelo de regresión. En este caso consideramos los modelos gbm, xgboost, y ligthgbm. Comenzamos definiendo los modelos de aprendizaje.\n\n# Preprocesado\npp_housing = \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n   po(\"imputemedian\", affect_columns = selector_type(\"numeric\")) %>>%\n   po(\"encode\", param_vals = list(method = \"one-hot\"))\n\n# Modelo de aprendizaje AdaBoost\n# ==============================\nlrn1 = lrn(\"regr.gbm\")\nhousing_gbm = as_learner(pp_housing %>>% lrn1)\n\n# Modelo de aprendizaje XGBoost\n# =======================================\nlrn2 = lrn(\"regr.xgboost\")\nhousing_xgboost = as_learner(pp_housing %>>% lrn2)\n\n# Modelo de aprendizaje lightGBM\n# =======================================\nlrn3 = lrn(\"regr.lightgbm\")\nhousing_lightgbm = as_learner(pp_housing %>>% lrn3)\n\nDefinimos el proceso de remuestreo necesario para la combinación de modelos y el proceso de combinación de todos los modelos definidos:\n\nset.seed(321)\n# Remuestreo\nremuestreo = rsmp(\"cv\", folds = 10)\n# Grid\ndesign = benchmark_grid(tsk_housing, \n                        list(housing_gbm, housing_xgboost, housing_lightgbm), \n                        remuestreo)\n# Combinación de soluciones\nbmr = benchmark(design)\n\nPodemos ver ahora los resultados obtenidos con cada algoritmo, utilizando el score smape:\n\nautoplot(bmr, measure = msr(\"regr.smape\"))\n\n\n\n\nEl modelo con un menor valor de smape es lightgbm, seguido muy cerca por gbm. De nuevo, analizamos este modelo buscando el óptimo de la tasa de aprendizaje.\n\nboost_regr_housing = lrn(\"regr.lightgbm\", \n                         learning_rate = to_tune(1e-04, 10, logscale = TRUE)\n                         )\ngr_housing =  as_learner(pp_housing %>>% boost_regr_housing)\n\n# Fijamos semilla para reproducibilidad del proceso\nset.seed(123)\n# Definimos instancia de optimización fijando el número de evaluaciones\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = tsk_housing,\n  learner = gr_housing,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"regr.smape\"),\n  term_evals = 50\n)\n\nPodemos ver el resultado del proceso de optimización con:\n\ninstance$result$regr.smape\n\n[1] 0.1595812\n\n\nEl valor de smape obtenido es el más bajo de todos los modelos propuestos hasta ahora, por lo que vamos a estudiar con algo más de detalle este modelo:\n\n# Modelo de aprendizaje\nboost_regr_housing = lrn(\"regr.lightgbm\", \n                         learning_rate = instance$result_x_domain$regr.lightgbm.learning_rate\n                         )\ngr_housing =  as_learner(pp_housing %>>% boost_regr_housing)\n\n# División de muestras\nset.seed(432)\nsplits = mlr3::partition(tsk_housing, ratio = 0.8)\ntsk_train_housing = tsk_housing$clone()$filter(splits$train)\ntsk_test_housing  = tsk_housing$clone()$filter(splits$test)\n\n# Entrenamiento del modelo\ngr_housing$train(tsk_train_housing)\n\nObtenemos ahora las predicciones del modelo y evaluamos los scores sobre la muestra de entrenamiento y validación.\n\n# Predicción de la muestra de entrenamiento y validación\npred_train = gr_housing$predict(tsk_train_housing)\npred_test = gr_housing$predict(tsk_test_housing)\n# scores de validación\nmeasures = msr(\"regr.smape\")\n# Muestra de entrenamiento\npred_train$score(measures)\n\nregr.smape \n 0.1305084 \n\n# Muestra de validación\npred_test$score(measures)\n\nregr.smape \n 0.1596983 \n\n\nEl resultado sigue siendo bastante bueno para la muestra de validación. Finalizamos con el análisis de validación y la representación de la curva de aprendizaje.\nEstudiamos la validez de la solución y la curva de aprendizaje.\n\n# Fijamos semilla\nset.seed(135)\n# Definimos proceso de validación cruzada kfold con k=10\nresamp = rsmp(\"cv\", folds = 10)\n# Remuestreo\nrr = resample(tsk_housing, gr_housing, resamp, store_models=TRUE)\n\nINFO  [19:23:41.180] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:23:41.859] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:23:42.531] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:23:43.213] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:23:43.912] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:23:44.603] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:23:45.358] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:23:46.090] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:23:46.825] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:23:47.579] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\n\n# Análisis de los valores obtenidos con los scores definidos anteriormente\nskim(rr$score(measures))\n\n\nData summary\n\n\nName\nrr$score(measures)\n\n\nNumber of rows\n10\n\n\nNumber of columns\n9\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nlist\n4\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntask_id\n0\n1\n9\n9\n0\n1\n0\n\n\nlearner_id\n0\n1\n39\n39\n0\n1\n0\n\n\nresampling_id\n0\n1\n2\n2\n0\n1\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\ntask\n0\n1\n1\n48\n48\n\n\nlearner\n0\n1\n10\n38\n38\n\n\nresampling\n0\n1\n1\n20\n20\n\n\nprediction\n0\n1\n10\n19\n19\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\niteration\n0\n1\n5.50\n3.03\n1.00\n3.25\n5.50\n7.75\n10.00\n▇▇▇▇▇\n\n\nregr.smape\n0\n1\n0.16\n0.00\n0.15\n0.16\n0.16\n0.16\n0.16\n▇▂▅▂▇\n\n\n\n\n\nEl valor medio del smape se sitúa en el 0.157 con una desviación típica del 0.001. El rango de scores es muy estrecho indicando que la solución es muy estable. Por último representamos la curva de aprendizaje.\n\nptr = seq(0.1, 0.9, 0.1)\nlcurve = learningcurve(tsk_housing, gr_housing, \"regr.smape\", ptr = ptr, rpeats = 10)\n\nINFO  [19:23:49.542] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:23:50.601] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:23:51.565] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:23:52.545] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:23:53.529] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:23:54.517] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:23:55.597] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:23:56.672] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:23:57.575] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:23:58.531] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\nINFO  [19:23:59.638] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:24:00.548] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:24:01.470] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:24:02.773] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:24:03.631] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:24:04.479] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:24:05.349] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:24:06.190] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:24:07.095] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:24:07.944] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\nINFO  [19:24:08.996] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:24:09.859] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:24:10.754] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:24:11.626] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:24:12.489] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:24:13.389] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:24:14.273] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:24:15.152] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:24:16.081] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:24:16.988] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\nINFO  [19:24:18.040] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:24:18.986] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:24:19.913] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:24:20.875] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:24:21.777] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:24:22.730] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:24:23.683] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:24:24.621] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:24:25.559] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:24:26.500] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\nINFO  [19:24:27.777] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:24:28.734] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:24:29.749] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:24:30.715] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:24:31.681] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:24:32.680] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:24:33.655] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:24:34.662] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:24:35.665] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:24:36.683] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\nINFO  [19:24:38.290] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:24:39.285] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:24:40.248] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:24:41.258] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:24:42.229] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:24:43.224] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:24:44.174] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:24:45.168] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:24:46.142] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:24:47.182] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\nINFO  [19:24:48.403] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:24:49.430] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:24:50.441] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:24:51.473] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:24:52.510] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:24:53.544] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:24:54.720] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:24:55.903] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:24:57.133] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:24:58.273] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\nINFO  [19:24:59.546] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:25:00.685] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:25:01.850] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:25:03.380] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:25:04.405] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:25:05.457] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:25:06.489] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:25:07.568] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:25:08.623] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:25:09.687] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\nINFO  [19:25:10.959] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 1/10)\nINFO  [19:25:12.098] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 2/10)\nINFO  [19:25:13.214] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 3/10)\nINFO  [19:25:14.314] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 4/10)\nINFO  [19:25:15.444] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 5/10)\nINFO  [19:25:16.564] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 6/10)\nINFO  [19:25:17.731] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 7/10)\nINFO  [19:25:18.834] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 8/10)\nINFO  [19:25:19.962] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 9/10)\nINFO  [19:25:21.107] [mlr3] Applying learner 'scale.imputemedian.encode.regr.lightgbm' on task 'housingCA' (iter 10/10)\n\ndatos = lcurve[lcurve$Sample==\"sco_test\",]\nggplot(datos, aes(ptr, MSR)) + \n    geom_line() +\n    labs(x =\"Proporción tamaño muestra entrenamiento\", y = \"sMAPE\") +\n    scale_x_continuous(breaks=ptr)\n\n\n\n\nComo ocurría con el banco de datos anterior, el smape se reduce cuando incrementa el tamaño de muestra de entrenamiento. Podemos tener algún problema de sobreaprendizaje que deberíamos controlar para evitar resultados sesgados. Un tamaño del 70% parece una buena opción."
  },
  {
    "objectID": "140_Boostingmodels.html#sec-140.5",
    "href": "140_Boostingmodels.html#sec-140.5",
    "title": "14  Modelos Boosting",
    "section": "14.5 Ejercicios",
    "text": "14.5 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos Mushroom4.3.4.\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos Hepatitis4.3.9.\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos Abalone4.3.1.\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos Us economic time series4.2.7.\nAjustar un modelo de aprendizaje automático basado en bosques aleatorios para el banco de datos QSAR4.2.8."
  },
  {
    "objectID": "04_NonSupervisedAA.html#modelos-reducción-de-la-dimensión",
    "href": "04_NonSupervisedAA.html#modelos-reducción-de-la-dimensión",
    "title": "Parte 4. Aprendizaje no supervisado",
    "section": "Modelos reducción de la dimensión",
    "text": "Modelos reducción de la dimensión\nSupongamos que disponemos de una muestra de \\(n\\) individuos medidos en \\(p\\) variables diferentes con \\(p\\) grande. La idea que subyace en los métodos de reducción de la dimensión es construir \\(k\\) variables artificiales, con \\(k << p\\), que poseen la misma capacidad explicativa que el conjunto original. De esta forma, cada sujeto pasa de ser representado por un vector de \\(p\\) dimensiones (que gráficamente resulta difícil de visualizar) por uno de \\(k\\) dimensiones. Si \\(k \\leq 3\\) resulta posible representar gráficamente la información contenida en todos los sujetos sin pérdida de capacidad explicativa. Los modelos de reducción de la dimensión más habituales son:\n\nComponentes principales que se obtienen como combinaciones lineales de las variables originales y se van construyendo según el orden de importancia en cuanto a la variabilidad total que recogen de la muestra. De modo ideal, se buscan \\(k << p\\) variables que sean combinaciones lineales de las \\(p\\) originales y que estén incorreladas, recogiendo la mayor parte de la información o variabilidad de los datos. Si las variables originales están incorreladas de partida, entonces no tiene sentido realizar un análisis de componentes principales. En mlr3 estos procedimientos se engloban dentro del preprocesamiento y por ese motivo vamos a estudiar esta técnica directamente desde las diferentes librerías de R que permiten su obtención.\nAnálisis Discriminante. Aunque esta englobada dentro de las técnicas de reducción de la dimensión, se utiliza en muchas ocasiones como un modelo de clasificación. El Análisis Discriminante es un método de clasificación supervisado de variables cualitativas en el que dos o más grupos son conocidos a priori y nuevas observaciones se clasifican en uno de ellos en función de sus características. Supongamos que un conjunto de objetos se clasifica en una serie de grupos; el Análisis Discriminante equivale a un análisis de regresión donde la variable dependiente es categórica y tiene como categorías la etiqueta de cada uno de los grupos, y donde las variables independientes son continuas y determinan a qué grupos pertenecen los objetos. Se trata de encontrar relaciones lineales entre las variables continuas que mejor discriminen en los grupos dados a los objetos. Además, se trata de definir una regla de decisión que asigne un objeto nuevo, que no sabemos clasificar previamente, a uno de los grupos prefijados. En este caso si disponemos de modelos de aprendizaje en mlr3.\nEscalas multidimiensionales. El escalamiento multidimensional tiene por objetivo representar los puntos que residen en un espacio de gran dimensión a uno de menor dimensión, preservando al máximo las distancias entre esos puntos. De este modo, las distancias o similitudes entre pares de puntos en el espacio de menor dimensión se aproximan mucho a sus distancias reales. Como las componentes principales necesitamos utilizar las librerías específicas de R."
  },
  {
    "objectID": "04_NonSupervisedAA.html#modelos-de-agrupación-o-cluster",
    "href": "04_NonSupervisedAA.html#modelos-de-agrupación-o-cluster",
    "title": "Parte 4. Aprendizaje no supervisado",
    "section": "Modelos de agrupación o cluster",
    "text": "Modelos de agrupación o cluster\nLos modelos de agrupación o cluster hacen referencia a un amplio abanico de algoritmos cuya finalidad es encontrar patrones o grupos (clusters) dentro de un conjunto de muestras. Las particiones se establecen de forma que, las observaciones que están dentro de un mismo grupo, son similares entre ellas y distintas a las observaciones de otros grupos. Se trata de un método de aprendizaje no supervisado, ya que el proceso no tiene en cuenta a qué grupo pertenece realmente cada observación (si es que existe tal información). Esta característica es la que diferencia al clustering de los métodos de clasificación en los que se emplea la verdadera clasificación durante su entrenamiento.\nDada la utilidad del clustering en disciplinas muy distintas (genómica, marketing…), se han desarrollado multitud de variantes y adaptaciones de sus métodos y algoritmos. Podemos distinguir tres grupos:\n\nAgrupación jerárquica: este tipo de algoritmos no requieren que el usuario especifique de antemano el número de grupos.\nAgrupación partitiva: este tipo de algoritmos requieren que el usuario especifique de antemano el número de clusters que se van a crear.\nMétodos combinados: algortimos que combinan los dos grupos anteriores."
  },
  {
    "objectID": "150_Discriminantmodels.html#sec-150.1",
    "href": "150_Discriminantmodels.html#sec-150.1",
    "title": "15  Análisis discriminante (AD)",
    "section": "15.1 Planteamiento del problema",
    "text": "15.1 Planteamiento del problema\nPara mostrar el uso del algoritmo de Análisis Discriminante comenzamos con la situación más sencilla donde disponemos de una única predictora y dos grupos.\nConsideramos una respuesta \\(Y\\) de tipo categórico con \\(K=2\\) grupos y una predictora \\(X\\) de tipo numérico. Se denota entonces:\n\n\\(\\pi_1\\) y \\(\\pi_2\\) como las probabilidades previas de que una observación aleatoria pertenezca a la clase \\(k\\) de la respuesta \\(Y\\).\n\\(f_k(x) \\equiv P(X=x|Y=k), k=1,2\\) son las funciones de densidad de probabilidad condicional de \\(X\\) para una observación que pertenece a la clase \\(k\\). Cuanto mayor sea \\(f_k(X)\\) mayor la probabilidad de que una observación de la clase \\(k\\) adquiera un valor de \\(X≈x\\).\n\\(P(Y=k|X=x)\\) son las probabilidades a posteriori de que una observación pertenezca a la clase \\(k\\) siendo \\(x\\) el valor del predictor.\n\nHaciendo uso del teorema de Bayes tenemos que:\n\\[P(Y=k|X=x) =  \\frac{\\pi_kf_k(x)}{\\sum_{i=1}^2 \\pi_kf_k(X)}\\]\nLa clasificación con menor error (clasificación de Bayes) se consigue asignando la observación a aquel grupo que maximice la probabilidad posterior. Dado que el denominador \\(\\sum_{i=1}^2 \\pi_kf_k(X)\\) es igual para todas las clases, la norma de clasificación es equivalente a decir que se asignará cada observación a aquel grupo para el que \\(\\pi_kf_k(x)\\) sea mayor. Si las probabilidades a priori son iguales la regla de clasificación asigna a la categoría \\(k\\) con mayor \\(f_k(x)\\).\nSi tenemos en cuenta los errores de clasificación \\(c(i|j)\\) de clasificar un objeto en el grupo \\(i\\) cuando realmente pertenece al \\(j\\), la regla de clasificación para clasificar en el grupo 2 frente al 1 viene dada por:\n\\[\\frac{f_2(x)\\pi_2}{c(2|1)} > \\frac{f_1(x)\\pi_1}{c(1|2)}.\\]\nDe forma análoga podemos establecer la regla de clasificación en el grupo 1 frente al 2. Si los errores de clasificación y probabilidades iniciales son iguales la regla de clasificación se simplifica a la evaluación de la función de densidad dentro de cada grupo.\nGeneralizar la regla de decisión cuando disponemos de un conjunto \\(X\\) de predictoras consiste simplemente en la evaluación de las probabilidades a priori y las densidades correspondientes.\nLos dos algoritmos más conocidos del análisis discriminante son el análisis discriminante lineal (LDA) y el análisis discriminante cuadrático (QDA). La principal diferencia entre ellos es que el LDA asume que la matriz de varianzas y covarianzas asociada con las predictoras en cada uno de los grupos son iguales, es decir, asumimos homogeneidad de varianzas entre los grupos, mientras que en QDA dichas matrices no tienen por qué ser iguales."
  },
  {
    "objectID": "150_Discriminantmodels.html#sec-150.2",
    "href": "150_Discriminantmodels.html#sec-150.2",
    "title": "15  Análisis discriminante (AD)",
    "section": "15.2 AD lineal",
    "text": "15.2 AD lineal\nLos aspectos teóricos relacionados con el LDA se estructuran presentando en primer lugar los fundamentos del algoritmo, el proceso de estimación necesario para evaluar las funciones discriminantes y finalizamos con el proceso de obtención de las funciones discriminantes canónicas. Comenzamos con la situación más sencilla donde consideramos dos grupos pero generalizaremos al caso de \\(k\\) grupos.\n\n15.2.1 Fundamentos\nPara que la clasificación basada en Bayes sea posible, se necesita conocer la probabilidad poblacional de que una observación cualquiera pertenezca a cada clase (\\(\\pi_k\\)) y la probabilidad poblacional de que una observación que pertenece a la clase \\(k\\) adquiera el valor \\(x\\) en el predictor \\(f_k(x)\\).\nEn el caso de la probabilidad a priori la estimación suele ser sencilla. La probabilidad de que una observación cualquiera pertenezca a la clase \\(k\\) es igual al número de observaciones de esa clase entre el número total de observaciones \\(\\hat{\\pi} = n_k/N\\), donde \\(n_k\\) es el número de observaciones en el grupo \\(k\\) y \\(N\\) es el número total de observaciones.\nLa estimación de \\(f_k(X)\\) no es tan directa y por ahora asumimos que dichas funciones de densidad son distribuciones normales multivariantes con vector de medias (\\(\\mu_i\\)) distintas para cada grupo pero con la misma matriz de varianzas-covarianzas (\\(\\Sigma\\)), es decir:\n\\[f_i(X) = \\frac{1}{(2\\pi)^{k/2}|\\Sigma|^{1/2}} exp\\left\\{-\\frac{1}{2}(x-\\mu_i)^t \\Sigma^{-1}(x-\\mu_i)\\right\\}.\\]\nDe esta forma, la regla de clasificación en el grupo 2 frente al grupo 1 (considerando logaritmos), asumiendo que los errores de clasificación son iguales, viene dada por:\n\\[log(\\pi_2) + log(f_2(X)) > log(\\pi_1) + log(f_1(X))\\]\nque sustituyendo por las funciones de densidad correspondientes tenemos que:\n\\[log(\\pi_2) - \\frac{1}{2}(x-\\mu_2)^t \\Sigma^{-1}(x-\\mu_2) > log(\\pi_1) + - \\frac{1}{2}(x-\\mu_1)^t \\Sigma^{-1}(x-\\mu_1)\\]\nque tras operar nos conduce a:\n\\[ (x-\\mu_1)^t \\Sigma^{-1}(x-\\mu_1) > (x-\\mu_2)^t \\Sigma^{-1}(x-\\mu_2) - 2log(\\pi_2/\\pi_1).\\]\nLos términos \\((x-\\mu_1)^t \\Sigma^{-1}(x-\\mu_1)\\) y \\((x-\\mu_2)^t \\Sigma^{-1}(x-\\mu_2)\\) son las distancias de Mahalanobis del punto \\(x\\) con respecto a las medias de cada uno de los grupos. Mediante cálculos sencillos la regla de clasificación anterior viene dada por:\n\\[w^{t}x = w^{t}\\left(\\frac{\\mu_1 + \\mu_2}{2}\\right)-log(\\pi_2/\\pi_1)\\]\ndonde \\(w=\\Sigma^{-1}(\\mu_2-\\mu_1)\\). La ecuación anterior corresponde con la de un hiperplano de forma que el procedimiento de clasificación puede resumirse así:\n\nDados los valores de \\(\\mu_1\\), \\(\\mu_2\\) y \\(\\Sigma\\) obtenemos en primer lugar el vector \\(w\\).\nEscribir la función discriminante como una combinación lineal de los valores de la variable con los pesos dados por el vector \\(w\\):\n\n\\[g(x) = w^{t}x,\\]\n\nIntroducir en esta función los valores observados para el nuevo individuo a clasificar, \\(x_0 = (x_{10},...,x_{k0})\\) y evaluar la regla de clasificación establecida.\n\nSi los costes y probabilidades a priori son iguales la regla de decisión se reduce entonces a clasificar en el grupo 2 si:\n\\[w^{t}x > w^{t}\\left(\\frac{\\mu_1+\\mu_2}{2}\\right)\\]\nEsta regla equivale a proyectar el punto x que queremos clasificar y las medias de ambas poblaciones sobre una recta, y después asignar el punto a aquella población de cuya media se encuentre más próxima en la proyección. La función \\(w^{t}x\\) se denomina función discriminante.\n\n\n15.2.2 Estimación de parámetros\nEn la práctica, a pesar de tener una certeza considerable de que \\(X\\) se distribuye de forma normal dentro de cada clase, los valores \\(\\mu_1...,\\mu_k\\), \\(\\pi_1,...,\\pi_k\\) y \\(\\Sigma\\) se desconocen, por lo que tienen que ser estimados a partir de las observaciones. En este caso abordamos directamente el caso de \\(K\\) grupos o poblaciones.\nLa matriz general de predictoras \\(X\\) de dimensiones q × n (q variables y n individuos), se puede particionar ahora en \\(K\\) matrices correspondientes a las subpoblaciones. Se denomina \\(x_{ijk}\\) a los elementos de estas submatrices, donde \\(i\\) representa el individuo, \\(j\\) la variable y \\(k\\) el grupo o submatriz. Llamaremos \\(n_k\\) al número de elementos en el grupo \\(k\\) y el número total de observaciones se denomina \\(n\\).\nSe denomina \\(x^{t}_{ik}\\) al vector fila que contiene los q valores de las variables para el individuo \\(i\\) en el grupo \\(k\\), es decir,\n\\[x^{t}_{ik} = (x_{i1k},...,x_{iqk})\\]\nEl vector de medias dentro de cada clase o grupo es:\n\\[\\bar{x}_k = \\frac{1}{n_k}\\sum_{i=1}^{n_k} x_{ik}\\]\nmientras que la matriz de varianzas-covarianzas para los sujetos del grupo \\(k\\) viene dada por:\n\\[\\hat{S}_k=\\frac{1}{n_k-1} \\sum_{i=1}^{n_k} (x_{ik}-\\bar{x}_k)(x_{ik}-\\bar{x}_k)^t.\\]\nSi los \\(k\\) grupos o clases tienen la misma matriz de varianzas-covarianzas una estimación de dicha matriz viene dada por:\n\\[\\hat{S}_{w} = \\sum_{k=1}^K \\frac{n_k-1}{n-k}\\hat{S}_k.\\]\nLlamaremos \\(W\\) a la matriz de sumas de cuadrados dentro de las clases que viene dada por:\n\\[W = (n-K)\\hat{S}_w.\\]\nPodemos utilizar ahora las estimaciones consideradas para obtener las correspondientes funciones discriminantes.\n\n\n15.2.3 Variables canócicas discriminantes\nEl enfoque anterior puede generalizarse para encontrar variables canónicas que tengan el máximo poder discriminante para clasificar nuevos elementos respecto a las poblaciones. El objetivo es, en lugar de trabajar con las variables originales \\(X\\), definir \\(r\\) variables canónicas, \\(z_i, i = 1,...,r\\) donde \\(r = min(K − 1, q)\\), que sean combinación lineal de las originales \\(z_i = w_i^tx\\) de modo que:\n\nLas medias de las poblaciones \\(\\mu_k\\) se expresan en términos de las variables canónicas donde \\(z_1,...,z_K\\) son vectores r × 1 cuyas coordenadas son las proyecciones de las medias sobre las \\(r\\) variables canónicas;\nSe hace lo mismo para el punto \\(x_0\\) a clasificar donde se denota a \\(z_0\\) como dicho vector;\nClasificamos el punto en aquella población de cuya media se encuentre más próxima, con la distancia euclídea, en el espacio de las variables canónicas \\(z\\); es decir, lo clasificaremos en la población \\(i\\) si\n\n\\[(z_0-z_i)'(z_0-z_i) = \\underset{k}{min} (z_0-z_k)'(z_0-z_k).\\]\nTan sólo nos resta establecer el procedimiento para la obtención de las variables canónicas. Para resolver este problema debemos buscar un vector \\(w\\) tal que, cuando proyectamos los puntos sobre él, se obtiene la máxima variabilidad entre los grupos en relación a la variabilidad dentro de los grupos. La media de las observaciones del grupo \\(k\\) en esta nueva variable será:\n\\[\\bar{z}_k = w^t\\bar{x}_k\\]\ny la media para todos los datos viene dada por:\n\\[\\bar{z}_T=w^t\\bar{x}_T.\\]\nSe desea encontrar el vector \\(w\\) de manera que la separación entre las medias de los grupos sea máxima. Una medida de la distancia entre las medias \\(z_1,..., z_K\\) es la suma de cuadrados entre las medias dada por\n\\[\\sum_{k=1}^K n_k(\\bar{z}_k-\\bar{z}_T)^2 = w^tBw,\\]\ncon \\(B\\) la matriz de suma de cuadrados entre grupos. Para juzgar si este término es grande o pequeño, debemos compararlo con la variabilidad intrínseca de los datos o variabilidad intra clase dada por:\n\\[\\sum_{j=1}^{n_k}\\sum_{k=1}^K (z_{ik}-\\bar{z}_k)^2 = w^tWw.\\]\nEn definitiva, el criterio para encontrar la mejor dirección de proyección consiste en maximizar la separación relativa entre las medias, dada por:\n\\[\\underset{w}{max} \\frac{w^tBw}{w^tWw}.\\]\nLa solución con este problema de maximización viene dada por los vectores propios asociados a los valores propios ordenados según su valor de mayor a menor de la matriz \\(W^{-1}B.\\) Por tanto, la primera función discriminante se corresponde con la combinación lineal de predictoras donde los valores de \\(w\\) son los valores del vector propio asociado con el mayor valor propio de \\(W^{-1}B\\). Los vectores propios de dicha matriz no serán, en general, ortogonales, y además el rango de dicha matriz será \\(r\\), es decir, el número máximo de funciones discriminantes que podemos construir. Tenemos entonces la funciones discriminantes dadas por la expresión:\n\\[z=U^tx\\]\ndonde \\(U^t\\) es la matriz de dimensiones \\(r \\times q\\) que contiene los vectores propios de \\(W^{-1}B\\) por filas. Las variables canónicas así obtenidas resuelven el problema de clasificación, de forma que, para clasificar un nuevo individuo \\(x_0\\) basta con calcular sus coordenadas \\(z_0\\) con la expresión anterior y asignarle al grupo de cuya media transformada esté más próxima mediante la distancia euclídea.\nSi trabajamos con variables estandarizadas los vectores propios obtenidos nos proporcionan además la relevancia de cada predictora en la función de discriminación. Así mismo podemos utilizar los valores propios obtenidos para valorar la capacidad discriminatoria del modelo o el número de funciones discriminantes necesarias para obtener una buena clasificación. Este procedimiento es similar al seguido con las componentes principales para la reducción de la dimensión.\n\n\n15.2.4 Probabilidades de cada clase\nUna alternativa a la solución anterior a partir de los estimadores anteriores viene dada por los logaritmos de las probabilidades de cada etiqueta \\(k\\) que se pueden obtener a partir de la expresiones:\n\\[log(P(y=k|x) = -0.5(x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + log(\\pi_k)) + Constante\\]\no alternativamente\n\\[log(P(y=k|x) = \\beta_k^{t}x+\\beta_{k0} + Constante\\]\ncon \\(\\beta_k = \\Sigma^{-1}\\mu_k\\) y \\(\\beta_{k0} = -0.5\\mu^t\\Sigma^{-1}\\mu_k + log(\\pi_k).\\)\nSi solo tenemos dos posibles etiquetas tan solo disponemos de una única ecuación y vector de pesos \\(\\beta\\).\n\n\n15.2.5 Precisión de la solución\nLa precisión o bondad de la solución obtenida se analiza mediante los procedimientos habituales de los problemas de clasificación entre los que el más relevante es el análisis de la matriz de confusión."
  },
  {
    "objectID": "150_Discriminantmodels.html#sec-150.3",
    "href": "150_Discriminantmodels.html#sec-150.3",
    "title": "15  Análisis discriminante (AD)",
    "section": "15.3 AD no lineal",
    "text": "15.3 AD no lineal\nEl análisis discriminante no lineal se asocia casi siempre con El clasificador cuadrático o Quadratic Discriminat Analysis QDA se asemeja en gran medida al LDA, con la única diferencia de que el QDA considera que cada clase k tiene su propia matriz de covarianzas (\\(\\Sigma_k\\)) y, como consecuencia, la función para estimar el logaritmo de la probabilidad de cada clase viene dada por:\n\\[log(P(y=k|x) = -0.5log|\\Sigma_k|-0.5(x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k) + log(\\pi_k)\\]\nEstas funciones no son lineales por lo que se generan límites de decisión curvos y pueden aplicarse a situaciones en las que la separación entre grupos no es lineal. Además, dado que consideramos matrices de varianzas-covarianzas distintas no resulta necesario la verificación de la hipótesis de homogeneidad."
  },
  {
    "objectID": "150_Discriminantmodels.html#sec-150.4",
    "href": "150_Discriminantmodels.html#sec-150.4",
    "title": "15  Análisis discriminante (AD)",
    "section": "15.4 AD en R",
    "text": "15.4 AD en R\nAunque en mlr3 se encuentran disponibles los modelos de aprendizaje classif.lda y classif.qda para el análisis discriminante lineal y cuadrático, es este caso optamos por utilizar directamente las funciones de R disponibles en las librerías MASS (instalada por defecto) y rda (que es necesario instalar). En los puntos siguientes se mostrara como hacer uso de las funciones de esas librerías para llevar a cabo el análisis discriminante. Para las soluciones gráficas deberemos instalar además los paquetes klaR y ggord. Para instalar este segundo es necesario ejecutar el código siguiente:\nEn primer lugar cargamos todas las librerías necesarias:\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(cvms)\nlibrary(kknn)\nlibrary(rpart.plot)\nlibrary(rda)\nlibrary(klaR)\nlibrary(ggord)\ntheme_set(theme_sjplot2())\n\n# Paquetes AA\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3tuningspaces)\nlibrary(gbm)\nlibrary(RWeka)\nlibrary(xgboost)\nlibrary(lightgbm)\n\n\n15.4.1 Bancos de datos\nPara ejemplificar el uso de los modelos de análisis dicriminante vamos a utilizar tres bancos de datos: water potability para la clasificación de dos grupos, Wine recognotion, y Abalone para la clasificación de tres grupos. A continuación presentamos los tres bancos de datos y el código para cargar las bases de datos correspondientes.\n\n15.4.1.1 Water Potability\nEl código para cargar el banco de datos es:\n\n# Leemos datos\nwaterpot = read_rds(\"waterpot.rds\")\n# creamos la tarea\ntsk_water = as_task_classif(waterpot, target = \"Potability\", positive=\"1\")\n# Generamos variable de estrato\ntsk_water$col_roles$stratum <- \"Potability\"\n\n\n\n15.4.1.2 Wine recognition\nEn este banco de datos se recoge el resultado de un análisis químico de vinos cultivados en la misma región de Italia pero procedentes de tres cultivos distintos. El análisis determinó las cantidades de 13 características que se encuentran en cada una de las muestras de vinos. El objetivo que perseguimos es clasificar cada muestra en una de estas tres clases de vino (Class label) en función de sus características de tipo numérico. Este banco de datos no contiene valores perdidos.\n\n# Leemos datos\nwinerecognition = read_rds(\"winerecognition.rds\")\nnames(winerecognition) = c(\"Class\", \"Alcohol\", \"Malic_acid\", \"Ash\", \"Alcalinity_of_ash\", \"Magnesium\", \"Total_phenols\", \"Flavanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\", \"Color_intensity\", \"Hue\", \"OD280_OD315_of_diluted_wines\", \"Proline\")\n# creamos la tarea\ntsk_wine = as_task_classif(winerecognition, target = \"Class\")\n# Generamos variable de estrato\ntsk_wine$col_roles$stratum <- \"Class\"\n\n\n\n15.4.1.3 Abalone\nEn este conjunto de datos se recoge información sobre los abulones, de la familia de los moluscos. Se está interesado en medir su desarrollo, que viene determinado principalmente por su desarrollo sexual. Concretamente se consideran tres estados de desarrollo asociados con el atributo Sex: M``(machos),F(hembras), eI` (infantil o sin desarrollo sexual). Para clasificar cada sujeto se utiliza un conjunto de características que son de tipo numérico.\n\n# Leemos los datos\nabalone = read_rds(\"abalone.rds\")\nnames(abalone) = c(\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\")  \n# creamos la tarea\ntsk_abalone = as_task_classif(abalone, target = \"Sex\")\n# Generamos variable de estrato\ntsk_abalone$col_roles$stratum <- \"Sex\"\n\n\n\n\n15.4.2 Modelos\nPara cada banco de datos probamos diferentes modelos de análisis discriminante para ver como afectan las predictoras en la clasificación a través de la construcción de las funciones discriminantes.\n\n15.4.2.1 Water potability\nComenzamos con el análisis del banco de datos de potabilidad del agua donde queremos discriminar entres dos grupos. En este caso solo podremos obtener una función discriminante como combinación de las predictoras consideradas. Comenzamos por dividir las muestras e imputar valores perdidos.\n\n# División de muestras\nset.seed(432)\nsplits = mlr3::partition(tsk_water, ratio = 0.8)\ntsk_train_water = tsk_water$clone()$filter(splits$train)\ntsk_test_water  = tsk_water$clone()$filter(splits$test)\n# preprocesado\npp_water = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) %>>%\n  po(\"imputemedian\", affect_columns = selector_type(\"numeric\"))\n# Obtención de muestras\ntrain = pp_water$train(tsk_train_water)\ntest = pp_water$train(tsk_test_water)\nwater_train = train[[1]]$data()\nwater_test = test[[1]]$data()\n\nComenzamos con el modelo discriminante lineal.\n\n# Modelo para la muestra de entrenamiento\nad.lineal = lda(Potability~., water_train)\n# Resultados del modelo\nad.lineal\n\nCall:\nlda(Potability ~ ., data = water_train)\n\nPrior probabilities of groups:\n        1         0 \n0.3900763 0.6099237 \n\nGroup means:\n  Chloramines Conductivity      Hardness Organic_carbon      Solids\n1  0.02445992  0.008382819 -0.0002709496    -0.04684461  0.04526142\n0 -0.01564333 -0.005361227  0.0001732857     0.02995945 -0.02894691\n     Turbidity     Sulfate Trihalomethanes          ph\n1 -0.007620776 -0.03646853   -0.0004807166  0.01402887\n0  0.004873863  0.01680100    0.0013832328 -0.01559109\n\nCoefficients of linear discriminants:\n                        LD1\nChloramines     -0.36974474\nConductivity    -0.10857240\nHardness         0.00524105\nOrganic_carbon   0.58084020\nSolids          -0.55670481\nTurbidity        0.12275871\nSulfate          0.43158436\nTrihalomethanes  0.01286696\nph              -0.37522197\n\n\nLos resultados del modelo son:\n\nProbabilidades a priori de cada uno de los grupos (ad.lineal$prior).\nMedias de las predictoras para cada uno de los grupos (ad.lineal$means).\nCoeficientes de la función discriminante para cada predictora (ad.lineal$scaling). En este caso los coeficientes positivos favorecen el tipo potable, frente a los negativos que favorecen el tipo no potable. La ecuación de la función discriminante viene dada por (redondeando a dos decimales los coeficientes):\n\n\\[LD1 = -0.11 Chloramines + 0.17 Conductivity + 0.01 Hardness + 0.64 Organic_carbon -0.74  Solids + 0.13Turbidity + 0.10Sulfate -0.10Trihalomethanes -0.18ph\\] Los coeficientes también nos permiten saber que predictoras son más relevantes en la función de discriminación. tan solo debemos ver que coeficientes son más grandes en valor absoluto. Para este modelo las predictoras Organic_carbon y Solids son las más relevantes en sentido positivo y negativo respectivamente.\nSi se dispone de más de una función discriminante la solución nos proporciona la proporción de variabilidad explicada (discriminación) de cada función.\nA continuación representamos las puntuaciones de la función discriminante tanto para la muestra de entrenamiento como de validación para valorar la capacidad de discriminación de dicha función. Recordemos que el análisis de reducción nos permite pasar del conjunto inicial de 9 predictoras a una única predictora dada por la función discriminante.\n\n# Muestra de entrenamiento\np <- predict(ad.lineal, water_train)\nldahist(data = p$x[,1], g = water_train$Potability)\n\n\n\n# Muestra de validación\np <- predict(ad.lineal, water_test)\nldahist(data = p$x[,1], g = water_test$Potability)\n\n\n\n\nEn ambas muestras podemos apreciar que las puntuaciones discriminantes para ambos grupos son muy similares, lo que sin duda provoca que nuestra función de discriminación no resulta muy adecuada. Para verificar este hecho vamos a obtener la matriz de confusión correspondiente a este modelo y calcularemos el porcentaje de clasificación correcta ponderada.\n\n# predicción validación\npred_test = data.frame(truth = water_test$Potability, response = predict(ad.lineal, water_test)$class)\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nComo era de esperar los resultados de la matriz de confusión muestran el mal funcionamiento del análisis planteado. Podemos ver el porcentaje de clasificación correcta ponderada con:\n\ncm$`Balanced Accuracy` \n\n[1] 0.5097656\n\n\nComo era de esperar el resultado obtenido es muy malo.\nComenzamos ahora el análisis utilizando un modelo discriminante cuadrático.\n\n# Modelo para la muestra de entrenamiento\nad.quad = qda(Potability~., water_train)\n# Resultados del modelo\nad.quad\n\nCall:\nqda(Potability ~ ., data = water_train)\n\nPrior probabilities of groups:\n        1         0 \n0.3900763 0.6099237 \n\nGroup means:\n  Chloramines Conductivity      Hardness Organic_carbon      Solids\n1  0.02445992  0.008382819 -0.0002709496    -0.04684461  0.04526142\n0 -0.01564333 -0.005361227  0.0001732857     0.02995945 -0.02894691\n     Turbidity     Sulfate Trihalomethanes          ph\n1 -0.007620776 -0.03646853   -0.0004807166  0.01402887\n0  0.004873863  0.01680100    0.0013832328 -0.01559109\n\n\nLa solución del análisis discriminante cuadrático solo nos proporciona las probabilidades a priori y las medias de cada predictora en cada nivel de la respuesta. En este caso no podemos analizar directamente las puntuaciones de la función discriminante, pero si podemos estudiar la matriz de confusión asociada al modelo.\n\n# predicción validación\npred_test = data.frame(truth = water_test$Potability, response = predict(ad.quad, water_test)$class)\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nLos resultados de la matriz de confusión muestran que este modelo tiene un comportamiento mejor que el lineal. Veamos el porcentaje de clasificación correcta ponderada.\n\ncm$`Balanced Accuracy` \n\n[1] 0.6476563\n\n\nEl resultado obtenido es casi tan bueno como el del mejor modelo que habíamos obtenido hasta ahora para este conjunto de datos.\n\n\n15.4.2.2 Wine recognition\nEn este caso disponemos de tres clases de vinos por lo que por defecto el análisis discriminante obtendrá dos funciones discriminantes en su solución. Comenzamos por el modelo lineal definiendo las muestras de entrenamiento y test.\n\n# División de muestras\nset.seed(432)\nsplits = mlr3::partition(tsk_wine, ratio = 0.8)\ntsk_train_wine = tsk_wine$clone()$filter(splits$train)\ntsk_test_wine  = tsk_wine$clone()$filter(splits$test)\n# preprocesado\npp_wine = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) \n# Obtención de muestras\ntrain = pp_wine$train(list(tsk_train_wine))\ntest = pp_wine$train(list(tsk_test_wine))\nwine_train = train[[1]]$data()\nwine_test = test[[1]]$data()\n\nComenzamos con el modelo discriminante lineal.\n\n# Modelo para la muestra de entrenamiento\nad.lineal = lda(Class~., wine_train)\n# Resultados del modelo\nad.lineal\n\nCall:\nlda(Class ~ ., data = wine_train)\n\nPrior probabilities of groups:\n        1         2         3 \n0.3309859 0.4014085 0.2676056 \n\nGroup means:\n  Alcalinity_of_ash    Alcohol        Ash Color_intensity  Flavanoids\n1        -0.7530190  0.9290783  0.2623347       0.2151580  0.97090136\n2         0.2474275 -0.8949264 -0.3883909      -0.8735059  0.02533243\n3         0.5602243  0.1932664  0.2581198       1.0441423 -1.23885033\n         Hue   Magnesium Malic_acid Nonflavanoid_phenols\n1  0.4831927  0.43590994 -0.3321547         -0.588689445\n2  0.4193928 -0.33253923 -0.3337043         -0.009089938\n3 -1.2267223 -0.04034292  0.9113793          0.741750800\n  OD280_OD315_of_diluted_wines Proanthocyanins    Proline Total_phenols\n1                    0.7560692       0.5466696  1.1669914    0.88203324\n2                    0.2617065       0.0384392 -0.7046074   -0.06383159\n3                   -1.3276979      -0.7338027 -0.3864730   -0.99518846\n\nCoefficients of linear discriminants:\n                                     LD1         LD2\nAlcalinity_of_ash             0.57237474 -0.62067698\nAlcohol                      -0.33382774  0.84308154\nAsh                          -0.11751893  0.71721561\nColor_intensity               1.05810079  0.62228850\nFlavanoids                   -1.63396480 -0.25165780\nHue                          -0.19664072 -0.48817442\nMagnesium                     0.01040250 -0.05100801\nMalic_acid                    0.29608846  0.24885233\nNonflavanoid_phenols         -0.13248561 -0.23038223\nOD280_OD315_of_diluted_wines -0.77246451  0.06190253\nProanthocyanins               0.01485653 -0.21661733\nProline                      -0.87734633  0.94935006\nTotal_phenols                 0.30663221 -0.23954012\n\nProportion of trace:\n   LD1    LD2 \n0.6883 0.3117 \n\n\nEn este caso podemos ver que la primera función discriminante alcanza el 68.8% de variabilidad explicada, mientras que la segunda alcanza el 31.2%. Recordemos que las funciones discriminantes se construyen por orden, de forma que las primeras siempre tiene mayor variabilidad explicada que las siguientes. Podemos ver la relevancia de cada predictora en las funciones discriminantes obtenidas mediante un gráfico de los coeficientes obtenidos en cada una de ellas.\n\ndf = as.data.frame(ad.lineal$scaling)\nggplot(df, aes(LD1, LD2, label = rownames(df))) + \n  geom_point() +\n  geom_label() +\n  geom_vline(xintercept = 0) + geom_hline(yintercept = 0)\n\n\n\n\nEn este gráfico las variables situadas más lejos del origen de coordenadas son las más relevantes en el análisis. Las predictoras más relevantes sobre la primera función discriminante son aquellas con mayor valor sobre el eje x (positivo o negativo), mientras que sobre la segunda son aquellas con mayor valor sobre el eje y (positivo o negativo). las que se sitúan sobre la diagonal contribuyen por igual en ambas funciones discriminantes.\nPara entender mejor los resultados del modelo realizamos otras representaciones. En primer lugar vemos las puntuaciones de las funciones discriminantes para cada grupo.\nNos vamos a centrar en los resultados sobre la muestra de entrenamiento:\n\n# primera función discriminante\np <- predict(ad.lineal, wine_train)\nldahist(data = p$x[,1], g = wine_train$Class)\n\n\n\n\nEn este caso las puntuaciones discriminantes si se separan para los tres grupos. El grupo 1 es el que tiene puntuaciones más bajas, mientras que el 3 es el que las tienen más altas. Veamos los resultados para la segunda función discriminante:\n\n# segunda función discriminante\nldahist(data = p$x[,2], g = wine_train$Class)\n\n\n\n\nEn este caso se puede distinguir el grupo 2 respecto de los otros dos que para esta función resultan indistinguibles. La solución nos indica que:\n\nEl grupo 1 viene caracterizado por puntuaciones bajas en LD1 y altas en LD2.\nEl grupo 2 viene caracterizado por puntuaciones medias en LD1 y bajas en LD2.\nEl grupo 3 viene caracterizado por puntuaciones altas en LD1 y altas en LD2.\n\nPara entender mejor la solución podemos representar la solución conjunta de las dos funciones discriminantes:\n\nggord(ad.lineal, wine_train$Class, ylim = c(-7, 5), xlim = c(-7, 8), txt = 3)\n\n\n\n\nCon el gráfico conjunto tratamos de representar la información de los grupos proporcionada por el análisis discriminante y la relevancia de las predictoras en cada una de ellas. Por ejemplo, podemos ver que el grupo 3 se caracteriza mayoritariamente por color_intensity. también podemos apreciar fácilmente si la solución proporcionada por el modelo es adecuada. Para relacionar más fácilmente el poder discriminatorio del modelo se suelen representar los mapas de clasificación. la forma habitual es obtener los mapas de clasificación dos a dos de todas las predictoras del modelo.\n\npartimat(Class~., data = wine_train, method = \"lda\", prec = 200, plot.matrix = TRUE, image.colors = c(\"darkgoldenrod1\", \"snow2\", \"skyblue2\"))\n\n\n\n\nDada la gran cantidad de predictoras se hace bastante difícil visualizar los resultados. Para mostrar mejor los resultados nos concentramos en las cuatro predictoras más relevantes.\n\npartimat(Class~Color_intensity + Flavanoids + Proline + Alcalinity_of_ash, data = wine_train, method = \"lda\", prec = 200, plot.matrix = TRUE, image.colors = c(\"darkgoldenrod1\", \"snow2\", \"skyblue2\"))\n\n\n\n\nEn los mapas de color podemos ver las funciones discriminantes, las zonas de clasificación, y los puntos incorrectamente clasificados (valores en rojo). Para finalizar el análisis de este modelo estudiamos la tabla de clasificación y el porcentaje de clasificación correcta.\n\n# predicción validación\npred_test = data.frame(truth = wine_test$Class, response = predict(ad.lineal, wine_test)$class)\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nSe observa que el porcentaje de clasificación obtenido con el modelo discriminante lineal es del 100%. No es necesario obtener el porcentaje de clasificación correcta. Aunque el modelo lineal proporciona unos resultados muy buenos vamos a explorar la solución cuadrática.\n\n# Modelo para la muestra de entrenamiento\nad.quad = qda(Class~., wine_train)\n# Resultados del modelo\nad.quad\n\nCall:\nqda(Class ~ ., data = wine_train)\n\nPrior probabilities of groups:\n        1         2         3 \n0.3309859 0.4014085 0.2676056 \n\nGroup means:\n  Alcalinity_of_ash    Alcohol        Ash Color_intensity  Flavanoids\n1        -0.7530190  0.9290783  0.2623347       0.2151580  0.97090136\n2         0.2474275 -0.8949264 -0.3883909      -0.8735059  0.02533243\n3         0.5602243  0.1932664  0.2581198       1.0441423 -1.23885033\n         Hue   Magnesium Malic_acid Nonflavanoid_phenols\n1  0.4831927  0.43590994 -0.3321547         -0.588689445\n2  0.4193928 -0.33253923 -0.3337043         -0.009089938\n3 -1.2267223 -0.04034292  0.9113793          0.741750800\n  OD280_OD315_of_diluted_wines Proanthocyanins    Proline Total_phenols\n1                    0.7560692       0.5466696  1.1669914    0.88203324\n2                    0.2617065       0.0384392 -0.7046074   -0.06383159\n3                   -1.3276979      -0.7338027 -0.3864730   -0.99518846\n\n\nVeamos las zonas de predicción correspondientes a este modelo para las mismas cuatro predictoras consideradas antes.\n\npartimat(Class~Color_intensity + Flavanoids + Proline + Alcalinity_of_ash, data = wine_train, method = \"qda\", prec = 200, plot.matrix = TRUE, image.colors = c(\"darkgoldenrod1\", \"snow2\", \"skyblue2\"))\n\n\n\n\nPodemos ver que las regiones de decisión ahora no se dividen únicamente mediante rectas (modelo lineal), sino mediante curvas. Veamos la capacidad de clasificación de este modelo.\n\n# predicción validación\npred_test = data.frame(truth = wine_test$Class, response = predict(ad.quad, wine_test)$class)\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nComo en el caso anterior la tablas de clasificación proporciona un porcentaje de acierto del 100%. Ambos modelos son igualmente buenos para este conjunto de datos.\n\n\n15.4.2.3 Abalone\nPara finalizar nos centraremos en el banco de datos Abalone. Comenzaremos de nuevo con el modelo lineal y luego compararemos los resultados con el cuadrático. Como siempre comenzamos con la preparación de los datos:\n\n# División de muestras\nset.seed(432)\nsplits = mlr3::partition(tsk_abalone, ratio = 0.8)\ntsk_train_abalone = tsk_abalone$clone()$filter(splits$train)\ntsk_test_abalone  = tsk_abalone$clone()$filter(splits$test)\n# preprocesado\npp_abalone = po(\"scale\", param_vals = list(center = TRUE, scale = TRUE)) \n# Obtención de muestras\ntrain = pp_abalone$train(list(tsk_train_abalone))\ntest = pp_abalone$train(list(tsk_test_abalone))\nabalone_train = train[[1]]$data()\nabalone_test = test[[1]]$data()\n\nComenzamos con el modelo discriminante lineal.\n\n# Modelo para la muestra de entrenamiento\nad.lineal = lda(Sex~., abalone_train)\n# Resultados del modelo\nad.lineal\n\nCall:\nlda(Sex ~ ., data = abalone_train)\n\nPrior probabilities of groups:\n        F         I         M \n0.3129862 0.3213645 0.3656493 \n\nGroup means:\n    Diameter     Height     Length      Rings Shell_weight Shucked_weight\nF  0.4754051  0.4468167  0.4614434  0.3677620    0.4612112      0.3989033\nI -0.8310516 -0.7533291 -0.8115554 -0.6371833   -0.8070032     -0.7660812\nM  0.3234662  0.2796279  0.3182821  0.2452175    0.3144799      0.3318481\n  Viscera_weight Whole_weight\nF      0.4682521    0.4504576\nI     -0.8204998   -0.8216986\nM      0.3203151    0.3366004\n\nCoefficients of linear discriminants:\n                      LD1        LD2\nDiameter       -1.1096845 -0.8741631\nHeight         -0.1910499 -0.4255549\nLength          0.7124536  0.2202397\nRings          -0.3332679  0.4250325\nShell_weight    0.2163388 -1.2061618\nShucked_weight  0.1245385  2.0237261\nViscera_weight -0.6412314 -2.1059409\nWhole_weight   -0.1802106  2.1471252\n\nProportion of trace:\n  LD1   LD2 \n0.985 0.015 \n\n\nLa primera función discriminante es la causante del 98.5% de variabilidad explicada, lo que implica que la segunda función tiene poco peso en el proceso de discriminación, para el caso del modelo lineal. Las predictoras más relevantes en este caso son Diameter y Viscera_weight en la parte negativa de la función, y Length en la parte positiva. Veamos el gráfico de las variables:\n\ndf = as.data.frame(ad.lineal$scaling)\nggplot(df, aes(LD1, LD2, label = rownames(df))) + \n  geom_point() +\n  geom_label() +\n  geom_vline(xintercept = 0) + geom_hline(yintercept = 0)\n\n\n\n\nRepresentamos ahora las puntuaciones de cada función discriminante en cada uno de los grupos:\n\n# primera función discriminante\np <- predict(ad.lineal, abalone_train)\nldahist(data = p$x[,1], g = abalone_train$Sex)\n\n\n\n\nCon la primera función discriminante parece que separamos el grupo I de los grupos F y M que resultan indistinguibles. Veamos los resultados para la segunda función discriminante:\n\n# segunda función discriminante\nldahist(data = p$x[,2], g = abalone_train$Sex)\n\n\n\n\nComo era de esperar, dado el bajo valor discriminante de esta función, no somo capaces de distinguir los tres grupos con la segunda función discriminante. En lugar de hacer los mapas de cada clase pasamos directamente a estudiar la matriz de confusión.\n\n# predicción validación\npred_test = data.frame(truth = abalone_test$Sex, response = predict(ad.lineal, abalone_test)$class)\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nLos porcentajes se encuentran bastante repartidos, pero si podemos ver que el mayor error de clasificación (con un 15.4%) se alcanza la intentar identificar los sujetos de los grupos M y F. Veamos el porcentaje de clasificación correcta ponderado:\n\ncm$`Balanced Accuracy` \n\n[1] 0.6460549\n\n\nEl porcentaje solo alcanza el 64.6%, es decir, nos estamos equivocando en 1 de cada 3 sujetos a la hora de clasificarlo en su grupo de origen. Veamos que ocurre si utilizamos un modelo cuadrático:\n\n# Modelo para la muestra de entrenamiento\nad.quad = qda(Sex~., abalone_train)\n# Resultados del modelo\nad.quad\n\nCall:\nqda(Sex ~ ., data = abalone_train)\n\nPrior probabilities of groups:\n        F         I         M \n0.3129862 0.3213645 0.3656493 \n\nGroup means:\n    Diameter     Height     Length      Rings Shell_weight Shucked_weight\nF  0.4754051  0.4468167  0.4614434  0.3677620    0.4612112      0.3989033\nI -0.8310516 -0.7533291 -0.8115554 -0.6371833   -0.8070032     -0.7660812\nM  0.3234662  0.2796279  0.3182821  0.2452175    0.3144799      0.3318481\n  Viscera_weight Whole_weight\nF      0.4682521    0.4504576\nI     -0.8204998   -0.8216986\nM      0.3203151    0.3366004\n\n\nAnalizamos la clasificación obtenida con este modelo:\n\n# predicción validación\npred_test = data.frame(truth = abalone_test$Sex, response = predict(ad.quad, abalone_test)$class)\n# matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nLa matriz de confusión obtenida es bastante similar a la del caso lineal, así que no esperamos una mejora sustancial en el porcentaje de clasificación correcta.\n\ncm$`Balanced Accuracy` \n\n[1] 0.6436839\n\n\nEl valor del 64.3% es muy similar al del modelo lineal. Esta visto que con las predictoras utilizadas no podemos construir un modelo discriminante que nos permita distinguir el sexo de cada uno de los elementos de la muestra con una alta precisión."
  },
  {
    "objectID": "150_Discriminantmodels.html#sec-150.5",
    "href": "150_Discriminantmodels.html#sec-150.5",
    "title": "15  Análisis discriminante (AD)",
    "section": "15.5 Ejercicios",
    "text": "15.5 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en análisis discriminante para el banco de datos Iris4.3.3.\nAjustar un modelo de aprendizaje automático basado en análisis discriminante para el banco de datos Breast cancer?sec-breastcancer.\nAjustar un modelo de aprendizaje automático basado en análisis discriminante para el banco de datos Wine quality4.3.8."
  },
  {
    "objectID": "160_PrinCompmodels.html#sec-160.1",
    "href": "160_PrinCompmodels.html#sec-160.1",
    "title": "16  Componentes principales (CP)",
    "section": "16.1 CP lineales",
    "text": "16.1 CP lineales\nConsideramos una matriz de variables \\(\\mathbf{X}\\) compuesta por \\(p\\) variables medidas en \\(n\\) sujetos que denotamos por:\n\\[\\mathbf{X} =\n\\begin{bmatrix}\nx_{11}&...&x_{1p}\\\\\nx_{21}&...&x_{2p}\\\\\n...&...&...\\\\\nx_{n1}&...&x_{np}\n\\end{bmatrix}\\]\ny cuya matriz de varianzas-covarianzas viene dada por \\(\\mathbf{\\Sigma}\\).\nEl procedimiento de componentes principales fue inicialmente desarrollado por Pearson a finales del siglo XIX y posteriormente por Hotelling en los años 30 del siglo XX. Sin embargo, hasta la aparición de los ordenadores no se empezó a popularizar. Para estudiar las relaciones que se presentan entre \\(p\\) variables correlacionadas (que miden información común) se puede transformar el conjunto original de variables en otro conjunto de nuevas variables incorreladas entre sí (que no tenga repetición o redundancia en la información) llamado conjunto de componentes principales.\nLas componentes principales se obtienen como combinaciones lineales de las variables originales y se van construyendo según el orden de importancia en cuanto a la variabilidad total que recogen de la muestra. De modo ideal, se buscan \\(k << p\\) variables que sean combinaciones lineales de las \\(p\\) originales y que estén incorreladas, recogiendo la mayor parte de la información o variabilidad de los datos. Si las variables originales están incorreladas de partida, entonces no tiene sentido realizar un análisis de componentes principales.\nSi denotamos por \\([\\mathbf{y}_1,...,\\mathbf{y}_p]\\) a las componentes principales, estas se pueden escribir como:\n\\[\\mathbf{y}_j = a_{j1}\\mathbf{x}_1 + a_{j2}\\mathbf{x}_2 + ... + a_{jp}\\mathbf{x}_p = \\mathbf{a}_j^{T}\\mathbf{x}, \\quad j=1,...,p\\]\nObviamente, si lo que queremos es maximizar la varianza, como veremos luego, una forma simple podría ser aumentar los coeficientes \\(a_{ij}\\). Por ello, para mantener la ortogonalidad de la transformación se impone que el módulo del vector \\(\\mathbf{a}_j^T = (a_{j1},...,a_{jp})\\) sea 1, es decir:\n\\[\\mathbf{a}_j^T \\mathbf{a}_j = \\sum_{i=1}^{p} a_{ji}^2= 1\\]\nLa primera componente se calcula eligiendo \\(\\mathbf{a}_1\\) de modo que \\(\\mathbf{y}_1\\) tenga la mayor varianza posible, sujeta a la restricción \\(\\mathbf{a}_j^T \\mathbf{a}_j = 1\\). La segunda componente principal se calcula obteniendo \\(\\mathbf{a}_2\\) de modo que la variable obtenida, \\(\\mathbf{y}_2\\) esté incorrelada con \\(\\mathbf{y}_1\\). Del mismo modo se eligen \\((\\mathbf{y}_1\\),…, \\(\\mathbf{y}_p)\\), incorrelados entre sí, de manera que las variables aleatorias obtenidas vayan teniendo cada vez menor varianza.\nLos términos \\(\\mathbf{a}_1,...,\\mathbf{a}_p\\) reciben el nombre de “loadings” y definen las componentes de forma única. Los loadings pueden interpretarse como el peso/importancia que tiene cada variable en cada componente y, por lo tanto, ayudan a conocer que tipo de información que recoge cada una de las componentes.\n\n16.1.1 Interpretación geométrica\nUna forma intuitiva de entender el proceso de PCA es interpretar las componentes principales desde un punto de vista geométrico. Supóngase un conjunto de observaciones para las que se dispone de dos variables \\((\\mathbf{X}_1, \\mathbf{X}_2)\\). El vector que define la primera componente principal sigue la dirección en la que las observaciones tienen más varianza (línea roja). La proyección de cada observación sobre esa dirección equivale al valor de la primera componente para dicha observación.\n\nLa segunda componente sigue la segunda dirección en la que los datos muestran mayor varianza y que no está correlacionada con la primera componente. La condición de no correlación entre componentes principales equivale a decir que sus direcciones son perpendiculares/ortogonales.\n\n\n\n16.1.2 Extracción de componentes\nEl primer paso en la obtención de las componentes principales es centrar todas las variables originales, es decir, se resta a cada valor la media de la variable a la que pertenece. Con esto se consigue que todas las variables tengan media cero.\nComenzamos el proceso obteniendo la primera componente principal, mediante el proceso de optimización:\n\\[\\underset{\\mathbf{a}_1}{max} \\text{ } Var(\\mathbf{y}_1) = \\underset{\\mathbf{a}_1}{max}  \\text{ }  Var(\\mathbf{a}_1^T \\mathbf{X}) = \\underset{\\mathbf{a}_1}{max}  \\text{ }  \\mathbf{a}_1^T \\mathbf{\\Sigma}\\mathbf{a}_1\\]\ncon la restricción:\n\\[\\mathbf{a}_1^T \\mathbf{a}_1 = 1.\\]\nUtilizando el procedimiento habitual basado en los multiplicadores de Lagrange el problema planteado es equivalente a optimizar la función:\n\\[f(\\mathbf{a}_1) = \\mathbf{a}_1^T \\mathbf{\\Sigma}\\mathbf{a}_1 - \\lambda (\\mathbf{a}_1^T \\mathbf{a}_1-1)\\]\nDerivando e igualando a cero se puede ver que \\(\\lambda\\) se corresponde con el mayor valor propio de \\(\\mathbf{\\Sigma}\\) y \\(\\mathbf{a}_1\\) con el correspondiente vector propio.\nPara obtener la segunda componente planteamos el problema de optimización:\n\\[\\underset{\\mathbf{a}_2}{max}  \\text{ }  \\mathbf{a}_2^T \\mathbf{\\Sigma}\\mathbf{a}_2\\]\ncon las restricciones:\n\\[\\mathbf{a}_2^T \\mathbf{a}_2 = 1 \\quad \\text{ y } \\quad \\mathbf{a}_2^T \\mathbf{a}_1 = 1.\\]\nUtilizando de nuevo los multiplicadores de Lagrange podemos ver que la segunda componente se corresponde con el vector propio asociado con el segundo mayor valor propio de \\(\\mathbf{\\Sigma}\\).\nSucesivamente se puede ver que las \\(p\\) componentes principales se corresponden con los \\(p\\) vectores propios asociados con los \\(p\\) valores propios, \\(\\lambda_1,...,\\lambda_p\\) (ordenados de mayor a menor) de \\(\\mathbf{\\Sigma}\\).\nPara obtener las componentes podemos usar entonces al descomposición de valores y vectores singulares (SVD) de \\(\\mathbf{\\Sigma}\\).\n\n\n16.1.3 Variabilidad asociada a cada componente\nUna de las preguntas más frecuentes que surge tras realizar un PCA es: ¿Cuánta información presente en el conjunto de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión?, o lo que es lo mismo, ¿Cuanta información es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporción de varianza explicada por cada componente principal.\nAsumiendo que las variables se han normalizado para tener media cero, y como hemos visto que cada autovalor corresponde a la varianza de la componente correspondiente, que se definía por medio del autovector \\(\\mathbf{a}_i\\), es decir, \\(Var(\\mathbf{y}_i) = \\lambda_i\\), la varianza total presente en el conjunto de datos se define como:\n\\[\\sum_{i=1}^p Var(\\mathbf{y_i}) = \\sum_{i=1}^p \\lambda_i = Tr(\\mathbf{\\Delta}),\\]\ndonde \\(\\mathbf{\\Delta}\\) es la matriz diagonal:\n\\[\\begin{bmatrix}\n\\lambda_1&0&...&0\\\\\n0&\\lambda_2&...&0\\\\\n...&...&...&...\\\\\n0&0&...&\\lambda_p\n\\end{bmatrix}\\]\nUtilizando las propiedades de la traza tenemos que:\n\\[Tr(\\mathbf{\\Delta}) = Tr(\\mathbf{A^t\\Sigma A}) = Tr(\\mathbf{\\Sigma A^tA}) = Tr(\\mathbf{\\Sigma}),\\]\ndonde \\(\\mathbf{A}\\) es la matriz con filas formadas por cada uno de los \\(p\\) vectores propios obtenidos.\nDe esta forma la variabilidad del banco de datos original se corresponde con la variabilidad de las componentes. Esto permite hablar del porcentaje de varianza total que recoge la primera componente principal como:\n\\[\\frac{\\lambda_1}{\\sum_{i=1}^p \\lambda_i}\\times 100\\]\ny del porcentaje de variabilidad recogido en las \\(k\\) primeras componentes:\n\\[\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}\\times 100\\]\n\n\n16.1.4 Número óptimo de componentes\nPor lo general, dada una matriz de datos de dimensiones n x p, el número de componentes principales que se pueden calcular es como máximo de n-1 o p (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de interés utilizar el número mínimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o método único que permita identificar cual es el número óptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporción de varianza explicada acumulada y seleccionar el número de componentes mínimo a partir del cual el incremento deja de ser sustancial.\nPara ello se suele hacer un gráfico de variabilidad explicada versus número de componentes para observar cuando el crecimiento de la curva resultante no resulta relevante.\n\n\n16.1.5 A tener en cuenta\nEl proceso de PCA estándar es determinista, genera siempre las mismas componentes principales, es decir, el valor de los loadings resultantes es el mismo. La única diferencia que puede darse es que el signo de todos los loadings esté invertido. Esto es así porque el vector de loadings determina la dirección de la componente, y dicha dirección es la misma independientemente del signo (la componente sigue una línea que se extiende en ambas direcciones).\nAl trabajar con varianzas, el método PCA es muy sensible a outliers, por lo que es recomendable estudiar si los hay. La detección de valores atípicos con respecto a una determinada dimensión es algo relativamente sencillo de hacer mediante comprobaciones gráficas. Sin embargo, cuando se trata con múltiples dimensiones el proceso se complica.\n\n\n16.1.6 Puntuaciones asociadas a las componentes\nUna vez obtenidas las componentes principales resulta posible obtener la puntuación que obtendría cada una de las muestras del banco de datos sin más que sustituir los valores correspondientes en cada una de las ecuaciones de las componentes. Para obtener las puntuaciones en la primera componente de todas las muestras que componen el banco de datos original tendríamos:\n\\[y_{1j} = a_{11}x_{j1} + a_{12}x_{j2}+...+a_{1p}x_{jp}, \\quad j=1,...,n\\]\nDe forma similar podemos obtener las puntuaciones en el resto de componentes:\n\\[y_{lj} = a_{l1}x_{jl} + a_{l2}x_{j2}+...+a_{lp}x_{jp}, \\quad l=1,...,p; j=1,...,n\\]\n\n\n16.1.7 Representaciones gráficas\nLa mejor forma de entender la solución pasa por diferentes representaciones gráficas tanto de las componentes obtenidas como de las puntuaciones asociadas con dichas componentes:\n\nGráfico de los pesos de las variables originales en cada una de las componentes. Generalmente se toma el gráfico de las dos primeras componentes para estudiar cómo influye cada una de las variables originales en su construcción.\nGráfico de las puntuaciones de las diferentes muestras con respecto a las primeras componentes para tratar de establecer patrones de comportamiento en los valores observados. Si además disponemos de una variable de etiquetas para los puntos podemos utilizarla para tratar de establecer dichos patrones.\nGráfico biplot donde integramos de forma conjunta tanto el gráfico de los pesos de las variables originales como el de las puntuaciones de las componentes. Este gráfico nos permite tanto estudiar la relevancia de las variables en cada componente, así como saber si los patrones de las puntuaciones se corresponden con ellas.\n\n\n\n16.1.8 Usos de las CP\nAunque las componentes principales constituyen por si mismo un algoritmo de aprendizaje automático, la realidad es que en muchas ocasiones se usa como complemento o paso previo de preparación de las muestras para el uso de un algoritmo supervisado. En este caso es necesario que la base de datos disponga de una variable respuesta (numérica o de etiquetas) que es utilizada en el algoritmo supervisado."
  },
  {
    "objectID": "160_PrinCompmodels.html#sec-160.2",
    "href": "160_PrinCompmodels.html#sec-160.2",
    "title": "16  Componentes principales (CP)",
    "section": "16.2 CP no lineales",
    "text": "16.2 CP no lineales\nExisten situaciones donde no resulta posible utilizar las componentes principales lineales ya que los comportamientos a reproducir no son exactamente lineales al igual que cuando estudiamos las máquinas de vector soporte. Por eso introducimos nuevos métodos de componentes principales que nos permiten trabajar en dichas situaciones.\nTodos los métodos de componentes principales que vamos a estudiar a continuación tienen en común que se utilizan para reducir la dimensión de un conjunto de datos. Cada uno de estos tiene matices que los hace únicos, entre ellos encontramos el método de componentes principales aleatorizadas (RPCA), componentes principales Kernel (KernelPCA), y componentes principales independientes (FastICA).\n\n16.2.1 CP aleatorizadas\nA diferencia del método básico de componentes principales (PCA), este método no solo se utiliza para reducir la dimensión de un conjunto de datos, sino también para encontrar patrones ocultos en estos. Se basa en el método de componentes principales, con una mayor eficiencia y estabilidad ya que utiliza un enfoque aleatorizado que permite conseguir mejores resultados.\nEn lugar de utilizar los datos originales, primero aplica una transformación aleatoria a los datos y luego calcula las componentes principales sobre los datos transformados. Esta transformación permite más velocidad de ejecución del algoritmo y menos requerimientos de memoria. Además, es más robusto y menos sensible a los valores atípicos.\n\n\n16.2.2 CP Kernel\nEste se utiliza para permitir que el algoritmo maneje datos que linealmente no se pueden separar. Cuando nuestros datos no tienen una naturaleza lineal y nuestro método es incapaz de capturarla buscamos un nuevo espacio donde estos puntos tengan una distribución más bien lineal, aplicamos el análisis de componentes principales reduciendo así su dimensionalidad y luego volvemos al espacio original. Es una técnica útil para el análisis de datos en muchas aplicaciones, especialmente en el aprendizaje automático y el análisis de imágenes.\nPara movernos entre un espacio y otro utilizamos las funciones kernel que ya vimos en SVM.\n\n\n16.2.3 CP Independientes\nFastICA utiliza un enfoque no lineal para permitir que el algoritmo maneje datos que linealmente no se pueden separar. En lugar de buscar componentes principales que capturen la mayor cantidad de varianza en los datos, este busca componentes independientes que sean lo más diferentes posibles entre sí, permitiendo al algoritmo capturar patrones no lineales en los datos originales y reducir la dimensión de manera efectiva. Se suele utilizar en el aprendizaje automático y en el análisis de señales."
  },
  {
    "objectID": "160_PrinCompmodels.html#sec-160.3",
    "href": "160_PrinCompmodels.html#sec-160.3",
    "title": "16  Componentes principales (CP)",
    "section": "16.3 CP en R",
    "text": "16.3 CP en R\nComo ocurría con el análisis discriminante la librería mlr3 pone a nuestra disposición dos pipiOps para llevar a cabo el preprocesamiento mediante componentes principales, pero en este caso vamos a optar por usar directamente las librerías y funciones de R para realizar un análisis más completo.\nLas librerías que vamos a utilizar (las deberemos instalar) son:\n\nFactoMineR y factoextra para el análisis de componentes principales lineal.\nrsvd para el análisis de componentes principales aleatorizadas.\nkernlab para el análisis de componentes principales kernel.\nfastICA para el análisis de componentes principales independientes.\n\nComenzamos cargando todas las librerías que hemos utilizado y las que terminamos de instalar:\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(cvms)\nlibrary(kknn)\nlibrary(rpart.plot)\nlibrary(rda)\nlibrary(klaR)\nlibrary(ggord)\ntheme_set(theme_sjplot2())\n\n# Paquetes AA\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3tuningspaces)\nlibrary(gbm)\nlibrary(RWeka)\nlibrary(xgboost)\nlibrary(lightgbm)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(rsvd)\nlibrary(kernlab)\nlibrary(fastICA)\n\n\n16.3.1 Bancos de datos\nPara ejemplificar el uso de los modelos de análisis de componentes principales vamos a utilizar tres bancos de datos: Breast Cancer Wisconsin, Abalone, y Gene expression leukemia. En concreto nos centramos en mostrar únicamente la aproximación de las componentes principales lineales. para el resto se puede acudir a las ayudas de las funciones en las librerías correspondientes.\nA continuación presentamos los tres bancos de datos y el código para cargar las bases de datos correspondientes.\n\n16.3.1.1 Breast Cancer Wisconsin\nEl código para cargar el banco de datos es:\n\n# Leemos datos\ncancer = read_rds(\"breastcancer.rds\")\n# Eliminamos la variable id\ncancer = cancer %>% dplyr::select(-id)\n\n\n\n16.3.1.2 Abalone\nEl código para cargar el banco de datos es:\n\n# Leemos los datos\nabalone = read_rds(\"abalone.rds\")\nnames(abalone) = c(\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\")  \n\n\n\n16.3.1.3 Gene expression leukemia\nEste archivo que contiene los niveles de expresión génica de 22284 genes (columnas) de 64 muestras (filas). Hay 5 tipos diferentes de leucemia representados en este conjunto de datos (columna “type”). Más información sobre este conjunto de datos, así como otros formatos de archivo como TAB y ARFF, visualización de datos y puntos de referencia de clasificación y agrupación están disponibles gratuitamente en el sitio web oficial de CuMiDa con el id GSE9476: http://sbcb.inf.ufrgs.br/cumida\n\n# Leemos los datos\ngeneexpleu = read_rds(\"geneexpressionleukemia.rds\")\n# Eliminamos el indicador de la muestra\ngeneexpleu = geneexpleu %>% dplyr::select(-samples)\n\n\n\n\n16.3.2 Modelos\nPara cada banco de datos probamos diferentes modelos de componentes principales para ver como afecta la posible reducción de la dimensión. Además, como en todos los ejemplos tenemos una variable que identifica el grupo al que pertenecen los sujetos, la utilizaremos para valorar el modelo obtenido.\n\n16.3.2.1 Breast Cancer Wisconsin\nComenzamos con el análisis de datos de cáncer de mama en wisconsin. En primer lugar vamos a construir las matrices de datos necesarias para el entrenamiento y validación del modelo.\n\n# Codificación de muestras 80%-20%\nids = sample(c(1, 2), size = nrow(cancer), replace = TRUE, prob = c(0.8, 0.2))\n# Muestras de variables\ntrain_cancer = cancer[ids == 1,]\ntest_cancer = cancer[ids==2, ]\n\nPara construir el modelo de CP utilizamos únicamente las características numéricas y nos guardaremos la variable de etiquetas (diagnosis) para valorar la solución obtenida. Para ello utilizamos la función princomp() de la librería FactoMineR. Los parámetros principales de esta función son:\n\nx: una matriz numérica o data frame que proporciona los datos para el análisis de componentes principales.\ncor: condición lógica que indica si debemos usar la matriz de correlaciones de x, es decir, si debemos escalar las variables. Por defecto el valor es FALSE.\nscores: una condición lógica que indica si debemos calcular las puntuaciones de cada muestra para cada una de las componentes principales extraídas. Por defecto el valor es TRUE.\n\nPara acceder a los resultados del análisis podemos utilizar las funciones:\n\nget_eigenvalue() para obtener la información sobre los valores propios.\nget_pca_var() para obtener información sobre las componentes principales. Podemos obtener las coordenadas, las contribuciones a las componentes, y la calidad de la representación con los métodos coord, contrib, y cos2.\n\nget_pca_ind() para obtener la información de las muestras sobre las componentes principales. Podemos obtener las coordenadas, contribuciones a las componentes, y la calidad de la representación con los métodos coord, contrib, y cos2.\n\nComenzamos obteniendo las CP para este conjunto de datos y analizamos numéricamente la calidad de la solución.\n\nX_train = train_cancer %>% dplyr::select(-diagnosis)\n# CP\ncancer_cp = princomp(X_train, scale = TRUE)\n# Resumen numérico del análisis\nget_eigenvalue(cancer_cp)\n\n         eigenvalue variance.percent cumulative.variance.percent\nDim.1  4.544424e+05     9.827174e+01                    98.27174\nDim.2  7.084840e+03     1.532074e+00                    99.80381\nDim.3  8.073646e+02     1.745900e-01                    99.97840\nDim.4  5.462594e+01     1.181269e-02                    99.99021\nDim.5  3.989682e+01     8.627561e-03                    99.99884\nDim.6  3.008991e+00     6.506848e-04                    99.99949\nDim.7  1.700384e+00     3.677027e-04                    99.99986\nDim.8  3.540018e-01     7.655177e-05                    99.99994\nDim.9  1.575402e-01     3.406757e-05                    99.99997\nDim.10 8.705421e-02     1.882520e-05                    99.99999\nDim.11 3.047705e-02     6.590566e-06                   100.00000\nDim.12 7.526044e-03     1.627483e-06                   100.00000\nDim.13 2.940581e-03     6.358914e-07                   100.00000\nDim.14 2.283617e-03     4.938249e-07                   100.00000\nDim.15 1.278475e-03     2.764661e-07                   100.00000\nDim.16 6.294514e-04     1.361169e-07                   100.00000\nDim.17 3.922398e-04     8.482061e-08                   100.00000\nDim.18 2.448805e-04     5.295464e-08                   100.00000\nDim.19 1.917317e-04     4.146138e-08                   100.00000\nDim.20 1.695831e-04     3.667181e-08                   100.00000\nDim.21 8.352442e-05     1.806189e-08                   100.00000\nDim.22 5.695513e-05     1.231637e-08                   100.00000\nDim.23 3.683906e-05     7.966331e-09                   100.00000\nDim.24 2.868066e-05     6.202103e-09                   100.00000\nDim.25 1.609069e-05     3.479560e-09                   100.00000\nDim.26 1.375499e-05     2.974472e-09                   100.00000\nDim.27 3.696495e-06     7.993554e-10                   100.00000\nDim.28 2.830228e-06     6.120279e-10                   100.00000\nDim.29 1.984835e-06     4.292142e-10                   100.00000\nDim.30 7.516780e-07     1.625480e-10                   100.00000\n\n\nCon las dos primeras CP alcanzamos un 99.80% de variabilidad explicada lo que nos da un solución muy precisa. Si nos quedamos con tres componentes tendríamos un 99.97% de variabilidad explicada. Sin embargo, para poder representar la solución en gráficos dimensionales nos quedaremos solo con las dos primeras componentes. Generamos el scree plot asociado a este modelo:\n\n# Resumen gráfico del análisis\nfviz_eig(cancer_cp)\n\n\n\n\nClaramente al solución con dos dimensiones parece la acertada en esta situación.\nEstudiamos con detalle las dos primeras componentes para determinar la contribución de cada característica original en su construcción:\n\n# Coordenadas de las características en las dos primeras componentes\nvar_pca = get_pca_var(cancer_cp)\nvar_pca$coord[,1:2]\n\n                                Dim.1         Dim.2\nradius_mean              3.433801e+00  7.454521e-01\ntexture_mean             1.480468e+00 -1.928105e-01\nperimeter_mean           2.367916e+01  4.978667e+00\narea_mean                3.510156e+02  7.148080e+01\nsmoothness_mean          2.970629e-03 -1.418917e-03\ncompactness_mean         2.696521e-02 -2.543761e-03\nconcavity_mean           5.446966e-02  3.903219e-03\nconcave_points_mean      3.219301e-02  2.924041e-03\nsymmetry_mean            4.519380e-03 -3.281438e-03\nfractal_dimension_mean  -1.916002e-03 -1.534539e-03\nradius_se                2.156650e-01 -3.947865e-04\ntexture_se              -3.967902e-02  3.367215e-02\nperimeter_se             1.527836e+00  9.242231e-02\narea_se                  3.845229e+01  1.846987e+00\nsmoothness_se           -4.888490e-04  2.349369e-04\ncompactness_se           3.541010e-03  5.660275e-04\nconcavity_se             5.173307e-03  1.751959e-03\nconcave_points_se        2.073223e-03  5.720825e-04\nsymmetry_se             -8.231274e-04  1.132139e-03\nfractal_dimension_se    -1.138313e-04 -2.232285e-05\nradius_worst             4.799605e+00 -1.205125e-01\ntexture_worst            2.042503e+00 -1.079647e+00\nperimeter_worst          3.312295e+01 -5.914483e-01\narea_worst               5.727569e+02 -4.410217e+01\nsmoothness_worst         4.179783e-03 -7.238337e-03\ncompactness_worst        6.699417e-02 -3.004328e-02\nconcavity_worst          1.089645e-01 -2.509081e-02\nconcave_points_worst     4.899289e-02 -5.216096e-03\nsymmetry_worst           1.092528e-02 -1.647904e-02\nfractal_dimension_worst  7.422932e-04 -5.467551e-03\n\n\nLos valores en valor absoluto más grandes en cada componente nos indican las características que más contribuyen en cada componente. Estos valores son los coeficientes asociados con las características consideradas en cada una de las componentes. Sin embargo, como el número de variables es grande nos va a resultar más útil presentar la solución mediante un gráfico. En concreto realizamos el gráfico de contribución de las características sobre las componentes:\n\nfviz_pca_var(cancer_cp, axes = c(1,2),\n             col.var = \"contrib\", \n             gradient.cols = c(\"aquamarine3\", \"darkgoldenrod1\", \"coral3\"),\n             repel = TRUE     \n             )\n\n\n\n\nPodemos ver dos variables que contribuyen tanto en la componente 1 y la 2 como son area_mean y area_worst, mientras que hay un gran conjunto que contribuyen en menor medida. para poder comprender mejor este gráfico vamos a seleccionar las cinco variables que más contribuyen en la elaboración de las componentes.\n\nfviz_pca_var(cancer_cp, axes = c(1,2),\n             select.var = list(contrib = 5),\n             col.var = \"contrib\", \n             gradient.cols = c(\"aquamarine3\", \"darkgoldenrod1\", \"coral3\"),\n             repel = TRUE     \n             )\n\n\n\n\nHemos identificado que las variables que más contribuyen so las relacionadas con el área y el perímetro del tumor. Para entender mejor el funcionamiento de las componentes representamos las muestras de entrenamiento a partir de los valores obtenidos por cada una de ellas en las dos componentes.\n\nfviz_pca_ind(cancer_cp, axes = c(1,2), geom = \"point\")\n\n\n\n\nLa nube de puntos se establece a lo largo de la componente 1, como era de esperar dado su gran capacidad explicativa, apreciándose una nube de puntos bastante concentrada en la parte negativa de la componente, mientras que es mucho más dispersa en la parte positiva. Para ver la posible asociación entre las componentes y la variable de etiquetas podemos repetir el gráfico anterior identificando cada punto según su etiqueta.\n\nfviz_pca_ind(cancer_cp, axes = c(1,2), geom = \"point\", \n             label = \"none\", \n             habillage = train_cancer$diagnosis,\n             addEllipses=TRUE, ellipse.level=0.95)\n\n\n\n\nEl gráfico resulta muy interesante porque somos capaces de agrupar en la parte negativa de la componente 1 casi todas las muestras identificadas como tumor benigno, mientras que en la parte positiva tenemos casi todos los malignos. Para ello hemos utilizado la información de todas las variables sin necesidad de ningún procedimiento de selección de características. El paso siguiente es vincular las muestras con las características utilizadas mediante un gráfico conjunto. En este caso los tumores malignos parecen asociarse con los valores más grandes en las variables relacionadas con el área y perímetro del tumor, ya que todas ellas contribuyen en la parte positiva de dicha componente. Este gráfico nos permite establecer una regla de clasificación a partir de la componente 1 para futuras observaciones, sin más que calcular el valor de dicha componente y asignar la etiqueta B si el valor es negativo, o M si es positivo. En temas siguientes veremos como podemos utilizar las técnicas de agrupación par establecer estas etiquetas sin necesidad de marcar un umbral de clasificación como es el cero en este caso.\nOtra forma de proceder es utilizar las dos componentes obtenidas y utilizarlas para construir un algoritmo de aprendizaje supervisado como por ejemplo kNN. Tengamos en cuenta que pasamos de un conjunto inicial de 30 características a una solución a partir de dos componentes.\nFinalizamos el análisis estudiando el comportamiento de las muestras de validación con el modelo obtenido. En primer lugar debemos obtener la predicción de los valores de las componentes para la muestra de validación.\n\n# Valores de validación\nX_test = test_cancer %>% dplyr::select(-diagnosis)\n# Predicción para el modelo propuesto\npredict.test = predict(cancer_cp, newdata = X_test)\n\nRepresentamos ahora la solución para la muestra de entrenamiento superponiendo los valores de la muestra de validación.\n\n# valores muestra de entrenamiento\np = fviz_pca_ind(cancer_cp, axes = c(1,2), geom = \"point\", label = \"none\")\n# valores muestra de validación\nfviz_add(p, predict.test[,1:2], color =\"blue\", geom = \"point\", addlabel = FALSE)\n\n\n\n\nSe puede ver como las predicciones e las muestras de validación se corresponde con las de la muestra de entrenamiento.\npara finalizar vemos como crear el nuevo banco de datos que nos permita definir la tarea de clasificación para ser utilizad por cualquier algoritmo de aprendizaje.\n\n# matriz con las dos primeras componentes\nind_pca = get_pca_ind(cancer_cp)$coord[,1:2]\n# predicciones para la muestra de validación en las dos primeras componentes\npredict.test = predict(cancer_cp, newdata = X_test)[,1:2]\n# Matriz conjunta de coordenadas\nX = rbind(ind_pca, predict.test)\n# vector target\ny = rbind(train_cancer[,\"diagnosis\"], test_cancer[,\"diagnosis\"])\n# matriz de datos\ndata_cp = as.tibble(data.frame(y, X))\n# Creación de task \ntsk_abalone = as_task_classif(data_cp, target = \"diagnosis\")\ntsk_abalone\n\n<TaskClassif:data_cp> (569 x 3)\n* Target: diagnosis\n* Properties: twoclass\n* Features (2):\n  - dbl (2): Dim.1, Dim.2\n\n\n\n\n16.3.2.2 Abalone\nRealizamos el análisis para el banco de datos Abalone. En primer lugar creamos la división en muestra de entrenamiento y validación de forma similar al banco de datos anterior.\n\n# Codificación de muestras 80%-20%\nids = sample(c(1, 2), size = nrow(abalone), replace = TRUE, prob = c(0.8, 0.2))\n# Muestras de variables\ntrain_abalone = abalone[ids == 1,]\ntest_abalone = abalone[ids==2, ]\n\nObtenemos las CP lineales para este conjunto de datos y analizamos numéricamente la calidad de la solución.\n\nX_train = train_abalone %>% dplyr::select(-Sex)\n# CP\nabalone_cp = princomp(X_train, scale = TRUE)\n# Resumen numérico del análisis\nget_eigenvalue(abalone_cp)\n\n        eigenvalue variance.percent cumulative.variance.percent\nDim.1 1.034609e+01     97.641532209                    97.64153\nDim.2 2.419785e-01      2.283678798                    99.92521\nDim.3 3.082687e-03      0.029092940                    99.95430\nDim.4 2.655909e-03      0.025065211                    99.97937\nDim.5 1.073762e-03      0.010133660                    99.98950\nDim.6 5.399308e-04      0.005095611                    99.99460\nDim.7 4.215568e-04      0.003978454                    99.99858\nDim.8 1.507935e-04      0.001423118                   100.00000\n\n\nEl resumen del modelo nos muestra que con las dos primeras componentes alcanzamos un 99.99% de variabilidad explicada, es decir, podemos reducir la información contenida en las 8 características originales con tan solo dos componentes. De hecho con la primera ya alcanzamos un 97.7% de variabilidad explicada.Podemos obtener el scree plot (o resumen gráfico del modelo) con la función fviz_eig:\n\n# Resumen gráfico del análisis\nfviz_eig(abalone_cp)\n\n\n\n\nEstudiamos con detalle las dos primeras componentes para determinar la contribución de cada característica original en su construcción:\n\n# Coordenadas de las características en las dos primeras componentes\nvar_pca = get_pca_var(abalone_cp)\nvar_pca$coord[,1:2]\n\n                    Dim.1       Dim.2\nLength         0.06888311  0.08964353\nDiameter       0.05867659  0.07273990\nHeight         0.02361168  0.02599891\nWhole_weight   0.27472170  0.41058411\nShucked_weight 0.09811647  0.19721625\nViscera_weight 0.05728304  0.08974881\nShell_weight   0.08979225  0.10014924\nRings          3.20014154 -0.04916569\n\n\nEn este caso si nos resulta más sencillo escribir las ecuaciones de la componentes que viene dadas por las expresiones:\n\\[CP1 = 0.068*Length + 0.058*Diameter + 0.024*Height  + 0.273*Whole + 0.097*Shucked + 0.057*Viscera + 0.090* Shell + 3.241*Rings\\] \\[CP2 = 0.089*Length + 0.073*Diameter + 0.025*Height  + 0.408*Whole + 0.195*Shucked + 0.089*Viscera + 0.101* Shell -0.048*Rings\\]\ndonde podemos ver que la variable Rings es la relevante en la CP1, mientras que Whole_weight y Shucked_weight lo son en la componente 2. Esas tres características son las más relevantes en la construcción de las componentes como podemos ver a partir de su contribución:\n\n# Contribuciones de las características en las dos primeras componentes\nvar_pca$contrib[,1:2]\n\n                      Dim.1      Dim.2\nLength          0.045861586  3.3209398\nDiameter        0.033277705  2.1865957\nHeight          0.005388615  0.2793402\nWhole_weight    0.729473460 69.6670491\nShucked_weight  0.093048083 16.0734297\nViscera_weight  0.031715798  3.3287448\nShell_weight    0.077929387  4.1449422\nRings          98.983305365  0.9989584\n\n\nEn los valores de las contribuciones podemos ver claramente las características más relevantes en cada una de las componentes. A continuación representamos gráficamente las contribuciones.\n\nfviz_pca_var(abalone_cp, axes = c(1,2),\n             col.var = \"contrib\", \n             gradient.cols = c(\"aquamarine3\", \"darkgoldenrod1\", \"coral3\"),\n             repel = TRUE     \n             )\n\n\n\n\nEn el gráfico se aprecia claramente la contribución de cada variables en cada una de las componentes. Como hay pocas características que contribuyan,estas tienden a agruparse en torno al cero. Para evitar esa aglomeración y que el gráfico sea más informativo vamos a representar solo las características más relevantes. En este caso seleccionamos las tres variables que más contribuyen a la solución.\n\nfviz_pca_var(abalone_cp, axes = c(1,2),\n             select.var = list(contrib = 3),\n             col.var = \"contrib\", \n             gradient.cols = c(\"aquamarine3\", \"darkgoldenrod1\", \"coral3\"),\n             repel = TRUE     \n             )\n\n\n\n\nPodemos trabajar ahora con las muestras consideradas en el entrenamiento representando sus valores en cada una de las componentes.\n\nfviz_pca_ind(abalone_cp, axes = c(1,2), geom = \"point\")\n\n\n\n\nEn el gráfico se representan cada una de las muestras con respecto a las dos primeras CP, es decir, los valores que obtenemos para ambas al sustituir los valores de sus características en las ecuaciones obtenidas. Para entender mejor el comportamiento de las muestras podemos representar los puntos anteriores coloreando cada uno de ellos con la variable Sex.\n\nfviz_pca_ind(abalone_cp, axes = c(1,2), geom = \"point\", \n             label = \"none\", \n             habillage = train_abalone$Sex,\n             addEllipses=TRUE, ellipse.level=0.95)\n\n\n\n\nPodemos ver como los sujetos identificados con el grupo I se sitúan en el tercer cuadrante principalmente mientras que los otros dos grupos se reparten por los otros tres cuadrantes. Este es el motivo por el cual en modelos anteriores el porcentaje de clasificación era tan bajo. Podemos comprobar eso con las elipses obtenidas para cada uno de los grupos donde vemos claramente intersecciones entre ellas.\nPara finalizar el análisis de la muestra de entrenamiento vamos a representar conjuntamente las muestras con la contribución de las variables para entender cuales de ellas caracterizan a las muestras más cercanas.\n\nfviz_pca_biplot(abalone_cp, axes = c(1,2), geom = \"point\", \n                label=\"var\", \n                habillage = train_abalone$Sex,\n                col.var = \"black\", \n                addEllipses=TRUE, ellipse.level=0.95)\n\n\n\n\nCon este gráfico podemos determinar que los sujetos en el grupo I tienen los valores más bajos en las variables que más contribuyen al análisis de CP. Finalizamos el análisis estudiando el comportamiento de las muestras de validación con el modelo obtenido. En primer lugar debemos obtener la predicción de los valores de las componentes para la muestra de validación.\n\n# Valores de validación\nX_test = test_abalone %>% dplyr::select(-Sex)\n# Predicción para el modelo propuesto\npredict.test = predict(abalone_cp, newdata = X_test)\n\nRepresentamos ahora la solución para la muestra de entrenamiento superponiendo los valores de la muestra de validación.\n\n# valores muestra de entrenamiento\np = fviz_pca_ind(abalone_cp, axes = c(1,2), geom = \"point\", label = \"none\")\n# valores muestra de validación\nfviz_add(p, predict.test[,1:2], color =\"blue\", geom = \"point\", addlabel = FALSE)\n\n\n\n\nComo era de esperar la predicción para la muestra de validación es compatible con los resultados de la muestra de entrenamiento.\nComo hemos hecho en el banco de datos anterior podemos definir una tarea de clasificación a partir de las componentes obtenidas para llevar a cabo un modelo de aprendizaje supervisado.\n\n# matriz con las dos primeras componentes\nind_pca = get_pca_ind(abalone_cp)$coord[,1:2]\n# predicciones para la muestra de validación en las dos primeras componentes\npredict.test = predict(abalone_cp, newdata = X_test)[,1:2]\n# Matriz conjunta de coordenadas\nX = rbind(ind_pca, predict.test)\n# vector target\ny = rbind(train_abalone[,\"Sex\"], test_abalone[,\"Sex\"])\n# matriz de datos\ndata_cp = as.tibble(data.frame(y, X))\n# Creación de task \ntsk_abalone = as_task_classif(data_cp, target = \"Sex\")\ntsk_abalone\n\n<TaskClassif:data_cp> (4177 x 3)\n* Target: Sex\n* Properties: multiclass\n* Features (2):\n  - dbl (2): Dim.1, Dim.2\n\n\n\n\n16.3.2.3 Gene expression leukemia\nEste ejemplo es el más habitual dentro de los modelos de reducción de la dimensión, ya que solo disponemos de 64 muestras, mientras que tenemos 22284 expresiones (características) para identificar el tipo de leucemia al que pertenecen. Conseguir reducir la dimensión de la matriz de datos para poder identificar las muestras adecuadamente es el objetivo de las CP en esta situación.\nEn primer lugar creamos la división en muestra de entrenamiento y validación de forma similar al banco de datos anterior.\n\n# Codificación de muestras 80%-20%\nids = sample(c(1, 2), size = nrow(geneexpleu), replace = TRUE, prob = c(0.8, 0.2))\n# Muestras de variables\ntrain_leukemia = geneexpleu[ids == 1,]\ntest_leukemia = geneexpleu[ids==2, ]\n\nObtenemos las CP lineales para este conjunto de datos y analizamos numéricamente la calidad de la solución. En este caso debemos cambiar la función que os permite realizar el análisis de componentes, dado que princomp sol funciona cuando el número de muestras es mayo que el de variables. En este caso utilizamos la función prcomp que tiene las mismas características que la anterior pero funciona en esta situación.\n\nX_train = train_leukemia %>% dplyr::select(-type)\n# CP\nleukemia_cp = prcomp(X_train, scale = TRUE)\n# Resumen numérico del análisis\nget_eigenvalue(leukemia_cp)\n\n         eigenvalue variance.percent cumulative.variance.percent\nDim.1  4.520739e+03     2.028784e+01                    20.28784\nDim.2  3.501512e+03     1.571383e+01                    36.00166\nDim.3  1.676743e+03     7.524765e+00                    43.52643\nDim.4  1.425329e+03     6.396485e+00                    49.92291\nDim.5  1.085828e+03     4.872897e+00                    54.79581\nDim.6  8.110920e+02     3.639959e+00                    58.43577\nDim.7  6.420111e+02     2.881170e+00                    61.31694\nDim.8  6.188420e+02     2.777194e+00                    64.09413\nDim.9  5.770235e+02     2.589523e+00                    66.68366\nDim.10 4.940824e+02     2.217306e+00                    68.90096\nDim.11 4.397460e+02     1.973460e+00                    70.87442\nDim.12 4.358849e+02     1.956132e+00                    72.83055\nDim.13 3.367410e+02     1.511202e+00                    74.34175\nDim.14 3.086688e+02     1.385221e+00                    75.72698\nDim.15 2.956141e+02     1.326635e+00                    77.05361\nDim.16 2.848350e+02     1.278261e+00                    78.33187\nDim.17 2.742650e+02     1.230826e+00                    79.56270\nDim.18 2.738469e+02     1.228950e+00                    80.79165\nDim.19 2.526948e+02     1.134025e+00                    81.92567\nDim.20 2.447249e+02     1.098258e+00                    83.02393\nDim.21 2.211324e+02     9.923815e-01                    84.01631\nDim.22 2.132694e+02     9.570945e-01                    84.97341\nDim.23 2.028438e+02     9.103075e-01                    85.88372\nDim.24 1.888337e+02     8.474341e-01                    86.73115\nDim.25 1.809886e+02     8.122274e-01                    87.54338\nDim.26 1.747806e+02     7.843677e-01                    88.32774\nDim.27 1.604691e+02     7.201416e-01                    89.04789\nDim.28 1.544187e+02     6.929888e-01                    89.74087\nDim.29 1.533225e+02     6.880693e-01                    90.42894\nDim.30 1.481118e+02     6.646850e-01                    91.09363\nDim.31 1.369662e+02     6.146666e-01                    91.70830\nDim.32 1.346512e+02     6.042777e-01                    92.31257\nDim.33 1.308830e+02     5.873668e-01                    92.89994\nDim.34 1.260239e+02     5.655607e-01                    93.46550\nDim.35 1.228773e+02     5.514395e-01                    94.01694\nDim.36 1.159545e+02     5.203722e-01                    94.53731\nDim.37 1.112905e+02     4.994415e-01                    95.03675\nDim.38 1.082983e+02     4.860132e-01                    95.52277\nDim.39 1.007091e+02     4.519548e-01                    95.97472\nDim.40 9.781492e+01     4.389666e-01                    96.41369\nDim.41 9.509857e+01     4.267763e-01                    96.84046\nDim.42 9.322575e+01     4.183716e-01                    97.25884\nDim.43 8.566623e+01     3.844466e-01                    97.64328\nDim.44 8.525253e+01     3.825900e-01                    98.02587\nDim.45 8.167464e+01     3.665334e-01                    98.39241\nDim.46 7.888189e+01     3.540003e-01                    98.74641\nDim.47 7.772464e+01     3.488069e-01                    99.09521\nDim.48 7.421464e+01     3.330550e-01                    99.42827\nDim.49 7.054615e+01     3.165918e-01                    99.74486\nDim.50 5.685270e+01     2.551394e-01                   100.00000\nDim.51 3.832475e-26     1.719910e-28                   100.00000\n\n\nPodemos ver que para alcanzar un porcentaje de clasificación correcta superior al 70% deberíamos considerar al menos 12 componentes. Esto puede parecer mucho pero hay que tener en cuenta que partimos de 22284 y nos estamos quedando con 12. Si nos parece que el porcentaje es demasiado bajo podríamos llegar hasta las 20 componentes con un porcentaje superior al 80%. El problema en esta situación es que los gráficos unidimensionales con las dos primeras componentes ya no resultan tan informativos como en los ejemplos anteriores dado que solo explican el 32.9% de la variabilidad.\nDe todas formas vamos a identificar las 10 expresiones más relevantes en las dos primeras componentes y representaremos las muestras etiquetadas para ver su comportamiento.\n\nfviz_pca_var(leukemia_cp, axes = c(1,2),\n             select.var = list(contrib = 10),\n             col.var = \"contrib\", \n             gradient.cols = c(\"aquamarine3\", \"darkgoldenrod1\", \"coral3\"),\n             repel = TRUE     \n             )\n\n\n\n\nPodemos ver las 10 expresiones que más contribuyen en la construcción de las dos primeras componentes, que recordemos son las más relevantes ya que las construimos por orden de importancia. Veamos el gráfico de muestras:\n\nfviz_pca_ind(leukemia_cp, axes = c(1,2), geom = \"point\", \n             label = \"none\", \n             habillage = train_leukemia$type,\n             addEllipses=TRUE, ellipse.level=0.95)\n\n\n\n\nSi ignoramos las observaciones del grupo AML podemos ver como con dos componentes somos capaces de ordenar los otros cuatro grupos de izquierda a derecha. Tengamos en cuenta que hemos pasado la información de 22284 característica a tan solo 2. Esta solución nos permite establecer scores arbitrarios de clasificación en cada uno de los grupos, pero para evitar sesgos vamos a construir un algoritmo de aprendizaje completo utilizando las CP en la tarea de preprocesado.\nEn primer lugar vamos a definir la tarea correspondiente, aplicaremos las componentes principales y utilizaremos los resultados en un modelo de aprendizaje. En este caso utilizaremos kNN por su sencillez y porque utiliza distancias que es al final lo que estamos observando en la solución de CP.\nPara poder utilizar el algoritmo de aprendizaje debemos generar en primer lugar con los que tenderemos que entrenarlo. para ellos vamos a seleccionar dentro de cada tipo de leucemia el 80% de los datos para entrenamiento y reservaremos el otro 20% para la validación. en este caso hacemos la construcción manualmente para que se entienda el proceso de selección de muestras y para poder usar la función prcomp directamente.\n\n# Generamos factor y contamos los elementos en cada grupo\ngeneexpleu$type = as.factor(geneexpleu$type)\ngrupos = levels(geneexpleu$type)\n# ordenamos los datos de acuerdo a los niveles del factor\ndf = geneexpleu[order(geneexpleu$type),]\n# tamaño de cada grupo\nngroup = df %>% \n  group_by(type) %>%\n  summarize(n = n()) \n# Bucle para la selección de la regla 80-20 dentro de cada grupo\nset.seed(1234)\nids = c()\nfor (i in 1:nrow(ngroup))\n{\n  id_0 = sample(c(1, 2), size = ngroup$n[i], replace = TRUE, prob = c(0.8, 0.2))\n  ids = c(ids, id_0)\n}\n# Muestras de entrenamiento y validación\ntrain_df = df[ids == 1,]\ntest_df = df[ids==2, ]\n\nProcedemos con el análisis de CP para la muestra de entrenamiento.\n\nX_train = train_df %>% dplyr::select(-type)\n# CP\nleukemia_cp = prcomp(X_train, scale = TRUE)\n# Resumen numérico del análisis\nget_eigenvalue(leukemia_cp)\n\n         eigenvalue variance.percent cumulative.variance.percent\nDim.1  4.411351e+03     1.979694e+01                    19.79694\nDim.2  3.401979e+03     1.526715e+01                    35.06409\nDim.3  1.613096e+03     7.239134e+00                    42.30322\nDim.4  1.419492e+03     6.370293e+00                    48.67351\nDim.5  1.064312e+03     4.776339e+00                    53.44985\nDim.6  8.118393e+02     3.643312e+00                    57.09316\nDim.7  7.066226e+02     3.171129e+00                    60.26429\nDim.8  6.057231e+02     2.718319e+00                    62.98261\nDim.9  5.830974e+02     2.616782e+00                    65.59939\nDim.10 4.512864e+02     2.025250e+00                    67.62464\nDim.11 4.326276e+02     1.941514e+00                    69.56616\nDim.12 4.187507e+02     1.879239e+00                    71.44540\nDim.13 3.265857e+02     1.465627e+00                    72.91102\nDim.14 3.087774e+02     1.385709e+00                    74.29673\nDim.15 2.979314e+02     1.337035e+00                    75.63377\nDim.16 2.700705e+02     1.212002e+00                    76.84577\nDim.17 2.665257e+02     1.196094e+00                    78.04186\nDim.18 2.516485e+02     1.129329e+00                    79.17119\nDim.19 2.359456e+02     1.058859e+00                    80.23005\nDim.20 2.330587e+02     1.045903e+00                    81.27596\nDim.21 2.169408e+02     9.735710e-01                    82.24953\nDim.22 2.161647e+02     9.700878e-01                    83.21961\nDim.23 2.059366e+02     9.241870e-01                    84.14380\nDim.24 1.936774e+02     8.691710e-01                    85.01297\nDim.25 1.830696e+02     8.215662e-01                    85.83454\nDim.26 1.814853e+02     8.144562e-01                    86.64900\nDim.27 1.735029e+02     7.786333e-01                    87.42763\nDim.28 1.647993e+02     7.395741e-01                    88.16720\nDim.29 1.537400e+02     6.899432e-01                    88.85715\nDim.30 1.505584e+02     6.756648e-01                    89.53281\nDim.31 1.441521e+02     6.469153e-01                    90.17973\nDim.32 1.358827e+02     6.098042e-01                    90.78953\nDim.33 1.331839e+02     5.976926e-01                    91.38722\nDim.34 1.303979e+02     5.851902e-01                    91.97241\nDim.35 1.270160e+02     5.700130e-01                    92.54243\nDim.36 1.230039e+02     5.520079e-01                    93.09443\nDim.37 1.205819e+02     5.411387e-01                    93.63557\nDim.38 1.160503e+02     5.208019e-01                    94.15637\nDim.39 1.144249e+02     5.135075e-01                    94.66988\nDim.40 1.096759e+02     4.921954e-01                    95.16208\nDim.41 1.089276e+02     4.888371e-01                    95.65091\nDim.42 1.008546e+02     4.526080e-01                    96.10352\nDim.43 9.688027e+01     4.347721e-01                    96.53829\nDim.44 8.988757e+01     4.033908e-01                    96.94169\nDim.45 8.776299e+01     3.938562e-01                    97.33554\nDim.46 8.632529e+01     3.874043e-01                    97.72295\nDim.47 8.281361e+01     3.716448e-01                    98.09459\nDim.48 7.973399e+01     3.578243e-01                    98.45241\nDim.49 7.666314e+01     3.440432e-01                    98.79646\nDim.50 7.388179e+01     3.315612e-01                    99.12802\nDim.51 7.175739e+01     3.220275e-01                    99.45005\nDim.52 6.561306e+01     2.944534e-01                    99.74450\nDim.53 5.693301e+01     2.554997e-01                   100.00000\nDim.54 3.668946e-26     1.646523e-28                   100.00000\n\n\nEl resultado obtenido es similar al anterior donde no hemos estratificado por type. En principio vamos a optar por una solución con 15 componentes donde alcanzamos el 75% de variabilidad explicada.\nPara facilitar la implementación del CP dentro del algoritmo de aprendizaje vamos a definir una función que nos de las puntuaciones de las CP (para un número fijo de componentes) junto con el vector de etiquetas. De esta forma utilizamos el resultado de esa función como entrada de datos del algoritmo de aprendizaje.\nPara generar la tarea debemos obtener las puntuaciones en esas 15 CP tanto de los datos de entrenamiento como los de validación.\n\n# Función para obtener las coordenadas de las muestras en las CP\nobtain_cp = function(dtrain, dtest, ncomp)\n{\n  # dtrain: datos de entrenamiento\n  # dtest: datos de validación\n  # ncomp: numero de componentes a extraer\n  \n  # Modelo CP\n  model_cp = prcomp(dtrain, scale = TRUE)\n  # Puntuaciones de la muestra de entrenamiento\n  cp_train = get_pca_ind(model_cp)$coord[,1:ncomp]\n  # Puntuaciones de la muestra de validación\n  cp_test = predict(model_cp, newdata = dtest)[,1:ncomp]\n  # matriz de puntuaciones\n  df = as.tibble(rbind(cp_train, cp_test))\n  return(df)\n}\n\n\n# Matriz de componentes y etiquetas para la definición de la tarea\nncp = 15\nX_test = test_df %>% dplyr::select(-type)\ndf_leukemia = obtain_cp(X_train, X_test, ncp)\ndf_leukemia$type = c(train_df$type, test_df$type)\n\nAhora ya estamos en disposición de definir la tarea de clasificación usando el algoritmo kNN.\n\n# Creación de task eliminado la columna que identifica os sujetos\ntsk_leukemia = as_task_classif(df_leukemia, target = \"type\")\ntsk_leukemia\n\n<TaskClassif:df_leukemia> (64 x 16)\n* Target: type\n* Properties: multiclass\n* Features (15):\n  - dbl (15): Dim.1, Dim.10, Dim.11, Dim.12, Dim.13, Dim.14, Dim.15,\n    Dim.2, Dim.3, Dim.4, Dim.5, Dim.6, Dim.7, Dim.8, Dim.9\n\n\nConfiguramos el algoritmo para trabajar con 4 vecinos, dado que el número de muestras es bastante pequeño. Además creamos las divisiones para el entrenamiento y la validación.\n\n# Definimos learner para predecir la probabilidad\nlknn_classif = lrn(\"classif.kknn\", k=4, scale = FALSE, predict_type = \"prob\")\n\n# Generamos variable de estrato\ntsk_leukemia$col_roles$stratum <- \"type\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_leukemia, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_leukemia = tsk_leukemia$clone()$filter(splits$train)\ntsk_test_leukemia  = tsk_leukemia$clone()$filter(splits$test)\n\nEvaluamos la capacidad de clasificación del modelo a partir de los scores habituales y del estudio de la matriz de confusión.\n\n# Entrenamiento\nlknn_classif$train(tsk_train_leukemia)\n# Predicción de la muestra de entrenamiento y validación\npred_train = lknn_classif$predict(tsk_train_leukemia)\npred_test = lknn_classif$predict(tsk_test_leukemia)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n classif.acc classif.bacc \n           1            1 \n\n# Muestra de validación\npred_test$score(measures)\n\n classif.acc classif.bacc \n           1            1 \n\n# Matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nLos resultados muestran una clasificación perfecta del 100%, es decir, con 15 componentes somo capaces de clasificar los diferentes tipos de tumor a partir de las 22248 expresiones. Sin embargo, vamos a ver que efecto tiene el reducir el número de componentes para obtener un modelo más sencillo. Veamos que ocurre si consideramos únicamente dos componentes principales.\n\ndf_leukemia = obtain_cp(X_train, X_test, 2)\ndf_leukemia$type = c(train_df$type, test_df$type)\n\nGeneramos y evaluamos el nuevo modelo\n\n# Creación de task eliminado la columna que identifica os sujetos\ntsk_leukemia = as_task_classif(df_leukemia, target = \"type\")\n# Definimos learner para predecir la probabilidad\nlknn_classif = lrn(\"classif.kknn\", k=4, scale = FALSE, predict_type = \"prob\")\n# Generamos variable de estrato\ntsk_leukemia$col_roles$stratum <- \"type\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_leukemia, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_leukemia = tsk_leukemia$clone()$filter(splits$train)\ntsk_test_leukemia  = tsk_leukemia$clone()$filter(splits$test)\n# Entrenamiento\nlknn_classif$train(tsk_train_leukemia)\n# Predicción de la muestra de entrenamiento y validación\npred_train = lknn_classif$predict(tsk_train_leukemia)\npred_test = lknn_classif$predict(tsk_test_leukemia)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n classif.acc classif.bacc \n   0.8627451    0.8630952 \n\n# Muestra de validación\npred_test$score(measures)\n\n classif.acc classif.bacc \n   0.8461538    0.8600000 \n\n# Matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nSi trabajamos con dos CP el porcentaje de clasificación correcta alcanza el 86% para la muestra de validación, y solo se observan errores de clasificación con los tipos AML y Bone_Marrow_CD34. Dado que hemos caído un 14% en el porcentaje de clasificación puede parecer que la solución no es muy buena, pero recordemos la reducción en el número de características consideradas. Veamos que ocurre si duplicamos el número de CP.\n\ndf_leukemia = obtain_cp(X_train, X_test, 4)\ndf_leukemia$type = c(train_df$type, test_df$type)\n\n\n# Creación de task eliminado la columna que identifica os sujetos\ntsk_leukemia = as_task_classif(df_leukemia, target = \"type\")\n# Definimos learner para predecir la probabilidad\nlknn_classif = lrn(\"classif.kknn\", k=4, scale = FALSE, predict_type = \"prob\")\n# Generamos variable de estrato\ntsk_leukemia$col_roles$stratum <- \"type\"\n# Fijamos semilla para asegurar la reproducibilidad del modelo\nset.seed(135)\n# Creamos la partición\nsplits = mlr3::partition(tsk_leukemia, ratio = 0.8)\n# Muestras de entrenamiento y validación\ntsk_train_leukemia = tsk_leukemia$clone()$filter(splits$train)\ntsk_test_leukemia  = tsk_leukemia$clone()$filter(splits$test)\n# Entrenamiento\nlknn_classif$train(tsk_train_leukemia)\n# Predicción de la muestra de entrenamiento y validación\npred_train = lknn_classif$predict(tsk_train_leukemia)\npred_test = lknn_classif$predict(tsk_test_leukemia)\n# scores de validación\nmeasures = msrs(c(\"classif.acc\", \"classif.bacc\"))\n# Muestra de entrenamiento\npred_train$score(measures)\n\n classif.acc classif.bacc \n   0.9607843    0.9571429 \n\n# Muestra de validación\npred_test$score(measures)\n\n classif.acc classif.bacc \n           1            1 \n\n# Matriz de confusión\ncm = confusion_matrix(pred_test$truth, pred_test$response)\nplot_confusion_matrix(cm$`Confusion Matrix`[[1]]) \n\n\n\n\nCon cuatro componentes pasamos a un 95% de clasificación correcta y un 100% de clasificación ponderada para la muestra de validación. Parece claro que una solución con cuatro componentes resulta muy aceptable en esta situación.\nEn este último ejemplo hemos visto como podemos combinar el método de CP y un algoritmo de clasificación, pero podríamos hacer algo similar en problemas de regresión donde nuestro objetivo es la predicción y no la clasificación. la forma de proceder es similar salvo por el hecho de que no podemos colorear los puntos en los gráficos de componentes."
  },
  {
    "objectID": "160_PrinCompmodels.html#sec-160.4",
    "href": "160_PrinCompmodels.html#sec-160.4",
    "title": "16  Componentes principales (CP)",
    "section": "16.4 Ejercicios",
    "text": "16.4 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en componentes principales y un algoritmo de clasificación para el banco de datos Iris4.3.3.\nAjustar un modelo de aprendizaje automático basado en componentes principales y un algoritmo de clasificación para el banco de datos Wine quality4.3.8.\nAjustar un modelo de aprendizaje automático basado en componentes principales y un algoritmo de clasificación para el banco de datos WGene expression breast cancer4.3.10.\nAjustar un modelo de aprendizaje automático basado en componentes principales y un algoritmo de clasificación para el banco de datos QSAR4.2.8.\nAjustar un modelo de aprendizaje automático basado en componentes principales y un algoritmo de clasificación para el banco de datos Meat spec4.2.5."
  },
  {
    "objectID": "170_MDSmodels.html#sec-170.1",
    "href": "170_MDSmodels.html#sec-170.1",
    "title": "17  Métodos de escalado multidimensional (MDS)",
    "section": "17.1 Escalado Multidimensional",
    "text": "17.1 Escalado Multidimensional\nEl MDS tiene por objetivo representar los puntos que residen en un espacio de gran dimensión a uno de menor dimensión, preservando al máximo las distancias entre esos puntos. De este modo, las distancias o similitudes entre pares de puntos en el espacio de menor dimensión se aproximan mucho a sus distancias reales.\nMDS puede ser utilizado como un método de preprocesamiento de datos en problemas de regresión y clasificación. Normalmente, la medida de distancia utilizada en el MDS es la distancia euclídea, aunque al aplicar el MDS puede utilizarse cualquier otra métrica de disimilitud adecuada.\nExisten dos tipos principales de MDS: métrico (clásico) y no métrico. Aunque ambos tienen como objetivo encontrar la mejor representación en dimensiones inferiores de los datos en dimensiones superiores, sus diferencias radican en el tipo de datos para los que están diseñados.\n\nEl MDS métrico (clásico) también se conoce como análisis de coordenadas principales (PCoA). No hay que confundirlo con el Análisis de Componentes Principales (PCA). El MDS métrico intenta modelar la similitud/disimilitud de los datos calculando las distancias entre cada par de puntos utilizando sus coordenadas geométricas. La clave aquí es la capacidad de medir una distancia utilizando una escala lineal. Por ejemplo, una distancia de 10 unidades se consideraría el doble de lejos que una distancia de 5 unidades.\nMDS no métrico: está diseñado para tratar datos ordinales. Por ejemplo, puede haber pedido a sus clientes que valoren sus productos en una escala de 1 a 5, donde 1 es terrible y 5 es asombroso. En este caso, un producto con una puntuación de 2 no es necesariamente el doble de bueno que un producto con una puntuación de 1. Lo que importa es el orden (1 < 2 < 3 < 4 < 5) y no el valor absoluto. Este es el tipo de situación en la que se utilizaría un MDS no métrico.\n\n\n17.1.1 MDS métrico\nEn general, la métrica MDS calcula las distancias entre cada par de puntos en el espacio original de alta dimensión y, a continuación, los representa en un espacio de menor dimensión preservando al máximo esas distancias entre puntos. En concreto el algoritmo MDS métrico puede describirse en dos pasos:\n\nCalculamos todas las distancias entre pares de puntos.\nCon las distancias originales conocidas, el algoritmo intenta resolver el problema de optimización encontrando un conjunto de coordenadas en un espacio de dimensiones inferiores que minimice el valor de Stress dado por:\n\n\\[Stress(x_1,...,x_n) = \\sqrt{\\sum_{i\\neq j = 1,...,N} (d_{ij} - ||x_i-x_j||)^2}\\]\ndonde \\(x_1,..x_n\\) son los puntos de datos en su nuevo conjunto de coordenadas en un espacio de dimensión inferior, \\(d_{ij}\\) es la distancia original entre dos puntos del espacio el la dimensión original, y \\(||x_i-x_j||\\) es la distancia entre los dos puntos correspondientes en el espacio de dimensiones reducido.\nSe pueden utilizar varios métodos para optimizar la función anterior, como el método de descenso más pronunciado de Kruskal o el método iterativo de mayorización de De Leeuw. Un aspecto importante a tener en cuenta es que los dos métodos antes mencionados son enfoques iterativos, a veces pueden proporcionar resultados diferentes, ya que son sensibles a la posición inicial de partida. Al final, se elige como resultado final la configuración con el valor de Stress más bajo.\n\n\n17.1.2 MDS no métrico\nEl MDS no métrico se centra en la ordenación de los datos. En este caso consideramos la matriz \\(S\\) de similitudes, y \\(X\\) la matriz de coordenadas de los puntos en el espacio original. Si \\(S_{ij} > S_{jk}\\) entonces las distancias deben verificar \\(d_{ij} < d_{jk}\\). Por este motivo, se habla de disimilitudes \\((\\delta_{ij})\\) en lugar de similitudes \\((S_{ij})\\). En este caso las disimilitudes pueden obtenerse fácilmente a partir de las similitudes mediante una simple transformación, por ejemplo \\(\\delta_{ij} = c_1-c_2S_{ij}\\) para algunas constantes reales \\(c_1\\) y \\(c_2\\). Un algoritmo sencillo para obtener una ordenación adecuada consiste en utilizar una regresión monotónica de \\(d_{ij}\\) sobre \\(\\delta_{ij}\\) que produce disparidades \\(\\hat{d}_{ij}\\) en el mismo orden que \\(\\delta_{ij}\\).\nUna solución trivial a este problema es situar todos los puntos en el origen. Para evitarlo, las disparidades \\(\\hat{d}_{ij}\\) se normalizan. Dado que sólo nos preocupamos por el orden relativo, nuestro objetivo debería ser invariable a la simple traslación y escalado; sin embargo, la función de stress utilizada en el MDS métrico es sensible al escalado. Para solucionarlo, el MDS no métrico puede utilizar una tensión normalizada, conocida como stress-1 definida como:\n\\[\\sqrt{\\frac{\\sum_{i<j} (d_{ij}-\\hat{d}_{ij})^2}{\\sum_{i<j} d_{ij}^2}}\\]\n\n\n17.1.3 Validación del modelo\nPara validar el modelo de escalado multidimensional se suele representar gráficamente el valor de stress versus el número de dimensiones consideradas. Se considera que el número óptimo de dimensiones se alcanza cuando la curva resultante se estabiliza sobre cierto valor de stress. En este caso el grid search puede no resultar útil si especificamos un número de dimensiones muy alto porque el valor de stress tiende a decrecer, y por tanto seleccionaríamos la solución con más dimensiones."
  },
  {
    "objectID": "170_MDSmodels.html#sec-170.2",
    "href": "170_MDSmodels.html#sec-170.2",
    "title": "17  Métodos de escalado multidimensional (MDS)",
    "section": "17.2 Ampliaciones del MDS",
    "text": "17.2 Ampliaciones del MDS\nLos métodos clásicos de reducción de la dimensión (PCA, MDS, Análisis factorial…) son fáciles de implementar, eficientes computacionalmente (ya que suelen ser métodos espectrales, es decir, basados en los valores y vectores propios de cierta matriz) y garantizan encontrar la estructura de nuestros datos si es que estos se encuentran sobre un subespacio lineal.\nEn PCA nuestros datos se proyectan en una menor dimensión que preserva la varianza; las componentes principales van en la dirección de máxima varianza de nuestros datos. Por otra parte, el escalado multidimensional clásico (métrico) encuentra una configuración de X que preserva las distancias originales entre individuos. Ambos métodos funcionarán bien siempre y cuando nuestros datos se encuentren sobre un subespacio lineal.\nPara solucionar estas deficiencias debemos definir un concepto fundamental; la geodésica. En geometría, la línea geodésica se define como la línea de mínima longitud que une dos puntos en una superficie dada, y está contenida en esta superficie. En un espacio euclídeo cualquier línea recta es geodésica. Para entender el comportamiento de la distancia geodésica tomamos como ejemplo de subespacio no lineal “Swiss Roll” (rollo suizo) que representamos en la imagen siguiente:\n\n\n\n\n\nEn esta situación la línea azul punteada es una recta geodésica en el espacio euclídeo y la línea azul continua es una recta geodésica en el subespacio lineal. Los puntos A y B se encuentran muy alejados en el brazo si tomamos la distancia geodésica; \\(d′\\), en cambio, parecerá que están muy cerca si consideramos una distancia euclídea; \\(d″\\). Solo la distancia geodésica puede reflejar la bidimensionalidad del Swiss Roll. En este caso, PCA y MDS “verán” únicamente la estructura euclídea, por tanto, nunca podrán encontrar la bidimensionalidad del subespacio considerado.\n\n17.2.1 Isomap\nComo acabamos de ver, nos gustaría considerar la distancia geodésica del subespacio y no la distancia euclídea. Pero si solo contamos con una nube de puntos y no conocemos la superficie sobre la que se encuentran los datos, ¿cómo podemos calcularla?\nA modo de ejemplo, consideremos una superficie circular sobre la que se encuentran dos observaciones: E y C. Supongamos que queremos llegar del punto C al punto E. La distancia que deberemos recorrer de C a E será la distancia geodésica; d′ ya que recorrer una distancia euclídea; d″ sería hacer trampas porque C no tiene por qué estar en un espacio euclídeo sino en uno no lineal. Pero si solo contamos con las coordenadas en el espacio euclídeo de E y C y desconocemos la superficie sobre la que se encuentran ambos, no podemos calcular d′. Aquí es donde entran en juego los vecinos de E que también se encuentran sobre la superficie no lineal. Una forma de aproximar la distancia geodésica entre E y C es encontrar la observación más cercana a E; r y medir la distancia euclídea entre r y E. Si r está muy cerca de E no hay grandes diferencias entre la distancia euclídea y la distancia geodésica. Una vez calculada \\(d(E,r)\\) podemos encontrar el amigo más cercano a r y medir su distancia euclídea, así hasta llegar a C. Si sumáramos todas estas distancias euclídeas, tendríamos una aproximación de d′:\n\\[d′=d(E,r)+d(r,x_i)+...+d(x_j,C)\\]\nMás formalmente, lo que vamos a querer hacer es construir un grafo de los k-vecinos más cercanos (aunque en el artículo original también se propone considerar como vecinos las observaciones que se encuentren en un radio fijo \\(\\epsilon\\)) y calcular la distancia entre E y C como la distancia del camino más corto en el grafo para ir de E a C.\nDe momento, tenemos solo las distancias geodésicas entre las observaciones que son vecinas, para las que no lo son la distancia entre estas es infinito. Pero, podemos calcular la distancia entre dos observaciones que no son vecinas; \\(x_i\\) y \\(x_j\\) como la distancia del camino más corto para ir de \\(x_i\\) a \\(x_j\\) (imagen siguiente). La línea azul representa la distancia euclídea y la linea roja representa la distancia usando los vecinos.\n\n\n\n\n\nLos algoritmos que se usan para encontrar este camino más corto entre dos nodos en el grafo son Dijkstra o Floyd. Ambos conocidos son bien conocidos dentro de la teoría de grafos en la búsqueda de caminos más cortos en tres dos localizaciones.\nUno de los primeros enfoques basados en al distancia geodésica es el algoritmo Isomap, abreviatura de Isometric Mapping. Isomap puede considerarse como una extensión de Multi-dimensional Scaling (MDS) o Kernel PCA. Busca una representación de un espacio multidimensional en otro inferior (generalmente dos dimensiones) que mantenga las distancias geodésicas entre todos los puntos, en lugar de las distancia euclídeas. El algoritmo Isomap se puede representar en los pasos siguientes:\n\nFijar k (el número de vecinos más cercanos), p (número de dimensiones a obtener en MDS), y considerar la matriz de datos X, de dimensiones d×m.\nBúsqueda de los k vecinos más próximos para cada punto del banco de datos, y construir el grafo correspondiente utilizando las distancias geodésicas.\nCalcular la matriz de distancias más cortas usando los algoritmos de Floyd o Dijkstra.\nAplicar MDS a a la matriz de distancias obtenidas en el paso anterior para reducir la dimensión manteniendo las distancias geodésicas.\n\n\n\n17.2.2 Linear Local Embedding (LLE)\nEl algoritmo LLE busca una proyección de los datos en una dimensión inferior que preserve las distancias dentro de los vecindarios locales. Puede considerarse como una serie de análisis de componentes principales locales que se comparan globalmente para encontrar la mejor representación no lineal. El algoritmo LLE trata de reducir un problema de n dimensiones a la vez que intenta preservar las características geométricas de la estructura original de características no lineales.\nLLE encuentra primero los k vecinos más cercanos de los puntos. A continuación, aproxima cada vector de datos como una combinación lineal ponderada de sus k vecinos más cercanos. Por último, calcula los pesos que mejor reconstruyen los vectores a partir de sus vecinos y, posteriormente, produce los vectores de baja dimensión mejor reconstruidos por estos pesos. El algoritmo se puede describir de la forma siguiente:\n\nEncontrar los K vecinos más próximos. Una de las ventajas del algoritmo LLE es que sólo hay que ajustar un parámetro: el valor de K, es decir, el número de vecinos más próximos que se consideran parte de un conglomerado. Si K se elige demasiado pequeño o demasiado grande, no podrá acomodarse a la geometría de los datos originales. Aquí, para cada punto de datos que tenemos, calculamos los K vecinos más próximos.\nHacemos una agregación ponderada de los vecinos de cada punto para construir un nuevo punto. Intentamos minimizar la función de coste, donde \\(X_j\\) es el vecino más cercano para el punto \\(X_i\\)\n\n\\[E(W) = \\sum_i |X_i - \\sum_j W_{ij}X_j|^2, \\text{ con } \\sum_j W_{ij} = 1\\]\n\nAhora definimos el nuevo espacio vectorial \\(Y\\) tal que minimizamos el coste para \\(Y\\) como los nuevos puntos\n\n\\[C(Y) = \\sum_i |Y_i - \\sum_j W_{ij}Y_j|^2\\]\n\n\n17.2.3 Diferencias entre ISOMAP y LLE\nPara ver las diferencias entre ambos algoritmos tomamos como ejemplo el conjunto de datos siguientes donde podremos ver como obtienen las distancias geodésicas y la elección de vecinos. El conjunto de datos son los conocidos datos en espiral:\n\n\n\n\n\nFijando el número de vecinos en 10 el algoritmo Isomap proporciona las siguientes relaciones de vecindad:\n En este caso las distancias euclidianas nos dan vecinos más cercanos que no necesariamente proporcionan los vecinos más cercanos a lo largo de figura, ya que se establecen conexiones entre diferentes niveles de la espiral.\nVeamos que ocurre con el algoritmo LLE cuando tomamos tres vecinos:\n Al reducir el número de vecinos las conexiones establecidas se restringen más al comportamiento de la espiral pero siguen existiendo conexiones entre diferentes niveles.\nEn los puntos siguientes veremos las funciones y librearías de R que nos permiten realizar el escaldo multidimensional en sus diferentes versiones, y lo aplicaremos en diferentes ejemplos."
  },
  {
    "objectID": "170_MDSmodels.html#sec-170.3",
    "href": "170_MDSmodels.html#sec-170.3",
    "title": "17  Métodos de escalado multidimensional (MDS)",
    "section": "17.3 MDS en R",
    "text": "17.3 MDS en R\nLa librería mlr3 no dispone de funciones específicas para llevar a cabo el escalamiento multidimensional por lo que recurrimos directamente a las funciones y librerías existentes para su presentación.\nLas funciones para llevar a cabo los diferentes tipos de escalamiento multidimensional presentado son:\n\ncmdscale de la librería stats para llevar a cabo el escalamiento multidimensional métrico. Los parámetros más relevantes de esta función son d, que contiene la matriz de distancias entre las muestras obtenida mediante la función dist, k que identifica el número máximo de dimensiones que deseamos representar, eig que indica si se deben devolver los valores propios como resultado de la función.\nisoMDS de la librería stats para llevar a cabo el escalamiento multidimensional no métrico. Los parámetros más relevantes de esta función son d, que contiene la matriz de distancias entre las muestras obtenida mediante la función dist, y k que identifica el número máximo de dimensiones que deseamos representar. También podemos fijar la tolerancia del proceso de convergencia mediante el parámetro tol.\nIsomap de la librería RDRToolbox para llevar a cabo el escalamiento Isomap. Los parámetros más relevantes de esta función son data, que contiene la matriz de datos, dims que identifica el número máximo de dimensiones que deseamos representar, y k que identifica el número de vecinos que vamos a utilizar en el proceso de construcción.\nLLE de la librería RDRToolbox para llevar a cabo el escalamiento Isomap. Los parámetros más relevantes de esta función son data, que contiene la matriz de datos, dim que identifica el número máximo de dimensiones que deseamos representar, y k que identifica el número de vecinos que vamos a utilizar en el proceso de construcción.\n\nPara utilizar estas dos últimas funciones hay que instalar la librería correspondiente mediante el código siguiente:\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"RDRToolbox\")\n\nCargamos el resto de librerías necesarias.\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(cvms)\nlibrary(kknn)\nlibrary(rpart.plot)\nlibrary(rda)\nlibrary(klaR)\nlibrary(ggord)\ntheme_set(theme_sjplot2())\n\n# Paquetes AA\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3tuningspaces)\nlibrary(gbm)\nlibrary(RWeka)\nlibrary(xgboost)\nlibrary(lightgbm)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(rsvd)\nlibrary(kernlab)\nlibrary(fastICA)\nlibrary(RDRToolbox)\n\n\n17.3.1 Bancos de datos\nPara ejemplificar el uso de los modelos de escalamiento multidimensional vamos a utilizar tres bancos de datos: Gene expression leukemia, Vehicle silhouettes, y Sales. A continuación presentamos los tres bancos de datos.\n\n17.3.1.1 Gene expression leukemia\nEste banco de datos lo hemos descrito en temas anteriores. Cargamos los datos y generamos un vector con la variable que identifica el tipo de leucemia.\n\n# Leemos los datos\ngeneexpleu = read_rds(\"geneexpressionleukemia.rds\")\n# Eliminamos el indicador de la muestra\ngeneexpleu = geneexpleu %>% dplyr::select(-samples)\n# Consideramos matriz de características y tipos\nX_genes = geneexpleu %>% dplyr::select(-type)\ny_genes = geneexpleu[,\"type\"]\n# Debemos cambiar los nombres pq no se respeta la convención de R ya que todas ellas empiezan por un número\nnames(X_genes) = str_replace_all(names(X_genes),\"[/_-]\",\"\")\nnames(X_genes) = paste(\"V\", names(X_genes), sep=\"_\")\n\n\n\n17.3.1.2 Vehicle silhouettes\nEste conjunto de datos recoge información de cuatro tipos diferentes de vehículos, utilizando un conjunto de características extraídas de su silueta. Para el experimento se utilizaron cuatro vehículos modelo “Corgie”: un bus de dos pisos, una camioneta Cheverolet, un Saab 9000 y un Opel Manta 400. El objetivo del estudio es clasificar una silueta dada como uno de cuatro tipos diferentes de vehículos. Todos los atributos son numéricos discretos salvo la última variable que registra el tipo de vehículo. No existen valores perdidos.\n\n# Leemos los datos\nvehicle = read_rds(\"vehicle.rds\")\n# Consideramos matriz de características y tipos\nX_vehicle = vehicle %>% dplyr::select(-Class)\ny_vehicle = vehicle[,\"Class\"]\n\n\n\n17.3.1.3 Sales\nContiene las cantidades compradas semanalmente de 800 productos a lo largo de 52 semanas. Todos los atributos son numéricos sin valores perdidos y van identificados mediante W(número de la semana). En este caso no tenemos posible variable target.\n\n# Leemos los datos\nsales = read_rds(\"sales.rds\")\n# Eliminamos la variable que identifica el código del producto y la almacenamos por si hiciera falta\nX_sales = sales %>% dplyr::select(-Product_Code)\ny_sales = sales[,\"Product_Code\"]\n\n\n\n\n17.3.2 Modelos\nPara cada banco de datos probamos los diferentes modelos de escalamiento multidimensional y valoramos las soluciones obtenidas.\n\n17.3.2.1 Gene expression leukemia\nComenzamos con el banco de datos de expresiones génicas. Para poder utilizar los algoritmos MDS debemos obtener en primer lugar la matriz de distancias entre muestras para lo que utilizaremos la distancia euclídea.\nComenzamos con los diferentes algoritmos.\n\nMDS métrico\nObtenemos las solución del MDS métrico para dos dimensiones.\n\n# Matriz de distancias\nmds_leukemia = X_genes %>%\n  dist() %>%        # Calculamos distancia\n  cmdscale(k=2, eig = TRUE) \n\nCreamos un tibble con las coordenadas y añadimos la columna que identifica el tipo de leucemia para poder representar la solución vinculada con el target original.\n\n# tibble de coordenadas\nmds_plot = as.tibble(mds_leukemia$points)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_genes$type\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nLa solución MDS parece identificar claramente los grupos PB, Bone_Marrow_CD34, y PBSC_CD34, mientras que parece confundir los grupos AML y Bone_Marrow. Al igual que ocurre con las CP podemos saber el porcentaje de variabilidad explicada por la solución MDS construida con:\n\nmds_leukemia$GOF\n\n[1] 0.4386531 0.4386531\n\n\nEl valor es bastante bajo indicando que la solución es algo pobre. Podemos valorar el ajuste comparando las distancias originales frente a las que obtendríamos con la solución MDS. Calculamos ambas distancias y las representamos gráficamente.\n\n# Distancias\noriginal = X_genes %>% dist()\npredicha = mds_leukemia$points %>% dist()\ndf = data.frame(original, predicha)\n# Gráfico\nggplot(df, aes(original, predicha)) + geom_point() + geom_abline(intercept = 0, slope = 1)\n\n\n\n\nla nube de puntos se alejan de la diagonal, que mostraría un ajuste perfecto, lo que demuestra que la solución no es adecuada. Pasamos el MDS no métrico.\n\n\nMDS no métrico\n\n# Matriz de distancias\nmdsnm_leukemia = X_genes %>%\n  dist() %>%        # Calculamos distancia\n  isoMDS(k=2) \n\ninitial  value 24.485837 \nfinal  value 24.470167 \nconverged\n\n\nEl valor de stress final es bastante algo lo que indica que el modelo puede no ser adecuado para este conjunto de datos. Representamos la solución identificando de nuevo por los tipos de leucemia.\n\n# tibble de coordenadas\nmds_plot = as.tibble(mdsnm_leukemia$points)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_genes$type\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nLa solución no métrica es prácticamente idéntica a la métrica lo que demuestra que la solución no es muy adecuada. Probamos con los otros algoritmos\n\n\nIsomap\nEn estos modelos no hace falta pasar la matriz de distancias ya que podemos utilizar los datos originales. Podemos probar diferentes configuraciones del número de vecinos pero para comenzar seleccionaremos k=5.\n\nisomap_leukemia = Isomap(as.matrix(X_genes), dims = 1:2, k = 5)\n\nComo en los modelos anteriores utilizamos las coordenadas de la solución obtenida para ver el comportamiento de los diferentes tipos de leucemia.\n\n# tibble de coordenadas\nmds_plot = as.tibble(isomap_leukemia$dim2)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_genes$type\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nLa solución con cinco vecinos del Isomap parece incluso peor que las soluciones MDS obtenidas antes. En este caso solo se identifica claramente el tipo Bone_Marrow_CD34. Veamos que ocurre si reducimos el número de vecinos a 3.\n\nisomap_leukemia = Isomap(as.matrix(X_genes), dims = 1:2, k = 3)\n# tibble de coordenadas\nmds_plot = as.tibble(isomap_leukemia$dim2)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_genes$type\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nLa solución es algo diferente y parecen identificarse dos tipos en lugar de uno solo. Por último probamos una solución con 10 vecinos para ver el efecto que tiene en la solución.\n\nisomap_leukemia = Isomap(as.matrix(X_genes), dims = 1:2, k = 10)\n# tibble de coordenadas\nmds_plot = as.tibble(isomap_leukemia$dim2)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_genes$type\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nEn este caso el único tipo que se identifica aisladamente es PB mientras que el resto está bastante disperso. No parece que ninguna de las soluciones propuestas nos sira para nuestro objetivo final. Veamos que ocurre cuando usamos LLE.\n\n\nLLE\nEn este caso comenzamos con 5 vecinos como en el caso de Isomap. Obtenemos las solución y la representamos gráficamente.\n\nlle_leukemia = LLE(as.matrix(X_genes), dim = 2, k = 5)\n# tibble de coordenadas\nmds_plot = as.tibble(lle_leukemia)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_genes$type\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nEn este caso si se identifican los cinco grupos claramente. De hecho las observaciones de tres de ellos se asignan en el mismo punto. El único tipo que parece disperso es AML, que ya vimos en análisis anteriores que era el más difícil de identificar. Esta solución resulta adecuada si estamos interesado sen diferenciar los tipos de tumor pero parece demasiado restrictiva. Veamos que ocurre si aumentamos el número de vecinos a 10.\n\nlle_leukemia = LLE(as.matrix(X_genes), dim = 2, k = 10)\n# tibble de coordenadas\nmds_plot = as.tibble(lle_leukemia)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_genes$type\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nEn este caso se diferencian los grupos PB, Bone_Marrow y PBSC_CD34, mientras que los otros dos aparecen muy mezclados. La solución LLE parece que muestra el mejor funcionamiento de todos los algoritmos utilizados para este banco de datos.\n\n\n\n17.3.2.2 Vehicle silhouettes\n\nMDS métrico\nObtenemos las solución del MDS métrico para dos dimensiones.\n\nmds_vehicle = X_vehicle %>%\n  dist() %>%        # Calculamos distancia\n  cmdscale(k=2, eig = TRUE) \n\nCreamos un tibble con las coordenadas y añadimos la columna que identifica el tipo de leucemia para poder representar la solución vinculada con el target original.\n\n# tibble de coordenadas\nmds_plot = as.tibble(mds_vehicle$points)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$vehicle = y_vehicle\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = vehicle)) + \n  geom_point()\n\n\n\n\nClaramente la solución métrica no permite caracterizar los cuatro grupos de vehículos. Veamos que ocurre con la solución no métrica.\n\n\nMDS no métrico\n\n# Matriz de distancias\nmdsnm_vehicle = X_vehicle %>%\n  dist() %>%        # Calculamos distancia\n  isoMDS(k=2) \n\ninitial  value 2.813486 \nfinal  value 2.813402 \nconverged\n\n\nEl valor de strees es bajo mostrando que la solución podría se adecuada. Veamos la representación gráfica de la solución.\n\n# tibble de coordenadas\nmds_plot = as.tibble(mdsnm_vehicle$points)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_vehicle\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nLa solución del modelo no métrico coincide con la del métrico, lo que era de esperar vistos lo valores de stress inicial y final.\n\n\nIsomap\nVeamos la solución Isomap con un número de vecinos k=5.\n\nisomap_vehicle = Isomap(as.matrix(X_vehicle), dims = 1:2, k = 5)\n\nComo en los modelos anteriores utilizamos las coordenadas de la solución obtenida para ver el comportamiento de los diferentes tipos de leucemia.\n\n# tibble de coordenadas\nmds_plot = as.tibble(isomap_vehicle$dim2)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_vehicle\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nLa solución se parece a las dos anteriores pero parece separar algo más los puntos y tiende a concentrar los del mismo tipo. Veamos que ocurre si aumentamos el número de vecinos a 10.\n\nisomap_vehicle = Isomap(as.matrix(X_vehicle), dims = 1:2, k = 10)\n# tibble de coordenadas\nmds_plot = as.tibble(isomap_vehicle$dim2)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_vehicle\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nLa solución es diferente a las anteriores pero sigue sin cumplir el objetivo que se persigue. Exploramos el algoritmo siguiente.\n\n\nLLE\nEn este caso comenzamos con 5 vecinos como en el caso de Isomap. Obtenemos las solución y la representamos gráficamente.\n\nlle_vehicle = LLE(as.matrix(X_vehicle), dim = 2, k = 5)\n# tibble de coordenadas\nmds_plot = as.tibble(lle_vehicle)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# añadimos tipo\nmds_plot$type = y_vehicle\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2, color = type)) + \n  geom_point()\n\n\n\n\nLa solución no resulta adecuada y se puede ver como cambiando el número de vecinos no mejoramos lo suficiente. Queda claro que la clasificación buscada resulta muy complicada con los datos recogidos. Esto puede ser debido a que hay una gran confusión en los tipos de vehículo atendiendo a las variables consideradas.\nVeamos que ocurre con el último ejemplo.\n\n\n\n17.3.2.3 Sales\nAnalizamos el fichero de ventas donde no tenemos variable target.\n\nMDS métrico\nObtenemos las solución del MDS métrico para dos dimensiones y representamos gráficamente la solución.\n\nmds_sales = X_sales %>%\n  dist() %>%        # Calculamos distancia\n  cmdscale(k=2, eig = TRUE) \n# tibble de coordenadas\nmds_plot = as.tibble(mds_sales$points)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2)) + \n  geom_point()\n\n\n\n\nEn el gráfico aparecen determinarse tres agrupaciones de puntos sobre los valores de la dimensión 1. Veamos la calidad de la solución representando las distancias originales frente a las predichas por el modelo.\n\n# Distancias\noriginal = X_sales %>% dist()\npredicha = mds_sales$points %>% dist()\ndf = data.frame(original, predicha)\n# Gráfico\nggplot(df, aes(original, predicha)) + geom_point() + geom_abline(intercept = 0, slope = 1)\n\n\n\n\nLas distancias obtenidas con el modelo se aproximan bastante bien a las originales salvo en las distancia bajas. Veamos que ocurre con la solución no métrica.\n\n\nMDS no métrico\n\n# Matriz de distancias\nmdsnm_sales = X_sales %>%\n  dist() %>%        # Calculamos distancia\n  isoMDS(k=2) \n\ninitial  value 6.265815 \nfinal  value 6.265459 \nconverged\n\n# tibble de coordenadas\nmds_plot = as.tibble(mdsnm_sales$points)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2)) + \n  geom_point()\n\n\n\n\nA la vista de los valores de stress la solución métrica y no métrica son muy similares. Pasamos a explorar el resto de soluciones.\n\n\nIsomap\nVeamos la solución Isomap con un número de vecinos k=5.\n\nisomap_sales = Isomap(as.matrix(X_sales), dims = 1:2, k = 5)\n# tibble de coordenadas\nmds_plot = as.tibble(isomap_sales$dim2)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2)) + \n  geom_point()\n\n\n\n\nLa configuración de la solución es bastante diferente a la de la solución MDS. Veamos que ocurre si variamos el número de vecinos. Cambiamos a 10 vecinos.\n\nisomap_sales = Isomap(as.matrix(X_sales), dims = 1:2, k = 10)\n# tibble de coordenadas\nmds_plot = as.tibble(isomap_sales$dim2)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2)) + \n  geom_point()\n\n\n\n\nAmpliar el número de vecinos no cambia la configuración de las dos dimensiones. Exploramos la solución LLE.\n\n\nLLE\nEn este caso comenzamos con 5 vecinos como en el caso de Isomap. Obtenemos las solución y la representamos gráficamente.\n\nlle_sales = LLE(as.matrix(X_sales), dim = 2, k = 5)\n# tibble de coordenadas\nmds_plot = as.tibble(lle_sales)\ncolnames(mds_plot) = c(\"Dim1\", \"Dim2\")\n# Gráfico MDS\nggplot(mds_plot, aes(Dim1, Dim2)) + \n  geom_point()\n\n\n\n\nLa solución agrupa conjuntos de muestras en una misma posición (por lo que no vemos una nube de puntos completa), lo cual dificulta la interpretación de la solución."
  },
  {
    "objectID": "170_MDSmodels.html#sec-170.4",
    "href": "170_MDSmodels.html#sec-170.4",
    "title": "17  Métodos de escalado multidimensional (MDS)",
    "section": "17.4 Ejercicios",
    "text": "17.4 Ejercicios\n\nAjustar un modelo de aprendizaje automático basado en escalas multidimensionales para el banco de datos Iris4.3.3.\nAjustar un modelo de aprendizaje automático basado en escalas multidimensionales para el banco de datos Wine quality4.3.8.\nAjustar un modelo de aprendizaje automático basado en escalas multidimensionales para el banco de datos WGene expression breast cancer4.3.10.\nAjustar un modelo de aprendizaje automático basado en escalas multidimensionales para el banco de datos QSAR4.2.8.\nAjustar un modelo de aprendizaje automático basado en escalas multidimensionales para el banco de datos Meat spec4.2.5."
  },
  {
    "objectID": "180_Clustermodels.html#sec-180.1",
    "href": "180_Clustermodels.html#sec-180.1",
    "title": "18  Análisis cluster",
    "section": "18.1 Definición de métricas",
    "text": "18.1 Definición de métricas\nPara proceder con cualquier proceso de agrupación es necesario establecer el concepto de métrica (divergencia o similitud) para un conjunto de muestras sobre las que se ha medido un conjunto de \\(k\\) características. El uso de un tipo de métrica u otro dependerá del tipo de características medidas sobre las muestras. En primer lugar definimos los conceptos de distancia y/o similitud y posteriormente veremos los tipos más habituales.\n\n18.1.1 Distancia\nUna función \\(d\\) se denomina distancia si para tres muestras \\(x\\), \\(y\\), \\(z\\) se verifica que:\n\n\\(d(x,y) \\geq 0\\)\n\\(d(x,y) = 0 \\Leftrightarrow x=y\\)\n\\(d(x,y)=d(y,x)\\)\n\\(d(x,z) \\leq d(x,y) + d(y,z)\\)\n\nLas métricas de distancia se utilizan principalmente para medir la cercanía o lejanía entre muestras que se han medido sobre características de tipo numérico.\n\n18.1.1.1 Tipos de distancias\nA continuación se presentan las medidas de distancia más habituales. En adelante consideramos dos muestras \\(x=(x_1,...,x_k)\\) e \\(y=(y_1,...,y_k)\\) medidas sobre \\(k\\) características.\nDistancia euclídea o distancia l2. La distancia euclídea entre dos muestras se define como la longitud del segmento que une ambos puntos, es decir,\n\\[d(x,y) = \\sqrt{\\sum_{i=1}^k (x_i-y_i)^2}\\]\nDistancia de Minkowski. La distancia de Minkowski de orden \\(q\\) se define como\n\\[d(x,y) = \\left(\\sum_{i=1}^k |x_i-y_i|^q\\right)^{1/q}\\]\nDistancia de Manhattan (ciudad) o distancia l1. La distancia de Manhattan se define como\n\\[d(x,y) = \\sum_{i=1}^k |x_i-y_i|\\]\nDistancia de Tchebychev. La distancia de Tchebychev se define como\n\\[d(x,y) = \\underset{i}{max} |x_i-y_i|\\]\nDistancia de Mahalanobis. La distancia de Mahalanobis se define como\n\\[d(x,y) = \\sqrt{(x-y)^t S^{-1}(x-y)}, \\] con \\(S\\) la matriz de varianzas-covarianzas entre \\(x\\) e \\(y\\).\nDistancia correlación. Se define la distancia correlación, a partir de la correlación entre \\(x\\) e \\(y\\) (\\(\\rho_{xy}\\)), como\n\\[d(x,y)=1-\\rho_{xy}\\] ### Similitud {#sec-180.1.2}\nUna función \\(s\\) se denomina similitud si para tres muestras \\(x\\), \\(y\\), \\(z\\) y un valor real finito arbitrario \\(s_0\\) se verifica que:\n\n\\(s(x,y) \\leq s_0\\)\n\\(s(x,x) = s_0\\)\n\\(s(x,y) = s(y,x)\\)\n\n\n\n18.1.1.2 Tipos de similitudes\nLas métricas de similitud se utilizan principalmente para establecer la cercanía o lejanía entre muestras que se han medido sobre características de tipo binario codificadas como 0-1. Dadas dos muestras de este tipo tenemos la tabla de asociación:\n\n\n\nx\n1\n0\nTotal\n\n\n\n\n1\na\nb\na+b\n\n\n0\nc\nd\nc+d\n\n\nTotal\na+c\nb+d\nt=a+b+c+d\n\n\n\nCoeficiente de adecuación simple (simple matching coefficient). El coeficiente de adecuación simple se define como\n\\[SMC = \\frac{a+d}{m}\\]\nÍndice de Jaccard. El índice de Jaccard se define como\n\\[J=\\frac{a}{c+b+a}\\]"
  },
  {
    "objectID": "180_Clustermodels.html#sec-180.2",
    "href": "180_Clustermodels.html#sec-180.2",
    "title": "18  Análisis cluster",
    "section": "18.2 Modelos de agrupación jerárquica",
    "text": "18.2 Modelos de agrupación jerárquica\nEste procedimiento intenta identificar grupos relativamente homogéneos de casos (o de variables) basándose en las características seleccionadas, mediante un algoritmo que comienza con cada caso (o cada variable) en un cluster diferente y combina los clusters hasta que sólo queda uno. Es posible analizar las variables brutas pero en el caso de variables de tipo numérico se suelen estandarizar.\nLos pasos a seguir con este algoritmo son:\n\nElegir distancia o similitud entre muestras y la elección del modelo de agrupación o distancia entre los diferentes clusters que se van formando.\nCalcular la distancia o similitud elegida entre cada par de muestras.\nComenzar el proceso de agrupación utilizando el modelo de agrupación o de distancia entre grupos prefijado hasta que todas las muestras se encuentren en un único grupo o cluster.\n\nEn primer lugar se presentan los procesos de agrupación de clusters más habituales.\n\n18.2.1 Distancias entre grupos\nAunque existe una gran variedad de definiciones de distancia entre grupos, aquí se presentan los más habituales. En adelante consideramos \\(A\\) y \\(B\\) dos grupos o clusters.\nVecino más cercano (Single linkage): se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (minimal intercluster dissimilarity). Tenemos entonces que:\n\\[d(A,B) = \\underset{i\\in A;j\\in B}{min} d(i,j)\\]\nEste modelo proporciona clusters más alargados.\nVecino más lejano (Complete linkage): se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (minimal intercluster dissimilarity). Tenemos entonces que:\n\\[d(A,B) = \\underset{i\\in A;j\\in B}{max} d(i,j)\\]\nEste modelo proporciona clusters más esféricos.\nPromedio de grupo (Average linkage): se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters (mean intercluster dissimilarity). Si \\(n_A\\) y \\(n_B\\) son el número de muestras en los clusters \\(A\\) y \\(B\\) respectivamente, tenemos que:\n\\[d(A,B) = \\frac{1}{n_a n_b}\\sum_{i\\in A;j\\in B} d(i,j)\\]\nEste modelo proporciona clusters más robustos.\nCentroide (Centroid linkage): se calcula el centroide de cada uno de los clusters y se selecciona la distancia entre ellos como la distancia entre los dos clusters. Si \\(\\bar{x}_A\\) y \\(\\bar{x}_B\\) son el vector promedio de las muestras en los clusters \\(A\\) y \\(B\\) respectivamente, tenemos que:\n\\[d(A,B) = d(\\bar{x}_A,\\bar{x}_B)\\]\nEste modelo proporciona clusters más robustos.\nWard (Ward linkage): se trata de un método general. La selección del par de clusters que se combinan en cada paso del agglomerative hierarchical clustering se basa en el valor óptimo de una función objetivo, pudiendo ser esta última cualquier función definida por el analista. El método Ward’s minimum variance es un caso particular en el que el objetivo es minimizar la suma total de varianza intracluster. En cada paso, se identifican aquellos 2 clusters cuya fusión conlleva menor incremento de la varianza total intracluster.\n\n\n18.2.2 Análisis de la agrupación\nEn un modelo de cluster jerárquico se pueden llegar a soluciones muy diferentes debido a la elección de la distancia entre pares de muestras y la distancia entre grupos. Para analizar las posibles soluciones existen diferentes herramientas gráficas y númericas que pasamos a analizar.\n\n18.2.2.1 Métodos gráficos\nLos métodos gráficos habituales son el dendograma y el scree plot.\n\nDendograma\nEl dendograma (o árbol invertido) es un diagrama de árbol que se utiliza para representar de forma gráfica la estructura de los grupos que se forman al aplicar un algoritmo de agrupamiento jerárquico y sus niveles de similitud. Cuanto más próximos más similares serán. Se puede representar de forma ascendente o descendente, pero siempre contiene la misma información.\nSu estructura consiste en un conjunto de ramas que se extienden desde un eje vertical y se ramifican hacia arriba. Los objetos se representan en las hojas de las ramas y los grupos en los nodos interiores de estas. La longitud de las ramas se utiliza para representar la distancia o similitud entre los objetos o grupos que conectan.\n\n\n\n\n\nEn el dendograma observamos la agrupación de los mamíferos según la dieta que siguen. Como hemos comentado anteriormente no indicamos a priori el número de grupos que se deben crear, por lo que dependerá del valor que escojamos.\nLos grupos se representan mediante líneas horizontales y las alturas representan distancias, de manera que la altura a la que se unen dos grupos es la distancia entre ellos. Por ejemplo, foca y delfín (seal y dolphin) son diferentes del resto, formando un grupo propio que no se ha unido al resto de animales hasta el último paso. Si cortamos con una línea horizontal a la altura 20, podemos observar que se forman tres grupos muy claros.\n\n\nGráfico scree\nEl gráfico scree se utiliza habitualmente para determinar el número de clusters óptimo mediante la representación de la distancia de agrupación versus el número de clusters considerados. Se considera que el número óptimo de clusters se alcanza cuando la curva obtenida se estabiliza sobre cierto valor. Sin embargo, este procedimiento resulta muy costoso computacionalmente si el número de muestras con el que trabajamos es muy alto.\nPodemos utilizar versiones de este gráfico con los criterios numéricos que vemos a continuación prefijando el número máximo de grupos a considerar.\n\n\n\n18.2.2.2 Métodos numéricos\nComo se analizó anteriormente, las tareas no supervisadas no tienen datos reales con los que comparar en la evaluación del modelo. Sin embargo, aún podemos medir la calidad de las asignaciones de conglomerados cuantificando qué tan estrechamente están relacionados los objetos dentro del mismo conglomerado (cohesión de conglomerados), así como qué tan distintos son los diferentes conglomerados entre sí (separación de conglomerados).\nDos medidas comunes son la medida de suma interna de cuadrados o suma de cuadrado intra grupos (WSS) y el coeficiente de silueta. WSS calcula la suma de diferencias al cuadrado entre observaciones y centroides, que es una cuantificación de la cohesión de los conglomerados (los valores más pequeños indican que los conglomerados son más compactos). El coeficiente de silueta cuantifica qué tan bien pertenece cada punto a su grupo asignado en comparación con los grupos vecinos, donde las puntuaciones más cercanas a 1 indican que están bien agrupados y las puntuaciones más cercanas a -1 indican que están mal agrupados."
  },
  {
    "objectID": "180_Clustermodels.html#sec-180.3",
    "href": "180_Clustermodels.html#sec-180.3",
    "title": "18  Análisis cluster",
    "section": "18.3 Modelos de agrupación partitivos",
    "text": "18.3 Modelos de agrupación partitivos\nLos algoritmos partitivos se diferencian principalmente de los modelos jerárquicos en que a priori se debe establecer el número de grupos (k) que se desean obtener. En muchas aplicaciones prácticas estos algoritmos de agrupación se utilizan como los modelos supervisados de clasificación.\nPara comenzar con este tipo de algoritmos necesitamos especificar los centroides que califican los k centroides iniciales. Un centroide es un punto que identifica a un cluster o grupo. Generalmente, se trata de un conjunto de puntos, en el que cada uno representa a un grupo de los que se han formado. Los resultados finales que se obtengan dependerán de la forma en la que se seleccionen inicialmente estos centroides.\nLo más usual es elegir un conjunto aleatorio de k puntos y asignarlos como centroides y cada paso que avance el algoritmo irá mejorando la precisión. La elección de estos puede afectar al rendimiento del algoritmo pero nunca afectará a su convergencia. A continuación presentamos los modelos partitivos más habituales.\n\n18.3.1 Algoritmo de k-medias\nEl algoritmo K-Means o K-Medias propuesto por Lloyd pretende partir un conjunto de N registros u observaciones en k grupos, de forma que su distancia al centroide de cada grupo sea mínima (o la similitud con respecto al centroide sea máxima). Los pasos del algoritmo son:\n\nPaso 1. Se seleccionan aleatoriamente k centroides (\\(C_i, i = 1,...,k\\)).\nPaso 2. Se asigna cada uno de los restantes N puntos al centroide \\(C_i\\) más cercano, utilizando como criterio que un punto es asignado al grupo \\(i\\) si la distancia al cuadrado del cada punto al centroide \\(C_i\\) es la menor de todas las obtenidas con respecto al resto de centroides.\nPaso 3. Recalcular los centroides a partir de los puntos asignados en cada uno de los grupos o cluster.\nPaso 4. Repetir los puntos 2 y 3 hasta que los grupos no cambien o se supere una tolerancia de usuario o un número máximo de iteraciones.\n\nPodemos ver el funcionamiento del algoritmo mediante la imagen siguiente:\n\n\n\n\n\nLa principal dificultad con este algoritmo es el establecimiento del número de clusters. Por ese motivo se suele usar en conjunto de datos conde tenemos un target categórico que nos indica el número de grupos que debemos considerar.\n\n\n18.3.2 Minibatch k-medias\nEl algoritmo de agrupación en clusters minibatch k-means es una versión del algoritmo estándar de k-medias que se puede usar en sustitución de este cuando se tratan grandes conjuntos de datos. Con el objetivo de reducir la dificultad computacional utiliza lotes o conjuntos de datos pequeños, aleatorios y de tamaño fijo para almacenarlos en la memoria y luego, con cada iteración, se recopila una muestra aleatoria de los datos y se usa para actualizar los clusters. A la vez que usa mini-lotes para reducir el tiempo de cálculo, intenta optimizar la función objetivo.\n\n\n18.3.3 Algoritmo DBSCAN\nEste es un algoritmo que se utiliza para identificar grupos (también conocidos como regiones densas), en los que no requiere que el usuario especifique el número de grupos a priori. Los grupos de puntos se asignan donde hay altas densidades y se calcula qué tan cercanos deben ser los puntos para que se consideren un miembro del grupo.\nPara identificar los grupos de una base de datos usando DBSCAN, el algoritmo realiza los siguientes pasos:\n\nSelecciona un punto al azar del conjunto de datos y recupera todos los puntos dentro de una distancia Epsilon desde ese punto.\nSi el número de puntos recuperados en el paso 1 es mayor o igual a MinPts, marca todos estos puntos como pertenecientes al mismo grupo y repite el proceso con la finalidad de identificar puntos adicionales que pertenecen al grupo.\nSi por el contrario, el número de puntos recuperados en el paso 1 es inferior a MinPts, el punto se marca como ruido.\nRepite los pasos 1, 2 y 3 hasta que se hayan procesado todos los puntos del conjunto de datos.\n\nLas instancias principales son aquellas que se encuentran en regiones densas. Todas las que estén en la vecindad de una instancia principal pertenecen al mismo grupo. Cualquier instancia que no sea una instancia central y no tenga una en su vecindad se considera una anomalía.\nEl algoritmo funciona bien si todos los clusters son lo suficientemente densos y si están bien separados por regiones de baja densidad. Es particularmente útil para encontrar grupos de forma arbitraria en grandes conjuntos de datos y es capaz de identificar grupos incluso en presencia de ruido o valores atípicos. Sin embargo, no es determinista y es sensible a la elección de Epsilon y MinPts. Estos parámetros deben elegirse cuidadosamente para obtener buenos resultados.\nDe esta forma el algoritmo DBSCAN permite identificar tres tipos de puntos:\n\nPunto núcleo (Core point): es aquel en el que al menos tiene minPts número de puntos (incluido el propio punto) en su región circundante dentro del radio eps.\nPunto frontera (Border point): es aquel en el que es alcanzable desde un punto núcleo y hay menos de minPts número de puntos dentro de su región circundante.\nValor atípico (Outlier): no es un punto central ni es accesible desde ningún punto central.\n\nEn la imagen siguiente se muestra el funcionamiento del algoritmo para dos grupos y con minPts igual a cuatro.\n\n\n\n\n\n\n\n18.3.4 Análisis de la agrupación\nEn todos los algoritmos partitivos el análisis de la agrupación obtenida se realiza con los mismos criterios que para los clusters jerárquicos."
  },
  {
    "objectID": "180_Clustermodels.html#sec-180.4",
    "href": "180_Clustermodels.html#sec-180.4",
    "title": "18  Análisis cluster",
    "section": "18.4 Análisis cluster en R",
    "text": "18.4 Análisis cluster en R\nPara llevar a cabo el análisis cluster vamos a utilizar diferentes modelos de aprendizaje que están definidos en mlr3. Son los más habituales pero hay muchos más definidos que pueden ser explorados en toro momento. Todos ellos se encuentran dentro de las librerías que hemos visto y en la mlr3cluster que cargaremos en este caso. En concreto nos centramos en los modelos:\n\nclust.hclust para cluster jerárquicos, cuyos parámetros principales son method para establecer el método de agrupación de los clusters, y distmethod para definir el tipo de distancia entre las muestras. Se pueden consultar las opciones disponibles en este enlace.\nclust.kmeans para el algoritmo de k-medias cuyos parámetros principales son centers que establece el número de clusters, iter.max que fija el número de iteraciones del método, y algorithm que establece el algoritmo utilizado en el proceso iterativo. Se pueden consultar las opciones disponibles en este enlace.\nclust.MBatchKMeans para el algoritmo minibatch k-means, cuyos parámetros principales son clusters para identificar el número de clusters considerados, batch_size para fijar el tamaño de cada uno de los batch, y max_iters que fija el número máximo de iteraciones del algoritmo. Este modelo utiliza la función MiniBatchKmeans() que se encuentra alojada en la librería ClusterR. Se pueden consultar las opciones disponibles en este enlace.\nclust.dbscan para el método dbscan, cuyos parámetros principales son eps para la tolerancia, y minPts para definir el número de puntos. Este modelo utiliza la función dbscan() que se encuentra alojada en la librería dbscan. Se pueden consultar las opciones disponibles en este enlace.\n\nA continuación cargamos todas las librerías necesarias para los análisis.\n\n# Paquetes anteriores\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(knitr) # para formatos de tablas\nlibrary(skimr)\nlibrary(DataExplorer)\nlibrary(GGally)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(cvms)\nlibrary(kknn)\nlibrary(rpart.plot)\nlibrary(rda)\nlibrary(klaR)\nlibrary(ggord)\ntheme_set(theme_sjplot2())\n\n# Paquetes AA\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3tuningspaces)\nlibrary(mlr3cluster)\nlibrary(cluster)\nlibrary(ClusterR)\nlibrary(dbscan)\nlibrary(gbm)\nlibrary(RWeka)\nlibrary(xgboost)\nlibrary(lightgbm)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(rsvd)\nlibrary(kernlab)\nlibrary(fastICA)\n\n\n18.4.1 Bancos de datos\nPara ejemplificar el uso de los modelos de cluster vamos a utilizar tres bancos de datos: Gene expression leukemia, Vehicle silhouettes, y Sales. Mostramos el uso de los modelos como herramienta individual y más tarde como acompañamiento de un modelo de componentes principales. A continuación presentamos los tres bancos de datos y la definición de la tarea de agrupación cada uno de ellos.\n\n18.4.1.1 Gene expression leukemia\nEste banco de datos lo hemos descrito en temas anteriores. Cargamos los datos, y generamos la tarea dejando fuera la característica que identifica el tipo de tumor.\n\n# Leemos los datos\ngeneexpleu = read_rds(\"geneexpressionleukemia.rds\")\n# Eliminamos el indicador de la muestra\ngeneexpleu = geneexpleu %>% dplyr::select(-samples)\n# Consideramos matriz de características y tipos\nX_genes = geneexpleu %>% dplyr::select(-type)\ny_genes = geneexpleu[,\"type\"]\n# Debemos cambiar los nombres pq no se respeta la convención de R ya que todas ellas empiezan por un número\nnames(X_genes) = str_replace_all(names(X_genes),\"[/_-]\",\"\")\nnames(X_genes) = paste(\"V\", names(X_genes), sep=\"_\")\n# Definimos la tarea de cluster \ntsk_genes = as_task_clust(X_genes)\n\n\n\n18.4.1.2 Vehicle silhouettes\nEste conjunto de datos recoge información de cuatro tipos diferentes de vehículos, utilizando un conjunto de características extraídas de su silueta. Para el experimento se utilizaron cuatro vehículos modelo “Corgie”: un bus de dos pisos, una camioneta Cheverolet, un Saab 9000 y un Opel Manta 400. El objetivo del estudio es clasificar una silueta dada como uno de cuatro tipos diferentes de vehículos. Todos los atributos son numéricos discretos salvo la última variable que registra el tipo de vehículo. No existen valores perdidos.\n\n# Leemos los datos\nvehicle = read_rds(\"vehicle.rds\")\n# Consideramos matriz de características y tipos\nX_vehicle = vehicle %>% dplyr::select(-Class)\ny_vehicle = vehicle[,\"Class\"]\n# Definimos la tarea de cluster cambiando los nombres\ntsk_vehicle = as_task_clust(X_vehicle)\n\n\n\n18.4.1.3 Sales\nContiene las cantidades compradas semanalmente de 800 productos a lo largo de 52 semanas. Todos los atributos son numéricos sin valores perdidos y van identificados mediante W(número de la semana). En este caso no tenemos posible variable target.\n\n# Leemos los datos\nsales = read_rds(\"sales.rds\")\n# Eliminamos la variable que identifica el código del producto y la almacenamos por si hiciera falta\nX_sales = sales %>% dplyr::select(-Product_Code)\ny_sales = sales[,\"Product_Code\"]\n# Definimos la tarea de cluster \ntsk_sales = as_task_clust(X_sales)\n\n\n\n\n18.4.2 Modelos de agrupación\nEn primer lugar comenzamos presentando los modelos de agrupación puros, es decir a partir de la variables originales, y en el punto siguiente veremos que ocurre cuando utilizamos los modelos de cluster como resultado del preprocesado mediante componentes principales. Cuando sea posible compararemos la solución del cluster con el target establecido en la base de datos.\n\n18.4.2.1 Gene expression leukemia\n\nModelo de agrupación jerárquica\nComenzamos con el análisis de cluster jerárquico utilizando el método de Ward que proporciona buenas soluciones en muchas situaciones prácticas, y la distancia euclídea. En este caso consideramos todos los datos para el análisis. En primer lugar establecemos el modelo de aprendizaje, y lo entrenamos con todos los datos disponibles. Esto nos permitirá realizar los gráficos por defecto para estos modelos de aprendizaje con la función autoplot. En la tarea de preprocesamiento únicamente escalamos las variables para reducir la variabilidad original.\n\n# Preprocesamiento\npp_genes =  \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\ngenes_lrn1 = as_learner(pp_genes %>>% lrn1)\n# Entrenamiento del modelo\ngenes_lrn1$train(tsk_genes)\n# Valoración del modelo\npr1 = genes_lrn1$train(tsk_genes)$predict(tsk_genes)\n# Valores de scores\npr1$score(msr(\"clust.silhouette\"), task = tsk_genes)\n\nclust.silhouette \n       0.1256943 \n\n\nEl valor del score es muy bajo indicando que el modelo establecido no parce muy adecuado. Obtenemos el dendograma y el gráfico scree asociado con el modelo obtenido.\n\nmodelo = genes_lrn1$model$clust.hclust$model\nplot(as.dendrogram(modelo))\n\n\n\n\nEn el dendograma obtenido se aprecian muchos grupos pero poco homogéneos. De hecho no queda muy clara cual sería la solución más correcta. Vamos a representar sobre el dendograma la solución con 5 grupos.\n\nplot(as.dendrogram(modelo))\nrect.hclust(modelo, k = 5, border = 1:5)\n\n\n\n\nUna vez fijada una solución el siguiente paso es la caracterización de los clusters obtenidos a partir de las características consideradas para su construcción. Sin embargo, cuando el número de características es muy elevado resulta imposible dicha tarea. Por ese motivo se recurre a un análisis de reducción de la dimensión antes del análisis cluster, ya que entonces si resulta posible caracterizar los clusters obtenidos.\nOtra opción como en este caso es comparar la solución cluster con el target original y analizar la matriz de confusión, para entender como se agrupan las muestras y si los grupos se corresponden con la información original. En este caso tomamos la solución con cinco clusters (ya que el target tenía cinco tipos de tumor) y comparamos los resultados.\n\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\", k = 5)\ngenes_lrn1 = as_learner(pp_genes %>>% lrn1)\n# Valores de predicción\npr1 = genes_lrn1$train(tsk_genes)$predict(tsk_genes)\n# Generamos la tabla de comparación\ntable(y_genes$type, pr1$partition)\n\n                  \n                    1  2  3  4  5\n  AML               2 16  0  8  0\n  Bone_Marrow       0  0 10  0  0\n  Bone_Marrow_CD34  7  1  0  0  0\n  PB                0  0  0  0 10\n  PBSC_CD34        10  0  0  0  0\n\n\nPodemos ver como los clusters 2 a 4 se caracterizan por contener sujetos con un tipo de leucemia específica, mientras que el cluster es una combinación de varios de ellos. Podríamos establecer la equivalencia:\n\nCluster 1: PBSC_CD34\nCluster 2: AML\nCluster 3: Bone_Marrow\nCluster 4: AML\nCluster 5: PB\n\nNo queda claro donde agregar el tipo Bone_Marrow_CD34, y aparecen dos grupos con tumores AML mayoritariamente. No parece que la solución obtenida permita clasificar adecuadamente los tipos de tumor, y queda claro que el tipo AML es el más difícil de aislar de forma independiente.\n\n\nModelo de k-medias\nUtilizamos ahora el algoritmo de k-medias fijando k igual a 5, el número de grupos que deseamos encontrar, para que coincida con el número de niveles del target. Definimos el modelo de aprendizaje, utilizando el mismo preprocesado que con el algoritmo anterior:\n\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.kmeans\", centers =  5)\ngenes_lrn1 = as_learner(pp_genes %>>% lrn1)\n# Entrenamiento del modelo\ngenes_lrn1$train(tsk_genes)\n\nPodemos analizar la solución obtenida identificando el cluster al que es asignado cada elemento de la muestra y comparando lo resultados con los datos originales.\n\n# Asignación del modelo\ngenes_lrn1$model$clust.kmeans$model$cluster\n\n [1] 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2 4 3 1 1 1 3 4 1 1 3 3 2 1 1 1 3 1 1 1\n[39] 4 2 4 1 1 2 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2\n\n# Tamaño de cada uno de los grupos\ngenes_lrn1$model$clust.kmeans$model$size\n\n[1] 14 14 22  4 10\n\n\nVeamos ahora como se corresponde la clasificación del algoritmo con los valores originales:\n\n# Generamos la tabla de comparación\ntable(y_genes$type, genes_lrn1$model$clust.kmeans$model$cluster)\n\n                  \n                    1  2  3  4  5\n  AML              13  4  5  4  0\n  Bone_Marrow       0  0 10  0  0\n  Bone_Marrow_CD34  1  0  7  0  0\n  PB                0  0  0  0 10\n  PBSC_CD34         0 10  0  0  0\n\n\nEl modelo se comporta de forma similar la jerárquico, incluso empeora la asignación a los grupos. Para verificar el mal comportamiento podemos valorar los scores asociados a este modelo:\n\n# Predicción para cada modelo\npr1 = genes_lrn1$train(tsk_genes)$predict(tsk_genes)\n# Valores de scores\npr1$score(msr(\"clust.silhouette\"), task = tsk_genes)\n\nclust.silhouette \n       0.1994495 \n\n\nEl resultado obtenido mejora los del modelo jerárquico a pesar de que la clasificación obtenida es menos clara. En este caso tenemos dos posibilidades de mejora. La primera consiste en determinar el número óptimo de grupos a considerar mediante una búsqueda automática, mientras que la segunda pasa por realizar el algoritmo de k-medias después del preprocesado mediante componentes principales. Esta segunda la exploraremos un poco más adelante en este mismo tema.\n\n\n\n18.4.2.2 Vehicle silhouettes\nRealizamos ahora el análisis para el banco de datos de vehículos.\n\nModelo de agrupación jerárquica\nDe nuevo utilizamos el método de Ward para obtener la solución jerárquica. De nuevo escalamos los valores de las características\n\n# Preprocesamiento\npp_vehicle =  \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\nvehicle_lrn1 = as_learner(pp_vehicle %>>% lrn1)\n# Entrenamiento del modelo\nvehicle_lrn1$train(tsk_vehicle)\n# Valoración del modelo\npr1 = vehicle_lrn1$train(tsk_vehicle)$predict(tsk_vehicle)\npr1$score(msr(\"clust.silhouette\"), task = tsk_vehicle)\n\nclust.silhouette \n        0.641126 \n\n\nEl score es bastante alto mostrando que el modelo de cluster obtenido parece bastante adecuado. Veamos el dendograma correspondiente:\n\nmodelo = vehicle_lrn1$model$clust.hclust$model\nplot(as.dendrogram(modelo))\n\n\n\n\nParecen apreciarse claramente una solución con tres grupos, pero para poder realizar las comparaciones necesarias con el target original, optamos por una solución con cuatro grupos que representamos gráficamente.\n\nplot(as.dendrogram(modelo))\nrect.hclust(modelo, k = 4, border = 1:4)\n\n\n\n\nA la vista del dendograma una solución con 4 grupos parece bastante buena para definir grupos homogéneos. Comparamos la solución del modelo con el target original.\n\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\", k = 4)\nvehicle_lrn1 = as_learner(pp_vehicle %>>% lrn1)\n# Valores de predicción\npr1 = vehicle_lrn1$train(tsk_vehicle)$predict(tsk_vehicle)\n# Generamos la tabla de comparación\ntable(y_vehicle, pr1$partition)\n\n         \ny_vehicle   1   2   3   4\n     bus   93  25  58  42\n     opel  41  78  53  40\n     saab  40  75  59  43\n     van  112   0  81   6\n\n\nLa solución obtenida es bastante mala porque no resulta posible identificar cada grupo con un tipo de vehículo ya que los datos están muy repartidos. Esto puede ser debido a que las variables no poseen suficiente información para clasificar las observaciones o que las utilizarlas directamente las distancia obtenidas no nos permiten realizar agrupaciones adecuadas. Como en el ejemplo anterior un preprocesado mediante CP podría resolver en parte este problema. Veamos que ocurre si utilizamos el algoritmo de k-medias.\n\n\nModelo de k-medias\nUtilizamos ahora el algoritmo de k-medias fijando k igual a 4.\n\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.kmeans\", centers =  4)\nvehicle_lrn1 = as_learner(pp_vehicle %>>% lrn1)\n# Entrenamiento del modelo\nvehicle_lrn1$train(tsk_vehicle)\n\nIdentificamos clusters y comparamos los resultados con los datos originales.\n\n# Asignación del modelo\nvehicle_lrn1$model$clust.kmeans$model$cluster\n\n  [1] 2 2 1 2 3 1 2 2 2 2 2 2 2 2 2 1 4 2 1 1 4 4 2 2 1 2 4 1 1 4 2 2 2 1 2 2 4\n [38] 3 1 4 1 4 4 2 1 4 4 4 4 2 4 2 1 2 1 2 2 4 1 4 1 4 4 4 2 4 4 1 2 1 1 1 2 4\n [75] 2 1 2 4 1 4 4 1 2 4 2 1 2 4 2 4 1 2 1 2 4 1 4 4 1 4 3 2 2 4 1 1 1 4 4 1 2\n[112] 2 4 4 4 2 1 1 4 2 4 4 2 4 4 4 4 4 2 1 1 2 2 4 1 3 4 2 4 2 2 4 1 4 2 1 2 2\n[149] 2 2 1 2 2 1 2 1 2 4 2 2 4 1 2 2 1 1 2 1 4 4 1 1 2 1 2 2 2 2 2 4 1 4 2 4 1\n[186] 2 2 2 1 2 1 2 2 1 2 4 1 4 4 4 2 2 1 1 2 2 2 4 4 1 2 2 2 1 4 2 4 1 4 2 1 4\n[223] 1 4 4 2 1 2 1 4 4 4 4 1 2 4 2 4 1 4 2 2 4 1 4 4 2 2 1 4 4 1 4 2 2 1 2 2 1\n[260] 1 4 2 2 2 1 4 4 2 2 4 4 2 2 2 1 2 4 4 1 2 2 4 4 1 4 2 2 4 1 4 2 3 2 2 1 2\n[297] 1 4 2 2 1 2 2 2 4 2 1 1 1 1 1 4 2 1 4 4 4 2 4 1 1 1 4 1 2 4 1 2 2 2 2 1 1\n[334] 4 1 1 4 1 2 2 2 4 4 1 1 1 1 2 2 2 1 4 2 4 1 2 2 1 2 1 1 1 2 2 4 1 4 4 4 2\n[371] 2 2 2 2 4 1 1 4 4 1 4 1 4 1 2 4 2 4 3 1 4 2 2 2 1 2 1 2 2 2 1 2 1 2 1 2 4\n[408] 4 2 2 2 4 4 2 4 1 2 2 4 2 4 1 2 4 2 2 1 2 1 2 1 1 4 4 1 2 4 4 2 1 1 4 2 1\n[445] 1 4 1 1 1 2 2 2 2 2 1 4 4 2 1 2 2 1 2 4 1 4 4 1 1 2 4 1 1 1 4 1 1 4 2 4 1\n[482] 1 2 2 4 4 1 2 4 1 1 2 4 1 1 2 1 4 4 1 1 1 4 4 1 1 1 2 2 1 4 2 1 2 4 4 2 1\n[519] 4 2 2 4 2 3 1 2 1 1 2 4 2 1 1 4 4 2 1 2 1 1 2 2 2 2 4 4 4 2 2 1 4 4 2 4 1\n[556] 2 1 4 4 1 1 2 1 2 2 2 1 2 4 2 1 2 2 4 1 1 1 1 2 4 4 4 1 1 1 2 1 4 2 1 4 4\n[593] 4 2 4 1 2 2 2 2 2 2 1 2 2 1 2 2 2 4 1 4 4 2 4 2 2 4 4 1 1 4 2 1 2 1 2 2 1\n[630] 2 4 1 4 1 4 4 2 4 2 1 1 4 1 2 2 4 2 4 1 2 1 4 2 2 2 4 4 4 2 1 2 1 4 2 2 2\n[667] 2 1 2 4 1 2 1 2 2 1 4 1 4 2 2 2 4 1 2 4 2 1 4 1 2 2 1 4 2 4 2 2 4 2 1 1 2\n[704] 2 1 1 3 2 4 2 1 1 1 1 2 1 2 2 1 1 2 1 2 1 2 4 1 2 4 1 1 1 2 1 4 4 1 1 1 2\n[741] 1 2 2 1 2 4 2 4 2 1 2 4 2 2 2 4 1 4 4 4 1 1 4 1 1 4 2 2 1 2 4 1 1 4 2 2 1\n[778] 1 1 4 1 2 1 1 4 4 1 4 1 2 4 2 1 1 2 4 2 1 1 2 2 4 2 2 1 4 2 1 4 4 1 4 2 4\n[815] 4 4 2 1 1 2 4 1 2 1 1 4 2 1 4 4 2 2 1 4 4 1 4 2 2 2 2 2 2 1 2 4\n\n# Tamaño de cada uno de los grupos\nvehicle_lrn1$model$clust.kmeans$model$size\n\n[1] 268 324   8 246\n\n\nVeamos ahora como se corresponde la clasificación del algoritmo con los valores originales:\n\n# Generamos la tabla de comparación\ntable(y_vehicle, vehicle_lrn1$model$clust.kmeans$model$cluster)\n\n         \ny_vehicle   1   2   3   4\n     bus   50  80   2  86\n     opel 112  65   0  35\n     saab 106  73   0  38\n     van    0 106   6  87\n\n\nLa agrupación es similar e incluso un poco peor que la del modelo jerárquico como podemos comprobar al analizar el score correspondiente.\n\n# Predicción para cada modelo\npr1 = vehicle_lrn1$train(tsk_vehicle)$predict(tsk_vehicle)\n# Valores de scores\npr1$score(msr(\"clust.silhouette\"), task = tsk_vehicle)\n\nclust.silhouette \n       0.2467293 \n\n\nEl resultado con el algoritmo de k-medias es muy inferior al del modelo jerárquico, o que nos haría decantarnos por este último para este banco de datos.\n\n\n\n18.4.2.3 Sales\nFinalizamos estos primeros modelos con el banco de datos de ventas. Procedemos de la misma forma que en ejemplos anteriores salvo por el hecho de que ahora no tenemos target y no podemos comparar nuestros resultados.\n\nModelo de agrupación jerárquica\nDe nuevo utilizamos el método de Ward para obtener la solución jerárquica y escalamos los valores de las características.\n\n# Preprocesamiento\npp_sales =  \n   po(\"scale\", param_vals = list(center = TRUE, scale = TRUE))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\nsales_lrn1 = as_learner(pp_sales %>>% lrn1)\n# Entrenamiento del modelo\nsales_lrn1$train(tsk_sales)\n# Valoración del modelo\npr1 = sales_lrn1$train(tsk_sales)$predict(tsk_sales)\npr1$score(msr(\"clust.silhouette\"), task = tsk_sales)\n\nclust.silhouette \n       0.7478163 \n\n\nEl coeficientes es bastante alto indicando que el modelo es capaz de establecer grupos más o menos homogéneos. Veamos el dendograma correspondiente.\n\nmodelo = sales_lrn1$model$clust.hclust$model\nplot(as.dendrogram(modelo))\n\n\n\n\nEn este caso se podría optar por una solución con tres o cuatro clusters. En este caso vamos a seleccionar la solución con cuatro grupos.\n\nplot(as.dendrogram(modelo))\nrect.hclust(modelo, k = 4, border = 1:4)\n\n\n\n\nA la vista del dendograma una solución con 4 grupos parece bastante buena para definir grupos homogéneos. En este caso vamos a caracterizar los grupos en función de las ventas realizadas en cada una de las semanas. para ellos añadiremos el grupo de pertenencia de cada muestra la banco de datos original, y realizaremos un estudio descriptivo y gráfico de ellos.\n\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\", k = 4)\nsales_lrn1 = as_learner(pp_sales %>>% lrn1)\n# Valores de predicción\npr1 = sales_lrn1$train(tsk_sales)$predict(tsk_sales)\n# Añadimos columna  de asignación (en formato factor) a matriz de datos de ventas\nX_sales$grupo = as.factor(pr1$partition)\n\nUna vez tenemos el dataframe con los datos originales y el grupo asignado calculamos las medias de cada variable para cada uno de los grupos:\n\nresumen = X_sales %>% \n  group_by(grupo) %>% \n  summarise_all(mean)\nresumen\n\n# A tibble: 4 × 53\n  grupo     W0     W1     W2     W3     W4     W5     W6     W7     W8     W9\n  <fct>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 1     12.2   12.0   12.2   12.4   12.6   12.8   12.6   12.4   12.7   12.7  \n2 2      3.25   3.48   3.42   3.61   3.61   3.58   3.71   3.99   3.55   3.72 \n3 3     34.0   35.6   37.1   38.6   37.2   36.3   38.0   37.1   38.6   37.5  \n4 4      0.231  0.221  0.217  0.221  0.298  0.251  0.281  0.221  0.234  0.258\n# ℹ 42 more variables: W10 <dbl>, W11 <dbl>, W12 <dbl>, W13 <dbl>, W14 <dbl>,\n#   W15 <dbl>, W16 <dbl>, W17 <dbl>, W18 <dbl>, W19 <dbl>, W20 <dbl>,\n#   W21 <dbl>, W22 <dbl>, W23 <dbl>, W24 <dbl>, W25 <dbl>, W26 <dbl>,\n#   W27 <dbl>, W28 <dbl>, W29 <dbl>, W30 <dbl>, W31 <dbl>, W32 <dbl>,\n#   W33 <dbl>, W34 <dbl>, W35 <dbl>, W36 <dbl>, W37 <dbl>, W38 <dbl>,\n#   W39 <dbl>, W40 <dbl>, W41 <dbl>, W42 <dbl>, W43 <dbl>, W44 <dbl>,\n#   W45 <dbl>, W46 <dbl>, W47 <dbl>, W48 <dbl>, W49 <dbl>, W50 <dbl>, …\n\n\nEl problema es que a tabla es demasiado grande y por tanto resulta difícil extraer conclusiones. Recolocamos los datos para poder representarlos gráficamente. Hacemos uso de la función pivot_longer que nos permite reestructurar los datos en un formato que la función gráfica puede interpretar fácilmente. Para utilizar esta función debemos identificar los nombres de las variables que deseamos reorganizar.\n\n# Reorganizamos los datos para tener solo tres columnas: grupo, semana, y ventas\ndf = resumen %>% pivot_longer(cols = colnames(resumen[,2:ncol(resumen)]), names_to = \"W\", values_to = \"Sales\") \nggplot(df, aes(W, Sales, col = grupo)) + geom_point()\n\n\n\n\nEl gráfico de medias nos permite identificar claramente las características de cada uno de los grupos. Por ejemplo, el grupo 3 viene caracterizado por las mayores ventas en todas las semanas, mientras que el cuatro contiene las ventas semanales más bajas. En este caso no establecemos el algoritmo de k-medias ya que no tenemos información suficiente para establecer el número de clusters inicial. Podríamos plantear soluciones para diferentes valores de k y seleccionar aquel con el mejor score.\nUna vez hemos visto los modelos de cluster aplicados directamente sobre los bancos de datos originales, vamos a ver como utilizar el algoritmo de componentes principales para refinar nuestras agrupaciones y conseguir soluciones más eficientes y que nos permitan caracterizar los datos de forma más precisa.\n\n\n\n\n18.4.3 Modelos de agrupación con CP\nComenzaremos todos los análisis determinado el número óptimo de componentes. A continuación extraeremos las coordenadas de todas las muestras en dichas componentes y procederemos con el análisis cluster. Como criterio valoraremos el número de componentes de las soluciones donde alcanzamos el 50%, 70%, y 80% de variabilidad explicada. Una vez establecido dicho número utilizaremos el pipeop pca para realizar el preprocesado de datos mediante componentes principales.\n\n18.4.3.1 Gene expression leukemia\nComenzamos con el análisis de CP y la selección del número de componentes.\n\n# CP\nleukemia_cp = prcomp(X_genes, scale = TRUE)\n# Resumen numérico del análisis\nresumen_cp = get_eigenvalue(leukemia_cp)\n# Componentes para el 50% de VE\nsum(resumen_cp$cumulative.variance.percent <= 50) + 1\n\n[1] 5\n\n# Componentes para el 70% de VE\nsum(resumen_cp$cumulative.variance.percent <= 70) + 1\n\n[1] 13\n\n# Componentes para el 80% de VE\nsum(resumen_cp$cumulative.variance.percent <= 80) + 1\n\n[1] 22\n\n\nEn este caso hay una gran diferencia en el número de componentes entre todas soluciones, aunque dado el gran número de variables originales (22283) es hasta cierto punto el comportamiento esperado. Para ver la potencia del análisis empezamos con la solución con cuatro componentes. Analizamos únicamente la solución del modelo jerárquico.\nEstablecemos el modelo de aprendizaje utilizando el preprocesado adecuado, y obtenemos el score correspondiente para comparar con el modelo sin preprocesado de CP:\n\n# Preprocesado\npp_genes =  \n   po(\"pca\", param_vals = list(center = TRUE, scale. = TRUE, rank. = 5))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\ngenes_lrn1 = as_learner(pp_genes %>>% lrn1)\n# Entrenamiento del modelo\ngenes_lrn1$train(tsk_genes)\n# Valoración del modelo\npr1 = genes_lrn1$train(tsk_genes)$predict(tsk_genes)\n# Valores de scores\npr1$score(msr(\"clust.silhouette\"), task = tsk_genes)\n\nclust.silhouette \n       0.1256943 \n\n\nEl score ha mejorado algo con respecto al modelo sin CP pero todavía resulta muy bajo. Veamos que ocurre si cambiamos la solución con 13 componentes.\n\n# Preprocesado\npp_genes =  \n   po(\"pca\", param_vals = list(center = TRUE, scale. = TRUE, rank. = 13))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\ngenes_lrn1 = as_learner(pp_genes %>>% lrn1)\n# Entrenamiento del modelo\ngenes_lrn1$train(tsk_genes)\n# Valoración del modelo\npr1 = genes_lrn1$train(tsk_genes)$predict(tsk_genes)\n# Valores de scores\npr1$score(msr(\"clust.silhouette\"), task = tsk_genes)\n\nclust.silhouette \n       0.1205582 \n\n\nEl coeficiente empeora el resultado anterior. Veamos que ocurre en el último caso:\n\n# Preprocesado\npp_genes =  \n   po(\"pca\", param_vals = list(center = TRUE, scale. = TRUE, rank. = 22))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\ngenes_lrn1 = as_learner(pp_genes %>>% lrn1)\n# Entrenamiento del modelo\ngenes_lrn1$train(tsk_genes)\n# Valoración del modelo\npr1 = genes_lrn1$train(tsk_genes)$predict(tsk_genes)\n# Valores de scores\npr1$score(msr(\"clust.silhouette\"), task = tsk_genes)\n\nclust.silhouette \n       0.1256943 \n\n\nA la vista de los resultados parece que una solución con cinco componentes es suficiente. En primer lugar vemos el dendograma correspondiente:\n\n# Preprocesado\npp_genes =  \n   po(\"pca\", param_vals = list(center = TRUE, scale. = TRUE, rank. = 5))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\ngenes_lrn1 = as_learner(pp_genes %>>% lrn1)\n# Entrenamiento del modelo\ngenes_lrn1$train(tsk_genes)\n# Dendograma\nmodelo = genes_lrn1$model$clust.hclust$model\nplot(as.dendrogram(modelo))\n\n\n\n\nEn este caso no queda clara que una solución con cinco clusters parece acertada, a pesar de que el target tiene cinco grupos. Veamos como se corresponden los clusters obtenidos con dicha variable. Planteamos cinco grupos para mantener la equivalencia con el target original.\n\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\", k = 5)\ngenes_lrn1 = as_learner(pp_genes %>>% lrn1)\n# Valores de predicción\npr1 = genes_lrn1$train(tsk_genes)$predict(tsk_genes)\n# Generamos la tabla de comparación\ntable(y_genes$type, pr1$partition)\n\n                  \n                    1  2  3  4  5\n  AML               2 16  3  5  0\n  Bone_Marrow       0  0 10  0  0\n  Bone_Marrow_CD34  7  1  0  0  0\n  PB                0  0  0  0 10\n  PBSC_CD34        10  0  0  0  0\n\n\nEn este caso si ocurre que algunos cluster identifican mayoritariamente cada tipo de tumor. El más problemático es el cluster 1 con los tipos AML, Bone_Marrow_CD34, y PBSC_CD34. Vista la agrupación obtenida la caracterización de los clusters que nos puede aportar información muy relevante sobre las características de cada tipo de tumor. Extraemos las coordenadas en las 5 componentes y las unimos con la solución obtenida.\n\n# Obtención de coordenadas\nleukemia_cp = prcomp(X_genes, scale = TRUE)\ncoor_cp = as.data.frame(get_pca_ind(leukemia_cp)$coord[,1:5])\n# Fusión de datos\ncoor_cp$cluster = as.factor(pr1$partition)\ncolnames(coor_cp)[1:5] = c(\"D01\", \"D02\",\"D03\",\"D04\",\"D05\")\n# Gráfico descriptivo de componentes\ndf = coor_cp %>% pivot_longer(cols = colnames(coor_cp[,1:(ncol(coor_cp)-1)]), names_to = \"CP\", values_to = \"Coordenadas\") \nggplot(df, aes(cluster, Coordenadas)) + \n  geom_boxplot() +\n  facet_wrap(vars(CP), nrow = 2)\n\n\n\n\nComo era de esperar, por la misma construcción de las componentes, las mayores diferencias entre los clusters se observan en las primera dimensiones. Por ejemplo, en la primera componente podemos establecer el orden siguiente en función de los valores observados (en orden descendente): C5 > C2 > C3 > C4 > C1, lo que implica teniendo en cuenta la tabla de coincidencias entre clasificación y valores originales, que las leucemias del tipo PB se pueden identificar si buscamos los valores más altos en la primera componente. Por otro lado los valores más bajos en dicha componente se pueden asociar con las leucemias de los tipos PBSC_CD34, AML, y Bone_Marrow_CD34. Así mismo podemos ver que los valores más bajos en la componente 2 se corresponden con el grupo 5 (PB), y los más altos con los grupos 1 y 2 (AML, Bone_Marrow_CD34). Veamos estas conclusiones de forma gráfica:\n\n# Añadimos el tipo de leucemia a las coordenadas y el cluster.\ncoor_cp$type = y_genes$type\n# Gráfico\nggplot(coor_cp, aes(D01, D02, color = cluster, shape = type)) +\n  geom_point(size = 3) \n\n\n\n\nEn el gráfico se observa claramente que el cluster 5 esta asociado con PB y se sitúa sobre el cuadrante IV. En los otros cuatro grupos hay mezclas de diferentes tipos de leucemia, aunque geográficamente si se aprecia cierta separación entre los diferentes cluster.\n\n\n18.4.3.2 Vehicle silhouettes\nComenzamos con el análisis de CP y la selección del número de componentes.\n\n# CP\nvehicle_cp = prcomp(X_vehicle, scale = TRUE)\n# Resumen numérico del análisis\nresumen_cp = get_eigenvalue(vehicle_cp)\n# Componentes para el 50% de VE\nsum(resumen_cp$cumulative.variance.percent <= 50) + 1\n\n[1] 1\n\n# Componentes para el 70% de VE\nsum(resumen_cp$cumulative.variance.percent <= 70) + 1\n\n[1] 3\n\n# Componentes para el 80% de VE \nsum(resumen_cp$cumulative.variance.percent <= 80) + 1 \n\n[1] 4\n\n\nEn este caso seleccionamos cuatro componentes ya que no añadimos mucha complejidad (muchas componentes) pero si ganamos un 10% de variabilidad explicada.\nEstablecemos el modelo de aprendizaje utilizando el preprocesado adecuado, y obtenemos el score correspondiente para comparar con el modelo sin preprocesado de CP:\n\n# Preprocesado\npp_vehicle =  \n   po(\"pca\", param_vals = list(center = TRUE, scale. = TRUE, rank. = 4))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\nvehicle_lrn1 = as_learner(pp_vehicle %>>% lrn1)\n# Entrenamiento del modelo\nvehicle_lrn1$train(tsk_vehicle)\n# Valoración del modelo\npr1 = vehicle_lrn1$train(tsk_vehicle)$predict(tsk_vehicle)\n# Valores de scores\npr1$score(msr(\"clust.silhouette\"), task = tsk_vehicle)\n\nclust.silhouette \n       0.6651938 \n\n\nEl valor obtenido del índice es superior al del modelo jerárquico original. Veamos el dendograma asociado:\n\n# Preprocesado\npp_genes =  \n   po(\"pca\", param_vals = list(center = TRUE, scale. = TRUE, rank. = 4))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\nvehicle_lrn1 = as_learner(pp_vehicle %>>% lrn1)\n# Entrenamiento del modelo\nvehicle_lrn1$train(tsk_vehicle)\n# Dendograma\nmodelo = vehicle_lrn1$model$clust.hclust$model\nplot(as.dendrogram(modelo))\n\n\n\n\nComo en el caso anterior vamos a mantener la solución con cuatro clusters para poder comparar la solución con el target original.\n\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\", k = 4)\nvehicle_lrn1 = as_learner(pp_vehicle %>>% lrn1)\n# Valores de predicción\npr1 = vehicle_lrn1$train(tsk_vehicle)$predict(tsk_vehicle)\n# Generamos la tabla de comparación\ntable(y_vehicle, pr1$partition)\n\n         \ny_vehicle   1   2   3   4\n     bus   44  63  56  55\n     opel  44  35 116  17\n     saab  53  35 110  19\n     van   68  55   0  76\n\n\nSe puede ver una gran mezcla en todos los clusters. Ahora caracterizamos los grupos en función de las coordenadas en las componentes principales consideradas:\n\n# Obtenición de coordenadas\nvehicle_cp = prcomp(X_vehicle, scale = TRUE)\ncoor_cp = as.data.frame(get_pca_ind(vehicle_cp)$coord[,1:4])\n# Fusión de datos\ncoor_cp$cluster = as.factor(pr1$partition)\ncolnames(coor_cp)[1:4] = c(\"D01\", \"D02\",\"D03\",\"D04\")\n# Gráfico descriptivo de componentes\ndf = coor_cp %>% pivot_longer(cols = colnames(coor_cp[,1:(ncol(coor_cp)-1)]), names_to = \"CP\", values_to = \"Coordenadas\") \nggplot(df, aes(cluster, Coordenadas)) + \n  geom_boxplot() +\n  facet_wrap(vars(CP), nrow = 2)\n\n\n\n\n¿Cómo interpretamos ahora los gráficos anteriores? Para finalizar utilizamos las dos primeras componentes para la representación gráfica de los clusters y target.\n\n# Añadimos el tipo de leucemia a las coordenadas y el cluster.\ncoor_cp$vehicle = y_vehicle\n# Gráfico\nggplot(coor_cp, aes(D01, D02, color = cluster, shape = vehicle)) +\n  geom_point(size = 3) \n\n\n\n\nAunque el gráfico de dispersión es capaz de separar los cuatro grupos (ese es el motivo por el que el índice es alto), también es cierto que la mezcla de tipos de vehículos es muy grande impidiendo una clasificación clara de todos ellos. tal vez si se aumenta el número de grupos podríamos tener una solución más efectiva. Vamos a ver que ocurre si consideramos 8 grupos en lugar de cuatro que parece una solución más adecuada para la obtención de clusters homogéneos.\n\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\", k = 8)\nvehicle_lrn1 = as_learner(pp_vehicle %>>% lrn1)\n# Valores de predicción\npr1 = vehicle_lrn1$train(tsk_vehicle)$predict(tsk_vehicle)\n# Generamos la tabla de comparación\ntable(y_vehicle, pr1$partition)\n\n         \ny_vehicle  1  2  3  4  5  6  7  8\n     bus  22  1 36  2 22 53 20 62\n     opel 15 33 88  0 29 17 28  2\n     saab 16 32 83  0 37 19 27  3\n     van  39 55  0  6 29 70  0  0\n\n\nEl reparto sigue siendo similar lo que nos impedirá obtener una buena clasificación.\n\n# Obtención de coordenadas\nvehicle_cp = prcomp(X_vehicle, scale = TRUE)\ncoor_cp = as.data.frame(get_pca_ind(vehicle_cp)$coord[,1:4])\n# Fusión de datos\ncoor_cp$cluster = as.factor(pr1$partition)\ncolnames(coor_cp)[1:4] = c(\"D01\", \"D02\",\"D03\",\"D04\")\n# Gráfico descriptivo de componentes\ndf = coor_cp %>% pivot_longer(cols = colnames(coor_cp[,1:(ncol(coor_cp)-1)]), names_to = \"CP\", values_to = \"Coordenadas\") \nggplot(df, aes(cluster, Coordenadas)) + \n  geom_boxplot() +\n  facet_wrap(vars(CP), nrow = 2)\n\n\n\n\nAl aumentar el número de clusters resulta más difícil extraer conclusiones sobre las características de los grupos- Veamos el gráfico de dispersión.\n\n# Añadimos el tipo de leucemia a las coordenadas y el cluster.\ncoor_cp$vehicle = y_vehicle\n# Gráfico\nggplot(coor_cp, aes(D01, D02, color = cluster, shape = vehicle)) +\n  geom_point(size = 3) \n\n\n\n\nAunque algunos grupos se identifican más claramente, también es cierto que en el cuadrante I hay una gran mezcla de subgrupos. Esta claro que el modelo de cluster no es capaz de clasificar adecuadamente este tipo de datos.\n\n\n18.4.3.3 Sales\nAmpliamos ahora el análisis del banco de datos Sales.\n\nX_sales = sales %>% dplyr::select(-Product_Code)\n# CP\nsales_cp = prcomp(X_sales, scale = TRUE)\n# Resumen numérico del análisis\nresumen_cp = get_eigenvalue(sales_cp)\n# Componentes para el 50% de VE\nsum(resumen_cp$cumulative.variance.percent <= 50) + 1\n\n[1] 1\n\n# Componentes para el 70% de VE\nsum(resumen_cp$cumulative.variance.percent <= 70) + 1\n\n[1] 1\n\n# Componentes para el 80% de VE \nsum(resumen_cp$cumulative.variance.percent <= 80) + 1 \n\n[1] 1\n\n\nCon la primera componente ya superamos el 80% de variabilidad explicada. Veamos toda la solución:\n\nresumen_cp \n\n        eigenvalue variance.percent cumulative.variance.percent\nDim.1  47.78910144      91.90211815                    91.90212\nDim.2   0.55213123       1.06179082                    92.96391\nDim.3   0.14797491       0.28456713                    93.24848\nDim.4   0.13595480       0.26145153                    93.50993\nDim.5   0.13219255       0.25421643                    93.76414\nDim.6   0.13080692       0.25155177                    94.01570\nDim.7   0.12715229       0.24452364                    94.26022\nDim.8   0.12416468       0.23877824                    94.49900\nDim.9   0.11564483       0.22239391                    94.72139\nDim.10  0.11108205       0.21361932                    94.93501\nDim.11  0.10604851       0.20393943                    95.13895\nDim.12  0.10381895       0.19965183                    95.33860\nDim.13  0.10118217       0.19458110                    95.53318\nDim.14  0.09765755       0.18780299                    95.72099\nDim.15  0.09366446       0.18012396                    95.90111\nDim.16  0.09062670       0.17428211                    96.07539\nDim.17  0.09004999       0.17317305                    96.24857\nDim.18  0.08702323       0.16735237                    96.41592\nDim.19  0.08581850       0.16503558                    96.58095\nDim.20  0.08313023       0.15986583                    96.74082\nDim.21  0.08106007       0.15588475                    96.89670\nDim.22  0.07692514       0.14793296                    97.04464\nDim.23  0.07608205       0.14631163                    97.19095\nDim.24  0.07418541       0.14266425                    97.33361\nDim.25  0.07358190       0.14150365                    97.47512\nDim.26  0.07174831       0.13797751                    97.61309\nDim.27  0.06924151       0.13315674                    97.74625\nDim.28  0.06717177       0.12917649                    97.87543\nDim.29  0.06491290       0.12483251                    98.00026\nDim.30  0.06293675       0.12103221                    98.12129\nDim.31  0.06094855       0.11720874                    98.23850\nDim.32  0.05962049       0.11465479                    98.35316\nDim.33  0.05791753       0.11137986                    98.46454\nDim.34  0.05652227       0.10869668                    98.57323\nDim.35  0.05327825       0.10245817                    98.67569\nDim.36  0.05087133       0.09782947                    98.77352\nDim.37  0.05065133       0.09740639                    98.87093\nDim.38  0.04912207       0.09446553                    98.96539\nDim.39  0.04770308       0.09173669                    99.05713\nDim.40  0.04626510       0.08897135                    99.14610\nDim.41  0.04385967       0.08434552                    99.23045\nDim.42  0.04285923       0.08242160                    99.31287\nDim.43  0.04274532       0.08220255                    99.39507\nDim.44  0.04171343       0.08021814                    99.47529\nDim.45  0.04013695       0.07718644                    99.55247\nDim.46  0.03864668       0.07432054                    99.62679\nDim.47  0.03581958       0.06888381                    99.69568\nDim.48  0.03556097       0.06838648                    99.76406\nDim.49  0.03328233       0.06400448                    99.82807\nDim.50  0.03219051       0.06190483                    99.88997\nDim.51  0.03014952       0.05797985                    99.94795\nDim.52  0.02706401       0.05204618                   100.00000\n\n\nCon las dos primeras CP alcanzamos un 92% de variabilidad explicada y optamos por esta solución para proceder con el análisis cluster.\n\n# Preprocesado\npp_sales =  \n   po(\"pca\", param_vals = list(center = TRUE, scale. = TRUE, rank. = 2))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\nsales_lrn1 = as_learner(pp_sales %>>% lrn1)\n# Entrenamiento del modelo\nsales_lrn1$train(tsk_sales)\n# Valoración del modelo\npr1 = sales_lrn1$train(tsk_sales)$predict(tsk_sales)\n# Valores de scores\npr1$score(msr(\"clust.silhouette\"), task = tsk_sales)\n\nclust.silhouette \n        0.751676 \n\n\nEl score ha mejorado teniendo en cuenta que hemos reducido a un análisis solo con dos predictoras. procedemos con el resto del análisis. Veamos el dendograma:\n\n# Preprocesado\npp_sales =  \n   po(\"pca\", param_vals = list(center = TRUE, scale. = TRUE, rank. = 2))\n# Modelo de aprendizaje\nlrn1 = lrn(\"clust.hclust\", method = \"ward.D\", distmethod = \"euclidean\")\nsales_lrn1 = as_learner(pp_sales %>>% lrn1)\n# Entrenamiento del modelo\nsales_lrn1$train(tsk_sales)\n# Dendograma\nmodelo = sales_lrn1$model$clust.hclust$model\nplot(as.dendrogram(modelo))\n\n\n\n\nUna solución con cinco grupos parece construir grupos homogéneos y distintos entre si. Veamos con algo más detalle esta solución tratando de caracterizar los grupos obtenidos:\n\n# Obtención de coordenadas\nsales_cp = prcomp(X_sales, scale = TRUE)\ncoor_cp = as.data.frame(get_pca_ind(sales_cp)$coord[,1:2])\n# Fusión de datos\ncoor_cp$cluster = as.factor(pr1$partition)\ncolnames(coor_cp)[1:2] = c(\"D01\", \"D02\")\n# Gráfico descriptivo de componentes\ndf = coor_cp %>% pivot_longer(cols = colnames(coor_cp[,1:(ncol(coor_cp)-1)]), names_to = \"CP\", values_to = \"Coordenadas\") \nggplot(df, aes(cluster, Coordenadas)) + \n  geom_boxplot() +\n  facet_wrap(vars(CP), nrow = 1)\n\n\n\n\nComo era de esperar casi toda la caracterización de los grupos se corresponde con la componente 1 ya que tenia más del 90% de variabilidad explicada. De acuerdo a ella nos resulta bastante fácil clasificar las diferentes semanas. El grupo 5 es que el que tiene los productos con mayores ventas y el grupo 4 el que tiene los productos con ventas más bajas. Veamos el gráfico de dispersión.\n\n# Añadimos el tipo de leucemia a las coordenadas y el cluster.\ncoor_cp$product = y_sales$Product_Code\n# Gráfico\nggplot(coor_cp, aes(D01, D02, color = cluster)) +\n  geom_point(size = 3) \n\n\n\n\nPodemos identificar claramente los clusters de izquierda a derecha (de menos a más ventas). Esto nos permite identificar rápidamente patrones de productos sin necesidad de revisar toda la tabla de datos. Podemos ver por ejemplo los productos que se corresponden con las menores ventas (grupo 4) y las mayores ventas (grupo 5).\n\n# Menores ventas\ncoor_cp[coor_cp$cluster == 4, \"product\"]\n\ncharacter(0)\n\n# mayores ventas\ncoor_cp[coor_cp$cluster == 5, \"product\"]\n\ncharacter(0)"
  },
  {
    "objectID": "180_Clustermodels.html#sec-180.5",
    "href": "180_Clustermodels.html#sec-180.5",
    "title": "18  Análisis cluster",
    "section": "18.5 Ejercicios",
    "text": "18.5 Ejercicios\nConsideramos diferentes ejercicios donde el target es categórico o numérico.\n\nAjustar un modelo de aprendizaje automático basado en algoritmo de cluster (independiente o como resultado de un preprocesado de componentes principales) para el banco de datos Iris4.3.3.\nAjustar un modelo de aprendizaje automático basado en algoritmo de cluster (independiente o como resultado de un preprocesado de componentes principales) para el banco de datos Wine quality4.3.8.\nAjustar un modelo de aprendizaje automático basado en algoritmo de cluster (independiente o como resultado de un preprocesado de componentes principales) para el banco de datos WGene expression breast cancer4.3.10.\nAjustar un modelo de aprendizaje automático basado en algoritmo de cluster (independiente o como resultado de un preprocesado de componentes principales) para el banco de datos QSAR4.2.8.\nAjustar un modelo de aprendizaje automático basado en algoritmo de cluster (independiente o como resultado de un preprocesado de componentes principales) para el banco de datos Meat spec4.2.5."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Cui, Boxuan. 2020. DataExplorer: Automate Data Exploration and\nTreatment. https://CRAN.R-project.org/package=DataExplorer.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times\nMade Easy with lubridate.”\nJournal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nKuhn, Max, and Hadley Wickham. 2020. Tidymodels: A Collection of\nPackages for Modeling and Machine Learning Using Tidyverse\nPrinciples. https://www.tidymodels.org.\n\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian\nPfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff,\nand Bernd Bischl. 2019. “mlr3: A\nModern Object-Oriented Machine Learning Framework in\nR.” Journal of Open Source Software,\nDecember. https://doi.org/10.21105/joss.01903.\n\n\nLang, Michel, and Patrick Schratz. 2023. Mlr3verse: Easily Install\nand Load the ’Mlr3’ Package Family.\n\n\nLüdecke, Daniel. 2023. sjPlot: Data Visualization for Statistics in\nSocial Science. https://CRAN.R-project.org/package=sjPlot.\n\n\nMohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2018.\nFoundations of Machine Learning. 2nd ed. Adaptive Computation\nand Machine Learning. Cambridge, MA: MIT Press.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz\nMarbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally:\nExtension to ’Ggplot2’. https://CRAN.R-project.org/package=GGally.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible\nSummaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2022. Stringr: Simple, Consistent Wrappers for\nCommon String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023. Forcats: Tools for Working with Categorical Variables\n(Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr:\nTidy Messy Data. https://CRAN.R-project.org/package=tidyr."
  }
]