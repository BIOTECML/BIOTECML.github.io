[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MachineLearning",
    "section": "",
    "text": "Deep Learning\nEl padre moderno del concepto de Aprendizaje Profundo o Deep Learning fue el británico Geoffrey Hinton, que investigó sobre este campo en los años 80 del siglo XX. Sin embargo, no fue hasta 2010 que se volvió popular debido a su capacidad para resolver problemas complejos y mejorar la precisión de los resultados obtenidos mediante técnicas de aprendizaje automático. La idea principal que hay detrás del concepto de Aprendizaje Profundo es observar el cerebro humano e inspirarse en él para intentar reproducir de forma informática su comportamiento.\nEn la imagen inferior podemos observar una neurona real, que está compuesta principalmente de tres partes: soma (cuerpo celular), dendritas (canales de entrada) y axón (canal de salida). Descrito de una forma muy simplificada, las neuronas procesan y transmiten información por medios electroquímicos. Cuando una neurona recibe, a través de las dendritas, una cantidad de estímulos mayor a un cierto umbral, esta se despolariza excitando, a través del axón, a otras neuronas próximas conectadas a través de las sinapsis.\n\n\n\n\n\nEl aprendizaje profundo se centra en el uso de redes neuronales tanto para la representación como para el procesamiento de información. El funcionamiento se basa en la estructura biológica del sistema neuronal humano, de modo que el algoritmo es capaz de “aprender” y mejorar de manera autónoma a partir de los datos de entrada. Para reproducir el comportamiento del sistema neuronal biológico es necesario introducir los conceptos de neurona artificial y red neuronal artificial que veremos en el punto siguiente.\nGeneralmente, cuando se habla de aprendizaje automático (Machine Learning) y aprendizaje profundo (Deep Learning), se tiende a pensar que son términos intercambiables, pero esto no es cierto. Aunque ambos están relacionados con el campo de la inteligencia artificial (IA), existen diferencias significativas entre ellos. A continuación, se explican las principales diferencias entre estos dos tipos de aprendizaje:\n\nEl aprendizaje profundo se enfoca principalmente en el uso de redes neuronales profundas para procesar la información recibida, utilizando modelos muy complejos y difíciles de interpretar. En cambio, el aprendizaje automático utiliza una amplia variedad de algoritmos, algunos de los cuales son más simples y fáciles de interpretar, aunque en algunos casos también se utilizan redes neuronales y la complejidad de los modelos puede variar.\nGeneralmente, el aprendizaje profundo requiere una cantidad mayor de datos para poder entrenar los modelos de manera efectiva debido a la complejidad de las redes neuronales y su capacidad para capturar o elaborar patrones sobre los datos más complejos.\nEn cuanto a las características, el aprendizaje automático requiere que los usuarios las creen e identifiquen con precisión, mientras que el aprendizaje profundo las aprende y crea nuevas de forma automática.\nPor último, el aprendizaje profundo se suele aplicar a problemas que requieren una mayor precisión, como por ejemplo en diagnósticos médicos o en el procesamiento del lenguaje natural.\n\nA continuación se presenta un diagrama sobre la integración del aprendizaje profundo y el aprendizaje automático dentro de la inteligencia artificial.\n\n\n\n\n\nPor último, veamos los campos de aplicación más habitual del aprendizaje profundo haciendo uso de la redes neuronales artificiales, entre los que destacan:\n\nProcesamiento del lenguaje natural. Se utiliza en tareas como el filtrado de correos electrónicos para detectar los mensajes spam, en la predicción y autocompletado de palabras en textos, en chatbots para comprender lo que el usuario está solicitando y proporcionar una respuesta adecuada, y en traducciones de textos de un idioma a otro de manera automática y precisa.\nProcesamiento de imágenes. Es un área en la que el aprendizaje profundo ha logrado grandes avances. Entre las tareas más destacadas se encuentran la clasificación y detección de objetos en imágenes, así como la segmentación y síntesis de imágenes.\nDiagnósticos médicos. Los avances ayudan a los médicos a mejorar la precisión y rapidez de los diagnósticos, lo que puede llevar a una mejor atención médica y resultados para los pacientes. Se utiliza en la detección y diagnóstico de enfermedades, la identificación de patrones en imágenes médicas y la predicción de la progresión de enfermedades.\nReconocimiento facial y de voz. Se utiliza para tareas relacionadas con la interacción con el ser humano generalmente, como la verificación de la identidad, la detección de emociones y la transcripción de voz a texto.\nAsistentes virtuales. Los asistentes virtuales, como Alexa, Siri y ok google utilizan el aprendizaje profundo para comprender y responder a las solicitudes del usuario, como por ejemplo para reproducir la canción que este solicita, realizar una llamada telefónica o buscar información en la web.\n\nLos contenidos de esta parte se estructuran de la siguiente forma:\n\nIntroducción al deep learning y proceso de aprendizaje de una red neuronal.\nRedes multicapas densas para problemas de clasificación y regresión con Keras.\nRedes convolucionales para el análisis de imagenes con Keras."
  },
  {
    "objectID": "10_IntroDL.html#conceptos-fundamentales-del-dl",
    "href": "10_IntroDL.html#conceptos-fundamentales-del-dl",
    "title": "1  Introducción",
    "section": "1.1 Conceptos fundamentales del DL",
    "text": "1.1 Conceptos fundamentales del DL\nComenzamos con los conceptos matemáticos de neurona artificial y red neuronal.\n\n1.1.1 Neurona artificial\nPara poder modelizar de forma matemática el funcionamiento de una neurona es necesario conocer su funcionamiento biológico. De esta forma nos resultará posible construir la denominada neurona artificial que trata de replicar el funcionamiento de una neurona real. El comportamiento de una neurona se puede representar mediante este sencillo esquema:\n\nLa señal entra en el núcleo de la neurona vía las dendritas o través de otra neurona.\nLa conexión sináptica de cada dendrita puede tener una fuerza (peso) diferente y ajustable.\nEn el núcleo, la señal de todas las dendritas (inputs) se combina (generalmente de forma aditiva) en un único efecto.\nSi la señal combinada es más fuerte que un umbral dado, entonces la neurona se activa a lo largo del axón, en el caso contrario permanece quieta, es decir, en la realización más sencilla, la intensidad de la señal tiene dos niveles posibles: encendido o apagado, es decir, 1 o 0, en función del valor del umbral. No se necesitan valores intermedios.\nSi la neurona se ha activado, el terminal del axón se conecta a las dendritas de otras neuronas o produce un estímulo de salida.\n\nTraduciendo esto a una receta matemática, se asignan a las celdas de entrada los números \\(x_1,...,x_n\\) (punto de datos de entrada). La fuerza de las conexiones sinápticas se controla con los pesos \\(w_1,...,w_n\\). A continuación, la señal combinada se define como la suma ponderada:\n\\[s=\\sum_{i=1}^n x_iw_i\\]\nLa señal se convierte en un argumento de la función de activación (\\(f\\)), que, en el caso más sencillo, adopta la forma de la función de salto (step), es decir, cuando la señal combinada \\(s\\) es mayor que el sesgo (umbral) \\(b\\), la señal que pasa por el axón es 1. En el caso contrario, el valor de la señal generada es 0 (no hay activación):\n\\[f(s,b) =\n\\begin{cases}\n1 \\text{ para } s \\geq b \\\\\n0 \\text{ para } s < b \\\\\n\\end{cases}\\]\nEsta representeción matemática es precisamente lo que necesitamos para imitar el prototipo biológico.\nExiste una conveniente convención notacional que se utiliza con frecuencia. En lugar de separar el sesgo de los datos de entrada, podemos tratarlos todos uniformemente. La condición de activación puede transformarse trivialmente como:\n\\[s\\geq b \\rightarrow \\sum_{i=1}^n x_iw_i - b \\geq 0 \\rightarrow \\sum_{i=1}^n x_iw_i - x_0w_0 \\geq 0 \\rightarrow \\sum_{i=0}^n x_iw_i \\geq 0,\\]\ndonde \\(x_0 = 1\\) y \\(w_0 = -b\\). En otras palabras, podemos tratar el sesgo como un peso en la arista conectada a una celda adicional con la entrada siempre fija a 1:\n\\[f(s,b) =\n\\begin{cases}\n1 \\text{ para } s \\geq 0 \\\\\n0 \\text{ para } s < 0 \\\\\n\\end{cases}\\]\ncon \\(s=\\sum_{i=0}^n x_iw_i\\). En la figura siguiente viene representado el comportamiento de la neurona artificial:\n\n\n\n\n\nLas ponderaciones \\(w_0,w_1,...,w_n\\) se denominan generalmente hiperparámetros. Determinan la funcionalidad de la neurona artificial y pueden modificarse durante el proceso de aprendizaje (entrenamiento, que analizaremos más adelante). Sin embargo, se mantienen fijos cuando se utiliza la neurona entrenada en una muestra de datos de entrada concreta.\nUna propiedad esencial de las neuronas artificiales es la no linealidad de la función de activación, lo que permite la construcción de estructuras neuronales muy complejas con gran capacidad de aprendizaje.\nA continuación vamos a ver como podemos implementar una neurona artificial utilizando una función muy simple, donde fijaremos los valores de \\(x\\) y \\(w\\), así como el balor de \\(b\\) para la activación de la neurona. En primer lugar definimos la función salto de activación de la neurona.\n\n# Función de activación\nsalto = function(s)\n{\n  activ = ifelse(s>0,1,0)\n  return(activ)\n}\n\nRepresentamos la función de activación para una secuencia de valores de s:\n\n# Valores a evalaur\nsval = seq(-2,2,length=100)\n# Función de activación\nres = salto(sval)\nplot(sval,res,type=\"l\", xlab=\"s\",ylab=\"Activación\")\n\n\n\n\nFunción de activación\n\n\n\n\nDefinimos ahora nuestra neurona artificail teniendo en cuenta que dada la construcción matemática se debe fijar que \\(x_0 = 0\\) y \\(w_0 = -b\\).\n\n# Función de activación\nneurona = function(x,w,b,f=salto)\n{\n  # Neurona artificial\n  \n  # Entradas\n  #   x: array de entradas  [x1, x2,...,xn]\n  #   w: array de pesos [w1, w2,...,wn]\n  #   f: función de activación. Por defecto función de salto\n\n  # Return\n  #   signal = x.w\n  \n  x = c(0,x)\n  w = c(-1*b,w)\n  return(f(sum(x*w)))\n}\n\nPodemos ver el funcionamiento de nuestra neurona con un ejemplo de muestra. la nerurona se activará cuando el resultado sea igual a 1, y no lo hará si el resultado es igual a cero.\n\nx = c(2,1.5,-0.7)\nw = c(1,2.5,-0.2)\nb = 4\n# Evaluamos\nneurona(x, w, b)\n\n[1] 1\n\n\nEn este caso la neurona se ha activado. Si cambiamos los valores de \\(b\\) podríamos tener diferentes representaciones de la activación de la neurona.\n\n\n1.1.2 Red neuronal artificial\nLas redes neuronales artificiales (RNA) son modelos computacionales que procesan información imitando el funcionamiento de las neuronas biológicas. El objetivo de las RNA es ayudar a que los sistemas informáticos puedan funcionar tal como un cerebro humano en cuanto a aprendizaje y pensamiento. De esta idea parte el concepto de “inteligencia artificial”.\nLa forma más común de representar la estructura de una red neuronal es mediante el uso de capas (layers), formadas a su vez por neuronas (unidades, units o neurons). Cada neurona, realiza una operación sencilla y está conectada a las neuronas de la capa anterior y de la capa siguiente mediante pesos, cuya función es regular la información que se propaga de una neurona a otra.\nLas redes neuronales artificiales están conformadas por 3 tipos de nodos o neuronas:\n\nNodos de entrada: reciben la información desde el exterior de la red (input).\nNodos de salida: envían la información hacia el exterior de la red (output).\nNodos ocultos: transmiten la información entre los nodos de la red. Por lo tanto, se encuentran en el medio de los nodos de entrada y de salida y no tienen contacto con el exterior.\n\nLas RNA suelen estar conformadas por múltiples capas de nodos ocultos, a estas se les llaman “capas de aprendizaje”. A mayor cantidad de capas, mayor es la profundidad de la red y mayor es la capacidad de aprendizaje. En este contexto, los nodos de entrada reciben una serie de datos desde el exterior, estos datos son enviados al interior de la red hacia los nodos ocultos. Los nodos ocultos van procesando, modificando y transfiriendo la información de una capa a otra. Este proceso es lo que se conoce como “aprendizaje”, pues cada capa de nodos ocultos va aprendiendo de las capas más externas. Dicha secuencia de aprendizaje es lo que da origen al Deep Learning. De aquí la inseparable relación entre redes neuronales artificiales y Deep Learning.\nCuando las redes neuronales son entrenadas, cada red crea, modifica o elimina conexiones entre los nodos con el fin de dar respuestas más acertadas ante el problema que se busca resolver.\nEn este punto una red neuronal formada por una única neurona (neurona artificial) se caracteriza por:\n\n\n\n\n\n\nUna capa de entrada que recibe los datos en bruto, es decir, el valor de los predictores.\nUna capa oculta que recibe los valores de la capa de entrada, ponderados por los pesos.\nUna capa de salida que combina los valores que salen de la capa intermedia y cuya información se propaga a otra capa si la función de activación así lo determina.\n\nLas funciones de activación convierten el valor neto de entrada en un nuevo valor, combinación de los input, pesos y bias. Es gracias a combinar funciones de activación no lineales con múltiples capas que los modelos de redes son capaces de aprender relaciones no lineales. La gran mayoría de funciones de activación convierten el valor de entrada neto de la neurona en un valor dentro del rango (0, 1) o (-1, 1). Cuando el valor de activación de una neurona (salida de su función de activación) es cero, se dice que la neurona está inactiva, ya que no pasa ningún tipo de información a las siguientes neuronas.\nEl modelo de red neuronal con una única capa (single-layer perceptron), aunque supuso un gran avance en el campo del Machine Learning, solo es capaz de aprender patrones sencillos. Para superar esta limitación, los investigadores descubrieron que, combinando múltiples capas ocultas, la red puede aprender relaciones mucho más complejas entre los predictores y la variable respuesta. A esta estructura se le conoce como perceptrón multicapa o multilayer perceptron (MLP), y puede considerarse como el primer modelo de Deep Learning.\nLa estructura de un perceptón multicapa consta de varias capas de neuronas ocultas. Cada neurona está conectada a todas las neuronas de la capa anterior y a las de la capa posterior. Aunque no es estrictamente necesario, todas las neuronas que forman parte de una misma capa suelen emplear la misma función de activación.\nCombinando múltiples capas ocultas y funciones de activación no lineales los modelos de redes pueden aprender prácticamente cualquier patrón. De hecho, está demostrado que, con suficientes neuronas, un MLP es un aproximador universal para cualquier función. A continuación, se muestra la estructura de un MLP con tres capas ocultas con 6, 6, y 8 neuronas en cada una de ellas y con cuatro capas de salida o valores de predicción de la red:"
  },
  {
    "objectID": "10_IntroDL.html#tipos-redes-neuronales",
    "href": "10_IntroDL.html#tipos-redes-neuronales",
    "title": "1  Introducción",
    "section": "1.2 Tipos redes neuronales",
    "text": "1.2 Tipos redes neuronales\nDentro de las redes neuronales encontramos diferentes tipos, distinguiéndose estos por sus características y aplicaciones particulares. A continuación, detallamos las que vamos a desarrollar en próximos cuadernos:\n\nRedes neuronales monocapa. También conocidas como perceptrones simples, son las redes neuronales más simples y están compuestas por una única capa de neuronas que realizan una combinación lineal sobre las entradas. Una vez modificadas, las trasladan a una capa de neuronas de salida donde se aplica una función de activación para generar una salida. Este tipo de redes se utilizan cuando se pretende realizar una clasificación binaria o linealmente separable.\n\n\n\n\n\n\n\nRedes neuronales multicapa. Son una generalización de las anteriores, compuestas por dos o más capas de neuronas. En estas se introducen las capas ocultas, las cuales permiten que la red neuronal aprenda características más complejas de los datos de entrada y se combinan con la capa de entrada y la capa de salida para formar una arquitectura de red más compleja. En este caso no podemos especificar una aplicación concreta, pues se utilizan en una amplia variedad de aplicaciones en diversas áreas.\n\n\n\n\n\n\n\nRedes neuronales convolucionales (CNN). Son una variante de las redes neuronales multicapa en las que cada neurona no se une con todas y cada una de las capas siguientes, sino que solo lo hace con un subgrupo de estas. Con esto se consigue reducir el número de neuronas y la complejidad computacional necesarias para su ejecución. Se utilizan para tareas de clasificación y detección de objetos en imágenes.\nRedes neuronales recurrentes (RNN). Son redes neuronales que no tienen la típica estructura de capas, sino que permiten conexiones arbitrarias entre las neuronas, incluso pudiendo crear ciclos. Esto les permite tener memoria y retroalimentación de información, lo que las hace útiles en tareas que requieren un contexto o una memoria de largo plazo. Son especialmente útiles en tareas que involucren secuencias de datos, como el procesamiento del lenguaje natural y el reconocimiento de voz.\n\n\n\n\n\n\n\nRedes neuronales de base radial (RBF). Son redes neuronales multicapa que utilizan funciones radiales para calcular la distancia entre los datos y un conjunto de puntos denominados centros. La salida proporcionada consiste en una combinación lineal de las funciones de activación radiales utilizadas por las neuronas de manera individual. Se utilizan comúnmente en tareas de regresión y clasificación, siendo especialmente adecuadas en problemas de alta dimensionalidad."
  },
  {
    "objectID": "10_IntroDL.html#terminología-básica",
    "href": "10_IntroDL.html#terminología-básica",
    "title": "1  Introducción",
    "section": "1.3 Terminología básica",
    "text": "1.3 Terminología básica\n\nNeurona o perceptrón: unidad básica de procesamiento en una red neuronal que procesa información mediante la aplicación de pesos y umbrales o sesgos para producir una salida.\nCapa: conjunto de neuronas que procesan la información de entrada y realizan una transformación no lineal para extraer características relevantes que sean útiles para la tarea que se esté abordando. En una red neuronal profunda distinguimos tres tipos de capas, las de entrada, las ocultas y las de salida, cumpliendo cada una su función específica en el procesamiento de la información.\nFunción de activación: función que se aplica a la salida de una neurona o de un conjunto de neuronas y transmite la información generada por la combinación lineal de los pesos y las entradas. Esto permite que la red pueda aprender y modelar relaciones entre los datos de entrada y la salida deseada.\nPesos: parámetros ajustables que se utilizan en una red neuronal para transformar las entradas en salidas. Se utilizan para ponderar la importancia de cada entrada en la salida de la neurona y, durante el entrenamiento de la red, se ajustan mediante un algoritmo de optimización para minimizar la función de pérdida.\nÉpoca: ciclo completo a través de todo el conjunto de datos de entrenamiento a través de una red neuronal. Durante una época, la red neuronal procesa las entradas de entrenamiento y ajusta los pesos correspondientes.\nCaracterística: representación numérica de una variable, imagen, texto, sonido u otro tipo de dato. Se utiliza como entrada, y su objetivo es capturar información relevante y discriminativa que permita al modelo realizar una tarea específica.\nBatch o lote: cantidad de datos que se utilizan para entrenar una red neuronal en cada iteración de aprendizaje."
  },
  {
    "objectID": "10_IntroDL.html#el-perceptrón-lineal",
    "href": "10_IntroDL.html#el-perceptrón-lineal",
    "title": "1  Introducción",
    "section": "1.4 El perceptrón lineal",
    "text": "1.4 El perceptrón lineal\nAntes de profundizar más en el proceso de entrenamiento de una red neuronal, que trataremos en los temas siguientes, vamos a ver cómo la neurona artificial definida anteriormente se puede utilizar como un clasificador binario. Distinguimos dos situaciones:\n\nse conoce de partida la regla de clasificación,\nno se conoce de partida la regla de clasificación.\n\n\n1.4.1 Regla conocida\nPara empezar, generamos 100 datos de entrenamiento como puntos aleatorios en un cuadrado unidad. Así, las coordenadas del punto \\(x_1\\) y \\(x_2\\) se toman en el intervalo \\([0,1]\\). Definimos dos categorías: una para los puntos situados por encima de la línea \\(x_1=x_2\\) y otra para los puntos situados por debajo. Durante la generación, comprobamos si \\(x_2 > x_1\\) o no, y asignamos una etiqueta 1 o 0 en función de que se cumpla la condición de clasificación establecida. Estas etiquetas son las respuestas “verdaderas” de la clasificación.\nPara la regla de clasificación establecida tenemos por tanto que:\n\\[x_2 > x_1 \\rightarrow s=-x_1+x_2>0\\]\nque en términos de una neurona artificial con función de activación de salto nos proporciona los pesos:\n\\[w_0 = -b = 0, w_1 = -1, w_2 = 1\\]\n\n# Valores simulados\nx1 = runif(100, 0, 1)\nx2 = runif(100, 0, 1)\nx = cbind(x1, x2)\n\n# Parámetros de la neurona \nw = c(-1, 1)\nb = 0\n\n# Valores de activación\nvalor = c()\nfor (i in 1:length(x1))\n{\n  valor[i] = neurona(x[i,], w, b)\n}\n# Resultado\ndf = data.frame(x1=x1,x2=x2,activ=as.factor(valor))\n\nVeamos la representación gráfica de los puntos junto con el valor de activación. Además añadimos la recta que determina la regla de clasificación.\n\nggplot(df, aes(x1,x2,color=activ)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(color = \"Activación\") +\n  scale_color_discrete(labels=c(\"No\", \"Si\")) \n\n\n\n\nClasificación binaria lineal\n\n\n\n\nComo era de esperar la neurona artificial proporciona el resultado adecuado en términos de la regla de clasificación dado que las muestras son separables linealmente. El problema aparece cuando queremos resolver el problema de clasificación de dos clases cuando estas no son separables linealmente como podemos ver en la imagen siguiente donde introducimos los hiperplanos de separación \\(x_2=x_1\\) y \\(x_2=0.1*x_1+0.5\\), es decir la regla de clasificación vieen dada por la combinación de las regiones que determinan los hiperplanos.\n\n# Valores simulados\nx1 = runif(100, 0, 1)\nx2 = runif(100, 0, 1)\nx = cbind(x1, x2)\n\n# Valores de activación en función d elas regiones definidas\nvalor = 1*((x2>x1) & (x2>0.1*x1+0.5))\n# Resultado\ndf = data.frame(x1=x1,x2=x2,activ=as.factor(valor))\n# Grafico\nggplot(df, aes(x1,x2,color=activ)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  geom_abline(intercept = 0.5, slope = 0.1) +\n  labs(color = \"Activación\") +\n  scale_color_discrete(labels=c(\"No\", \"Si\")) \n\n\n\n\nClasificación binaria no lineal\n\n\n\n\nEste problema se puede resolver fácilmente si consideramos que una parte de los datos son separados por una neurona y otra parte por otra, y así sucesivamente hasta conseguir separar los dos grupos de la forma más precisa posible. Podemos establecer tantas neuronas como sean necesarias para tener en cuenta todas las posibles ecuaciones lineales necesarias para separar los datos de ambas muestras.\nImaginemos ahora que tenemos más condiciones de este tipo: dos, tres, etc., en general \\(k\\) condiciones independientes. Tomando una conjunción de estas condiciones podemos construir regiones como se muestra, por ejemplo, en la figura siguiente:\n\n\n\n\n\nque no son más que regiones convexas en el plano obtenidas, de izquierda a derecha, con una condición de desigualdad, y una conjunción de 2, 3 o 4 condiciones de desigualdad, obteniéndose polígonos con las dos últimas. Claramente \\(k\\) condiciones de desigualdad se pueden imponer con \\(k\\) neuronas artificiales.\nEn la situación del ejemplo anterior utilizando la función de activación de salto tendríamos dos neuronas:\n\nNeurona 1 con pesos \\(w_0 =0, w_1=-1, w_2=1\\)\n\nNeurona 2 con pesos \\(w_0 =-0.5, w_1=-0.1, w_2=1\\)\n\nAhora sólo nos resta combinar la información de esas dos neuronas en una tercera para deteminar la clasificación de cada punto. Esta neurona final actúa como un operador lógico con cuatro posibilidades en función de la activación o no activación de las dos primeras neuronas. Podemos considerar esta neurona 3 con pesos \\(w_0=1.5, w_1=1, w_2=1\\) para reflejar el hecho de que combinamos las dos anteriores y que sólo una de las cuatro opciones lógicas nos activará esta última neurona. Las situaciones lógicas son:\n\nNo se activa la neurona 1 ni la neurona 2.\nSe activa la neurona 1 y no se activa la neurona 2.\nNo se activa la neurona 1 y se activa la neurona 2.\nSe activan ambas neuronas.\n\nSe pude crear ahora un función que evalué de acuerdo al algoritmo establecido con las dos neuronas construyendo así nuestra primera red neuronal.\nLas arquitecturas de redes para \\(k\\) = 1, 2, 3 ó 4 condiciones se muestran en la figura siguiente. Yendo de izquierda a derecha desde el segundo panel, tenemos redes con dos capas de neuronas y con neuronas en la capa intermedia, que proporcionan las condiciones de desigualdad, y una neurona en la capa de salida (ya que sólo debemos clasificar en dos grupos).\n\n\n\n\n\nEn la interpretación geométrica, la primera capa de neuronas representa los \\(k\\) semiplanos, y la neurona de la segunda capa corresponde a una región convexa con \\(k\\) lados. La situación se generaliza de forma obvia a los datos en más dimensiones. En ese caso tenemos más puntos negros en las entradas de la figura anterior. Geométricamente, para \\(n=3\\) tratamos con planos divisorios y poliedros convexos, y para \\(n>3\\) con hiperplanos divisores y polítopos convexos.\n\n\n1.4.2 Regla desconocida\nLlegados a este punto, puede parecer que los resultados que hemos obtenido en el punto anterior son bastante triviales. Esto se debe a que conocíamos la regla de clasificación y por tanto era posible obtener los pesos asociados con la neurona de la red. En situaciones reales disponemos de los datos de entrada pero se desconocen la regla o reglas de clasificación. En otras palabras, necesitamos encontrar la regla o reglas de clasificación correspondientes, lo que equivale a encontrar los pesos de las neuronas artificiales consideradas que nos permitan una mejor clasificación.\nEn el punto siguiente vemos el proceso de estimación de los pesos de la neurona artificial para el problema de clasificación binaria con una única neurona artificial. En próximos temas veremos como estimar los pesos en estructuras de redes con capas ocultas y donde incorporamos más de una neurona.\n\n1.4.2.1 Algoritmo perceptrón\nLa solución para el problema de clasificación binario pasa por disponer de un procedimiento algorítmico sistemático que funcione sin esfuerzo para ésta y cualquier otra situación similar. La respuesta es el ya mencionado algoritmo del perceptrón.\nEn la situación del ejemplo anterior, y antes de presentar el algoritmo de estimación de los pesos, observemos que la neurona artificial con algún conjunto de pesos \\(w_0, w_1,w_2\\) siempre da alguna respuesta para un punto de datos etiquetado, correcta o incorrecta. Consideramos el conjunto de datos inicial y vemos qué clasificación obtenemos cuando fijamos unos pesos que no se corresponden con los correspondientes con la regla de clasificación.\nLa idea general del algortimo perceptrón es utilizar las respuestas erróneas para ajustar inteligentemente, en pequeños pasos, los pesos, de forma que después de un número suficiente de iteraciones obtengamos todas las respuestas correctas para la muestra de entrenamiento.\nLa base del algoritmo (que estudiaremos con más detalle en el próximo cuaderno) se basa en el método del descenso del gradiente y viene dado por el siguiente proceso iterativo:\n\nSe hace una primera iteración con los pesos iniciales y se obtiene la clasificación con ellos.\nSi para un punto dado el resultado obtenido \\(y_0\\) es igual al valor verdadero \\(y_t\\) (la etiqueta), es decir, la respuesta es correcta, no hacemos nada. Sin embargo, si es incorrecta, cambiamos un poco los pesos, de forma que disminuya la probabilidad de obtener una respuesta errónea. Si consideramos \\(\\epsilon\\) (tasa de aprendizaje) como un valor pequeño que debemos fijar al inicio, y \\(x_i\\) como los inputs de la muestra \\(i\\) con \\(i=1,...,n\\).valor pequeño, entonces la regla iterativa viene dada por:\n\n\\[w_i \\rightarrow w_i + \\epsilon(y_t-y_0)x_i,\\]\n\nEl proceso se detiene cuando la diferencia de los pesos entre dos iteraciones seguidas está por debajo de un umbral prefijado.\n\nVeamos como funciona el algoritmo en la práctica. Supongamos que \\(x_i >0\\). Si la etiqueta predicha para dicha muestra es \\(y_t = 1\\) mientras que la etiqueta original era \\(y_0=0\\), el peso \\(w_i\\) se incrementa. Entonces \\(wx\\) se incrementa e \\(y_0=f(wx)\\) está más cerca de obtener el verdadero valor de 1 (ya que estamos utilizando la función de salto). Por otro lado, si la etiqueta \\(y_t=0\\) es menor que la respuesta encontrada \\(y_0=1\\), entonces el peso \\(w_i\\) decrece mientras que \\(wx\\) crece, e \\(y_0=f(wx)\\) esta más cerca de obtener el verdadero valor de 0. Si \\(x_i < 0\\) podemos ver que el funcionamiento es análogo. Cuando la respuesta es correcta, \\(y_t =y_0\\) entonces no es necesario cambiar los pesos.\nLa fórmula anterior puede utilizarse muchas veces para el mismo punto de la muestra de entrenamiento. A continuación, hacemos un bucle sobre todos los puntos de la muestra, y todo el procedimiento se puede seguir repitiendo en muchas rondas para obtener pesos estables (que no cambien más a medida que continuamos el procedimiento, o que cambien ligeramente).\nNormalmente, en este tipo de algoritmos la velocidad de aprendizaje \\(\\epsilon\\) disminuye en las rondas sucesivas. Esto es técnicamente muy importante, porque unas actualizaciones demasiado grandes podrían estropear la solución obtenida.\nGráficamente podemos ver el funcionamiento del algoritmo:"
  },
  {
    "objectID": "20_TrainDL.html#preprocesado-de-inputs",
    "href": "20_TrainDL.html#preprocesado-de-inputs",
    "title": "2  Entrenamiento de la red neuronal",
    "section": "2.1 Preprocesado de inputs",
    "text": "2.1 Preprocesado de inputs\nA la hora de entrenar modelos basados en redes neuronales es necesario realizar, al menos, dos tipos de transformaciones de los datos. Procedemos igual que en cualquier otro algoritmo de aprendizaje automático.\nCodificación (One hot ecoding) de las variables categóricas\nLa binarización (one-hot-encoding) consiste en crear nuevas variables dummy con cada uno de los niveles de las variables cualitativas. Por ejemplo, una variable llamada color que contenga los niveles rojo, verde y azul, se convertirá en tres nuevas variables (color_rojo, color_verde, color_azul), todas con el valor 0 excepto la que coincide con la observación, que toma el valor 1.\nEstandarización y escalado de variables numéricas\nCuando los predictores son numéricos, la escala en la que se miden, así como la magnitud de su varianza, pueden influir en gran medida en el modelo. Si no se igualan de alguna forma los predictores, aquellos que se midan en una escala mayor o que tengan más varianza dominarán el modelo aunque no sean los que más relación tengan con la variable respuesta. Existen principalmente dos estrategias para evitarlo:\n\nCentrado: consiste en restarle a cada valor la media del predictor al que pertenece. Si los datos están almacenados en un dataframe, el centrado se consigue restándole a cada valor la media de la columna en la que se encuentra. Como resultado de esta transformación, todos los predictores pasan a tener una media de cero, es decir, los valores se centran en torno al origen.\nNormalización (estandarización): consiste en transformar los datos de forma que todos los predictores estén aproximadamente en la misma escala. Las dos opciones habituales son:\n\nNormalización Z-score (StandardScaler): dividir cada predictor entre su desviación típica después de haber sido centrado, de esta forma, los datos pasan a tener una distribución normal.\nEstandarización max-min (MinMaxScaler): transformar los datos de forma que estén dentro del rango [0, 1]."
  },
  {
    "objectID": "20_TrainDL.html#funciones-de-activación",
    "href": "20_TrainDL.html#funciones-de-activación",
    "title": "2  Entrenamiento de la red neuronal",
    "section": "2.2 Funciones de activación",
    "text": "2.2 Funciones de activación\nComo ya vimos en el tema anterior, las funciones de activación controlan en gran medida qué información se propaga desde una capa a la siguiente (forward propagation). Estas funciones convierten el valor neto de entrada a la red neuronal, combinación de los inputs, pesos y sesgos, en un nuevo valor. En el cuaderno anterior ya vimos el comportamiento de la función de activación de salto, pero no es la única existente. En concreto, el uso de funciones de activación no lineales con múltiples capas es lo que permite que los modelos de redes sean capaces de aprender relaciones no lineales.\nLa gran mayoría de funciones de activación convierten el valor de entrada neto de la neurona en un valor dentro del rango (0, 1) o (-1, 1). Cuando el valor de activación de una neurona (salida de su función de activación) es cero, se dice que la neurona está inactiva, ya que no pasa ningún tipo de información a las siguientes neuronas. A continuación, se describen las funciones de activación más empleadas así como la derivada de dicha función que será utilizada en el proceso de entrenamiento de la red.\n\n2.2.1 Sigmoide\nLa función de activación sigmoide acepta un número como entrada y devuelve un número entre 0 y 1. Es fácil de usar y tiene todas las cualidades deseables de las funciones de activación: no linealidad, diferenciación continua, monotonicidad y un rango de salida establecido.\nSe utiliza principalmente en problemas de clasificación binaria, ya que su salida puede interpretarse como probabilidades, ya que nos proporciona la probabilidad de existencia de una clase determinada. Matemáticamente se define como:\n\\[sigmoid(s) = S(s) = \\frac{1}{1+e^{-s}}.\\] La derivada de la función viene dada por la expresión:\n\\[S'(s) = S(s)(1-S(s))\\]\nEntre las ventajas y desventajas del uso de esta función podemos mencionar:\n\nEs de naturaleza no lineal. Las combinaciones de esta función también son no lineales, y dará una activación analógica, a diferencia de la función de activación de salto. Además, esta función presenta un gradiente suave y es efectiva para el problema de clasificación.\nEl resultado de la función de activación siempre va a estar en el rango \\((0,1)\\) en comparación con \\((-∞, ∞)\\) de la función de activación lineal. Como resultado, hemos definido un rango para nuestras activaciones.\nLa función sigmoide da lugar a un problema de “gradientes evanescentes” (“Vanishing gradients”) y los sigmoides saturan y matan los gradientes. El problema de “gradientes evanescentes” es frecuente en el entrenamiento de redes neuronales. Dado que la función de activación tiene un rango de salida pequeño (de 0 a 1), un gran cambio en el input de la función de activación creará una pequeña modificación en la salida. Por lo tanto, la derivada también se vuelve pequeña. Estas funciones de activación sólo se utilizan en redes poco profundas con pocas capas. Cuando estas funciones de activación se aplican a una red multicapa, el gradiente puede llegar a ser demasiado pequeño para el entrenamiento esperado.\nSu resultado no está centrado en cero, y hace que las actualizaciones del gradiente fluctúen lejos en diferentes direcciones.\nEl valor de salida está entre cero y uno, por lo que dificulta la optimización.\nEn ocasiones la red se niega a aprender más o es extremadamente lenta.\n\n\n\n2.2.2 Tangente hiperbólica\nLa función tangente hiperbólica comprime un número real al rango [-1, 1]. Es no lineal, pero es diferente de la anterior (Sigmoid), y su salida está centrada en cero. Su definición formal es:\n\\[TanH(s) = \\frac{e^s-e^{-s}}{e^s+e^{-s}}\\]\nLa ventaja que tiene esta función de activación es que los inputs negativos se convierten en valores fuertemente negativos y los inputs positivos se convierten en valores fuertemente positivos. Los resultados tienden a los extremos. Al igual que en la función sigmoide, es diferenciable y monótona mientras que su derivada no lo es. Esta función de activación se utiliza principalmente para la clasificación entre dos clases, ya que si la tendencia es hacia uno de los lados la función lo arrastrará más aún para ese lado.\nLa derivada de la función viene dada por la expresión:\n\\[TanH'(s) = 1-Tanh^2(s)\\] Entre sus ventajas e inconvenientes podemos destacar:\n\nTanH también tiene el problema del gradiente evanescente, pero el gradiente es más fuerte en TanH que en sigmoide (las derivadas son más pronunciadas).\nTanH está centrada en cero, y los gradientes no tienen que moverse en una dirección específica.\n\n\n\n2.2.3 ReLU (Rectified linear Unit)\nReLU significa Unidad Lineal Rectificada y es una de las funciones de activación más utilizadas en las aplicaciones. Ha resuelto el problema del gradiente evanescente porque el valor máximo del gradiente de la función ReLU es uno. También resuelve el problema de la saturación de la neurona, ya que la pendiente nunca es cero para la función ReLU. El rango de ReLU está entre 0 e infinito.\nFormalmente se define como:\n\\[ReLU(s) = max\\{0,s\\}\\]\n¿Cuál es la diferencia de esta función con la de salto que la hace tan interesante? La clave está en que todos los valores negativos se vuelven cero, y eso significa que cualquier entrada negativa dada a la función de activación de ReLU convierte el valor en cero inmediatamente. Esto puede ayudar mucho en la simplificación computacional ya que todos los valores iguales a 0 son inmediatamente descartados (dichas neuronas son irrelevantes). A su vez esto puede disminuir la capacidad del modelo para ajustarse o entrenarse a partir de los datos correctamente. Es el motivo por el que su uso esta muy extendido en las redes convolucionales.\nEn este caso la derivada de la función ReLU toma el valor 1 si \\(s>0\\) y \\(0\\) en otro caso.\nEntre las ventajas e incovenientes de esta función podemos destacar:\n\nDado que sólo se activa un cierto número de neuronas, la función ReLU es mucho más eficiente desde el punto de vista computacional que las funciones sigmoide y TanH.\nReLU acelera la convergencia del descenso gradiente hacia el mínimo global de la función de pérdida gracias a su propiedad lineal y no saturante.\nUna de sus limitaciones es que sólo debe utilizarse dentro de las capas ocultas de un modelo de red neuronal artificial.\nAlgunos gradientes pueden ser frágiles durante el entrenamiento. En otras palabras, para activaciones en la región (\\(x<0\\)) de ReLU, el gradiente será 0 debido a lo cual los pesos no se ajustarán durante el descenso. Es decir, las neuronas que entren en ese estado dejarán de responder a las variaciones en la entrada (simplemente porque el gradiente es 0, nada cambia). A esto se le llama el problema del ReLU moribundo.\n\n\n\n2.2.4 Softmax\nLa función Softmax es una de las funciones estrella en la última capa de la red, ya que permite realizar una clasificación categórica multiclase.\nEs una combinación de muchos sigmoides. Al igual que la función sigmoide, devuelve una probabilidad, sólo que en este caso lo hace para cada clase/etiqueta. Más concretamente la función Softmax devuelve la probabilidad de la clase actual con respecto a las demás. Esto significa que también tiene en cuenta la posibilidad de que existan otras clases.\nSi \\(s_i\\) donde \\(i\\) identifica la clase \\(i\\) de un conjunto de \\(k\\) clases esta función se puede expresar como:\n\\[Softmax(s_i) = \\frac{exp(s_i)}{\\sum_{j=1}^k exp(s_j)}\\]\nEntre las ventajas e inconvenientes podemos destacar que:\n\nImita mejor la etiqueta codificada que los valores absolutos.\nSe perdería información si se utilizaran valores absolutos (módulo), pero la exponencial se encarga de esto por sí sola.\nDebería utilizarse también para tareas de clasificación y regresión multietiqueta.\n\n\n\n2.2.5 Otras funciones de activación\nExisten más funciones de activación pero hemos preferido centrarnos en estas porque son las de uso más habitual. Más adelante introduciremos nuevas funciones de activación cuando sea necesario."
  },
  {
    "objectID": "20_TrainDL.html#funciones-de-pérdida",
    "href": "20_TrainDL.html#funciones-de-pérdida",
    "title": "2  Entrenamiento de la red neuronal",
    "section": "2.3 Funciones de pérdida",
    "text": "2.3 Funciones de pérdida\nCuando se trabaja en un problema de aprendizaje automático o aprendizaje profundo se utilizan funciones de pérdida/coste para optimizar el modelo durante el entrenamiento. El objetivo es casi siempre minimizar la función de pérdida. Cuanto menor sea la pérdida, mejor será el modelo. En las redes neuronales este aspecto resulta muy relevante ya que la predición o salida de la red depende de la estructura de la red (nodos y capas ocultas). En este apartado se presenta la notación y las funciones utilizadas en las redes neuronales tanto para las tareas de clasificación como de regresión.\nPara ejemplificar supongamos que tenemos una red con una arquitectura \\([p,m,1]\\), es decir con \\(p\\) inputs, \\(m\\) neuronas artificiales en una única capa, y un nodo de salida. Consideramos:\n\n\\((X,y)\\) el conjunto de valores (inputs y respuesta) observados para \\(n\\) muestras, de forma que \\(x^{(i)}\\) e \\(y^{(i)}\\) son los inputs y respuesta de la muestra \\(i\\) con \\(i=1,...,n\\).\n\\(W\\) la matriz de pesos de la red.\n\\(f\\) la función de activación utilizada para obtener la respuesta para un input dado como\n\n\\[\\hat{y}^{(i)} = f(x^{(i)};W)\\]\nDefinimos entonces la función de pérdida (\\(L\\)) para una muestra específica como\n\\[L(\\hat{y}^{(i)}, y^{(i)})= L(f(x^{(i)};W), y^{(i)})\\]\nde forma que la pérdida empírica para el conjunto de muestras dado viene dada para una matriz de pesos \\(W\\) como\n\\[J(W)=\\frac{1}{n} \\sum_{i=1}^n L(f(x^{(i)};W), y^{(i)})\\]\nA continuación, veremos la expresión específica para la pérdida empírica en función del tipo de repuesta que tratamos de predecir con la red o tipo de tarea que deseamos resolver: clasificación binaria, regresión o clasificación múltiple.\n\n2.3.1 Clasificación binaria\nPara tareas de clasificación binaria donde la respuesta observada \\(y^{(i)}\\) para cada una de las muestras consideradas sólo toma valores 0 o 1, se define la pérdida de entropía cruzada binaria (“Binary Cross-Entropy”) como:\n\\[\\begin{eqnarray}\nJ(W) =&-\\frac{1}{n}\\sum_{i=1}^n \\left [ y^{(i)}log(f(x^{(i)};W)) + (1-y^{(i)})log(1-f(x^{(i)};W))\\right ]\\\\\n=&-\\frac{1}{n}\\sum_{i=1}^n \\left [ y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})\\right ]\n\\end{eqnarray}\\]\ndonde \\(p^{(i)} = f(x^{(i)};W)\\) es la probabilidad predicha de clasificación de la clase 1 mediante la red neuronal considerada con pesos estimados \\(W\\).\nLa principal ventaja de esta función de pérdida es que es diferenciable, pero entre las deventajas debemos destacar que tiene múltiples mínimos locales y que no es una medida de error muy intuitiva.\n\n\n2.3.2 Clasificación múltiple\nPara tareas de clasificación múltiple donde la respuesta observada \\(y^{(i)}\\) para cada una de las muestras consideradas puede tomar valores en el conjunto \\(\\{1, 2,...,M\\}\\), se define la pérdida de entropía cruzada binaria para clasificaciones múltiples (“Binary Cross Entropy for Multi-Class classification”) como:\n\\[\\begin{eqnarray}\nJ(W) =&-\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^M \\left [ y^{(i,j)}log(f(x^{(i)};W)) \\right ]\\\\\n&-\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^M \\left [ y^{(i,j)}log(p^{(i,j)}) \\right ]\n\\end{eqnarray}\\]\ndonde \\(y^{(i,j)}\\) es el elento \\(j\\) del vector \\((y^{(i,1)}, y^{(i,2)},..., y^{(i,M)})\\) que representa la codificación hot-encoding para la respuesta \\(y^{(i)}\\), y \\(p^{(i,j)}\\) representa la probabilidad predicha por la red neuronal de que la muestra \\(i\\) pertenezca a la clase \\(j\\).\n\n\n2.3.3 Predicción\nEn tareas de predicción podemos considerar diferentes funciones de pérdida.\n\n2.3.3.1 Error cuadrático medio\nEl error cuadrático medio o mean squared error (MSE) es la función de pérdida más sencilla y común para evaluar la solución obtenida en una tarea de predicción. Si \\(y^{(i)}\\) denota el valor real de la repuesta e \\(\\hat{y}^{(i)}\\) el valor predicho mediante la red considerada con pesos W, para calcular el MSE, se toma la diferencia entre el valor real y la predicción del modelo, se eleva al cuadrado y se calcula la media de todo el conjunto de datos:\n\\[\\begin{eqnarray}\nJ(W) =&-\\frac{1}{n}\\sum_{i=1}^n \\left ( y^{(i)}-f(x^{(i)};W) \\right )^2\\\\\n&-\\frac{1}{n}\\sum_{i=1}^n \\left ( y^{(i)}-\\hat{y}^{(i)} \\right )^2\n\\end{eqnarray}\\]\nLas principales ventajas de esta función de pérdida son que es muy fácil de interpretar, siempre es diferenciable y sólo tiene un mínimo local. Entre las desventajas podemos decir que el error está en unidades al cuadrado, y que es muy sensible a la presencia de observaciones anómalas.\n\n\n2.3.3.2 Error absoluto medio\nEl error absoluto medio o mean absolute error (MAE) es otra función de pérdida muy sencilla. Si \\(y^{(i)}\\) denota el valor real de la repuesta e \\(\\hat{y}^{(i)}\\) el valor predicho mediante la red considerada con pesos W, para calcular el MAE, se toma la diferencia en valor absoluto entre el valor real y la predicción del modelo, promediando para todo el conjunto de datos:\n\\[\\begin{eqnarray}\nJ(W) =&-\\frac{1}{n}\\sum_{i=1}^n  \\left |y^{(i)}-f(x^{(i)};W) \\right |\\\\\n&-\\frac{1}{n}\\sum_{i=1}^n \\left | y^{(i)}-\\hat{y}^{(i)} \\right |\n\\end{eqnarray}\\]\nLas principales ventajas de esta función de pérdida son que su resultado está en las mismas unidades que la variable respuesta, y que es insensible a observaciones anómalas. La principal desventaja es que la función de pérdida no es diferenciable y no se puede utilizar directamente en procesos de optimización."
  },
  {
    "objectID": "20_TrainDL.html#proceso-de-optimización",
    "href": "20_TrainDL.html#proceso-de-optimización",
    "title": "2  Entrenamiento de la red neuronal",
    "section": "2.4 Proceso de optimización",
    "text": "2.4 Proceso de optimización\nComo hemos visto en el punto anterior todas las funciones de pérdida dependen de los valores de los pesos \\(W\\) de la arquitectura de red considerada. Por tanto, para optimizar la arquitectura de dicha red es necesario obtener los valores de \\(W\\) que minimicen el error de predicción o clasificación dependiendo de la tarea de interés. En el cuaderno siguiente estudiaremos con detalle los algoritmos de optimización utilizados habitualmente, pero en este presentanmos las ideas generales. Distinguimos entre arquitecturas de redes con una única capa y otras con múltiples capas ocultas.\n\n2.4.1 Arquitectura de red con una capa oculta\nSupongamos que disponemos de \\(n\\) muestras con una arquitectura de red \\([p,m,1]\\) con matriz de datos \\((X,y)\\), \\(W\\) matriz de pesos, y \\(f\\) la función de activación. Independientemente de si la tarea es de clasificación o predicción, para obtener los valores óptimos de la matriz de pesos de la red, \\(W^*\\), debemos minimizar la función de pérdida empírica considerada con respecto a \\(W\\), es decir:\n\\[W^* = \\underset{W}{argmin} \\quad J(W)\\]\n\\[W^* = \\underset{W}{argmin} \\quad \\frac{1}{n} L(f(x^{(i)};W),y^{(i)})\\]\nPor tanto, para encontrar el óptimo queda claro que necesitamos la derivadas de la función de activación considerada.\n\n\n2.4.2 Arquitectura de red con múltiples capas ocultas\nSi tenemos una arquitectura de red con múltiples capas ocultas la matriz de pesos se organiza para las diferentes capas consideradas, es decir consideramos:\n\\[W = \\{W^{(0)}, W^{(1)},...\\}\\]\ndonde \\(W^{(i)}\\) es la matriz de pesos correspondiente a la capa oculta \\(i+1\\). Recordemos que el proceso de propagación hacia adelante utiliza la salida de una capa oculta para valorar la activación de la capa siguiente en función de los pesos de dicha capa. En esta situación el proceso de optimización viene dado por:\n\\[W^* = \\underset{W}{argmin} \\quad J(W)\\]\n\\[W^* = \\underset{W}{argmin} \\quad \\frac{1}{n} L(f(x^{(i)};W),y^{(i)})\\]\ndonde \\(W = \\{W^{(0)}, W^{(1)},...\\}\\).\nEn este caso el proceso de optimización resulta algo más complejo, ya que debemos enlazar las derivadas de las funciones de activación en cada una de las capas ocultas para obtener los pesos de la arquitectura de la red. Es lo que denominamos como algortimo de Backpropagation para combinar el proceso de optimización de las diferentes capas, mientras que utilizamos el método del gradiente descendente para estimar los pesos de una capa en específico."
  },
  {
    "objectID": "20_TrainDL.html#algoritmo-de-backpropagation",
    "href": "20_TrainDL.html#algoritmo-de-backpropagation",
    "title": "2  Entrenamiento de la red neuronal",
    "section": "2.5 Algoritmo de backpropagation",
    "text": "2.5 Algoritmo de backpropagation\nBackpropagation es el algoritmo que permite cuantificar la influencia que tiene cada peso y bias de la red en sus predicciones. Para conseguirlo, hace uso de la regla de la cadena (chain rule) para calcular el gradiente, que no es más que el vector formado por las derivadas parciales de una función.\nEn el caso de las redes, la derivada parcial del error respecto a un parámetro (peso o bias) mide cuanta “responsabilidad” ha tenido ese parámetro en el error cometido. Gracias a esto, se puede identificar qué pesos de la red hay que modificar para mejorarla.\nDe hecho el objetivo principal de este algoritmo es encontrar las derivadas de la pérdida o el error con respecto a cada peso de la red, y actualizar estos pesos en la dirección opuesta a sus derivadas respetadas, de modo que se muevan hacia los mínimos globales o locales de la función de coste o error.\nPara entender mejor el funcionamiento de este algoritmo, y ver cómo integrar los algortimos de optimización de pesos en el proceso, presentamos su funcionamiento teórico para una arquitectura de red con una capa oculta.\nPor simplicidad consideramos la red que viene representada en la figura siguiente:\n\n\n\n\n\ndonde tenemos una red con una única capa coculta, y con función de pérdida empírica para la matriz de pesos \\(W=\\{W1,W2\\}\\) dada por \\(J(W)\\). Como ya vimos en el cuaderno anterior el objetivo es encontrar los valores de \\(W\\) que minimizan la función de pérdida. Utilizando la regla de la cadena el proceso de optimización se puede escribir como:\n\\[\\frac{\\partial J(W)}{\\partial W1} = \\frac{\\partial J(W)}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial Z} \\frac{\\partial Z}{\\partial W1}\\]\nEste proceso se repite para cada peso de la red utilizando los gradientes de capas posteriores.\nPor tanto, para resolver el entrenamiento del modelo es necesario determinar los gradientes anteriores actualizando los pesos de cada capa de la red para reducir la función de pérdida considerada."
  },
  {
    "objectID": "20_TrainDL.html#algoritmos-de-optimización",
    "href": "20_TrainDL.html#algoritmos-de-optimización",
    "title": "2  Entrenamiento de la red neuronal",
    "section": "2.6 Algoritmos de optimización",
    "text": "2.6 Algoritmos de optimización\nLos algoritmos optimizadores son métodos de optimización que ayudan a mejorar el rendimiento de un modelo de aprendizaje profundo. Estos algoritmos de optimización u optimizadores afectan en gran medida a la precisión y la velocidad de entrenamiento del modelo de aprendizaje profundo. Pero antes de nada, surge la pregunta de qué es un optimizador.\nMientras se entrena el modelo de aprendizaje profundo los optimizadores modifican los pesos de cada iteración (epoch) y minimizan la función de pérdida. Un optimizador es una función o un algoritmo que ajusta los atributos de la red neuronal, como los pesos y las tasas de aprendizaje. De este modo, ayuda a reducir la pérdida global y a mejorar la precisión. El problema de elegir los pesos adecuados para el modelo es una tarea de enormes proporciones, ya que un modelo de aprendizaje profundo suele constar de millones de parámetros. Esto plantea la necesidad de elegir un algoritmo de optimización adecuado para su aplicación. De ahí que la comprensión de estos algoritmos de aprendizaje automático sea necesaria para los científicos de datos antes de sumergirse a fondo en este campo.\nSe pueden utilizar diferentes optimizadores en el modelo de aprendizaje automático para cambiar sus pesos y su tasa de aprendizaje. Sin embargo, elegir el mejor optimizador depende de la aplicación. Como principiante, un mal pensamiento que nos viene a la mente es que probamos todas las posibilidades y elegimos la que muestra los mejores resultados. Esto puede estar bien al principio, pero cuando se trata de cientos de gigabytes de datos, incluso una sola epoch puede llevar un tiempo considerable. Así que elegir un algoritmo al azar puede resultar en una pérdida de tiempo.\nEste apartado cubrirá varios optimizadores de aprendizaje profundo, como Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent, Adagrad, RMSProp, AdaDelta, y Adam.\nAntes de continuar, recordemos algunos términos con los que nos debemos familiarizar:\n\nEpoch (Época) - Denota el número de veces que el algoritmo se ejecuta en todo el conjunto de datos de entrenamiento.\nBatch (Lote) - Número de muestras que se tomarán para actualizar los parámetros del modelo.\nLearning rate (Tasa de aprendizaje) - Es un parámetro que proporciona al modelo una escala de cuánto deben actualizarse los pesos del modelo.\n\n\n2.6.1 Descenso del gradiente\nEl algortimo de descenso del gradiente puede considerarse como el más famoso de todos los optimizadores. En concreto permite minimizar una función haciendo actualizaciones de sus parámetros en la dirección del valor negativo de su gradiente. Aplicado a las redes neuronales y, como ya vimos en el cuaderno de introducción a las redes neuronales, este algoritmo se implementa de la forma siguiente:\n\nFijamos unos pesos inciales \\(W\\) y evaluamos la función de pérdida correspondiente.\nHasta alcanzar convergencia, es decir, hasta alcanzar un mínimo local, procedemos de la siguiente forma fijando una tasa de aprendizaje \\(\\eta\\):\n\n\nCalcular el gradiente \\(\\frac{\\partial J(W)}{\\partial W_t}\\)\nActualizar los pesos mediante la expresión siguiente y reevaluar la función de pérdida\n\n\\[ W_{t+1} = W_t - \\eta  \\frac{\\partial J(W)}{\\partial W_t}\\]\n\nUna vez alcanzada la convergencia devolvemos los pesos de la última iteración y el valor de la función de pérdida corrrespondiente.\n\nEl algoritmo de descenso del gradiente funciona bien en la mayoría de situaciones pero, sin embargo, también tiene algunos inconvenientes:\n\nEs costoso calcular los gradientes si el tamaño de los datos es enorme.\nEl descenso de gradiente funciona bien para funciones convexas, pero no sabe qué distancia recorrer a lo largo del gradiente para funciones no convexas.\nHay que fijar una tasa de aprendizaje que puede afectar en gran medida al proceso de optimización, ya que en ocasiones puede ralentizar mucho el proceso o nos puede llevar a un mínimo local que no es óptimo.\n\nLa función siguiente nos permite ver el funcionamiento del descenso del gradiente para un learning rate y función específica.\n\n# Función para visualizar el algoritmo del gradiente descendente\nver_descenso_gradiente = function(learning_rate, f)\n{\n    # Visualización gráfica del método del descenso del gradiente\n\n    # learning_rate: ratio de aprendizaje\n    # f: función a optimizar\n\n    # return: solución gráfica del algoritmo\n    \n      # Número máximo de iteraciones\n      maximum_iterations = 1000\n      # Iteración actual\n      current_iteration = 0\n      # precisión de la solución\n      precision_value=1e-6\n      previous_step_size = 0.5\n      current_x_value = 1\n      h = 0.01\n      x_iterativo=c()\n      fx_iterativo=c()\n      while((previous_step_size>precision_value) & (current_iteration<maximum_iterations))\n      {\n            previous_x_value = current_x_value\n            #versión numérica\n            gradient_of_y=(f(current_x_value + h) - f(current_x_value - h))/(2*h)\n            current_x_value = current_x_value - learning_rate * gradient_of_y\n            x_iterativo = c(x_iterativo, current_x_value)\n            previous_step_size = abs(current_x_value - previous_x_value)\n            fx_iterativo = c(fx_iterativo, f(current_x_value))\n            current_iteration = current_iteration + 1\n        }\n        x = seq(-2, 5, length=100)\n        plot(x, f(x),\"l\", xlab=\"x\",ylab=\"f(x)\", main =paste(\"LR= \",learning_rate))\n        lines(x_iterativo, fx_iterativo,col=\"red\",lwd=3)\n        points(x_iterativo, fx_iterativo,col=\"red\",pch=16)\n}\n\nVeamos si el algoritmo es capaz de llegar al mínimo de la función \\(f(x) = x^2-6x+5\\)\n\n# Definimos la función objetivo\nf1x = function(x)\n{\n  return(x*x-6*x+5)\n}\n\n# Descenso del gradiente para diferentes tasas de aprendizaje\nlr =c(1,0.9,0.5,0.1,0.01,0.001, 0.0001,0.00001)\nfor(i in 1:length(lr))\n{\n  ver_descenso_gradiente(lr[i], f1x)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe los resultados obtenidos podemos ver que la tasa de aprendizaje juega un papel relevante en la convergencia del algoritmo, ya que en ocasiones necesitamos las 1000 iteraciones prefijadas y no alcanzamos el óptimo, mientras que en otras situaciones con pocas iteraciones somos capaces de alcanzar el óptimo. Incluso con una función tan sencilla como esta el algoritmo muestra comportamientos inadecuados.\n\n\n2.6.2 Descenso del gradiente estocástico\nAl final de la sección anterior, vimos que utilizar el descenso de gradiente podría no ser la mejor opción para encontrar el óptimo de la función. Para abordar el problema se plantea el algoritmo del descenso de gradiente estocástico. El término estocástico proviene de la aleatoriedad en la que se basa el algoritmo. En el descenso del gradiente estocástico, en lugar de tomar todo el conjunto de datos para cada iteración, seleccionamos aleatoriamente los lotes de datos. Esto significa que sólo tomamos unas pocas muestras del conjunto de datos.\nEl procedimiento consiste en seleccionar primero los parámetros iniciales y la tasa de aprendizaje para, a continuación, barajar aleatoriamente los datos en cada iteración para alcanzar un mínimo aproximado.\nDado que no utilizamos todo el conjunto de datos, el camino que recorre el algoritmo está lleno de ruido en comparación con el algoritmo del descenso del gradiente. Por lo tanto, SGD utiliza un mayor número de iteraciones para alcanzar los mínimos locales. Al aumentar el número de iteraciones, aumenta el tiempo total de cálculo. Pero incluso después de aumentar el número de iteraciones, el coste computacional sigue siendo menor que el del optimizador de descenso del gradiente.\nA continuación vemos la estructura del algoritmo:\n\nFijamos unos pesos inciales \\(W\\) y evaluamos la función de pérdida correspondiente.\nHasta alcanzar convergencia, es decir, hasta alcanzar un mínimo local, procedemos de la siguiente forma fijando una tasa de aprendizaje \\(\\eta\\):\n\n\nElegimos aleatoriamente un punto \\(i\\) de la muestra.\nCalcular el gradiente sobre dicho punto \\(\\frac{\\partial J_i(W)}{\\partial W_t}\\)\nActualizar los pesos\n\n\\[ W_{t+1} = W_{t} - \\eta  \\frac{\\partial J_i(W)}{\\partial W_t}\\]\n\nUna vez alcanzada la convergencia devolvemos los pesos de la última iteración y el valor de la función de pérdida correspondiente.\n\n\n\n2.6.3 Descenso del gradiente por mini lotes\nEn esta variante del algortimo de descenso del gradiente, en lugar de tomar todos los datos de entrenamiento, sólo se utiliza un subconjunto del conjunto de datos para calcular la función de pérdida. Dado que utilizamos un lote de datos en lugar de todo el conjunto de datos, se necesitan menos iteraciones. Por este motivo, el algoritmo de descenso del gradiente por mini lotes es más rápido que los algoritmos de descenso del gradiente anteriores. Además es más eficiente y robusto que las variantes anteriores del descenso del gradiente. Como el algoritmo utiliza el procesamiento por lotes, no es necesario cargar todos los datos de entrenamiento en la memoria, lo que hace que el proceso sea más eficiente de implementar. Además, la función de coste del algoritmo de descenso del gradiente contiene más variabilidad que las de los algortimos anteriores pero es más suave que la del algoritmo de descenso del gradiente estocástico. Por todo ello, el descenso del gradiente por mini lotes es ideal y proporciona un buen equilibrio entre velocidad y precisión.\nOtra ventaja importante es que la estructura del algoritmo permite la paralelización de cálculos, lo que permite acelerar todavía más el proceso de aprendizaje.\nA pesar de todo, el algoritmo de descenso del gradiente por mini lotes también tiene algunas desventajas. Necesita un hiperparámetro que es el “tamaño de mini lotes”, que debe ajustarse para lograr la precisión requerida. Aunque el tamaño de lote de 32 se considera adecuado para casi todos los casos. Además, en algunos casos, resulta en una precisión final pobre. Por ello, es necesario buscar también otras alternativas.\nA continuación vemos la estructura del algoritmo:\n\nFijamos unos pesos inciales \\(W\\) y evaluamos la función de pérdida correspondiente.\nHasta alcanzar convergencia, es decir, hasta alcanzar un mínimo local, procedemos de la siguiente forma fijando una tasa de aprendizaje \\(\\eta\\):\n\n\nElegimos aleatoriamente un lote de tamaño \\(B\\) de muestras.\nCalcular el gradiente como\n\n\\[\\frac{\\partial J_i(W)}{\\partial W_t} = \\frac{1}{B} \\sum_{k=1}^B \\frac{\\partial J_k(W)}{\\partial W_t}\\]\n\nActualizar los pesos\n\n\\[ W_{t+1} = W_{t} - \\eta  \\frac{\\partial J_i(W)}{\\partial W_t}\\]\n\nUna vez alcanzada la convergencia devolvemos los pesos de la última iteración y el valor de la función de pérdida corrrespondiente.\n\n\n\n2.6.4 Descenso del gradiente adaptativo (Adagrad)\nEl algoritmo de descenso del gradiente adaptativo es ligeramente diferente de otros algoritmos de descenso del gradiente. Esto se debe a que utiliza diferentes tasas de aprendizaje para cada iteración. El cambio en la tasa de aprendizaje depende de la diferencia en los parámetros durante el entrenamiento. Cuanto más cambian los parámetros, menos cambia la tasa de aprendizaje. Esta modificación es muy beneficiosa porque los conjuntos de datos del mundo real contienen características tanto dispersas como densas. Por lo tanto, es injusto tener el mismo valor de tasa de aprendizaje para todas las características.\nEl algoritmo Adagrad utiliza la siguiente fórmula para actualizar los pesos, donde \\(\\eta_t\\) denota las diferentes tasas de aprendizaje en cada iteración, \\(\\eta\\) es la tasa de aprendizaje inicial que actúa como una constante, y \\(\\epsilon\\) es un valor positivo pequeño para evitar la división por 0:\n\\[W_t = W_{t-1} - \\alpha_t \\frac{\\partial J(W)}{\\partial W_{t-1}}\\]\n\\[\\alpha_t = \\frac{\\eta}{\\sqrt{\\eta_t + \\epsilon}}\\]\nEl algoritmo completo queda de la forma siguiente:\n\nFijamos unos pesos inciales \\(W\\), una tasa de aprendizaje \\(\\eta\\), un valor de \\(\\epsilon\\) y evaluamos la función de pérdida correspondiente.\nHasta alcanzar convergencia, es decir, hasta alcanzar un mínimo local, procedemos de la siguiente forma en cada iteración \\(t\\):\n\n\nCalcular el gradiente\n\n\\[\\frac{\\partial J(W)}{\\partial W_{t}}\\]\n\nActualizar las tasas de aprendizaje \\(\\alpha_t\\)\n\n\\[\\alpha_{t+1} = \\frac{\\eta}{\\sqrt{\\eta_t + \\epsilon}}\\]\n\nActualizar los pesos\n\n\\[ W_{t+1} = W_{t} - \\alpha_t \\frac{\\partial J(W)}{\\partial W_{t}}\\]\n\nUna vez alcanzada la convergencia devolvemos los pesos de la última iteración y el valor de la función de pérdida corrrespondiente.\n\nLa ventaja de utilizar Adagrad es que elimina la necesidad de modificar manualmente la tasa de aprendizaje. Es más fiable que los algoritmos de descenso gradiente y sus variantes, y alcanza la convergencia a mayor velocidad.\nUn inconveniente del optimizador AdaGrad es que disminuye la tasa de aprendizaje de forma agresiva y monotónica. Puede llegar un momento en que la tasa de aprendizaje sea extremadamente pequeña. Esto se debe a que los gradientes al cuadrado en el denominador siguen acumulándose y, por tanto, la parte del denominador sigue aumentando. Debido a las pequeñas tasas de aprendizaje, el modelo acaba siendo incapaz de adquirir más conocimientos y, por tanto, la precisión del modelo se ve comprometida.\n\n\n2.6.5 RMS Prop\nRMS Prop es uno de los optimizadores más populares entre los entusiastas del aprendizaje profundo. Esto se debe quizás a que no ha sido publicado pero sigue siendo muy conocido en la comunidad. RMS Prop es idealmente una extensión del trabajo RPPROP. Resuelve el problema de los gradientes variables. El problema de los gradientes es que algunos son pequeños mientras que otros pueden ser enormes. Por lo tanto, definir una única tasa de aprendizaje puede no ser la mejor idea. RPPROP utiliza el signo del gradiente, adaptando el tamaño del paso individualmente para cada peso. En este algoritmo, primero se comparan los signos de los dos gradientes. Si tienen el mismo signo, vamos en la dirección correcta, aumentando el tamaño del paso en una pequeña fracción. Si tienen signos opuestos, debemos disminuir el tamaño del paso. Entonces limitamos el tamaño del paso y ya podemos pasar a la actualización del peso.\nEl problema con RPPROP es que no funciona bien con grandes conjuntos de datos y cuando queremos realizar actualizaciones en mini lotes. Por lo tanto, lograr la robustez de RPPROP y la eficiencia de los mini-lotes simultáneamente fue la principal motivación detrás del surgimiento de RMS Prop. RMS Prop es un avance en el optimizador AdaGrad, ya que reduce la tasa de aprendizaje monotónicamente decreciente.\nEl algoritmo se centra principalmente en acelerar el proceso de optimización disminuyendo el número de evaluaciones de la función para alcanzar el mínimo local. El algoritmo mantiene la media móvil de los gradientes al cuadrado para cada peso y divide el gradiente por la raíz cuadrada del cuadrado medio. La actualización de los pesos se obtiene como:\n\\[W_{t+1} = W_{t} - \\eta_t \\frac{\\partial J(W)}{\\partial W_t}\\]\ncon\n\\[\\eta_t = \\frac{\\eta}{\\sqrt{v_{t+1}}+\\epsilon}\\]\n\\[v_{t+1} = \\alpha v_t + \\left(1-\\alpha \\left[\\frac{\\partial J(W)}{\\partial W_t}\\right]^2\\right)\\]\ndonde \\(\\eta\\) es la taza global de aprendizaje, \\(\\epsilon\\) es un valor infinitesimal (del orden de \\(10^{-7}\\) o \\(10^{-8}\\)) para evitar errores de división por cero, y \\(v_{t+1}\\) es la estimación del segundo momento.\nEn términos más sencillos, si existe un parámetro debido al cual la función de coste oscila mucho, queremos penalizar la actualización de este parámetro. Supongamos que ha construido un modelo para clasificar una variedad de peces. El modelo se basa principalmente en el factor “color” para diferenciar los peces. Debido a esto, comete muchos errores. Lo que hace RMS Prop es penalizar el parámetro “color” para que pueda basarse también en otras características. Esto evita que el algoritmo se adapte demasiado rápido a los cambios en el parámetro “color” en comparación con otros parámetros. Este algoritmo tiene varias ventajas en comparación con las versiones anteriores de los algoritmos de descenso del gradiente. El algoritmo converge rápidamente y requiere menos ajustes que los algoritmos de descenso del gradiente y sus variantes.\nEl problema con RMS Prop es que la tasa de aprendizaje tiene que definirse manualmente, y el valor sugerido no funciona para todas las aplicaciones.\n\n\n2.6.6 AdaDelta\nAdaDelta puede considerarse una versión más robusta del optimizador AdaGrad. Se basa en el aprendizaje adaptativo y está diseñado para hacer frente a importantes inconvenientes del optimizador AdaGrad y RMS Prop. El principal problema de los dos optimizadores anteriores es que la tasa de aprendizaje inicial debe definirse manualmente. Otro problema es la tasa de aprendizaje decreciente, que llega a ser infinitesimalmente pequeña en algún momento. Debido a esto, un cierto número de iteraciones más tarde, el modelo ya no puede aprender nuevos conocimientos.\n\n\n2.6.7 Adam\nEl nombre Adam procede de adaptive moment estimation (estimación adaptativa del momento). Este algoritmo de optimización es una extensión del descenso del gradiente estocástico para actualizar los pesos de la red durante el entrenamiento. A diferencia de mantener una única tasa de aprendizaje durante el entrenamiento con el descenso del gradiente estocástico (SGD), el optimizador Adam actualiza la tasa de aprendizaje para cada peso de red individualmente. Los creadores del algoritmo de optimización Adam conocen las ventajas de los algoritmos AdaGrad y RMSProp, que también son extensiones de los algoritmos de descenso del gradiente estocástico. De ahí que los optimizadores Adam hereden las características de los algoritmos Adagrad y RMSProp. En Adam, en lugar de adaptar las tasas de aprendizaje basándose en el primer momento (media) como en RMS Prop, también utiliza el segundo momento de los gradientes. Nos referimos a la varianza no centrada por el segundo momento de los gradientes (no restamos la media).\nEl optimizador Adam tiene varias ventajas, por lo que se utiliza ampliamente. Se ha adaptado como punto de referencia para trabajos de aprendizaje profundo y se recomienda como algoritmo de optimización por defecto. Además, el algoritmo es fácil de implementar, tiene un tiempo de ejecución más rápido, bajos requisitos de memoria y requiere menos ajustes que cualquier otro algoritmo de optimización.\nLa actualización de los pesos viene dada por la expresión:\n\\[w_{t+1} = w_t - \\eta \\frac{m_t}{\\sqrt{v_{t+1} + \\epsilon}}\\]\ndonde\n\\[m_{t+1}  = \\beta m_t + (1-\\beta)\\frac{\\partial J(W)}{\\partial W_t}\\]\n\\[v_{t+1} = \\alpha v_t + (1-\\alpha)\\left[\\frac{\\partial J(W)}{\\partial W_t}\\right]^2\\]\ncon \\(v_{t+1}\\) es la estimación del segundo momento, y \\(m_{t+1}\\) es el promedio exponencial del momento.\n\n\n\n\n\n\nPara complemetar los aspectos teóricos de los diferentes algortimos de optimización y otros que no hemos presentado en este apartado se puede consultar el libro gratuito que aparece en este enlace http://d2l.ai/chapter_optimization/index.html.."
  },
  {
    "objectID": "20_TrainDL.html#hiperparámetros-de-la-red",
    "href": "20_TrainDL.html#hiperparámetros-de-la-red",
    "title": "2  Entrenamiento de la red neuronal",
    "section": "2.7 Hiperparámetros de la red",
    "text": "2.7 Hiperparámetros de la red\nLa gran “flexibilidad” que tienen las redes neuronales es un arma de doble filo. Por un lado, son capaces de generar modelos que aprenden relaciones muy complejas, sin embargo, sufren fácilmente el problema de sobreajuste (overfitting) lo que los incapacita al tratar de predecir nuevas observaciones. La forma de minimizar este problema y conseguir modelos útiles pasa por configurar de forma adecuada sus hiperparámetros.\n\n2.7.1 Número y tamaño de capas\nLa arquitectura de una red, el número de capas y el número de neuronas que forman parte de cada capa, determinan en gran medida la complejidad del modelo y con ello su potencial capacidad de aprendizaje.\nLas capas de entrada y salida son sencillas de establecer. La capa de entrada tiene tantas neuronas como predictores y la capa de salida tiene una neurona en problemas de regresión y tantas como clases en problemas de clasificación. En la mayoría de implementaciones, estos valores se establecen automáticamente en función del conjunto de entrenamiento. El usuario suele especificar únicamente el número de capas intermedias (ocultas) y el tamaño de las mismas.\nCuantas más neuronas y capas, mayor la complejidad de las relaciones que puede aprender el modelo. Sin embargo, dado que en cada neurona está conectada por pesos al resto de neuronas de las capas adyacentes, el número de parámetros a aprender aumenta y con ello el tiempo de entrenamiento.\n\n\n2.7.2 Ratio de aprendizaje\nEl learning rate o ratio de aprendizaje establece cómo de rápido pueden cambiar los parámetros de un modelo a medida que se optimiza (aprende). Este hiperparámetro es uno de los más complicados de establecer, ya que depende mucho de los datos e interacciona con el resto de hiperparámetros. Si el learning rate es muy grande, el proceso de optimización puede ir saltando de una región a otra sin que el modelo sea capaz de aprender. Si por el contrario, el learning rate es muy pequeño, el proceso de entrenamiento puede tardar demasiado y no llegar a completarse. Algunas de las recomendaciones heurísticas basadas en prueba y error son:\n\nUtilizar un learning rate lo más pequeño posible siempre y cuando el tiempo de entrenamiento no supere las limitaciones temporales disponibles.\nNo utilizar un valor constante de learning rate durante todo el proceso de entrenamiento. Por lo general, utilizar valores mayores al inicio y pequeños al final.\n\nDe hecho, el algoritmo de optimización establecido para el proceso de entrenamiento ya implementa las diferentes posibilidades de ratio de aprendizaje."
  },
  {
    "objectID": "20_TrainDL.html#regularización",
    "href": "20_TrainDL.html#regularización",
    "title": "2  Entrenamiento de la red neuronal",
    "section": "2.8 Regularización",
    "text": "2.8 Regularización\nLas redes neuronales pueden aprender a representar relaciones complejas entre las entradas y salidas de la red. Este poder de representación las ayuda a rendir mejor que los algoritmos tradicionales de aprendizaje automático en tareas de visión por ordenador y procesamiento del lenguaje natural. Sin embargo, uno de los retos asociados al entrenamiento de redes neuronales es el sobreajuste.\nCuando una red neuronal se adapta en exceso al conjunto de datos de entrenamiento, aprende una representación excesivamente compleja que modela el conjunto de datos de entrenamiento demasiado bien. Como resultado, su rendimiento es excepcional en el conjunto de datos de entrenamiento, pero su generalización a los datos de prueba es deficiente.\nLas técnicas de regularización ayudan a mejorar la capacidad de generalización de una red neuronal reduciendo el sobreajuste. Para ello, minimizan la complejidad innecesaria y exponen la red a datos más diversos. A continuación hacemos un repaso de las técnicas de regularización más habituales en el periodo de entrenamiento de la red.\n\n2.8.1 Parada temprana (early stopping)\nLa parada temprana es una de las técnicas de regularización más sencillas e intuitivas. Consiste en detener el entrenamiento de la red neuronal en una época anterior, de ahí su nombre.\nPero, ¿cómo y cuándo se detiene? A medida que se entrena la red neuronal durante muchas épocas, el error de entrenamiento disminuye.\nSi el error de entrenamiento es demasiado bajo y se acerca arbitrariamente a cero, la red se ajustará en exceso al conjunto de datos de entrenamiento. Una red neuronal de este tipo es un modelo de alta varianza que funciona mal en datos de prueba que nunca ha visto antes a pesar de su rendimiento casi perfecto en las muestras de entrenamiento.\nPor lo tanto, heurísticamente, si podemos evitar que la pérdida de entrenamiento sea arbitrariamente baja, es menos probable que el modelo se ajuste en exceso al conjunto de datos de entrenamiento y generalizará mejor.\n¿Cómo lo hacemos en la práctica?\n\n2.8.1.1 Métricas de validación\nUn método sencillo consiste en controlar métricas como el error de validación y la precisión de validación a medida que avanza el entrenamiento de la red neuronal, y utilizarlas para decidir cuándo parar.\nSi vemos que el error de validación no disminuye significativamente o aumenta en un intervalo de épocas, digamos p épocas, podemos detener el entrenamiento. También podemos reducir la tasa de aprendizaje y entrenar unas cuantas épocas más antes de parar. En la imagen siguiente (extraída de https://www.pinecone.io/learn/regularization-in-neural-networks/) podemos ver el cambio en las métricas y el punto de parada temprana.\n\n\n\n\n\nDe forma equivalente, se puede pensar en términos de la precisión de la red neuronal en los conjuntos de datos de entrenamiento y validación. Detenerse antes de tiempo cuando el error de validación empieza a aumentar (o deja de disminuir) equivale a detenerse cuando la precisión de validación empieza a disminuir.\n #### Monitorizando el cambio en el vector de pesos\nOtra forma de saber cuándo parar es controlar el cambio en los pesos de la red. Sean \\(w^{(t)}\\) y \\(w^{(t-k)}\\) los vectores de pesos en las épocas \\(t\\) y \\(t-k\\) respectivamente. Podemos calcular la norma l2 para la diferencia de los vectores anteriores y detener el entrenamiento cuando esta sea suficientemente pequeña, digamos una cantidad \\(epsilon\\), es decir:\n\\[||w^{(t)} - w^{(t-k)}|| < \\epsilon\\]\nPero este enfoque de utilizar la norma del vector diferencia no es muy fiable. ¿Por qué? Algunos pesos pueden haber cambiado mucho en las últimas k épocas, mientras que otros pueden haber sufrido cambios insignificantes. Por lo tanto, la norma del vector diferencia resultante puede ser pequeña a pesar del cambio drástico en ciertos componentes del vector peso.\nUn enfoque mejor es calcular el cambio en componentes individuales del vector de pesos. Si el cambio máximo (en todos los componentes) es inferior a \\(\\epsilon\\) podemos concluir que los pesos no están cambiando significativamente, por lo que podemos detener el entrenamiento de la red neuronal. Matemáticamente:\n\\[\\underset{i}{max} |w_i^{(t)} - w_i^{(t-k)}| < \\epsilon\\]\n\n\n\n2.8.2 Aumento de datos (data augmentation)\nEl aumento de datos es una técnica de regularización que ayuda a una red neuronal a generalizar mejor exponiéndola a un conjunto más diverso de ejemplos de entrenamiento. Como las redes neuronales profundas requieren un gran conjunto de datos de entrenamiento, el aumento de datos también es útil cuando no tenemos datos suficientes para entrenar una red neuronal.\nTomemos el ejemplo del aumento de datos de imágenes. Supongamos que tenemos un conjunto de datos con N ejemplos de entrenamiento en C clases. Podemos aplicar ciertas transformaciones a estas N imágenes para construir un conjunto de datos mayor.\n\n\n\n\n\n¿Qué es una transformación válida? Es cualquier operación que no altere la etiqueta original de los datos. Por ejemplo, un panda es un panda, esté mirando a la derecha o a la izquierda, situado cerca del centro de la imagen o en una de las esquinas.\nEn resumen: podemos aplicar cualquier transformación invariante de la etiqueta para realizar el aumento de datos. He aquí algunos ejemplos:\n\nTransformaciones del espacio de color, como el cambio de las intensidades de los píxeles.\nRotación y reflejo.\nInyección de ruido, distorsión y desenfoque.\n\nAdemás de las transformaciones básicas del espacio de color y de la imagen geométrica, existen nuevas técnicas de aumento de la imagen. Mixup es una técnica de regularización que utiliza una combinación convexa de entradas existentes para aumentar el conjunto de datos.\nSupongamos que \\(x_i\\) y \\(x_j\\) son muestras de entrada pertenecientes a las clases \\(i\\) y \\(j\\), respectivamente; \\(y_i\\) y \\(y_j\\) son los vectores unidireccionales correspondientes a las etiquetas de clase \\(i\\) y \\(j\\), respectivamente. Se puede formar forma una nueva imagen tomando una combinación convexa de \\(x_i\\) y \\(x_j\\):\n\\[\\begin{eqnarray}\n\\tilde{x} = &\\lambda x_i +(1-\\lambda)x_j\\\\\n\\tilde{y} = &\\lambda y_i +(1-\\lambda)y_j\\\\\n\\end{eqnarray}\\]\ncon \\(\\lambda \\in [0,1]\\).\nOtros métodos de aumento de datos son Cutout, CutMix y AugMix. Cutout implica la eliminación aleatoria de partes de una imagen de entrada durante el entrenamiento. CutMix sustituye las secciones eliminadas por partes de otra imagen. AugMix es una técnica de regularización que hace que una red neuronal sea robusta a los cambios de distribución. A diferencia de Mixup, que utiliza imágenes de dos clases diferentes, AugMix realiza una serie de transformaciones en la misma imagen y, a continuación, utiliza una composición de estas imágenes transformadas para obtener la imagen resultante.\n\n\n2.8.3 Penalización de pesos\nEn este caso actuamos como en otros muchos algoritmos de aprendiaje automático donde se añaden restricciones o penalizaciones sobre los coeficientes del modelo utilizado. En este caso se trata de introducir penalizaciones sobre los pesos de la red neuronal.\n\n2.8.3.1 Regularización L2\nLa idea detrás de este tipo de regularización es reducir el valor de los parámetros para que sean pequeños. Esta técnica introduce un término adicional de penalización en la función de coste original, añadiendo a su valor la suma de los cuadrados de los parámetros, es decir, consideramos una nueva función de coste que viene dada por:\n\\[J_2(W) = J(W) + \\lambda \\sum w_i^2\\]\nLa mala noticia es que este nuevo término puede ser alto; tanto que la red minimizaría la función de coste haciendo los parámetros muy cercanos a 0, lo que no sería nada conveniente. Es por ello que multiplicaremos ese sumando por una constante (\\(\\lambda\\)) pequeña, cuyo valor escogeremos de forma arbitraria (0.1, 0.01, …). La actualización de pesos en el proceso de entrenamiento de la red viene dado entonces (para SGD) por:\n\\[ W_{t+1} = W_{t} - \\eta  \\left[\\frac{\\partial J(W)}{\\partial W_t} +2\\lambda W_t\\right]\\]\nEl parámetro \\(\\lambda\\) controla la regularización, de forma que si deseamos más podemos aumentar el valor de \\(\\lambda\\).\n\n\n2.8.3.2 Regularización L1\nExiste otra técnica muy parecida a la anterior denominada regularización L1 donde los parámetros en el sumatorio del término de penalización no se elevan al cuadrado, sino que se usa su valor absoluto:\n\\[J_1(W) = J(W) + \\lambda \\sum |w_i|\\]\nde forma que la actualización de pesos viene dada por:\n\\[W_{t+1} = W_{t} - \\eta  \\left[\\frac{\\partial J(W)}{\\partial W_t} +\\lambda sgn(W_t)\\right]\\]\ndonde \\(sgn()\\) es la función signo.\nEsta variante empuja el valor de los parámetros hacia valores más pequeños, haciendo incluso que la influencia de algunas variables de entrada sea nula en la salida de la red, lo que supone una selección de variables automática. El resultado es una una mejor generalización, pero sólo hasta cierto punto (la elección del valor de λ cobra más importancia en este caso).\n\n\n2.8.3.3 Decaimiento de pesos (weight decay)\nEsta técnica podríamos decir que es idéntica a la regularización L2, pero aplicada en otro punto. En lugar de introducir la penalización como un sumando en la función de coste, la añadimos como un término extra en la fórmula de actualización de los pesos:\n\\[W_{t+1} = W_{t} - \\eta  \\left[\\frac{\\partial J(W)}{\\partial W_t} +\\lambda W_t\\right]\\]\nComo vemos esta actualización es prácticamente igual a la actualización de los pesos en la regularización L2, salvo que en este caso no aparece un 2 multiplicando en el término añadido.\n\n\n\n2.8.4 Drop out\nDrop out es uno de los tipos de técnicas de regularización más interesantes. También produce muy buenos resultados y, en consecuencia, es la técnica de regularización más utilizada en el campo del aprendizaje profundo.\nPara entender cómo funciona el abandono, conviene repasar el concepto de modelos de conjunto.\nEn el aprendizaje automático tradicional, los modelos de conjunto ayudan a reducir el sobreajuste y a mejorar el rendimiento del modelo. Para un problema de clasificación simple, podemos adoptar uno de los siguientes enfoques:\n\nEntrenar varios clasificadores para resolver la misma tarea.\nEntrenar diferentes instancias del mismo clasificador para diferentes subconjuntos del conjunto de datos de entrenamiento.\n\nPara un modelo de clasificación simple, una técnica de conjunto como el bagging implica entrenar el mismo clasificador en diferentes subconjuntos de datos de entrenamiento, muestreados con reemplazo. Supongamos que hay N instancias. En el momento de la prueba, cada clasificador pasa por la muestra de prueba y se utiliza un conjunto de sus predicciones.\nEn general, el rendimiento de un conjunto es al menos tan bueno como el de los modelos individuales; no puede ser peor que el de los modelos individuales.\nSi trasladáramos esta idea a las redes neuronales, podríamos intentar hacer lo siguiente (identificando al mismo tiempo las limitaciones de este enfoque):\n\nEntrenar varias redes neuronales con diferentes arquitecturas. Entrenar una red neuronal en diferentes subconjuntos de los datos de entrenamiento. Sin embargo, entrenar múltiples redes neuronales es prohibitivamente caro.\nIncluso si entrenamos N redes neuronales diferentes, ejecutar el punto de datos a través de cada uno de los N modelos -en el momento de la prueba- introduce una sobrecarga computacional sustancial.\n\nPara entender el drop out, digamos que la estructura de nuestra red neuronal es parecida a la que se muestra a continuación (red densa donde todas las neuronas están interconectadas):\n\n\n\n\n\n¿Qué hace drop out? En cada iteración, selecciona aleatoriamente algunos nodos y los elimina junto con todas sus conexiones entrantes y salientes, como se muestra a continuación\n\n\n\n\n\nAsí, cada iteración tiene un conjunto diferente de nodos, lo que da lugar a un conjunto diferente de resultados. También puede considerarse una técnica de conjunto en el aprendizaje automático. Los modelos de conjunto suelen funcionar mejor que un modelo único, ya que capturan más aleatoriedad. Del mismo modo, el abandono también funciona mejor que un modelo de red neuronal normal.\nCon un abandono de 0,5, hay un 50% de posibilidades de que cada neurona participe en el entrenamiento dentro de cada lote de entrenamiento. El resultado es una arquitectura de red ligeramente diferente para cada lote. Equivale a entrenar redes neuronales diferentes en subconjuntos diferentes de los datos de entrenamiento.\nEsta probabilidad de elegir cuántos nodos deben abandonarse es el hiperparámetro de la función de abandono. Como se ve en la imagen anterior, el drop out puede aplicarse tanto a las capas ocultas como a las capas de entrada.\nLa matriz de pesos se inicializa una vez al principio del entrenamiento. En general, para el lote k-ésimo, la retropropagación se produce sólo a lo largo de los caminos de las neuronas presentes para ese lote. Esto significa que sólo se actualizan los pesos correspondientes a las neuronas que están presentes.\nEn el momento de la prueba, todas las neuronas están presentes en la red. Entonces, ¿cómo tenemos en cuenta los abandonos durante el entrenamiento? Ponderamos la salida de cada neurona con la misma probabilidad p, proporcional a la fracción de tiempo que la neurona estuvo presente durante el entrenamiento.\n\n\n2.8.5 Normalización por lotes\nLa normalización en lotes consiste básicamente en añadir un paso extra, habitualmente entre las neuronas y la función de activación, con la idea de normalizar las activaciones de salida. Lo ideal es que la normalización se hiciera usando la media y la varianza de todo el conjunto de entrenamiento, pero si estamos aplicando el descenso del gradiente estocástico para entrenar la red, se usará la media y la varianza de cada mini-lote de entrada.\nNota: cada salida de cada neurona se normalizará de forma independiente, lo que quiere decir que en cada iteración se calculará la media y la varianza de cada salida para el mini-lote en curso.\nA continuación de la normalización se añaden 2 parámetros: un bias como sumando, y otra constante similar a un bias pero que aparece multiplicando cada activación. Esto se hace para que el rango de la entrada escale fácilmente hasta el rango de salida, lo que ayudará mucho a nuestra red a la hora de ajustar a los datos de entrada, y reducirá las oscilaciones de la función de coste. Como consecuencia de esto podremos aumentar la tasa de aprendizaje (no hay tanto riesgo de acabar en un mínimo local) y la convergencia hacia el mínimo global se producirá más rápidamente.\nLa normalización por lotes es más una técnica de ayuda al entrenamiento que una estrategia de regularización en sí misma. Esto último se logra realmente aplicando algo adicional conocido como momentum. La idea de este momentum es que cuando introduzcamos un nuevo mini-lote de entrada (N muestras procesadas en paralelo) no se usen una media y una desviación muy distintas a las de la iteración anterior, para lo que se tendrá en cuenta el histórico, y se elegirá una constante que pondere la importancia de los valores del mini-lote actual frente a los valores del anterior. Gracias a todo esto se conseguirá reducir el sobreajuste."
  }
]