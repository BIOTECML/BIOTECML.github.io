# Modelos de conjunto (Ensemble models) {#sec-130}

Como ya se coment√≥ en temas anteriores todos los modelos de aprendizaje autom√°tico sufren el problema de equilibrio entre sesgo y varianza. A medida que aumenta la complejidad de un modelo, este dispone de mayor flexibilidad para adaptarse a las observaciones, reduciendo as√≠ el sesgo y mejorando su capacidad predictiva. Sin embargo, alcanzado un determinado grado de flexibilidad, aparece el problema de sobreajuste, el modelo se ajusta tanto a los datos de entrenamiento que es incapaz de predecir correctamente nuevas observaciones.

Los m√©todos de modelos conjuntos (*Ensemble Models*) combinan m√∫ltiples modelos en uno nuevo con el objetivo de lograr un equilibro entre sesgo y varianza, tratando de obtener mejores predicciones o clasificaciones que cualquiera de los modelos individuales originales.

El principal reto no es obtener modelos individuales muy precisos y/o complejos, sino obtener modelos que cometan diferentes tipos de errores. Por ejemplo, si se utilizan conjuntos para la clasificaci√≥n, se pueden conseguir altas precisiones si diferentes modelos base clasifican mal diferentes ejemplos de entrenamiento, incluso si la precisi√≥n del clasificador base es baja. De esta forma la combinaci√≥n de todos los clasificadores nos acerca a la soluci√≥n √≥ptima.

En la pr√°ctica existen dos metodolog√≠as para la obtenci√≥n de modelo de conjunto: **Bagging**, o m√©todo a partir de modelos individuales independientes, y **Boosting** o m√©todo a partir de modelos secuenciales.

En la metodolog√≠a ***Bagging*** se ajustan m√∫ltiples modelos, cada uno con un subconjunto distinto de los datos de entrenamiento. En esta situaci√≥n los modelos que forman el agregado participan aportando de forma individual su predicci√≥n o clasificaci√≥n. Como valor final, se toma la media de todas las predicciones de los modelos individuales si estamos en un problema de regresi√≥n o la clase m√°s frecuente del conjunto de soluciones aportadas por todos los clasificadores individuales. Los algoritmos m√°s habituales dentro de este grupo son los de **voto por mayor√≠a**, **bosques aleatorios**, y **clasificadores *Bagging***. A continuaci√≥n se presenta una imagen de este proceso:

![](https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/bagging-1.png){fig-align="center" width="550" height="400"}

En este tema nos centramos en el estudio de los m√©todos de bagging, mientras que en el tema siguiente abordaremos los modelos de boosting.

## Modelos b√°sicos Bagging {#sec-130.1}

El t√©rmino ***bagging*** es el diminutivo de *bootstrap agregation* (*bootstrap* agregativo), y hace referencia al empleo del muestreo repetido con reposici√≥n *bootstrapping* con el fin de reducir la varianza de algunos modelos de aprendizaje estad√≠stico haciendo uso del famoso resultado estad√≠stico conocido como teorema central del l√≠mite.

Dicho teorema nos dice que si disponemos de n variables aleatorias independientes con varianza $\sigma^2$ para cada una de ellas, entonces la varianza de la media de todas ellas es $\sigma^2/n$. En otras palabras, promediando un conjunto de observaciones se reduce la varianza.

Bas√°ndose en esta idea, una forma de reducir la varianza y aumentar la precisi√≥n de un m√©todo predictivo es obtener m√∫ltiples muestras de la poblaci√≥n, ajustar un modelo distinto con cada una de ellas, y hacer la media (la moda en el caso de variables cualitativas) de las predicciones resultantes. Como en la pr√°ctica no se suele tener acceso a m√∫ltiples muestras, se puede simular el proceso recurriendo a la t√©cnica de *bootstrap*, que genera pseudo-muestras a partir del conjunto de datos disponible mediante muestreo con reemplazamiento. Se ajusta entonces los diferentes modelos individuales propuestos a cada una de las pseudo-muestras y se agregan los resultados obtenidos en funci√≥n de la variable objetivo considerada.

Los dos algoritmos de *bagging* m√°s utilizados son el de **voto por mayor√≠a** o ***bagging*** **por lotes**.

### Voto por mayor√≠a {#sec-130.1.1}

El m√©todo de voto por mayor√≠a es el m√°s habitual dentro de los modelos de conjunto por agregaci√≥n. Supongamos que ajustamos $L$ modelos de aprendizaje distintos sobre toda la muestra de entrenamiento para resolver un problema de clasificaci√≥n o regresi√≥n. Obtenemos ahora la predicci√≥n para la muestra de test ( de tama√±o $m$) para cada uno de los $L$ modelos mediante:

-   los valores predichos $\hat{y}_1,\hat{y}_2,..., \hat{y}_m$ si estamos en un problema de regresi√≥n, o
-   los valores de clasificaci√≥n predichos $\hat{c}_1,\hat{c}_2,..., \hat{c}_m$ si nos enfrentamos a un problema de clasificaci√≥n donde la variable respuesta puede tomar $k$ valores distintos.

Para la construcci√≥n de la predicci√≥n para el modelo de conjunto se procede obteniendo:

-   la media de los valores predichos en un problema de regresi√≥n

$$\hat{y}_{ensemble} = \frac{\sum_{i=1}^m \hat{y}_i}{m},$$

-   la moda de los valores de clasificaci√≥n en un problema de clasificaci√≥n

$$\hat{c}_{ensemble} = \text{moda}\{\hat{c}_1,\hat{c}_2,..., \hat{c}_m\}.$$

En el problema de clasificaci√≥n si los $L$ modelos considerados pueden estimar las probabilidades de cada clase tendremos una matriz estimada de probabilidades de dimensiones $Lxk$,

```{=tex}
\begin{equation}
\begin{pmatrix}
p_{1,1} & p_{1,2} &...& p_{1,k}\\
p_{2,1} & p_{2,2} &...& p_{2,k}\\
... & ... &...& ...\\
p_{L-1,1} & p_{L-1,2} &...& p_{L-1,k}\\
p_{L,1} & p_{L,2} &...& p_{L,k}
\end{pmatrix}
\end{equation}
```
En esta situaci√≥n tomamos como clase resultante la que proporcione un mayor valor promedio (por columnas) de las probabilidades obtenidas. Este procedimiento suele dar mejores resultados que el voto por mayor√≠a est√°ndar ya que nos permite tener en cuenta la variabilidad de las predicciones.

### Bagging por lotes {#sec-130.1.2}

En el *bagging* por lotes en lugar de entrenar diferentes algoritmos sobre el conjunto completo de datos de entrenamiento y promediar sus resultados como hace el voto por mayor√≠a, este m√©todo entrena un √∫nico o m√∫ltiples clasificadores/regresores en diferentes subconjuntos o submuestras de los datos de entrenamiento y agrega los resultados en todos los subconjuntos de forma similar al voto por mayor√≠a.

Este m√©todo de *bagging* por lotes resulta de gran utilidad para reducir el problema de sobreajuste cuando trabajamos con modelos complejos como los proporcionados por los √°rboles de decisi√≥n. Estos modelos basados en √°rboles son los que veremos m√°s tarde bajo el nombre de **bosques aleatorios**.

Dada la naturaleza de construcci√≥n de los estimadores de *bagging* por lotes donde usamos submuestras con reemplazamiento resulta posible estimar el error de test sin necesidad de recurrir a m√©todos de validaci√≥n cruzada. Las submuestras que son utilizadas sobre cada modelo de aprendizaje se conocen como "*in of bag*" mientras que las que no son utilizadas se conocen como "*out of bag*". Estas √∫ltimas son utilizadas como si fuera una muestra de validaci√≥n para valorar el error cometido dentro de cada uno de los modelos de aprendizaje que conforman el modelo conjunto. Definimos as√≠ el OOB error asociado con cada uno de los modelos individuales a partir de esa muestra. Dos limitaciones de este proceso son:

-   El *Out-of-Bag Error* no es adecuado cuando las observaciones tienen una relaci√≥n temporal (series temporales). Como la selecci√≥n de las observaciones que participan en cada entrenamiento es aleatoria, no respetan el orden temporal y se estar√≠a introduciendo informaci√≥n a futuro.

-   El preprocesado de los datos de entrenamiento se hace de forma conjunta, por lo que las observaciones *out-of-bag* pueden sufrir "*data leakage*", es decir utilizan informaci√≥n de la muestra de entrenamiento para el ajuste del modelo. De ser as√≠, las estimaciones del OOB-error son demasiado optimistas.

En el m√©todo de *bagging* por lotes resulta necesario tambi√©n establecer ciertos hiperpar√°metros asociados con la forma de selecci√≥n de las submuestras, el proceso de reemplazamiento y los correspondientes al modelo de aprendizaje de cada submuestra. En la descripci√≥n de cada uno de esos algoritmos que veremos m√°s adelante se presentar√°n las caracter√≠sticas propias de cada uno de ellos. Por el momento incluiremos los correspondientes a la selecci√≥n de submuestras.

## Modelos b√°sicos Bagging en mlr3 {#sec-130.2}

Aunque la librer√≠a `mlr3` nos permite programar manualmente un algoritmo de bagging, lo habitual es utilizar los pipelines pre-construidos para bagging que est√°n disponibles con `ppl("bagging")`. Para configurar estos pipelines debemos a√±adir como par√°metros:

-   El `learner` o `graphlearner` que utilizaremos en el proceso de bagging.

-   El n√∫mero de iteraciones o repeticiones que usaremos durante el proceso de bagging, identificado con el par√°metro `iterations`.

-   La proporci√≥n de muestras en el conjunto de entrenamiento, identificado con el par√°metro `frac`.

-   El `PipeOp` que utilizamos para promediar los resultados, identificado con el par√°metro `averager`. Las opciones por defecto son:

    -   `po("classifavg", collect_multiplicity = TRUE))` para el voto por mayor√≠a (problemas de clasificaci√≥n) guardando las predicciones individuales de cada modelo.
    -   `po("regravg", collect_multiplicity = TRUE))` para el valor promedio de todos los modleos (problemas de regresi√≥n) guardando las predicciones individuales de cada modelo.

El resultado de esta funci√≥n es un `graphlearner` con los m√©todos asociados habituales.

Antes de presentar los bancos de datos que utilizaremos para mostrar el uso de los modelos de bagging m√°s b√°sicos, cargamos todas las librer√≠as necesarias.

```{r}
#| label: bgg-001
#| message: false
#| results: false
#| warning: false

# Paquetes anteriores
library(tidyverse)
library(sjPlot)
library(knitr) # para formatos de tablas
library(skimr)
library(DataExplorer)
library(GGally)
library(gridExtra)
library(ggpubr)
library(cvms)
library(kknn)
library(rpart.plot)
theme_set(theme_sjplot2())

# Paquetes AA
library(mlr3verse)
library(mlr3tuning)
library(mlr3tuningspaces)
```

### Bancos de datos {#sec-130.2.1}

Para ejemplificar el uso de los modelos de bagging b√°sicos vamos a utilizar tres bancos de datos: `Stroke`, `Water Potability`, y `Housing in California` que se pueden consultar en el tema [-@sec-40]. De los tres con el √∫nico con el que no hemos trabajado hasta ahora es `Water Potability`. A continuaci√≥n se muestra el c√≥digo necesario para la carga de cada uno de esos bancos de datos, y la creaci√≥n de la tarea correspondiente. Los dos primeros corresponden a problemas de clasificaci√≥n mientras que el √∫ltimo se corresponde con un problema de regresi√≥n.

#### Stroke

El c√≥digo para este banco de datos es:

```{r}
#| label: bgg-002
#| warning: false
#| message: false

# Leemos datos
stroke = read_rds("stroke.rds")
# Eliminamos la variable id
stroke = stroke %>% dplyr::select(-id)
# creamos la tarea
tsk_stroke = as_task_classif(stroke, target = "stroke")
# Generamos variable de estrato
tsk_stroke$col_roles$stratum <- "stroke"
```

#### Water Potability

El agua potable es el derecho humano m√°s b√°sico y un factor importante para la salud. El conjunto de datos Water potability, tiene por objetivo estudiar la potabilidad del agua utilizando varias propiedades qu√≠micas debido a su importancia como cuesti√≥n de salud y desarrollo a nivel nacional, regional y local. En algunas regiones, se ha demostrado que las inversiones en abastecimiento de agua y saneamiento pueden producir un beneficio econ√≥mico neto, ya que la reducci√≥n de los efectos adversos para la salud y los costes de la atenci√≥n sanitaria superan los costes de las intervenciones. EL target de inter√©s `potability` indica si la muestra de agua es potable o no en funci√≥n de: `pH`, `Hardness`, `Solids`, `Chloramines`, `Sulfate`, `Conductivity`, `Organic_carbon`, `Trihalomethanes`, y `Turbidity`. Este banco de datos contiene valores perdidos en diferentes variables.

```{r}
#| label: bgg-003
#| warning: false
#| message: false

# Leemos datos
waterpot = read_rds("waterpot.rds")
# creamos la tarea
tsk_water = as_task_classif(waterpot, target = "Potability")
# Generamos variable de estrato
tsk_water$col_roles$stratum <- "Potability"
```

Valoramos la presencia de missings

```{r}
#| label: bgg-004
#| warning: false
#| message: false

tsk_water$missings()
```

Aparecen valores perdidos en tres de las posibles predictoras. Representamos los datos:

```{r}
#| label: bgg-005
#| warning: false
#| message: false
#| fig-width: 14
#| fig-height: 14

autoplot(tsk_water, type = "pairs")
```

#### Housing in California

Cargamos los datos correspondientes:

```{r}
#| label: bgg-006
#| warning: false
#| message: false

# Carga de datos
housingCA = read_rds("housingCA.rds")
# Creaci√≥n de task
tsk_housing = as_task_regr(housingCA, target = "median_house_value")
```

### Modelos {#sec-130.2.2}

Elaboramos nuestros primeros modelos de bagging para los diferentes bancos de datos. Utilizamos el voto por mayor√≠a y el promedio de predicciones para resolver las tareas de clasificaci√≥n y regresi√≥n.

#### Stroke

Para el proceso de bagging hemos de definir en primer caso el algoritmo de aprendizaje asociado. Utilizamos como base los modelos de √°rboles de decisi√≥n vistos en el tema anterior.

```{r}
#| label: bgg-007
#| warning: false
#| message: false

# Preprocesamiento
pp_stroke =  po("imputemedian", affect_columns = selector_type("numeric"))
# Modelo de aprendizaje combinando preprocesado y algoritmo
dt_classif_stroke = as_learner(pp_stroke %>>% 
                                  lrn("classif.rpart", keep_model = TRUE, predict_type = "prob", 
                                      cp = 0.0003562633,
                                      minsplit = 10, 
                                      maxdepth = 6))
dt_classif_stroke$id = "TreeDecision"
```

Definimos ahora el grpahlearner asociado con el proceso de bagging utilizando la funci√≥n `ppl` y comparamos con el resultado de un √∫nico √°rbol de decisi√≥n. Realizamos un proceso de validaci√≥n cruzada con cinco repeticiones para obtener resultados m√°s estables:

```{r}
#| label: bgg-008
#| warning: false
#| message: false

set.seed(345)
# Graphleaner para bagging
bgg_stroke = ppl("bagging", dt_classif_stroke,
  iterations = 10, frac = 0.8, averager = po("classifavg", collect_multiplicity = TRUE))
bgg_stroke = as_learner(bgg_stroke)
bgg_stroke$id = "Bagging"
# Comparaci√≥n de modelos simple y ponderado
learners = c(dt_classif_stroke, bgg_stroke)
bmr = benchmark(benchmark_grid(tsk_stroke, learners,
  rsmp("cv", folds = 5)))
# Resultados individuales
bmr$score(msr("classif.bacc"))
# Resultados agregados
bmr$aggregate(msr("classif.bacc"))
```

Como se puede ver en los resultados obtenidos el modelo de conjunto proporciona casi los mismo resultados que el modelo individual. En este caso la ponderaci√≥n no mejora ya que el modelo individual tambi√©n era bastante malo. Veamos gr√°ficamente al comparaci√≥n entre lo scores del modelo basal y del ponderado.

```{r}
#| label: bgg-009
#| warning: false
#| message: false

autoplot(bmr, measure = msr("classif.bacc"))
```

Podemos obtener los resultados del modelo bagging de forma similar a otros modelos de aprendizaje autom√°tico:

```{r}
#| label: bgg-010
#| warning: false
#| message: false

# Entrenamos el modelo
bgg_stroke$train(tsk_stroke)
# Construimos la predicci√≥n
pred = bgg_stroke$predict(tsk_stroke)
pred
```

Evaluamos la matriz de confusi√≥n:

```{r}
#| label: bgg-011
#| warning: false
#| message: false

cm = confusion_matrix(pred$truth, pred$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

#### Water Potability

En este caso vamos a utilizar como modelo base de aprendizaje un modelo de regresi√≥n log√≠stica donde estamos interesados en conocer la probabilidad de que una muestra se clasifique como agua potable en funci√≥n de las predictoras consideradas. Dado que todas las predictoras son num√©ricas (y relacionadas entre si como hemos visto en la interpretaci√≥n gr√°fica de los datos) y algunas contienen valores perdidos consideramos un modelo penalizado (en este caso elastic net con peso igual a 0.5) y el correspondiente preprocesado. En primer lugar generamos la tarea de nuevo identificando la categor√≠a de inter√©s:

```{r}
#| label: bgg-012
#| warning: false
#| message: false

# creamos la tarea
tsk_water = as_task_classif(waterpot, target = "Potability", positive = "1")
# Generamos variable de estrato
tsk_water$col_roles$stratum <- "Potability"
```

```{r}
#| label: bgg-013
#| warning: false
#| message: false

# Preprocesado
pp_water = po("imputemedian", affect_columns = selector_type("numeric")) %>>% 
  po("scale", param_vals = list(center = TRUE, scale = TRUE)) 
# Definimos learner basal
learner = lrn("classif.cv_glmnet", type.logistic = "Newton", standardize = FALSE,
              alpha = 0.5, predict_type = "prob")
# Graphlearner: Preprocesado y learner
logm_classif_water = as_learner(pp_water %>>% learner)
logm_classif_water$id = "LogisticReg"
```

Constru√≠os el modelo de bagging y vemos los resultados obtenidos:

```{r}
#| label: bgg-014
#| warning: false
#| message: false

set.seed(345)
# Graphleaner para bagging
bgg_water = ppl("bagging", logm_classif_water,
  iterations = 10, frac = 0.8, averager = po("classifavg", collect_multiplicity = TRUE))
bgg_water = as_learner(bgg_water)
bgg_water$id = "Bagging"
# Comparaci√≥n de modelos simple y ponderado
learners = c(logm_classif_water, bgg_water)
bmr = benchmark(benchmark_grid(tsk_water, learners,
  rsmp("cv", folds = 5)))
# Resultados individuales
bmr$score(msr("classif.bacc"))
# Resultados agregados
bmr$aggregate(msr("classif.bacc"))
```

Podemos ver que el comportamiento con ambas modelizaciones nos proporciona el mismo resultado. La soluci√≥n es bastante estable aunque bastante deficiente. Analizamos con detalle el modelo bagging que hemos construido:

```{r}
#| label: bgg-015
#| warning: false
#| message: false

# Entrenamos el modelo
bgg_water$train(tsk_water)
# Construimos la predicci√≥n
pred = bgg_water$predict(tsk_water)
# matriz de confusi√≥n
cm = confusion_matrix(pred$truth, pred$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

Se ve claramente el mal funcionamiento del modelo ya que clasifica todas las muestras como no potable, a pesar de que el n√∫mero de muestras que originalmente eran potables es bastante elevado.

#### Housing in California

En este caso nos enfrentamos a un problema de regresi√≥n, pero en lugar de utilizar un modelo lineal como modelo basal vamos a utilizar un √°rbol de decisi√≥n con las configuraciones por defecto. En primer lugar configuramos el modelo basal.

```{r}
#| label: bgg-016
#| warning: false
#| message: false

# Preprocesamiento
pp_housing = 
   po("scale", param_vals = list(center = TRUE, scale = TRUE)) %>>%
   po("imputemedian", affect_columns = selector_type("numeric")) 
# Modelo de aprendizaje combinando preprocesado y algoritmo
dt_regr_housing = as_learner(pp_housing %>>% 
                                  lrn("regr.rpart", keep_model = TRUE))
dt_regr_housing$id = "TreeDecision"
```

Establecemos ahora el modelo bagging

```{r}
#| label: bgg-017
#| warning: false
#| message: false

set.seed(345)
# Graphleaner para bagging
bgg_housing = ppl("bagging", dt_regr_housing,
  iterations = 10, frac = 0.8, averager = po("regravg", collect_multiplicity = TRUE))
bgg_housing = as_learner(bgg_housing)
bgg_housing$id = "Bagging"
# Comparaci√≥n de modelos simple y ponderado
learners = c(dt_regr_housing, bgg_housing)
bmr = benchmark(benchmark_grid(tsk_housing, learners,
  rsmp("cv", folds = 5)))
# Resultados individuales
bmr$score(msr("regr.smape"))
# Resultados agregados
bmr$aggregate(msr("regr.smape"))
```

Aunque los resultados son muy similares para ambas modelizaciones, podemos ver que el `sMAPE` es algo inferior para el modelo bagging que para el modelo basal. Mejoramos, aunque muy levemente, la capacidad explicativa de nuestro modelo. En este caso tambi√©n podr√≠amos evaluar la capacidad del modelo mediante la representaci√≥n gr√°fica de los valores observados frente a los predichos por el modelo.

```{r}
#| label: bgg-018
#| warning: false
#| message: false

# Entrenamiento
bgg_housing$train(tsk_housing)
# Predicci√≥n
pred = bgg_housing$predict(tsk_housing)
# Gr√°fico
autoplot(pred, type = "xy") + labs(title = "Observados vs predichos")
```

Claramente la nube de puntos es muy dispersa y el modelo no es capaz de ajustar correctamente.

Los modelos de bagging tienden a producir soluciones muy similares a los de los modelos de base utilizados, dado que siempre se utilizan todas las observaciones y todas las predictoras en cada iteraci√≥n de la ponderaci√≥n. Sin embargo, se pueden construir otro tipo de modelos de bagging en los que en cada iteraci√≥n se puede usar un conjunto de entrenamiento diferente y un conjunto de predictoras, en lugar de todas ellas. Estos son los modelos de bosques aleatorios que pasamos a describir a continuaci√≥n.

## Bosques aleatorios (Random Forests) {#sec-130.3}

Un modelo de **bosque aleatorio** est√° formado por un conjunto de √°rboles de decisi√≥n individuales, cada uno entrenado con una muestra ligeramente distinta de los datos de entrenamiento generada mediante *bootstrapping*. La predicci√≥n de una nueva observaci√≥n se obtiene agregando las predicciones de todos los √°rboles individuales que forman el modelo.

Muchos m√©todos predictivos generan modelos globales en los que disponemos de una √∫nica ecuaci√≥n o modelo de predicci√≥n. Sin embargo, en situaciones complejas con m√∫ltiples predictores, que interaccionan entre ellos de forma compleja y no lineal, es muy dif√≠cil encontrar un modelo predictivo lo suficientemente preciso. Como ya hemos visto anteriormente los √°rboles de decisi√≥n nos permiten obtener un modelo con el que podemos manejar de forma sencilla relaciones complejas entre las posibles predictoras

Ahora, como ya hemos visto, la utilizaci√≥n de los √°rboles de decisi√≥n no est√° exenta de dificultades y por ese motivo se introduce aqu√≠ el algoritmo de bosque aleatorio que es un m√©todo de conjunto *bagging* que nos permite mejorar la capacidad predictiva de los √°rboles de decisi√≥n individuales.

Entre las ventajas del uso de este tipo de algoritmo podemos destacar:

-   Son capaces de seleccionar predictores de forma autom√°tica.
-   Pueden aplicarse a problemas de regresi√≥n y clasificaci√≥n.
-   Al tratarse de m√©todos no param√©tricos no es necesario que se cumpla ning√∫n tipo de distribuci√≥n espec√≠fica.
-   Por lo general, requieren mucha menos limpieza y preprocesado de los datos en comparaci√≥n a otros m√©todos de aprendizaje estad√≠stico (por ejemplo, no requieren estandarizaci√≥n).
-   No se ven muy influenciados por *outliers*.
-   Son muy √∫tiles en la exploraci√≥n de datos, permiten identificar de forma r√°pida y eficiente las variables (predictores) m√°s importantes.
-   Gracias al *Out-of-Bag Error* puede estimarse su error de validaci√≥n sin necesidad de recurrir a estrategias computacionalmente costosas como la validaci√≥n cruzada.

Entre las desventajas podemos destacar:

-   Al combinar m√∫ltiples √°rboles, se pierde la interpretabilidad que tienen los modelos basados en un √∫nico √°rbol.
-   Cuando tratan con predictores continuos pierden parte de su informaci√≥n al categorizarlas en el momento de la divisi√≥n de los nodos.
-   Por la forma de construcci√≥n de los √°rboles de decisi√≥n los predictores continuos o predictores cualitativos con muchos niveles tienen mayor probabilidad de contener, solo por azar, alg√∫n punto de corte √≥ptimo, por lo que suelen verse favorecidos en la creaci√≥n de los √°rboles.
-   No son capaces de extrapolar fuera del rango de los predictores observados en los datos de entrenamiento.

### Algortimo Bosques aleatorios {#sec-130.3.1}

Antes de presentar el algoritmo espec√≠fico del bosque aleatorio es necesario conocer como funciona el proceso de *bagging* para un √∫nico √°rbol de decisi√≥n. Dicho algoritmo se organiza en tres pasos:

1.  Generar ùêµ *pseudo-training sets* mediante *bootstrapping* a partir de la muestra de entrenamiento original.
2.  Entrenar un √°rbol con cada una de las ùêµ muestras del paso 1. Cada √°rbol se crea sin apenas restricciones y no se somete a *pruning*, por lo que tiene varianza alta pero poco sesgo. En la mayor√≠a de casos, la √∫nica regla de parada es el n√∫mero m√≠nimo de observaciones que deben tener los nodos terminales. El valor √≥ptimo de este hiperpar√°metro puede obtenerse comparando el *out of bag error* o mediante validaci√≥n cruzada.
3.  Para cada una de la muestras de validaci√≥n, se obtiene la predicci√≥n en cada uno de los ùêµ √°rboles. El valor final de la predicci√≥n se obtiene como la media de las ùêµ predicciones en el caso de variables cuantitativas y como la clase predicha m√°s frecuente (moda) para variables cualitativas.

En el algoritmo descrito, el n√∫mero de √°rboles creados no es un hiperpar√°metro cr√≠tico en cuanto a que, por mucho que se incremente el n√∫mero, no se aumenta el riesgo de *overfitting*. Alcanzado un determinado n√∫mero de √°rboles, la reducci√≥n del *test error* se estabiliza. A pesar de ello, cada √°rbol ocupa memoria, por lo que no conviene almacenar m√°s de los necesarios.

El algoritmo de *Random Forest* es una modificaci√≥n del proceso de *bagging* anterior que consigue mejorar los resultados gracias a que considera √°rboles lo m√°s independientes posibles.

Sup√≥ngase un conjunto de datos en el que hay un predictor muy influyente, junto con otros moderadamente influyentes. En este escenario, todos o casi todos los √°rboles creados en el proceso de *bagging* estar√°n dominados por el mismo predictor y ser√°n muy parecidos entre ellos. Como consecuencia de la alta correlaci√≥n entre los √°rboles, el proceso de *bagging* apenas conseguir√° disminuir la varianza y, por lo tanto, tampoco mejorar el modelo. *Random forest* evita este problema haciendo una selecci√≥n aleatoria de $ùëö$ predictores antes de evaluar cada divisi√≥n. De esta forma, un promedio de $(ùëù‚àíùëö)/ùëù$ divisiones no contemplar√° el predictor influyente, permitiendo que otros predictores puedan ser seleccionados. A√±adiendo este paso extra se consigue descorrelacionar los √°rboles todav√≠a m√°s, con lo que su agregaci√≥n consigue una mayor reducci√≥n de la varianza. Algunas recomendaciones para la selecci√≥n de $m$ son:

-   La ra√≠z cuadrada del n√∫mero total de predictores para problemas de clasificaci√≥n:

$$m \approx \sqrt{p}$$

-   Un tercio del n√∫mero de predictores para problemas de regresi√≥n:

$$m \approx p/3$$

-   Si los predictores est√°n muy correlacionados, valores peque√±os de $ùëö$ consiguen mejores resultados.

### Predicci√≥n mediante bosque aleatorio {#sec-130.3.2}

Para realizar la predicci√≥n de un bosque aleatorio utilizamos el principio de *bagging*, de forma que, una vez determinamos el nodo terminal al que es asignada la observaci√≥n a predecir en cada uno de los √°rboles, utilizamos las observaciones contenidas en dicho nodo terminal para la predicci√≥n individual de cada uno de ellos. Si estamos en un modelo de regresi√≥n obtenemos la media de todas las observaciones del nodo terminal en cada √°rbol, mientras que si estamos en un problema de clasificaci√≥n actuamos mediante el voto por mayor√≠a.

Una vez obtenemos las predicciones individuales la predicci√≥n conjunta se obtiene a partir de la media o de la categor√≠a m√°s frecuente de todas ellas en funci√≥n de que estemos en un problema de predicci√≥n o clasificaci√≥n.

Sin embargo, en los problemas de regresi√≥n la predicci√≥n de un √°rbol de regresi√≥n puede verse como una variante de vecinos cercanos en la que, solo las observaciones que forman parte del mismo nodo terminal que la observaci√≥n predicha tienen influencia. Siguiendo esta aproximaci√≥n, la predicci√≥n del √°rbol se define como la media ponderada de todas las observaciones de entrenamiento, donde el peso de cada observaci√≥n depende √∫nicamente de si forma parte o no del mismo nodo terminal, es decir, definimos los pesos del √°rbol j como un vector de $n$ componentes donde cada una de las componentes toma el valor $w_j = 1/n_j$ si la observaci√≥n pertenece al nodo terminal $j$ con $n_j$ observaciones, y 0 en otro caso. Para el bosque aleatorio esto equivale a la media ponderada de todas las observaciones, empleando como pesos la media de los vectores de pesos de los $M$ √°rboles considerados, es decir,

$$\hat{w}=\frac{\sum_{i=1}^{M}  w_i}{M}$$

$$y_{pred} = \sum_{i=1}^n \hat{w}_i y_i$$

### Importancia de los predictores {#sec-130.3.3}

Si bien es cierto que el bosque aleatorio consigue mejorar la capacidad predictiva en comparaci√≥n a los modelos basados en un √∫nico √°rbol, esto tiene un coste asociado, la interpretabilidad del modelo se reduce. Al tratarse de una combinaci√≥n de m√∫ltiples √°rboles, no es posible obtener una representaci√≥n gr√°fica sencilla del modelo y no es inmediato identificar de forma visual que predictores son m√°s importantes. Sin embargo, se han desarrollado nuevas estrategias para cuantificar la importancia de los predictores que hacen de los modelos de bosque aleatorio una herramienta muy potente, no solo para predecir, sino tambi√©n para el an√°lisis exploratorio. Dos de estas medidas son: importancia por permutaci√≥n e impureza de nodos.

#### Importancia por permutaci√≥n

Identifica la influencia que tiene cada predictor sobre una determinada m√©trica de evaluaci√≥n del modelo (estimada por *out-of-bag error* o validaci√≥n cruzada). El valor asociado con cada predictor se obtiene de la siguiente forma:

1.  Crear el conjunto de √°rboles que forman el modelo.

2.  Calcular una determinada m√©trica de error (mse, *classification error*, ...). Este es el valor de referencia ($ùëíùëüùëü_0$).

3.  Para cada predictor $ùëó$:

-   Permutar en todos los √°rboles del modelo los valores del predictor $ùëó$ manteniendo el resto constante.

-   Recalcular la m√©trica tras la permutaci√≥n, ll√°mese ($ùëíùëüùëü_ùëó$).

-   Calcular el incremento en la m√©trica debido a la permutaci√≥n del predictor $ùëó$

$$\%I_ùëó=100*\frac{err_j-err_0}{err_0}$$

Si el predictor permutado estaba contribuyendo al modelo, es de esperar que el modelo aumente su error, ya que se pierde la informaci√≥n que proporcionaba esa variable. El porcentaje en que se incrementa el error debido a la permutaci√≥n del predictor $ùëó$ puede interpretarse como la influencia que tiene $ùëó$ sobre el modelo. Algo que suele llevar a confusiones es el hecho de que este incremento puede resultar negativo. Si la variable no contribuye al modelo, es posible que, al reorganizarla aleatoriamente, solo por azar, se consiga mejorar ligeramente el modelo, por lo que $(ùëíùëüùëü_ùëó‚àíùëíùëüùëü_0)$ es negativo. A modo general, se puede considerar que estas variables tienen una importancia pr√≥xima a cero.

Aunque esta estrategia suele ser la m√°s recomendada, cabe tomar algunas precauciones en su interpretaci√≥n. Lo que cuantifican es la influencia que tienen los predictores sobre el modelo, no su relaci√≥n con la variable respuesta. ¬øPor qu√© es esto tan importante? Sup√≥ngase un escenario en el que se emplea esta estrategia con la finalidad de identificar qu√© predictores est√°n relacionados con el peso de una persona, y que dos de los predictores son: el √≠ndice de masa corporal (IMC) y la altura. Como IMC y altura est√°n muy correlacionados entre s√≠ (la informaci√≥n que aportan es redundante), cuando se permute uno de ellos, el impacto en el modelo ser√° m√≠nimo, ya que el otro aporta la misma informaci√≥n. Como resultado, estos predictores aparecer√°n como poco influyentes aun cuando realmente est√°n muy relacionados con la variable respuesta. Una forma de evitar problemas de este tipo es, siempre que se excluyan predictores de un modelo, comprobar el impacto que tiene en su capacidad predictiva.

#### Incremento de la pureza de los nodos

Cuantifica el incremento total en la pureza de los nodos debido a divisiones en las que participa el predictor (promedio de todos los √°rboles). La forma de calcularlo es la siguiente: en cada divisi√≥n de los √°rboles, se registra el descenso conseguido en la medida empleada como criterio de divisi√≥n (√≠ndice Gini, MSE, entrop√≠a, ...). Para cada uno de los predictores, se calcula el descenso medio conseguido en el conjunto de √°rboles que forman el conjunto. Cuanto mayor sea este valor medio, mayor la contribuci√≥n del predictor en el modelo.

### Hiperpar√°metros relevantes en el bosque aleatorio {#sec-130.3.4}

Del conjunto de hiperpar√°metros que se pueden modificar en el bosque aleatorio los dos m√°s interesantes son el n√∫mero de √°rboles considerados y el n√∫mero m√°ximo de predictoras usadas en la construcci√≥n de cada √°rbol.

Lo habitual es proceder de forma individual estudiando la influencia de cada uno de los hiperpar√°metros respecto de la capacidad predictiva del modelo utilizando el *out of bag score* (aunque se puede configurar el algoritmo para utilizar otra). Esas curvas de influencia nos permiten determinar la evoluci√≥n del error del modelo con respecto a ese hiperpar√°metro y obtener as√≠ el conjunto √≥ptimo de valores.

Sin embargo, aunque el an√°lisis individual de los hiperpar√°metros es √∫til para entender su impacto en el modelo e identificar rangos de inter√©s, la b√∫squeda final no debe hacerse de forma secuencial, ya que cada hiperpar√°metro interacciona con los dem√°s. Es preferible recurrir a *grid search* o *random search* para analizar varias combinaciones de hiperpar√°metros. Los dos m√©todos m√°s habituales son el *grid search* basado en el *out of bag* o el *grid search* basado en validaci√≥n cruzada.

### Codificaci√≥n de predictoras cualitativas {#sec-130.3.5}

Los modelos basados en √°rboles de decisi√≥n, entre ellos *Random Forest*, son capaces de utilizar predictores categ√≥ricos en su forma natural sin necesidad de convertirlos en variables *dummy* mediante *one hot encoding*. Sin embargo, en la pr√°ctica, depende de la implementaci√≥n que tenga la librer√≠a o software utilizado. Esto tiene impacto directo en la estructura de los √°rboles generados y, en consecuencia, en los resultados predictivos del modelo y en la importancia calculada para los predictores.

Entre las dificultades m√°s relevantes al utilizar *one hot encoding* se pueden destacar:

-   El entrenamiento de los modelos es m√°s costoso cuando se aplica *one hot encoding* debido al aumento de dimensionalidad al crear las nuevas variables *dummy*, obligando a que el algoritmo tenga que analizar muchos m√°s puntos de divisi√≥n.
-   Al convertir una variable categ√≥rica en m√∫ltiples variables *dummy* su importancia queda diluida, dificultando que el modelo pueda aprender de ella y perdiendo as√≠ capacidad predictiva. Este efecto es mayor cuantos m√°s niveles tiene la variable original.
-   Al diluir la importancia de los predictores categ√≥ricos, estos tienen menos probabilidad de ser seleccionados por el modelo, lo que desvirt√∫a las m√©tricas que miden la importancia de los predictores.

Por el momento, en Scikit-Learn es necesario hacer *one hot encoding* para convertir las variables categ√≥ricas en variables *dummy* si deseamos usar *random forest*. La implementaci√≥n de `H2O` s√≠ permite utilizar directamente variables categ√≥ricas.

## Bosque aleatorio en mlr3 {#sec-130.4}

Los algoritmos b√°sicos de bosques aleatorios que podemos encontrar en `mlr3`:

-   `regr.ranger` para abordar tareas de regresi√≥n.
-   `classif.ranger` para abordar tareas de clasificaci√≥n.

Estos algoritmos son una implementaci√≥n r√°pida de bosques aleatorios o partici√≥n recursiva, particularmente adecuada para datos de alta dimensi√≥n.

Otros algoritmos disponibles en la librer√≠a `mlr3extralearners` son:

-   `classif.randomForest` y `regr.randomForest`, para tareas de clasificaci√≥n y regresi√≥n.
-   `classif.rfsrc` y `regr.rfsrc`, utilizando programaci√≥n en paralelo, y que se pueden utilizar tanto en problemas de clasificaci√≥n, regresi√≥n, supervivencia, y otros m√°s.
-   `classif.cforest` y `regr.cforest`, que son algoritmos para la partici√≥n recursiva basada en modelos que produce un √°rbol con modelos ajustados asociados con cada nodo terminal.

En este tema nosotros nos centramos en los modelos b√°sicos de bosques aleatorios cuyos hiperpar√°metros m√°s relevantes son:

-   `importance`: Modo de importancia utilizado en la construcci√≥n del bosque aleatorio "none", "impurity", "impurity_corrected", "permutation". La medida de 'impureza' es el √≠ndice de Gini para la clasificaci√≥n, la varianza de las respuestas para la regresi√≥n.
-   `max_depth`: profundidad m√°xima del √°rbol. Un valor de 0 corresponde a un √°rbol sin limitaciones.
-   `min.node.size`: N√∫mero de observaciones m√≠nimo en los nodos terminales.
-   `mtry`: N√∫mero de variables a considerar en la divisi√≥n de cada nodo. El valor predeterminado es la ra√≠z cuadrada (redondeada hacia abajo) de las variables num√©ricas. Alternativamente, una funci√≥n de un solo argumento devuelve un n√∫mero entero, dado el n√∫mero de variables independientes.
-   `mtry.ratio`: Proporci√≥n de variables a considerar en la divisi√≥n de cada nodo que toma valores en el intervalo $[0,1]$.
-   `splitrule`: regla de divisi√≥n utilizada. Para tareas de clasificaci√≥n se considera `gini` (valor por defecto), `extratrees`, y `hellinger`. Para tareas de regresi√≥n se considera `variance` (valor por defecto), `extratrees`, `max-stat`, y `beta`.
-   `sample.fraction`: Fracci√≥n de observaciones para la muestra. El valor predeterminado es 1 para muestreo con reemplazo.
-   `num.trees`: n√∫mero de √°rboles considerados en la construcci√≥n del bosque aleatorio. El valor por defecto es 500.
-   `oob.error`: condici√≥n l√≥gica que indica se se debe calcular el error de predicci√≥n oob.

En los puntos siguientes analizamos los modelos b√°sicos de bosques aleatorios para cada uno de nuestros problemas, y finalizaremos con la optimizaci√≥n de par√°metros para alcanzar el mejor modelo posible. Se deja para el lector la modelizaci√≥n con otros algoritmos de bosques aleatorios.

### Modelos de bosques aleatorios {#sec-130.4.1}

Como en el caso de √°rboles aleatorios no resulta necesario estandarizar las variables num√©ricas pero si es necesario imputar los valores perdidos en la tarea de preprocesamiento. Sin embargo, para que los resultados sean comparables con los obtenidos en el punto anterior vamos a realizar todo el preprocesamiento.

#### Breast Cancer Wisconsin

Definimos el algoritmo de aprendizaje asociado as√≠ como las tareas de preprocesamiento.

```{r}
#| label: bgg-019
#| warning: false
#| message: false

# Preprocesamiento
pp_stroke =  po("imputemedian", affect_columns = selector_type("numeric"))
# Modelo de aprendizaje combinando preprocesado y algoritmo
rf_classif_stroke = as_learner(pp_stroke %>>% 
                                  lrn("classif.ranger", importance = "impurity"))
rf_classif_stroke$id = "RandomForest"
```

Comenzamos con le entrenamiento del modelo definiendo en primer lugar las muestras de entrenamiento y validaci√≥n:

```{r}
#| label: bgg-020
#| warning: false
#| message: false

# Divisi√≥n de muestras
set.seed(432)
# Creamos la partici√≥n
splits = mlr3::partition(tsk_stroke, ratio = 0.8)
# Muestras de entrenamiento y validaci√≥n
tsk_train_stroke = tsk_stroke$clone()$filter(splits$train)
tsk_test_stroke  = tsk_stroke$clone()$filter(splits$test)
# Entrenamiento del modelo
rf_classif_stroke$train(tsk_train_stroke)
# modelo construido
modelo = rf_classif_stroke$model$classif.ranger$model
```

Analizamos ahora los resultados del modelo obtenido. Comenzamos con los resultados generales del modelo:

```{r}
#| label: bgg-021
#| warning: false
#| message: false

# Caracter√≠sticas del modelo
modelo
```

Podemos ver que el modelo obtenido tiene un 4.94% de error de predicci√≥n. Estudiamos ahora la contribuci√≥n de cada predictora en el modelo obtenido partir de la importancia de cada una de ellas.

```{r}
#| label: bgg-022
#| warning: false
#| message: false

# Importancia de las predictoras (ordenada)
sort(modelo$variable.importance, decreasing = TRUE)
```

Las tres predictoras con una mayor contribuci√≥n en la construcci√≥n del bosque aleatorio son `avg_glucose_level`, `age`, y `bmi`, muy por encima del resto de predictoras. En muchas situaciones pr√°cticas se puede utilizar el resultado de la importancia para construir un nuevo modelo de aprendizaje basado √∫nicamente en dichas predictoras. En este caso nos podr√≠amos plantear un modelo de regresi√≥n log√≠stica o un √°rbol de decisi√≥n que solo contemplara dichas variables.

Por el momento nos centramos en ampliar el aprendizaje sobre nuestro modelo analizando las predicciones para la muestra de entrenamiento y validaci√≥n.

```{r}
#| label: bgg-023
#| warning: false
#| message: false

# Predicci√≥n de la muestra de entrenamiento y validaci√≥n
pred_train = rf_classif_stroke$predict(tsk_train_stroke)
pred_test = rf_classif_stroke$predict(tsk_test_stroke)
# scores de validaci√≥n
measures = msrs(c("classif.acc", "classif.bacc"))
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci√≥n
pred_test$score(measures)
```

De nuevo el porcentaje de clasificaci√≥n correcta ponderado muestra valores muy bajos en comparaci√≥n con el no ponderado. Este comportamiento es similar al visto en modelos anteriores. Analizamos la tabla de confusi√≥n de la muestra de validaci√≥n:

```{r}
#| label: bgg-024
#| warning: false
#| message: false

cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

Se puede ver claramente que le modelo no proporciona una soluci√≥n adecuada ya que no es capaz de clasificar correctamente ninguna de las muestras originales correspondientes a sujetos que han sufrido un ictus. En el proceso de optimizaci√≥n trataremos de mejorar los resultados de este algoritmo.

Para comenzar el proceso de optimizaci√≥n nos centramos en los hiperpar√°metros: `mtry.ratio`, `num.trees`, y `sample.fraction`. Establecemos el proceso de optimizaci√≥n como en otras ocasiones aumentando el n√∫mero de iteraciones a 50 debido al alto n√∫mero de predictoras involucradas. Necesitamos recorrer el espacio de b√∫squeda (sin mucho coste computacional) para tratar de acercarnos al √≥ptimo.

```{r}
#| label: bgg-025
#| message: false
#| warning: false
#| results: hide

rf_classif_stroke = lrn("classif.ranger", importance = "impurity",
                         mtry.ratio = to_tune(1e-02, 1, logscale = TRUE),
                         num.trees = to_tune(100, 1000),
                         sample.fraction = to_tune(1e-01, 1, logscale = TRUE)
                         )
gr_stroke =  pp_stroke %>>% rf_classif_stroke
gr_stroke = GraphLearner$new(gr_stroke)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci√≥n fijando el n√∫mero de evaluaciones
instance = tune(
  tuner = tnr("random_search"),
  task = tsk_stroke,
  learner = gr_stroke,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.bacc"),
  term_evals = 50
)
```

Podemos ver los resultados obtenidos con

```{r}
#| label: bgg-026
#| message: false
#| warning: false

instance$result_y
instance$result_x_domain
```

donde observamos los valores √≥ptimos de los hiperpar√°metros y el porcentaje de clasificaci√≥n alcanzado del 50.3%. La soluci√≥n √≥ptima es muy similar a la opci√≥n por defecto. No hemos mejorado pr√°cticamente nada respecto de la soluci√≥n inicial.

#### Water Potability

Como en otras situaciones empezamos por el modelo de aprendizaje por defecto.

```{r}
#| label: bgg-027
#| warning: false
#| message: false

# Preprocesado
pp_water = po("imputemedian", affect_columns = selector_type("numeric")) %>>% 
  po("scale", param_vals = list(center = TRUE, scale = TRUE)) 
# Modelo de aprendizaje combinando preprocesado y algoritmo
rf_classif_water = as_learner(pp_water %>>% 
                                  lrn("classif.ranger", importance = "impurity"))
rf_classif_water$id = "RandomForest"
```

Comenzamos con le entrenamiento del modelo definiendo en primer lugar las muestras de entrenamiento y validaci√≥n:

```{r}
#| label: bgg-028
#| warning: false
#| message: false

# Divisi√≥n de muestras
set.seed(432)
# Creamos la partici√≥n
splits = mlr3::partition(tsk_water, ratio = 0.8)
# Muestras de entrenamiento y validaci√≥n
tsk_train_water = tsk_water$clone()$filter(splits$train)
tsk_test_water  = tsk_water$clone()$filter(splits$test)
# Entrenamiento del modelo
rf_classif_water$train(tsk_train_water)
# modelo construido
modelo = rf_classif_water$model$classif.ranger$model
```

Analizamos ahora los resultados del modelo obtenido. Comenzamos con los resultados generales del modelo:

```{r}
#| label: bgg-029
#| warning: false
#| message: false

# Caracter√≠sticas del modelo
modelo
```

Podemos ver que el modelo obtenido tiene un 33.44% de error de predicci√≥n. Estudiamos ahora la contribuci√≥n de cada predictora en el modelo obtenido partir de la importancia de cada una de ellas.

```{r}
#| label: bgg-030
#| warning: false
#| message: false

# Importancia de las predictoras (ordenada)
sort(modelo$variable.importance, decreasing = TRUE)
```

En este caso no hay muchas diferencias entre las contribuciones de las predictoras, lo que no nos permite destacar una predictora o un grupo de ellas sobre el resto. Finalizamos con el an√°lisis de predicci√≥n:

```{r}
#| label: bgg-031
#| warning: false
#| message: false

# Predicci√≥n de la muestra de entrenamiento y validaci√≥n
pred_train = rf_classif_water$predict(tsk_train_water)
pred_test = rf_classif_water$predict(tsk_test_water)
# scores de validaci√≥n
measures = msrs(c("classif.acc", "classif.bacc"))
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci√≥n
pred_test$score(measures)
# matriz de confusi√≥n
cm = confusion_matrix(pred_test$truth, pred_test$response)
plot_confusion_matrix(cm$`Confusion Matrix`[[1]]) 
```

En este caso el porcentaje de clasificaci√≥n correcta ponderada se sit√∫a en el 61.75%. El mayor error de clasificaci√≥n se produce al predecir las muestras clasificadas originalmente como potables, ya que el 25.5% de ellas son clasificadas por el modelo como no potables. Sin embargo, si hemos mejorado los resultados del modelo bagging planteado anteriormente.

Veamos que ocurre al intentar optimizar los hiperpar√°metros del modelo. En este caso reducimos la b√∫squeda de `mtry` dado que tenemos menos predictoras, y reducimos le intervalo de b√∫squeda de `num.trees`. Adem√°s aumentamos el n√∫mero de evaluaciones del algoritmo de b√∫squeda ya que utilizamos `grid_search` y deseamos una b√∫squeda fina.

```{r}
#| label: bgg-032
#| message: false
#| warning: false
#| results: hide

rf_classif_water = lrn("classif.ranger", importance = "impurity",
                         mtry.ratio = to_tune(1e-02, 1, logscale = TRUE),
                         num.trees = to_tune(100, 2000),
                         sample.fraction = to_tune(1e-01, 1, logscale = TRUE)
                         )
gr_water =  as_learner(pp_water %>>% rf_classif_water)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci√≥n fijando el n√∫mero de evaluaciones
instance = tune(
  tuner = tnr("grid_search"),
  task = tsk_water,
  learner = gr_water,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.bacc"),
  term_evals = 50
)
```

Podemos ver los resultados obtenidos con:

```{r}
#| label: bgg-033
#| message: false
#| warning: false

instance$result_y
instance$result_x_domain
```

La soluci√≥n √≥ptima alcanza un porcentaje de clasificaci√≥n correcta ponderada del 61.82%, pr√°cticamente igual al del modelo sin optimizaci√≥n. Sin modificar m√°s hiperpar√°metros la soluci√≥n obtenida en ambas situaciones es muy similar.

#### Housing in California

En este caso nos enfrentamos a un problema de regresi√≥n. En primer lugar configuramos el modelo por defecto

```{r}
#| label: bgg-034
#| warning: false
#| message: false

# Preprocesamiento
pp_housing = 
   po("scale", param_vals = list(center = TRUE, scale = TRUE)) %>>%
   po("imputemedian", affect_columns = selector_type("numeric")) 
# Modelo de aprendizaje combinando preprocesado y algoritmo
rf_regr_housing = as_learner(pp_housing %>>% 
                                  lrn("regr.ranger", importance = "impurity"))
rf_regr_housing$id = "RandomForest"
```

Comenzamos con le entrenamiento del modelo definiendo en primer lugar las muestras de entrenamiento y validaci√≥n:

```{r}
#| label: bgg-035
#| warning: false
#| message: false

# Divisi√≥n de muestras
set.seed(432)
# Creamos la partici√≥n
splits = mlr3::partition(tsk_housing, ratio = 0.8)
# Muestras de entrenamiento y validaci√≥n
tsk_train_housing = tsk_housing$clone()$filter(splits$train)
tsk_test_housing  = tsk_housing$clone()$filter(splits$test)
# Entrenamiento del modelo
rf_regr_housing$train(tsk_train_housing)
# modelo construido
modelo = rf_regr_housing$model$regr.ranger$model
```

Analizamos ahora los resultados del modelo obtenido. Comenzamos con los resultados generales del modelo:

```{r}
#| label: bgg-036
#| warning: false
#| message: false

# Caracter√≠sticas del modelo
modelo
```

En este caso podemos ver que le $R^2$ obtenido con este modelo se sit√∫a en el 82.11%, lo que no es un valor fant√°stico pero si bastante alto dado el gran n√∫mero de predictoras y muestras consideradas. Podemos evaluar la importancia de las predictoras

```{r}
#| label: bgg-037
#| warning: false
#| message: false

# Importancia de las predictoras (ordenada)
sort(modelo$variable.importance, decreasing = TRUE)
```

Todas contribuyen aunque en mayor medida el precio de la vivienda viene determinando por `median_income`, `longitude`. `latitude`, y `ocean_proximity`. Veamos el `sMAPE` que obtenemos con este modelo:

```{r}
#| label: bgg-038
#| warning: false
#| message: false

# Predicci√≥n de la muestra de entrenamiento y validaci√≥n
pred_train = rf_regr_housing$predict(tsk_train_housing)
pred_test = rf_regr_housing$predict(tsk_test_housing)
# scores de validaci√≥n
measures = msr("regr.smape")
# Muestra de entrenamiento
pred_train$score(measures)
# Muestra de validaci√≥n
pred_test$score(measures)
```

Podemos ver como los valores son mejores que los obtenidos con el modelo de bagging inicial, demostrando que el modelo random forest consigue mejorar la predicci√≥n del target. Para finalizar exploramos la optimizaci√≥n de hiperpar√°metros con un esquema similar al del ejemplo anterior.

```{r}
#| label: bgg-039
#| message: false
#| warning: false
#| results: hide

rf_regr_housing = lrn("regr.ranger", importance = "impurity",
                         mtry.ratio = to_tune(1e-02, 1, logscale = TRUE),
                         num.trees = to_tune(100, 2000),
                         sample.fraction = to_tune(1e-01, 1, logscale = TRUE)
                         )
gr_housing =  as_learner(pp_housing %>>% rf_regr_housing)

# Fijamos semilla para reproducibilidad del proceso
set.seed(123)
# Definimos instancia de optimizaci√≥n fijando el n√∫mero de evaluaciones
instance = tune(
  tuner = tnr("grid_search"),
  task = tsk_housing,
  learner = gr_housing,
  resampling = rsmp("cv", folds = 3),
  measures = msr("regr.smape"),
  term_evals = 50
)
```

Podemos ver los resultados obtenidos con:

```{r}
#| label: bgg-040
#| message: false
#| warning: false

instance$result_y
instance$result_x_domain
```

A pesar de la b√∫squeda que hemos hecho (sobre todo computacionalmente hablando) el `smape` es pr√°cticamente id√©ntico al obtenido con el modelo sin optimizaci√≥n. Es posible que alguna combinaci√≥n pueda alcanzar un valor m√°s bajo, pero el tiempo computacional puede ser excesivo sino se usa m√°s de un procesador.

## Ejercicios {#sec-130.5}

1.  Ajustar un modelo de aprendizaje autom√°tico basado en bosques aleatorios para el banco de datos `Mushroom`[-@sec-mushroom].
2.  Ajustar un modelo de aprendizaje autom√°tico basado en bosques aleatorios para el banco de datos `Hepatitis`[-@sec-hepatitis].
3.  Ajustar un modelo de aprendizaje autom√°tico basado en bosques aleatorios para el banco de datos `Abalone`[-@sec-abalone].
4.  Ajustar un modelo de aprendizaje autom√°tico basado en bosques aleatorios para el banco de datos `Us economic time series`[-@sec-usaets].
5.  Ajustar un modelo de aprendizaje autom√°tico basado en bosques aleatorios para el banco de datos `QSAR`[-@sec-qsar].
