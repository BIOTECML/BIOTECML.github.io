<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MachineLearning - 2&nbsp; Entrenamiento de la red neuronal</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./30_RMDDL.html" rel="next">
<link href="./10_IntroDL.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Entrenamiento de la red neuronal</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MachineLearning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Deep Learning</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_IntroDL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducci√≥n</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20_TrainDL.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Entrenamiento de la red neuronal</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./30_RMDDL.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Redes multicapa densas con Keras</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./40_AplMD.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Aplicaciones Redes multicapa densas</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./50_ConvNN.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Redes convolucionales</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#preprocesado-de-inputs" id="toc-preprocesado-de-inputs" class="nav-link active" data-scroll-target="#preprocesado-de-inputs"><span class="toc-section-number">2.1</span>  Preprocesado de inputs</a></li>
  <li><a href="#funciones-de-activaci√≥n" id="toc-funciones-de-activaci√≥n" class="nav-link" data-scroll-target="#funciones-de-activaci√≥n"><span class="toc-section-number">2.2</span>  Funciones de activaci√≥n</a>
  <ul>
  <li><a href="#sigmoide" id="toc-sigmoide" class="nav-link" data-scroll-target="#sigmoide"><span class="toc-section-number">2.2.1</span>  Sigmoide</a></li>
  <li><a href="#tangente-hiperb√≥lica" id="toc-tangente-hiperb√≥lica" class="nav-link" data-scroll-target="#tangente-hiperb√≥lica"><span class="toc-section-number">2.2.2</span>  Tangente hiperb√≥lica</a></li>
  <li><a href="#relu-rectified-linear-unit" id="toc-relu-rectified-linear-unit" class="nav-link" data-scroll-target="#relu-rectified-linear-unit"><span class="toc-section-number">2.2.3</span>  ReLU (Rectified linear Unit)</a></li>
  <li><a href="#softmax" id="toc-softmax" class="nav-link" data-scroll-target="#softmax"><span class="toc-section-number">2.2.4</span>  Softmax</a></li>
  <li><a href="#otras-funciones-de-activaci√≥n" id="toc-otras-funciones-de-activaci√≥n" class="nav-link" data-scroll-target="#otras-funciones-de-activaci√≥n"><span class="toc-section-number">2.2.5</span>  Otras funciones de activaci√≥n</a></li>
  </ul></li>
  <li><a href="#funciones-de-p√©rdida" id="toc-funciones-de-p√©rdida" class="nav-link" data-scroll-target="#funciones-de-p√©rdida"><span class="toc-section-number">2.3</span>  Funciones de p√©rdida</a>
  <ul>
  <li><a href="#clasificaci√≥n-binaria" id="toc-clasificaci√≥n-binaria" class="nav-link" data-scroll-target="#clasificaci√≥n-binaria"><span class="toc-section-number">2.3.1</span>  Clasificaci√≥n binaria</a></li>
  <li><a href="#clasificaci√≥n-m√∫ltiple" id="toc-clasificaci√≥n-m√∫ltiple" class="nav-link" data-scroll-target="#clasificaci√≥n-m√∫ltiple"><span class="toc-section-number">2.3.2</span>  Clasificaci√≥n m√∫ltiple</a></li>
  <li><a href="#predicci√≥n" id="toc-predicci√≥n" class="nav-link" data-scroll-target="#predicci√≥n"><span class="toc-section-number">2.3.3</span>  Predicci√≥n</a>
  <ul class="collapse">
  <li><a href="#error-cuadr√°tico-medio" id="toc-error-cuadr√°tico-medio" class="nav-link" data-scroll-target="#error-cuadr√°tico-medio"><span class="toc-section-number">2.3.3.1</span>  Error cuadr√°tico medio</a></li>
  <li><a href="#error-absoluto-medio" id="toc-error-absoluto-medio" class="nav-link" data-scroll-target="#error-absoluto-medio"><span class="toc-section-number">2.3.3.2</span>  Error absoluto medio</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#proceso-de-optimizaci√≥n" id="toc-proceso-de-optimizaci√≥n" class="nav-link" data-scroll-target="#proceso-de-optimizaci√≥n"><span class="toc-section-number">2.4</span>  Proceso de optimizaci√≥n</a>
  <ul>
  <li><a href="#arquitectura-de-red-con-una-capa-oculta" id="toc-arquitectura-de-red-con-una-capa-oculta" class="nav-link" data-scroll-target="#arquitectura-de-red-con-una-capa-oculta"><span class="toc-section-number">2.4.1</span>  Arquitectura de red con una capa oculta</a></li>
  <li><a href="#arquitectura-de-red-con-m√∫ltiples-capas-ocultas" id="toc-arquitectura-de-red-con-m√∫ltiples-capas-ocultas" class="nav-link" data-scroll-target="#arquitectura-de-red-con-m√∫ltiples-capas-ocultas"><span class="toc-section-number">2.4.2</span>  Arquitectura de red con m√∫ltiples capas ocultas</a></li>
  </ul></li>
  <li><a href="#algoritmo-de-backpropagation" id="toc-algoritmo-de-backpropagation" class="nav-link" data-scroll-target="#algoritmo-de-backpropagation"><span class="toc-section-number">2.5</span>  Algoritmo de backpropagation</a></li>
  <li><a href="#algoritmos-de-optimizaci√≥n" id="toc-algoritmos-de-optimizaci√≥n" class="nav-link" data-scroll-target="#algoritmos-de-optimizaci√≥n"><span class="toc-section-number">2.6</span>  Algoritmos de optimizaci√≥n</a>
  <ul>
  <li><a href="#descenso-del-gradiente" id="toc-descenso-del-gradiente" class="nav-link" data-scroll-target="#descenso-del-gradiente"><span class="toc-section-number">2.6.1</span>  Descenso del gradiente</a></li>
  <li><a href="#descenso-del-gradiente-estoc√°stico" id="toc-descenso-del-gradiente-estoc√°stico" class="nav-link" data-scroll-target="#descenso-del-gradiente-estoc√°stico"><span class="toc-section-number">2.6.2</span>  Descenso del gradiente estoc√°stico</a></li>
  <li><a href="#descenso-del-gradiente-por-mini-lotes" id="toc-descenso-del-gradiente-por-mini-lotes" class="nav-link" data-scroll-target="#descenso-del-gradiente-por-mini-lotes"><span class="toc-section-number">2.6.3</span>  Descenso del gradiente por mini lotes</a></li>
  <li><a href="#descenso-del-gradiente-adaptativo-adagrad" id="toc-descenso-del-gradiente-adaptativo-adagrad" class="nav-link" data-scroll-target="#descenso-del-gradiente-adaptativo-adagrad"><span class="toc-section-number">2.6.4</span>  Descenso del gradiente adaptativo (Adagrad)</a></li>
  <li><a href="#rms-prop" id="toc-rms-prop" class="nav-link" data-scroll-target="#rms-prop"><span class="toc-section-number">2.6.5</span>  RMS Prop</a></li>
  <li><a href="#adadelta" id="toc-adadelta" class="nav-link" data-scroll-target="#adadelta"><span class="toc-section-number">2.6.6</span>  AdaDelta</a></li>
  <li><a href="#adam" id="toc-adam" class="nav-link" data-scroll-target="#adam"><span class="toc-section-number">2.6.7</span>  Adam</a></li>
  </ul></li>
  <li><a href="#hiperpar√°metros-de-la-red" id="toc-hiperpar√°metros-de-la-red" class="nav-link" data-scroll-target="#hiperpar√°metros-de-la-red"><span class="toc-section-number">2.7</span>  Hiperpar√°metros de la red</a>
  <ul>
  <li><a href="#n√∫mero-y-tama√±o-de-capas" id="toc-n√∫mero-y-tama√±o-de-capas" class="nav-link" data-scroll-target="#n√∫mero-y-tama√±o-de-capas"><span class="toc-section-number">2.7.1</span>  N√∫mero y tama√±o de capas</a></li>
  <li><a href="#ratio-de-aprendizaje" id="toc-ratio-de-aprendizaje" class="nav-link" data-scroll-target="#ratio-de-aprendizaje"><span class="toc-section-number">2.7.2</span>  Ratio de aprendizaje</a></li>
  </ul></li>
  <li><a href="#regularizaci√≥n" id="toc-regularizaci√≥n" class="nav-link" data-scroll-target="#regularizaci√≥n"><span class="toc-section-number">2.8</span>  Regularizaci√≥n</a>
  <ul>
  <li><a href="#parada-temprana-early-stopping" id="toc-parada-temprana-early-stopping" class="nav-link" data-scroll-target="#parada-temprana-early-stopping"><span class="toc-section-number">2.8.1</span>  Parada temprana (early stopping)</a>
  <ul class="collapse">
  <li><a href="#m√©tricas-de-validaci√≥n" id="toc-m√©tricas-de-validaci√≥n" class="nav-link" data-scroll-target="#m√©tricas-de-validaci√≥n"><span class="toc-section-number">2.8.1.1</span>  M√©tricas de validaci√≥n</a></li>
  </ul></li>
  <li><a href="#aumento-de-datos-data-augmentation" id="toc-aumento-de-datos-data-augmentation" class="nav-link" data-scroll-target="#aumento-de-datos-data-augmentation"><span class="toc-section-number">2.8.2</span>  Aumento de datos (data augmentation)</a></li>
  <li><a href="#penalizaci√≥n-de-pesos" id="toc-penalizaci√≥n-de-pesos" class="nav-link" data-scroll-target="#penalizaci√≥n-de-pesos"><span class="toc-section-number">2.8.3</span>  Penalizaci√≥n de pesos</a>
  <ul class="collapse">
  <li><a href="#regularizaci√≥n-l2" id="toc-regularizaci√≥n-l2" class="nav-link" data-scroll-target="#regularizaci√≥n-l2"><span class="toc-section-number">2.8.3.1</span>  Regularizaci√≥n L2</a></li>
  <li><a href="#regularizaci√≥n-l1" id="toc-regularizaci√≥n-l1" class="nav-link" data-scroll-target="#regularizaci√≥n-l1"><span class="toc-section-number">2.8.3.2</span>  Regularizaci√≥n L1</a></li>
  <li><a href="#decaimiento-de-pesos-weight-decay" id="toc-decaimiento-de-pesos-weight-decay" class="nav-link" data-scroll-target="#decaimiento-de-pesos-weight-decay"><span class="toc-section-number">2.8.3.3</span>  Decaimiento de pesos (weight decay)</a></li>
  </ul></li>
  <li><a href="#drop-out" id="toc-drop-out" class="nav-link" data-scroll-target="#drop-out"><span class="toc-section-number">2.8.4</span>  Drop out</a></li>
  <li><a href="#normalizaci√≥n-por-lotes" id="toc-normalizaci√≥n-por-lotes" class="nav-link" data-scroll-target="#normalizaci√≥n-por-lotes"><span class="toc-section-number">2.8.5</span>  Normalizaci√≥n por lotes</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Entrenamiento de la red neuronal</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>El proceso de entrenamiento de una red neuronal consiste en ajustar el valor de los pesos y sesgos (umbrales) de forma que, las predicciones que se generen, tengan el menor error posible. Gracias a esto, el modelo es capaz de identificar qu√© predictores tienen mayor influencia y de qu√© forma est√°n relacionados entre ellos y con la variable respuesta.</p>
<p>La idea intuitiva de c√≥mo entrenar una red neuronal es la siguiente:</p>
<ol type="1">
<li><p>Iniciar la red con valores aleatorios de los pesos y sesgos.</p></li>
<li><p>Para cada conjunto de entrenamiento definido por los <em>inputs</em> y la respuesta <span class="math inline">\((ùëã, ùë¶)\)</span>, se debe calcular el error que comete la red al hacer sus predicciones, promediando los errores de todas las observaciones.</p></li>
<li><p>Identificar la responsabilidad que ha tenido cada peso y sesgo en el error de la predicci√≥n.</p></li>
<li><p>Modificar ligeramente los pesos y sesgos de la red (de forma proporcional a su responsabilidad en el error) en la direcci√≥n correcta para que se reduzca el error.</p></li>
<li><p>Repetir los pasos 2, 3, 4 y 5 hasta que la red sea suficientemente buena.</p></li>
</ol>
<p>Si bien la idea parece sencilla, alcanzar una forma de implementarla ha requerido la combinaci√≥n de m√∫ltiples m√©todos matem√°ticos, en concreto, el algoritmo de retropropagaci√≥n (<em>backpropagation</em>) y la optimizaci√≥n por descenso del gradiente (<em>gradient descent</em>).</p>
<p>En este tema profundizaremos en los conceptos relacionados con el entrenamiento de una red neuronal.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Paquetes anteriores</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">library</span>(sjPlot)</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="fu">library</span>(knitr) <span class="co"># para formatos de tablas</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="fu">theme_set</span>(<span class="fu">theme_sjplot2</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="preprocesado-de-inputs" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="preprocesado-de-inputs"><span class="header-section-number">2.1</span> Preprocesado de inputs</h2>
<p>A la hora de entrenar modelos basados en redes neuronales es necesario realizar, al menos, dos tipos de transformaciones de los datos. Procedemos igual que en cualquier otro algoritmo de aprendizaje autom√°tico.</p>
<p><strong>Codificaci√≥n (<em>One hot ecoding</em>) de las variables categ√≥ricas</strong></p>
<p>La binarizaci√≥n (<em>one-hot-encoding</em>) consiste en crear nuevas variables <em>dummy</em> con cada uno de los niveles de las variables cualitativas. Por ejemplo, una variable llamada color que contenga los niveles rojo, verde y azul, se convertir√° en tres nuevas variables (color_rojo, color_verde, color_azul), todas con el valor 0 excepto la que coincide con la observaci√≥n, que toma el valor 1.</p>
<p><strong>Estandarizaci√≥n y escalado de variables num√©ricas</strong></p>
<p>Cuando los predictores son num√©ricos, la escala en la que se miden, as√≠ como la magnitud de su varianza, pueden influir en gran medida en el modelo. Si no se igualan de alguna forma los predictores, aquellos que se midan en una escala mayor o que tengan m√°s varianza dominar√°n el modelo aunque no sean los que m√°s relaci√≥n tengan con la variable respuesta. Existen principalmente dos estrategias para evitarlo:</p>
<ul>
<li><p>Centrado: consiste en restarle a cada valor la media del predictor al que pertenece. Si los datos est√°n almacenados en un <em>dataframe</em>, el centrado se consigue rest√°ndole a cada valor la media de la columna en la que se encuentra. Como resultado de esta transformaci√≥n, todos los predictores pasan a tener una media de cero, es decir, los valores se centran en torno al origen.</p></li>
<li><p>Normalizaci√≥n (estandarizaci√≥n): consiste en transformar los datos de forma que todos los predictores est√©n aproximadamente en la misma escala. Las dos opciones habituales son:</p>
<ul>
<li>Normalizaci√≥n Z-score (<em>StandardScaler</em>): dividir cada predictor entre su desviaci√≥n t√≠pica despu√©s de haber sido centrado, de esta forma, los datos pasan a tener una distribuci√≥n normal.</li>
<li>Estandarizaci√≥n max-min (<em>MinMaxScaler</em>): transformar los datos de forma que est√©n dentro del rango [0, 1].</li>
</ul></li>
</ul>
</section>
<section id="funciones-de-activaci√≥n" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="funciones-de-activaci√≥n"><span class="header-section-number">2.2</span> Funciones de activaci√≥n</h2>
<p>Como ya vimos en el tema anterior, las funciones de activaci√≥n controlan en gran medida qu√© informaci√≥n se propaga desde una capa a la siguiente (<em>forward propagation</em>). Estas funciones convierten el valor neto de entrada a la red neuronal, combinaci√≥n de los <em>inputs</em>, pesos y sesgos, en un nuevo valor. En el cuaderno anterior ya vimos el comportamiento de la funci√≥n de activaci√≥n de salto, pero no es la √∫nica existente. En concreto, el uso de funciones de activaci√≥n no lineales con m√∫ltiples capas es lo que permite que los modelos de redes sean capaces de aprender relaciones no lineales.</p>
<p>La gran mayor√≠a de funciones de activaci√≥n convierten el valor de entrada neto de la neurona en un valor dentro del rango (0, 1) o (-1, 1). Cuando el valor de activaci√≥n de una neurona (salida de su funci√≥n de activaci√≥n) es cero, se dice que la neurona est√° inactiva, ya que no pasa ning√∫n tipo de informaci√≥n a las siguientes neuronas. A continuaci√≥n, se describen las funciones de activaci√≥n m√°s empleadas as√≠ como la derivada de dicha funci√≥n que ser√° utilizada en el proceso de entrenamiento de la red.</p>
<section id="sigmoide" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="sigmoide"><span class="header-section-number">2.2.1</span> Sigmoide</h3>
<p>La funci√≥n de activaci√≥n sigmoide acepta un n√∫mero como entrada y devuelve un n√∫mero entre 0 y 1. Es f√°cil de usar y tiene todas las cualidades deseables de las funciones de activaci√≥n: no linealidad, diferenciaci√≥n continua, monotonicidad y un rango de salida establecido.</p>
<p>Se utiliza principalmente en problemas de clasificaci√≥n binaria, ya que su salida puede interpretarse como probabilidades, ya que nos proporciona la probabilidad de existencia de una clase determinada. Matem√°ticamente se define como:</p>
<p><span class="math display">\[sigmoid(s) = S(s) = \frac{1}{1+e^{-s}}.\]</span> La derivada de la funci√≥n viene dada por la expresi√≥n:</p>
<p><span class="math display">\[S'(s) = S(s)(1-S(s))\]</span></p>
<p>Entre las ventajas y desventajas del uso de esta funci√≥n podemos mencionar:</p>
<ul>
<li>Es de naturaleza no lineal. Las combinaciones de esta funci√≥n tambi√©n son no lineales, y dar√° una activaci√≥n anal√≥gica, a diferencia de la funci√≥n de activaci√≥n de salto. Adem√°s, esta funci√≥n presenta un gradiente suave y es efectiva para el problema de clasificaci√≥n.</li>
<li>El resultado de la funci√≥n de activaci√≥n siempre va a estar en el rango <span class="math inline">\((0,1)\)</span> en comparaci√≥n con <span class="math inline">\((-‚àû, ‚àû)\)</span> de la funci√≥n de activaci√≥n lineal. Como resultado, hemos definido un rango para nuestras activaciones.</li>
<li>La funci√≥n sigmoide da lugar a un problema de ‚Äúgradientes evanescentes‚Äù (<em>‚ÄúVanishing gradients‚Äù</em>) y los sigmoides saturan y matan los gradientes. El problema de ‚Äúgradientes evanescentes‚Äù es frecuente en el entrenamiento de redes neuronales. Dado que la funci√≥n de activaci√≥n tiene un rango de salida peque√±o (de 0 a 1), un gran cambio en el <em>input</em> de la funci√≥n de activaci√≥n crear√° una peque√±a modificaci√≥n en la salida. Por lo tanto, la derivada tambi√©n se vuelve peque√±a. Estas funciones de activaci√≥n s√≥lo se utilizan en redes poco profundas con pocas capas. Cuando estas funciones de activaci√≥n se aplican a una red multicapa, el gradiente puede llegar a ser demasiado peque√±o para el entrenamiento esperado.</li>
<li>Su resultado no est√° centrado en cero, y hace que las actualizaciones del gradiente fluct√∫en lejos en diferentes direcciones.</li>
<li>El valor de salida est√° entre cero y uno, por lo que dificulta la optimizaci√≥n.</li>
<li>En ocasiones la red se niega a aprender m√°s o es extremadamente lenta.</li>
</ul>
</section>
<section id="tangente-hiperb√≥lica" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="tangente-hiperb√≥lica"><span class="header-section-number">2.2.2</span> Tangente hiperb√≥lica</h3>
<p>La funci√≥n tangente hiperb√≥lica comprime un n√∫mero real al rango [-1, 1]. Es no lineal, pero es diferente de la anterior (<em>Sigmoid</em>), y su salida est√° centrada en cero. Su definici√≥n formal es:</p>
<p><span class="math display">\[TanH(s) = \frac{e^s-e^{-s}}{e^s+e^{-s}}\]</span></p>
<p>La ventaja que tiene esta funci√≥n de activaci√≥n es que los <em>inputs</em> negativos se convierten en valores fuertemente negativos y los <em>inputs</em> positivos se convierten en valores fuertemente positivos. Los resultados tienden a los extremos. Al igual que en la funci√≥n sigmoide, es diferenciable y mon√≥tona mientras que su derivada no lo es. Esta funci√≥n de activaci√≥n se utiliza principalmente para la clasificaci√≥n entre dos clases, ya que si la tendencia es hacia uno de los lados la funci√≥n lo arrastrar√° m√°s a√∫n para ese lado.</p>
<p>La derivada de la funci√≥n viene dada por la expresi√≥n:</p>
<p><span class="math display">\[TanH'(s) = 1-Tanh^2(s)\]</span> Entre sus ventajas e inconvenientes podemos destacar:</p>
<ul>
<li>TanH tambi√©n tiene el problema del gradiente evanescente, pero el gradiente es m√°s fuerte en TanH que en sigmoide (las derivadas son m√°s pronunciadas).</li>
<li>TanH est√° centrada en cero, y los gradientes no tienen que moverse en una direcci√≥n espec√≠fica.</li>
</ul>
</section>
<section id="relu-rectified-linear-unit" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="relu-rectified-linear-unit"><span class="header-section-number">2.2.3</span> ReLU (Rectified linear Unit)</h3>
<p>ReLU significa Unidad Lineal Rectificada y es una de las funciones de activaci√≥n m√°s utilizadas en las aplicaciones. Ha resuelto el problema del gradiente evanescente porque el valor m√°ximo del gradiente de la funci√≥n ReLU es uno. Tambi√©n resuelve el problema de la saturaci√≥n de la neurona, ya que la pendiente nunca es cero para la funci√≥n ReLU. El rango de ReLU est√° entre 0 e infinito.</p>
<p>Formalmente se define como:</p>
<p><span class="math display">\[ReLU(s) = max\{0,s\}\]</span></p>
<p>¬øCu√°l es la diferencia de esta funci√≥n con la de salto que la hace tan interesante? La clave est√° en que todos los valores negativos se vuelven cero, y eso significa que cualquier entrada negativa dada a la funci√≥n de activaci√≥n de ReLU convierte el valor en cero inmediatamente. Esto puede ayudar mucho en la simplificaci√≥n computacional ya que todos los valores iguales a 0 son inmediatamente descartados (dichas neuronas son irrelevantes). A su vez esto puede disminuir la capacidad del modelo para ajustarse o entrenarse a partir de los datos correctamente. Es el motivo por el que su uso esta muy extendido en las redes convolucionales.</p>
<p>En este caso la derivada de la funci√≥n ReLU toma el valor 1 si <span class="math inline">\(s&gt;0\)</span> y <span class="math inline">\(0\)</span> en otro caso.</p>
<p>Entre las ventajas e incovenientes de esta funci√≥n podemos destacar:</p>
<ul>
<li>Dado que s√≥lo se activa un cierto n√∫mero de neuronas, la funci√≥n ReLU es mucho m√°s eficiente desde el punto de vista computacional que las funciones sigmoide y TanH.</li>
<li>ReLU acelera la convergencia del descenso gradiente hacia el m√≠nimo global de la funci√≥n de p√©rdida gracias a su propiedad lineal y no saturante.</li>
<li>Una de sus limitaciones es que s√≥lo debe utilizarse dentro de las capas ocultas de un modelo de red neuronal artificial.</li>
<li>Algunos gradientes pueden ser fr√°giles durante el entrenamiento. En otras palabras, para activaciones en la regi√≥n (<span class="math inline">\(x&lt;0\)</span>) de ReLU, el gradiente ser√° 0 debido a lo cual los pesos no se ajustar√°n durante el descenso. Es decir, las neuronas que entren en ese estado dejar√°n de responder a las variaciones en la entrada (simplemente porque el gradiente es 0, nada cambia). A esto se le llama el problema del ReLU moribundo.</li>
</ul>
</section>
<section id="softmax" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="softmax"><span class="header-section-number">2.2.4</span> Softmax</h3>
<p>La funci√≥n <em>Softmax</em> es una de las funciones estrella en la √∫ltima capa de la red, ya que permite realizar una clasificaci√≥n categ√≥rica multiclase.</p>
<p>Es una combinaci√≥n de muchos sigmoides. Al igual que la funci√≥n sigmoide, devuelve una probabilidad, s√≥lo que en este caso lo hace para cada clase/etiqueta. M√°s concretamente la funci√≥n <em>Softmax</em> devuelve la probabilidad de la clase actual con respecto a las dem√°s. Esto significa que tambi√©n tiene en cuenta la posibilidad de que existan otras clases.</p>
<p>Si <span class="math inline">\(s_i\)</span> donde <span class="math inline">\(i\)</span> identifica la clase <span class="math inline">\(i\)</span> de un conjunto de <span class="math inline">\(k\)</span> clases esta funci√≥n se puede expresar como:</p>
<p><span class="math display">\[Softmax(s_i) = \frac{exp(s_i)}{\sum_{j=1}^k exp(s_j)}\]</span></p>
<p>Entre las ventajas e inconvenientes podemos destacar que:</p>
<ul>
<li>Imita mejor la etiqueta codificada que los valores absolutos.</li>
<li>Se perder√≠a informaci√≥n si se utilizaran valores absolutos (m√≥dulo), pero la exponencial se encarga de esto por s√≠ sola.</li>
<li>Deber√≠a utilizarse tambi√©n para tareas de clasificaci√≥n y regresi√≥n multietiqueta.</li>
</ul>
</section>
<section id="otras-funciones-de-activaci√≥n" class="level3" data-number="2.2.5">
<h3 data-number="2.2.5" class="anchored" data-anchor-id="otras-funciones-de-activaci√≥n"><span class="header-section-number">2.2.5</span> Otras funciones de activaci√≥n</h3>
<p>Existen m√°s funciones de activaci√≥n pero hemos preferido centrarnos en estas porque son las de uso m√°s habitual. M√°s adelante introduciremos nuevas funciones de activaci√≥n cuando sea necesario.</p>
</section>
</section>
<section id="funciones-de-p√©rdida" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="funciones-de-p√©rdida"><span class="header-section-number">2.3</span> Funciones de p√©rdida</h2>
<p>Cuando se trabaja en un problema de aprendizaje autom√°tico o aprendizaje profundo se utilizan funciones de p√©rdida/coste para optimizar el modelo durante el entrenamiento. El objetivo es casi siempre minimizar la funci√≥n de p√©rdida. Cuanto menor sea la p√©rdida, mejor ser√° el modelo. En las redes neuronales este aspecto resulta muy relevante ya que la predici√≥n o salida de la red depende de la estructura de la red (nodos y capas ocultas). En este apartado se presenta la notaci√≥n y las funciones utilizadas en las redes neuronales tanto para las tareas de clasificaci√≥n como de regresi√≥n.</p>
<p>Para ejemplificar supongamos que tenemos una red con una arquitectura <span class="math inline">\([p,m,1]\)</span>, es decir con <span class="math inline">\(p\)</span> <em>inputs</em>, <span class="math inline">\(m\)</span> neuronas artificiales en una √∫nica capa, y un nodo de salida. Consideramos:</p>
<ul>
<li><span class="math inline">\((X,y)\)</span> el conjunto de valores (<em>inputs</em> y respuesta) observados para <span class="math inline">\(n\)</span> muestras, de forma que <span class="math inline">\(x^{(i)}\)</span> e <span class="math inline">\(y^{(i)}\)</span> son los <em>inputs</em> y respuesta de la muestra <span class="math inline">\(i\)</span> con <span class="math inline">\(i=1,...,n\)</span>.</li>
<li><span class="math inline">\(W\)</span> la matriz de pesos de la red.</li>
<li><span class="math inline">\(f\)</span> la funci√≥n de activaci√≥n utilizada para obtener la respuesta para un <em>input</em> dado como</li>
</ul>
<p><span class="math display">\[\hat{y}^{(i)} = f(x^{(i)};W)\]</span></p>
<p>Definimos entonces la funci√≥n de p√©rdida (<span class="math inline">\(L\)</span>) para una muestra espec√≠fica como</p>
<p><span class="math display">\[L(\hat{y}^{(i)}, y^{(i)})= L(f(x^{(i)};W), y^{(i)})\]</span></p>
<p>de forma que la p√©rdida emp√≠rica para el conjunto de muestras dado viene dada para una matriz de pesos <span class="math inline">\(W\)</span> como</p>
<p><span class="math display">\[J(W)=\frac{1}{n} \sum_{i=1}^n L(f(x^{(i)};W), y^{(i)})\]</span></p>
<p>A continuaci√≥n, veremos la expresi√≥n espec√≠fica para la p√©rdida emp√≠rica en funci√≥n del tipo de repuesta que tratamos de predecir con la red o tipo de tarea que deseamos resolver: clasificaci√≥n binaria, regresi√≥n o clasificaci√≥n m√∫ltiple.</p>
<section id="clasificaci√≥n-binaria" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="clasificaci√≥n-binaria"><span class="header-section-number">2.3.1</span> Clasificaci√≥n binaria</h3>
<p>Para tareas de clasificaci√≥n binaria donde la respuesta observada <span class="math inline">\(y^{(i)}\)</span> para cada una de las muestras consideradas s√≥lo toma valores 0 o 1, se define la p√©rdida de entrop√≠a cruzada binaria (‚Äú<em>Binary Cross-Entropy</em>‚Äù) como:</p>
<span class="math display">\[\begin{eqnarray}
J(W) =&amp;-\frac{1}{n}\sum_{i=1}^n \left [ y^{(i)}log(f(x^{(i)};W)) + (1-y^{(i)})log(1-f(x^{(i)};W))\right ]\\
=&amp;-\frac{1}{n}\sum_{i=1}^n \left [ y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})\right ]
\end{eqnarray}\]</span>
<p>donde <span class="math inline">\(p^{(i)} = f(x^{(i)};W)\)</span> es la probabilidad predicha de clasificaci√≥n de la clase 1 mediante la red neuronal considerada con pesos estimados <span class="math inline">\(W\)</span>.</p>
<p>La principal ventaja de esta funci√≥n de p√©rdida es que es diferenciable, pero entre las deventajas debemos destacar que tiene m√∫ltiples m√≠nimos locales y que no es una medida de error muy intuitiva.</p>
</section>
<section id="clasificaci√≥n-m√∫ltiple" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="clasificaci√≥n-m√∫ltiple"><span class="header-section-number">2.3.2</span> Clasificaci√≥n m√∫ltiple</h3>
<p>Para tareas de clasificaci√≥n m√∫ltiple donde la respuesta observada <span class="math inline">\(y^{(i)}\)</span> para cada una de las muestras consideradas puede tomar valores en el conjunto <span class="math inline">\(\{1, 2,...,M\}\)</span>, se define la p√©rdida de entrop√≠a cruzada binaria para clasificaciones m√∫ltiples (‚Äú<em>Binary Cross Entropy for Multi-Class classification</em>‚Äù) como:</p>
<span class="math display">\[\begin{eqnarray}
J(W) =&amp;-\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^M \left [ y^{(i,j)}log(f(x^{(i)};W)) \right ]\\
&amp;-\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^M \left [ y^{(i,j)}log(p^{(i,j)}) \right ]
\end{eqnarray}\]</span>
<p>donde <span class="math inline">\(y^{(i,j)}\)</span> es el elento <span class="math inline">\(j\)</span> del vector <span class="math inline">\((y^{(i,1)}, y^{(i,2)},..., y^{(i,M)})\)</span> que representa la codificaci√≥n <em>hot-encoding</em> para la respuesta <span class="math inline">\(y^{(i)}\)</span>, y <span class="math inline">\(p^{(i,j)}\)</span> representa la probabilidad predicha por la red neuronal de que la muestra <span class="math inline">\(i\)</span> pertenezca a la clase <span class="math inline">\(j\)</span>.</p>
</section>
<section id="predicci√≥n" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="predicci√≥n"><span class="header-section-number">2.3.3</span> Predicci√≥n</h3>
<p>En tareas de predicci√≥n podemos considerar diferentes funciones de p√©rdida.</p>
<section id="error-cuadr√°tico-medio" class="level4" data-number="2.3.3.1">
<h4 data-number="2.3.3.1" class="anchored" data-anchor-id="error-cuadr√°tico-medio"><span class="header-section-number">2.3.3.1</span> Error cuadr√°tico medio</h4>
<p>El error cuadr√°tico medio o <em>mean squared error</em> (MSE) es la funci√≥n de p√©rdida m√°s sencilla y com√∫n para evaluar la soluci√≥n obtenida en una tarea de predicci√≥n. Si <span class="math inline">\(y^{(i)}\)</span> denota el valor real de la repuesta e <span class="math inline">\(\hat{y}^{(i)}\)</span> el valor predicho mediante la red considerada con pesos W, para calcular el MSE, se toma la diferencia entre el valor real y la predicci√≥n del modelo, se eleva al cuadrado y se calcula la media de todo el conjunto de datos:</p>
<span class="math display">\[\begin{eqnarray}
J(W) =&amp;-\frac{1}{n}\sum_{i=1}^n \left ( y^{(i)}-f(x^{(i)};W) \right )^2\\
&amp;-\frac{1}{n}\sum_{i=1}^n \left ( y^{(i)}-\hat{y}^{(i)} \right )^2
\end{eqnarray}\]</span>
<p>Las principales ventajas de esta funci√≥n de p√©rdida son que es muy f√°cil de interpretar, siempre es diferenciable y s√≥lo tiene un m√≠nimo local. Entre las desventajas podemos decir que el error est√° en unidades al cuadrado, y que es muy sensible a la presencia de observaciones an√≥malas.</p>
</section>
<section id="error-absoluto-medio" class="level4" data-number="2.3.3.2">
<h4 data-number="2.3.3.2" class="anchored" data-anchor-id="error-absoluto-medio"><span class="header-section-number">2.3.3.2</span> Error absoluto medio</h4>
<p>El error absoluto medio o <em>mean absolute error</em> (MAE) es otra funci√≥n de p√©rdida muy sencilla. Si <span class="math inline">\(y^{(i)}\)</span> denota el valor real de la repuesta e <span class="math inline">\(\hat{y}^{(i)}\)</span> el valor predicho mediante la red considerada con pesos W, para calcular el MAE, se toma la diferencia en valor absoluto entre el valor real y la predicci√≥n del modelo, promediando para todo el conjunto de datos:</p>
<span class="math display">\[\begin{eqnarray}
J(W) =&amp;-\frac{1}{n}\sum_{i=1}^n  \left |y^{(i)}-f(x^{(i)};W) \right |\\
&amp;-\frac{1}{n}\sum_{i=1}^n \left | y^{(i)}-\hat{y}^{(i)} \right |
\end{eqnarray}\]</span>
<p>Las principales ventajas de esta funci√≥n de p√©rdida son que su resultado est√° en las mismas unidades que la variable respuesta, y que es insensible a observaciones an√≥malas. La principal desventaja es que la funci√≥n de p√©rdida no es diferenciable y no se puede utilizar directamente en procesos de optimizaci√≥n.</p>
</section>
</section>
</section>
<section id="proceso-de-optimizaci√≥n" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="proceso-de-optimizaci√≥n"><span class="header-section-number">2.4</span> Proceso de optimizaci√≥n</h2>
<p>Como hemos visto en el punto anterior todas las funciones de p√©rdida dependen de los valores de los pesos <span class="math inline">\(W\)</span> de la arquitectura de red considerada. Por tanto, para optimizar la arquitectura de dicha red es necesario obtener los valores de <span class="math inline">\(W\)</span> que minimicen el error de predicci√≥n o clasificaci√≥n dependiendo de la tarea de inter√©s. En el cuaderno siguiente estudiaremos con detalle los algoritmos de optimizaci√≥n utilizados habitualmente, pero en este presentanmos las ideas generales. Distinguimos entre arquitecturas de redes con una √∫nica capa y otras con m√∫ltiples capas ocultas.</p>
<section id="arquitectura-de-red-con-una-capa-oculta" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="arquitectura-de-red-con-una-capa-oculta"><span class="header-section-number">2.4.1</span> Arquitectura de red con una capa oculta</h3>
<p>Supongamos que disponemos de <span class="math inline">\(n\)</span> muestras con una arquitectura de red <span class="math inline">\([p,m,1]\)</span> con matriz de datos <span class="math inline">\((X,y)\)</span>, <span class="math inline">\(W\)</span> matriz de pesos, y <span class="math inline">\(f\)</span> la funci√≥n de activaci√≥n. Independientemente de si la tarea es de clasificaci√≥n o predicci√≥n, para obtener los valores √≥ptimos de la matriz de pesos de la red, <span class="math inline">\(W^*\)</span>, debemos minimizar la funci√≥n de p√©rdida emp√≠rica considerada con respecto a <span class="math inline">\(W\)</span>, es decir:</p>
<p><span class="math display">\[W^* = \underset{W}{argmin} \quad J(W)\]</span></p>
<p><span class="math display">\[W^* = \underset{W}{argmin} \quad \frac{1}{n} L(f(x^{(i)};W),y^{(i)})\]</span></p>
<p>Por tanto, para encontrar el √≥ptimo queda claro que necesitamos la derivadas de la funci√≥n de activaci√≥n considerada.</p>
</section>
<section id="arquitectura-de-red-con-m√∫ltiples-capas-ocultas" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="arquitectura-de-red-con-m√∫ltiples-capas-ocultas"><span class="header-section-number">2.4.2</span> Arquitectura de red con m√∫ltiples capas ocultas</h3>
<p>Si tenemos una arquitectura de red con m√∫ltiples capas ocultas la matriz de pesos se organiza para las diferentes capas consideradas, es decir consideramos:</p>
<p><span class="math display">\[W = \{W^{(0)}, W^{(1)},...\}\]</span></p>
<p>donde <span class="math inline">\(W^{(i)}\)</span> es la matriz de pesos correspondiente a la capa oculta <span class="math inline">\(i+1\)</span>. Recordemos que el proceso de propagaci√≥n hacia adelante utiliza la salida de una capa oculta para valorar la activaci√≥n de la capa siguiente en funci√≥n de los pesos de dicha capa. En esta situaci√≥n el proceso de optimizaci√≥n viene dado por:</p>
<p><span class="math display">\[W^* = \underset{W}{argmin} \quad J(W)\]</span></p>
<p><span class="math display">\[W^* = \underset{W}{argmin} \quad \frac{1}{n} L(f(x^{(i)};W),y^{(i)})\]</span></p>
<p>donde <span class="math inline">\(W = \{W^{(0)}, W^{(1)},...\}\)</span>.</p>
<p>En este caso el proceso de optimizaci√≥n resulta algo m√°s complejo, ya que debemos enlazar las derivadas de las funciones de activaci√≥n en cada una de las capas ocultas para obtener los pesos de la arquitectura de la red. Es lo que denominamos como algortimo de <em>Backpropagation</em> para combinar el proceso de optimizaci√≥n de las diferentes capas, mientras que utilizamos el m√©todo del gradiente descendente para estimar los pesos de una capa en espec√≠fico.</p>
</section>
</section>
<section id="algoritmo-de-backpropagation" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="algoritmo-de-backpropagation"><span class="header-section-number">2.5</span> Algoritmo de backpropagation</h2>
<p><code>Backpropagation</code> es el algoritmo que permite cuantificar la influencia que tiene cada peso y bias de la red en sus predicciones. Para conseguirlo, hace uso de la regla de la cadena (<em>chain rule</em>) para calcular el gradiente, que no es m√°s que el vector formado por las derivadas parciales de una funci√≥n.</p>
<p>En el caso de las redes, la derivada parcial del error respecto a un par√°metro (peso o bias) mide cuanta ‚Äúresponsabilidad‚Äù ha tenido ese par√°metro en el error cometido. Gracias a esto, se puede identificar qu√© pesos de la red hay que modificar para mejorarla.</p>
<p>De hecho el objetivo principal de este algoritmo es encontrar las derivadas de la p√©rdida o el error con respecto a cada peso de la red, y actualizar estos pesos en la direcci√≥n opuesta a sus derivadas respetadas, de modo que se muevan hacia los m√≠nimos globales o locales de la funci√≥n de coste o error.</p>
<p>Para entender mejor el funcionamiento de este algoritmo, y ver c√≥mo integrar los algortimos de optimizaci√≥n de pesos en el proceso, presentamos su funcionamiento te√≥rico para una arquitectura de red con una capa oculta.</p>
<p>Por simplicidad consideramos la red que viene representada en la figura siguiente:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ia4legos/Deeplearning/main/images/Backward001.png" width="650" height="100" class="figure-img"></p>
</figure>
</div>
<p>donde tenemos una red con una √∫nica capa coculta, y con funci√≥n de p√©rdida emp√≠rica para la matriz de pesos <span class="math inline">\(W=\{W1,W2\}\)</span> dada por <span class="math inline">\(J(W)\)</span>. Como ya vimos en el cuaderno anterior el objetivo es encontrar los valores de <span class="math inline">\(W\)</span> que minimizan la funci√≥n de p√©rdida. Utilizando la regla de la cadena el proceso de optimizaci√≥n se puede escribir como:</p>
<p><span class="math display">\[\frac{\partial J(W)}{\partial W1} = \frac{\partial J(W)}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial Z} \frac{\partial Z}{\partial W1}\]</span></p>
<p>Este proceso se repite para cada peso de la red utilizando los gradientes de capas posteriores.</p>
<p>Por tanto, para resolver el entrenamiento del modelo es necesario determinar los gradientes anteriores actualizando los pesos de cada capa de la red para reducir la funci√≥n de p√©rdida considerada.</p>
</section>
<section id="algoritmos-de-optimizaci√≥n" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="algoritmos-de-optimizaci√≥n"><span class="header-section-number">2.6</span> Algoritmos de optimizaci√≥n</h2>
<p>Los algoritmos optimizadores son m√©todos de optimizaci√≥n que ayudan a mejorar el rendimiento de un modelo de aprendizaje profundo. Estos algoritmos de optimizaci√≥n u optimizadores afectan en gran medida a la precisi√≥n y la velocidad de entrenamiento del modelo de aprendizaje profundo. Pero antes de nada, surge la pregunta de qu√© es un optimizador.</p>
<p>Mientras se entrena el modelo de aprendizaje profundo los optimizadores modifican los pesos de cada iteraci√≥n (<em>epoch</em>) y minimizan la funci√≥n de p√©rdida. Un optimizador es una funci√≥n o un algoritmo que ajusta los atributos de la red neuronal, como los pesos y las tasas de aprendizaje. De este modo, ayuda a reducir la p√©rdida global y a mejorar la precisi√≥n. El problema de elegir los pesos adecuados para el modelo es una tarea de enormes proporciones, ya que un modelo de aprendizaje profundo suele constar de millones de par√°metros. Esto plantea la necesidad de elegir un algoritmo de optimizaci√≥n adecuado para su aplicaci√≥n. De ah√≠ que la comprensi√≥n de estos algoritmos de aprendizaje autom√°tico sea necesaria para los cient√≠ficos de datos antes de sumergirse a fondo en este campo.</p>
<p>Se pueden utilizar diferentes optimizadores en el modelo de aprendizaje autom√°tico para cambiar sus pesos y su tasa de aprendizaje. Sin embargo, elegir el mejor optimizador depende de la aplicaci√≥n. Como principiante, un mal pensamiento que nos viene a la mente es que probamos todas las posibilidades y elegimos la que muestra los mejores resultados. Esto puede estar bien al principio, pero cuando se trata de cientos de gigabytes de datos, incluso una sola <em>epoch</em> puede llevar un tiempo considerable. As√≠ que elegir un algoritmo al azar puede resultar en una p√©rdida de tiempo.</p>
<p>Este apartado cubrir√° varios optimizadores de aprendizaje profundo, como Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent, Adagrad, RMSProp, AdaDelta, y Adam.</p>
<p>Antes de continuar, recordemos algunos t√©rminos con los que nos debemos familiarizar:</p>
<ul>
<li><em>Epoch</em> (√âpoca) - Denota el n√∫mero de veces que el algoritmo se ejecuta en todo el conjunto de datos de entrenamiento.</li>
<li><em>Batch</em> (Lote) - N√∫mero de muestras que se tomar√°n para actualizar los par√°metros del modelo.</li>
<li><em>Learning rate</em> (Tasa de aprendizaje) - Es un par√°metro que proporciona al modelo una escala de cu√°nto deben actualizarse los pesos del modelo.</li>
</ul>
<section id="descenso-del-gradiente" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="descenso-del-gradiente"><span class="header-section-number">2.6.1</span> Descenso del gradiente</h3>
<p>El algortimo de descenso del gradiente puede considerarse como el m√°s famoso de todos los optimizadores. En concreto permite minimizar una funci√≥n haciendo actualizaciones de sus par√°metros en la direcci√≥n del valor negativo de su gradiente. Aplicado a las redes neuronales y, como ya vimos en el cuaderno de introducci√≥n a las redes neuronales, este algoritmo se implementa de la forma siguiente:</p>
<ol type="1">
<li><p>Fijamos unos pesos inciales <span class="math inline">\(W\)</span> y evaluamos la funci√≥n de p√©rdida correspondiente.</p></li>
<li><p>Hasta alcanzar convergencia, es decir, hasta alcanzar un m√≠nimo local, procedemos de la siguiente forma fijando una tasa de aprendizaje <span class="math inline">\(\eta\)</span>:</p></li>
</ol>
<ul>
<li>Calcular el gradiente <span class="math inline">\(\frac{\partial J(W)}{\partial W_t}\)</span></li>
<li>Actualizar los pesos mediante la expresi√≥n siguiente y reevaluar la funci√≥n de p√©rdida</li>
</ul>
<p><span class="math display">\[ W_{t+1} = W_t - \eta  \frac{\partial J(W)}{\partial W_t}\]</span></p>
<ol start="3" type="1">
<li>Una vez alcanzada la convergencia devolvemos los pesos de la √∫ltima iteraci√≥n y el valor de la funci√≥n de p√©rdida corrrespondiente.</li>
</ol>
<p>El algoritmo de descenso del gradiente funciona bien en la mayor√≠a de situaciones pero, sin embargo, tambi√©n tiene algunos inconvenientes:</p>
<ul>
<li>Es costoso calcular los gradientes si el tama√±o de los datos es enorme.</li>
<li>El descenso de gradiente funciona bien para funciones convexas, pero no sabe qu√© distancia recorrer a lo largo del gradiente para funciones no convexas.</li>
<li>Hay que fijar una tasa de aprendizaje que puede afectar en gran medida al proceso de optimizaci√≥n, ya que en ocasiones puede ralentizar mucho el proceso o nos puede llevar a un m√≠nimo local que no es √≥ptimo.</li>
</ul>
<p>La funci√≥n siguiente nos permite ver el funcionamiento del descenso del gradiente para un learning rate y funci√≥n espec√≠fica.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Funci√≥n para visualizar el algoritmo del gradiente descendente</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>ver_descenso_gradiente <span class="ot">=</span> <span class="cf">function</span>(learning_rate, f)</span>
<span id="cb2-3"><a href="#cb2-3"></a>{</span>
<span id="cb2-4"><a href="#cb2-4"></a>    <span class="co"># Visualizaci√≥n gr√°fica del m√©todo del descenso del gradiente</span></span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="co"># learning_rate: ratio de aprendizaje</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>    <span class="co"># f: funci√≥n a optimizar</span></span>
<span id="cb2-8"><a href="#cb2-8"></a></span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="co"># return: soluci√≥n gr√°fica del algoritmo</span></span>
<span id="cb2-10"><a href="#cb2-10"></a>    </span>
<span id="cb2-11"><a href="#cb2-11"></a>      <span class="co"># N√∫mero m√°ximo de iteraciones</span></span>
<span id="cb2-12"><a href="#cb2-12"></a>      maximum_iterations <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb2-13"><a href="#cb2-13"></a>      <span class="co"># Iteraci√≥n actual</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>      current_iteration <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb2-15"><a href="#cb2-15"></a>      <span class="co"># precisi√≥n de la soluci√≥n</span></span>
<span id="cb2-16"><a href="#cb2-16"></a>      precision_value<span class="ot">=</span><span class="fl">1e-6</span></span>
<span id="cb2-17"><a href="#cb2-17"></a>      previous_step_size <span class="ot">=</span> <span class="fl">0.5</span></span>
<span id="cb2-18"><a href="#cb2-18"></a>      current_x_value <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb2-19"><a href="#cb2-19"></a>      h <span class="ot">=</span> <span class="fl">0.01</span></span>
<span id="cb2-20"><a href="#cb2-20"></a>      x_iterativo<span class="ot">=</span><span class="fu">c</span>()</span>
<span id="cb2-21"><a href="#cb2-21"></a>      fx_iterativo<span class="ot">=</span><span class="fu">c</span>()</span>
<span id="cb2-22"><a href="#cb2-22"></a>      <span class="cf">while</span>((previous_step_size<span class="sc">&gt;</span>precision_value) <span class="sc">&amp;</span> (current_iteration<span class="sc">&lt;</span>maximum_iterations))</span>
<span id="cb2-23"><a href="#cb2-23"></a>      {</span>
<span id="cb2-24"><a href="#cb2-24"></a>            previous_x_value <span class="ot">=</span> current_x_value</span>
<span id="cb2-25"><a href="#cb2-25"></a>            <span class="co">#versi√≥n num√©rica</span></span>
<span id="cb2-26"><a href="#cb2-26"></a>            gradient_of_y<span class="ot">=</span>(<span class="fu">f</span>(current_x_value <span class="sc">+</span> h) <span class="sc">-</span> <span class="fu">f</span>(current_x_value <span class="sc">-</span> h))<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>h)</span>
<span id="cb2-27"><a href="#cb2-27"></a>            current_x_value <span class="ot">=</span> current_x_value <span class="sc">-</span> learning_rate <span class="sc">*</span> gradient_of_y</span>
<span id="cb2-28"><a href="#cb2-28"></a>            x_iterativo <span class="ot">=</span> <span class="fu">c</span>(x_iterativo, current_x_value)</span>
<span id="cb2-29"><a href="#cb2-29"></a>            previous_step_size <span class="ot">=</span> <span class="fu">abs</span>(current_x_value <span class="sc">-</span> previous_x_value)</span>
<span id="cb2-30"><a href="#cb2-30"></a>            fx_iterativo <span class="ot">=</span> <span class="fu">c</span>(fx_iterativo, <span class="fu">f</span>(current_x_value))</span>
<span id="cb2-31"><a href="#cb2-31"></a>            current_iteration <span class="ot">=</span> current_iteration <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb2-32"><a href="#cb2-32"></a>        }</span>
<span id="cb2-33"><a href="#cb2-33"></a>        x <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">5</span>, <span class="at">length=</span><span class="dv">100</span>)</span>
<span id="cb2-34"><a href="#cb2-34"></a>        <span class="fu">plot</span>(x, <span class="fu">f</span>(x),<span class="st">"l"</span>, <span class="at">xlab=</span><span class="st">"x"</span>,<span class="at">ylab=</span><span class="st">"f(x)"</span>, <span class="at">main =</span><span class="fu">paste</span>(<span class="st">"LR= "</span>,learning_rate))</span>
<span id="cb2-35"><a href="#cb2-35"></a>        <span class="fu">lines</span>(x_iterativo, fx_iterativo,<span class="at">col=</span><span class="st">"red"</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb2-36"><a href="#cb2-36"></a>        <span class="fu">points</span>(x_iterativo, fx_iterativo,<span class="at">col=</span><span class="st">"red"</span>,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb2-37"><a href="#cb2-37"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Veamos si el algoritmo es capaz de llegar al m√≠nimo de la funci√≥n <span class="math inline">\(f(x) = x^2-6x+5\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Definimos la funci√≥n objetivo</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>f1x <span class="ot">=</span> <span class="cf">function</span>(x)</span>
<span id="cb3-3"><a href="#cb3-3"></a>{</span>
<span id="cb3-4"><a href="#cb3-4"></a>  <span class="fu">return</span>(x<span class="sc">*</span>x<span class="dv">-6</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a>}</span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co"># Descenso del gradiente para diferentes tasas de aprendizaje</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>lr <span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.9</span>,<span class="fl">0.5</span>,<span class="fl">0.1</span>,<span class="fl">0.01</span>,<span class="fl">0.001</span>, <span class="fl">0.0001</span>,<span class="fl">0.00001</span>)</span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(lr))</span>
<span id="cb3-10"><a href="#cb3-10"></a>{</span>
<span id="cb3-11"><a href="#cb3-11"></a>  <span class="fu">ver_descenso_gradiente</span>(lr[i], f1x)</span>
<span id="cb3-12"><a href="#cb3-12"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="20_TrainDL_files/figure-html/trainDL-002-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="20_TrainDL_files/figure-html/trainDL-002-2.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="20_TrainDL_files/figure-html/trainDL-002-3.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="20_TrainDL_files/figure-html/trainDL-002-4.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="20_TrainDL_files/figure-html/trainDL-002-5.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="20_TrainDL_files/figure-html/trainDL-002-6.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="20_TrainDL_files/figure-html/trainDL-002-7.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="20_TrainDL_files/figure-html/trainDL-002-8.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>De los resultados obtenidos podemos ver que la tasa de aprendizaje juega un papel relevante en la convergencia del algoritmo, ya que en ocasiones necesitamos las 1000 iteraciones prefijadas y no alcanzamos el √≥ptimo, mientras que en otras situaciones con pocas iteraciones somos capaces de alcanzar el √≥ptimo. Incluso con una funci√≥n tan sencilla como esta el algoritmo muestra comportamientos inadecuados.</p>
</section>
<section id="descenso-del-gradiente-estoc√°stico" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="descenso-del-gradiente-estoc√°stico"><span class="header-section-number">2.6.2</span> Descenso del gradiente estoc√°stico</h3>
<p>Al final de la secci√≥n anterior, vimos que utilizar el descenso de gradiente podr√≠a no ser la mejor opci√≥n para encontrar el √≥ptimo de la funci√≥n. Para abordar el problema se plantea el algoritmo del descenso de gradiente estoc√°stico. El t√©rmino estoc√°stico proviene de la aleatoriedad en la que se basa el algoritmo. En el descenso del gradiente estoc√°stico, en lugar de tomar todo el conjunto de datos para cada iteraci√≥n, seleccionamos aleatoriamente los lotes de datos. Esto significa que s√≥lo tomamos unas pocas muestras del conjunto de datos.</p>
<p>El procedimiento consiste en seleccionar primero los par√°metros iniciales y la tasa de aprendizaje para, a continuaci√≥n, barajar aleatoriamente los datos en cada iteraci√≥n para alcanzar un m√≠nimo aproximado.</p>
<p>Dado que no utilizamos todo el conjunto de datos, el camino que recorre el algoritmo est√° lleno de ruido en comparaci√≥n con el algoritmo del descenso del gradiente. Por lo tanto, SGD utiliza un mayor n√∫mero de iteraciones para alcanzar los m√≠nimos locales. Al aumentar el n√∫mero de iteraciones, aumenta el tiempo total de c√°lculo. Pero incluso despu√©s de aumentar el n√∫mero de iteraciones, el coste computacional sigue siendo menor que el del optimizador de descenso del gradiente.</p>
<p>A continuaci√≥n vemos la estructura del algoritmo:</p>
<ol type="1">
<li><p>Fijamos unos pesos inciales <span class="math inline">\(W\)</span> y evaluamos la funci√≥n de p√©rdida correspondiente.</p></li>
<li><p>Hasta alcanzar convergencia, es decir, hasta alcanzar un m√≠nimo local, procedemos de la siguiente forma fijando una tasa de aprendizaje <span class="math inline">\(\eta\)</span>:</p></li>
</ol>
<ul>
<li>Elegimos aleatoriamente un punto <span class="math inline">\(i\)</span> de la muestra.</li>
<li>Calcular el gradiente sobre dicho punto <span class="math inline">\(\frac{\partial J_i(W)}{\partial W_t}\)</span></li>
<li>Actualizar los pesos</li>
</ul>
<p><span class="math display">\[ W_{t+1} = W_{t} - \eta  \frac{\partial J_i(W)}{\partial W_t}\]</span></p>
<ol start="3" type="1">
<li>Una vez alcanzada la convergencia devolvemos los pesos de la √∫ltima iteraci√≥n y el valor de la funci√≥n de p√©rdida correspondiente.</li>
</ol>
</section>
<section id="descenso-del-gradiente-por-mini-lotes" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="descenso-del-gradiente-por-mini-lotes"><span class="header-section-number">2.6.3</span> Descenso del gradiente por mini lotes</h3>
<p>En esta variante del algortimo de descenso del gradiente, en lugar de tomar todos los datos de entrenamiento, s√≥lo se utiliza un subconjunto del conjunto de datos para calcular la funci√≥n de p√©rdida. Dado que utilizamos un lote de datos en lugar de todo el conjunto de datos, se necesitan menos iteraciones. Por este motivo, el algoritmo de descenso del gradiente por mini lotes es m√°s r√°pido que los algoritmos de descenso del gradiente anteriores. Adem√°s es m√°s eficiente y robusto que las variantes anteriores del descenso del gradiente. Como el algoritmo utiliza el procesamiento por lotes, no es necesario cargar todos los datos de entrenamiento en la memoria, lo que hace que el proceso sea m√°s eficiente de implementar. Adem√°s, la funci√≥n de coste del algoritmo de descenso del gradiente contiene m√°s variabilidad que las de los algortimos anteriores pero es m√°s suave que la del algoritmo de descenso del gradiente estoc√°stico. Por todo ello, el descenso del gradiente por mini lotes es ideal y proporciona un buen equilibrio entre velocidad y precisi√≥n.</p>
<p>Otra ventaja importante es que la estructura del algoritmo permite la paralelizaci√≥n de c√°lculos, lo que permite acelerar todav√≠a m√°s el proceso de aprendizaje.</p>
<p>A pesar de todo, el algoritmo de descenso del gradiente por mini lotes tambi√©n tiene algunas desventajas. Necesita un hiperpar√°metro que es el ‚Äútama√±o de mini lotes‚Äù, que debe ajustarse para lograr la precisi√≥n requerida. Aunque el tama√±o de lote de 32 se considera adecuado para casi todos los casos. Adem√°s, en algunos casos, resulta en una precisi√≥n final pobre. Por ello, es necesario buscar tambi√©n otras alternativas.</p>
<p>A continuaci√≥n vemos la estructura del algoritmo:</p>
<ol type="1">
<li><p>Fijamos unos pesos inciales <span class="math inline">\(W\)</span> y evaluamos la funci√≥n de p√©rdida correspondiente.</p></li>
<li><p>Hasta alcanzar convergencia, es decir, hasta alcanzar un m√≠nimo local, procedemos de la siguiente forma fijando una tasa de aprendizaje <span class="math inline">\(\eta\)</span>:</p></li>
</ol>
<ul>
<li>Elegimos aleatoriamente un lote de tama√±o <span class="math inline">\(B\)</span> de muestras.</li>
<li>Calcular el gradiente como</li>
</ul>
<p><span class="math display">\[\frac{\partial J_i(W)}{\partial W_t} = \frac{1}{B} \sum_{k=1}^B \frac{\partial J_k(W)}{\partial W_t}\]</span></p>
<ul>
<li>Actualizar los pesos</li>
</ul>
<p><span class="math display">\[ W_{t+1} = W_{t} - \eta  \frac{\partial J_i(W)}{\partial W_t}\]</span></p>
<ol start="3" type="1">
<li>Una vez alcanzada la convergencia devolvemos los pesos de la √∫ltima iteraci√≥n y el valor de la funci√≥n de p√©rdida corrrespondiente.</li>
</ol>
</section>
<section id="descenso-del-gradiente-adaptativo-adagrad" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="descenso-del-gradiente-adaptativo-adagrad"><span class="header-section-number">2.6.4</span> Descenso del gradiente adaptativo (Adagrad)</h3>
<p>El algoritmo de descenso del gradiente adaptativo es ligeramente diferente de otros algoritmos de descenso del gradiente. Esto se debe a que utiliza diferentes tasas de aprendizaje para cada iteraci√≥n. El cambio en la tasa de aprendizaje depende de la diferencia en los par√°metros durante el entrenamiento. Cuanto m√°s cambian los par√°metros, menos cambia la tasa de aprendizaje. Esta modificaci√≥n es muy beneficiosa porque los conjuntos de datos del mundo real contienen caracter√≠sticas tanto dispersas como densas. Por lo tanto, es injusto tener el mismo valor de tasa de aprendizaje para todas las caracter√≠sticas.</p>
<p>El algoritmo Adagrad utiliza la siguiente f√≥rmula para actualizar los pesos, donde <span class="math inline">\(\eta_t\)</span> denota las diferentes tasas de aprendizaje en cada iteraci√≥n, <span class="math inline">\(\eta\)</span> es la tasa de aprendizaje inicial que act√∫a como una constante, y <span class="math inline">\(\epsilon\)</span> es un valor positivo peque√±o para evitar la divisi√≥n por 0:</p>
<p><span class="math display">\[W_t = W_{t-1} - \alpha_t \frac{\partial J(W)}{\partial W_{t-1}}\]</span></p>
<p><span class="math display">\[\alpha_t = \frac{\eta}{\sqrt{\eta_t + \epsilon}}\]</span></p>
<p>El algoritmo completo queda de la forma siguiente:</p>
<ol type="1">
<li><p>Fijamos unos pesos inciales <span class="math inline">\(W\)</span>, una tasa de aprendizaje <span class="math inline">\(\eta\)</span>, un valor de <span class="math inline">\(\epsilon\)</span> y evaluamos la funci√≥n de p√©rdida correspondiente.</p></li>
<li><p>Hasta alcanzar convergencia, es decir, hasta alcanzar un m√≠nimo local, procedemos de la siguiente forma en cada iteraci√≥n <span class="math inline">\(t\)</span>:</p></li>
</ol>
<ul>
<li>Calcular el gradiente</li>
</ul>
<p><span class="math display">\[\frac{\partial J(W)}{\partial W_{t}}\]</span></p>
<ul>
<li>Actualizar las tasas de aprendizaje <span class="math inline">\(\alpha_t\)</span></li>
</ul>
<p><span class="math display">\[\alpha_{t+1} = \frac{\eta}{\sqrt{\eta_t + \epsilon}}\]</span></p>
<ul>
<li>Actualizar los pesos</li>
</ul>
<p><span class="math display">\[ W_{t+1} = W_{t} - \alpha_t \frac{\partial J(W)}{\partial W_{t}}\]</span></p>
<ol start="3" type="1">
<li>Una vez alcanzada la convergencia devolvemos los pesos de la √∫ltima iteraci√≥n y el valor de la funci√≥n de p√©rdida corrrespondiente.</li>
</ol>
<p>La ventaja de utilizar Adagrad es que elimina la necesidad de modificar manualmente la tasa de aprendizaje. Es m√°s fiable que los algoritmos de descenso gradiente y sus variantes, y alcanza la convergencia a mayor velocidad.</p>
<p>Un inconveniente del optimizador AdaGrad es que disminuye la tasa de aprendizaje de forma agresiva y monot√≥nica. Puede llegar un momento en que la tasa de aprendizaje sea extremadamente peque√±a. Esto se debe a que los gradientes al cuadrado en el denominador siguen acumul√°ndose y, por tanto, la parte del denominador sigue aumentando. Debido a las peque√±as tasas de aprendizaje, el modelo acaba siendo incapaz de adquirir m√°s conocimientos y, por tanto, la precisi√≥n del modelo se ve comprometida.</p>
</section>
<section id="rms-prop" class="level3" data-number="2.6.5">
<h3 data-number="2.6.5" class="anchored" data-anchor-id="rms-prop"><span class="header-section-number">2.6.5</span> RMS Prop</h3>
<p>RMS Prop es uno de los optimizadores m√°s populares entre los entusiastas del aprendizaje profundo. Esto se debe quiz√°s a que no ha sido publicado pero sigue siendo muy conocido en la comunidad. RMS Prop es idealmente una extensi√≥n del trabajo RPPROP. Resuelve el problema de los gradientes variables. El problema de los gradientes es que algunos son peque√±os mientras que otros pueden ser enormes. Por lo tanto, definir una √∫nica tasa de aprendizaje puede no ser la mejor idea. RPPROP utiliza el signo del gradiente, adaptando el tama√±o del paso individualmente para cada peso. En este algoritmo, primero se comparan los signos de los dos gradientes. Si tienen el mismo signo, vamos en la direcci√≥n correcta, aumentando el tama√±o del paso en una peque√±a fracci√≥n. Si tienen signos opuestos, debemos disminuir el tama√±o del paso. Entonces limitamos el tama√±o del paso y ya podemos pasar a la actualizaci√≥n del peso.</p>
<p>El problema con RPPROP es que no funciona bien con grandes conjuntos de datos y cuando queremos realizar actualizaciones en mini lotes. Por lo tanto, lograr la robustez de RPPROP y la eficiencia de los mini-lotes simult√°neamente fue la principal motivaci√≥n detr√°s del surgimiento de RMS Prop. RMS Prop es un avance en el optimizador AdaGrad, ya que reduce la tasa de aprendizaje monot√≥nicamente decreciente.</p>
<p>El algoritmo se centra principalmente en acelerar el proceso de optimizaci√≥n disminuyendo el n√∫mero de evaluaciones de la funci√≥n para alcanzar el m√≠nimo local. El algoritmo mantiene la media m√≥vil de los gradientes al cuadrado para cada peso y divide el gradiente por la ra√≠z cuadrada del cuadrado medio. La actualizaci√≥n de los pesos se obtiene como:</p>
<p><span class="math display">\[W_{t+1} = W_{t} - \eta_t \frac{\partial J(W)}{\partial W_t}\]</span></p>
<p>con</p>
<p><span class="math display">\[\eta_t = \frac{\eta}{\sqrt{v_{t+1}}+\epsilon}\]</span></p>
<p><span class="math display">\[v_{t+1} = \alpha v_t + \left(1-\alpha \left[\frac{\partial J(W)}{\partial W_t}\right]^2\right)\]</span></p>
<p>donde <span class="math inline">\(\eta\)</span> es la taza global de aprendizaje, <span class="math inline">\(\epsilon\)</span> es un valor infinitesimal (del orden de <span class="math inline">\(10^{-7}\)</span> o <span class="math inline">\(10^{-8}\)</span>) para evitar errores de divisi√≥n por cero, y <span class="math inline">\(v_{t+1}\)</span> es la estimaci√≥n del segundo momento.</p>
<p>En t√©rminos m√°s sencillos, si existe un par√°metro debido al cual la funci√≥n de coste oscila mucho, queremos penalizar la actualizaci√≥n de este par√°metro. Supongamos que ha construido un modelo para clasificar una variedad de peces. El modelo se basa principalmente en el factor ‚Äúcolor‚Äù para diferenciar los peces. Debido a esto, comete muchos errores. Lo que hace RMS Prop es penalizar el par√°metro ‚Äúcolor‚Äù para que pueda basarse tambi√©n en otras caracter√≠sticas. Esto evita que el algoritmo se adapte demasiado r√°pido a los cambios en el par√°metro ‚Äúcolor‚Äù en comparaci√≥n con otros par√°metros. Este algoritmo tiene varias ventajas en comparaci√≥n con las versiones anteriores de los algoritmos de descenso del gradiente. El algoritmo converge r√°pidamente y requiere menos ajustes que los algoritmos de descenso del gradiente y sus variantes.</p>
<p>El problema con RMS Prop es que la tasa de aprendizaje tiene que definirse manualmente, y el valor sugerido no funciona para todas las aplicaciones.</p>
</section>
<section id="adadelta" class="level3" data-number="2.6.6">
<h3 data-number="2.6.6" class="anchored" data-anchor-id="adadelta"><span class="header-section-number">2.6.6</span> AdaDelta</h3>
<p>AdaDelta puede considerarse una versi√≥n m√°s robusta del optimizador AdaGrad. Se basa en el aprendizaje adaptativo y est√° dise√±ado para hacer frente a importantes inconvenientes del optimizador AdaGrad y RMS Prop. El principal problema de los dos optimizadores anteriores es que la tasa de aprendizaje inicial debe definirse manualmente. Otro problema es la tasa de aprendizaje decreciente, que llega a ser infinitesimalmente peque√±a en alg√∫n momento. Debido a esto, un cierto n√∫mero de iteraciones m√°s tarde, el modelo ya no puede aprender nuevos conocimientos.</p>
</section>
<section id="adam" class="level3" data-number="2.6.7">
<h3 data-number="2.6.7" class="anchored" data-anchor-id="adam"><span class="header-section-number">2.6.7</span> Adam</h3>
<p>El nombre Adam procede de <em>adaptive moment estimation</em> (estimaci√≥n adaptativa del momento). Este algoritmo de optimizaci√≥n es una extensi√≥n del descenso del gradiente estoc√°stico para actualizar los pesos de la red durante el entrenamiento. A diferencia de mantener una √∫nica tasa de aprendizaje durante el entrenamiento con el descenso del gradiente estoc√°stico (SGD), el optimizador Adam actualiza la tasa de aprendizaje para cada peso de red individualmente. Los creadores del algoritmo de optimizaci√≥n Adam conocen las ventajas de los algoritmos AdaGrad y RMSProp, que tambi√©n son extensiones de los algoritmos de descenso del gradiente estoc√°stico. De ah√≠ que los optimizadores Adam hereden las caracter√≠sticas de los algoritmos Adagrad y RMSProp. En Adam, en lugar de adaptar las tasas de aprendizaje bas√°ndose en el primer momento (media) como en RMS Prop, tambi√©n utiliza el segundo momento de los gradientes. Nos referimos a la varianza no centrada por el segundo momento de los gradientes (no restamos la media).</p>
<p>El optimizador Adam tiene varias ventajas, por lo que se utiliza ampliamente. Se ha adaptado como punto de referencia para trabajos de aprendizaje profundo y se recomienda como algoritmo de optimizaci√≥n por defecto. Adem√°s, el algoritmo es f√°cil de implementar, tiene un tiempo de ejecuci√≥n m√°s r√°pido, bajos requisitos de memoria y requiere menos ajustes que cualquier otro algoritmo de optimizaci√≥n.</p>
<p>La actualizaci√≥n de los pesos viene dada por la expresi√≥n:</p>
<p><span class="math display">\[w_{t+1} = w_t - \eta \frac{m_t}{\sqrt{v_{t+1} + \epsilon}}\]</span></p>
<p>donde</p>
<p><span class="math display">\[m_{t+1}  = \beta m_t + (1-\beta)\frac{\partial J(W)}{\partial W_t}\]</span></p>
<p><span class="math display">\[v_{t+1} = \alpha v_t + (1-\alpha)\left[\frac{\partial J(W)}{\partial W_t}\right]^2\]</span></p>
<p>con <span class="math inline">\(v_{t+1}\)</span> es la estimaci√≥n del segundo momento, y <span class="math inline">\(m_{t+1}\)</span> es el promedio exponencial del momento.</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container" title="algoritmos">
<p><strong>Para complemetar los aspectos te√≥ricos de los diferentes algortimos de optimizaci√≥n y otros que no hemos presentado en este apartado se puede consultar el libro gratuito que aparece en este enlace http://d2l.ai/chapter_optimization/index.html..</strong></p>
</div>
</div>
</div>
</section>
</section>
<section id="hiperpar√°metros-de-la-red" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="hiperpar√°metros-de-la-red"><span class="header-section-number">2.7</span> Hiperpar√°metros de la red</h2>
<p>La gran ‚Äúflexibilidad‚Äù que tienen las redes neuronales es un arma de doble filo. Por un lado, son capaces de generar modelos que aprenden relaciones muy complejas, sin embargo, sufren f√°cilmente el problema de sobreajuste (<em>overfitting</em>) lo que los incapacita al tratar de predecir nuevas observaciones. La forma de minimizar este problema y conseguir modelos √∫tiles pasa por configurar de forma adecuada sus hiperpar√°metros.</p>
<section id="n√∫mero-y-tama√±o-de-capas" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="n√∫mero-y-tama√±o-de-capas"><span class="header-section-number">2.7.1</span> N√∫mero y tama√±o de capas</h3>
<p>La arquitectura de una red, el n√∫mero de capas y el n√∫mero de neuronas que forman parte de cada capa, determinan en gran medida la complejidad del modelo y con ello su potencial capacidad de aprendizaje.</p>
<p>Las capas de entrada y salida son sencillas de establecer. La capa de entrada tiene tantas neuronas como predictores y la capa de salida tiene una neurona en problemas de regresi√≥n y tantas como clases en problemas de clasificaci√≥n. En la mayor√≠a de implementaciones, estos valores se establecen autom√°ticamente en funci√≥n del conjunto de entrenamiento. El usuario suele especificar √∫nicamente el n√∫mero de capas intermedias (ocultas) y el tama√±o de las mismas.</p>
<p>Cuantas m√°s neuronas y capas, mayor la complejidad de las relaciones que puede aprender el modelo. Sin embargo, dado que en cada neurona est√° conectada por pesos al resto de neuronas de las capas adyacentes, el n√∫mero de par√°metros a aprender aumenta y con ello el tiempo de entrenamiento.</p>
</section>
<section id="ratio-de-aprendizaje" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="ratio-de-aprendizaje"><span class="header-section-number">2.7.2</span> Ratio de aprendizaje</h3>
<p>El <em>learning rate</em> o ratio de aprendizaje establece c√≥mo de r√°pido pueden cambiar los par√°metros de un modelo a medida que se optimiza (aprende). Este hiperpar√°metro es uno de los m√°s complicados de establecer, ya que depende mucho de los datos e interacciona con el resto de hiperpar√°metros. Si el <em>learning rate</em> es muy grande, el proceso de optimizaci√≥n puede ir saltando de una regi√≥n a otra sin que el modelo sea capaz de aprender. Si por el contrario, el <em>learning rate</em> es muy peque√±o, el proceso de entrenamiento puede tardar demasiado y no llegar a completarse. Algunas de las recomendaciones heur√≠sticas basadas en prueba y error son:</p>
<ul>
<li><p>Utilizar un <em>learning rate</em> lo m√°s peque√±o posible siempre y cuando el tiempo de entrenamiento no supere las limitaciones temporales disponibles.</p></li>
<li><p>No utilizar un valor constante de <em>learning rate</em> durante todo el proceso de entrenamiento. Por lo general, utilizar valores mayores al inicio y peque√±os al final.</p></li>
</ul>
<p>De hecho, el algoritmo de optimizaci√≥n establecido para el proceso de entrenamiento ya implementa las diferentes posibilidades de ratio de aprendizaje.</p>
</section>
</section>
<section id="regularizaci√≥n" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="regularizaci√≥n"><span class="header-section-number">2.8</span> Regularizaci√≥n</h2>
<p>Las redes neuronales pueden aprender a representar relaciones complejas entre las entradas y salidas de la red. Este poder de representaci√≥n las ayuda a rendir mejor que los algoritmos tradicionales de aprendizaje autom√°tico en tareas de visi√≥n por ordenador y procesamiento del lenguaje natural. Sin embargo, uno de los retos asociados al entrenamiento de redes neuronales es el sobreajuste.</p>
<p>Cuando una red neuronal se adapta en exceso al conjunto de datos de entrenamiento, aprende una representaci√≥n excesivamente compleja que modela el conjunto de datos de entrenamiento demasiado bien. Como resultado, su rendimiento es excepcional en el conjunto de datos de entrenamiento, pero su generalizaci√≥n a los datos de prueba es deficiente.</p>
<p>Las t√©cnicas de regularizaci√≥n ayudan a mejorar la capacidad de generalizaci√≥n de una red neuronal reduciendo el sobreajuste. Para ello, minimizan la complejidad innecesaria y exponen la red a datos m√°s diversos. A continuaci√≥n hacemos un repaso de las t√©cnicas de regularizaci√≥n m√°s habituales en el periodo de entrenamiento de la red.</p>
<section id="parada-temprana-early-stopping" class="level3" data-number="2.8.1">
<h3 data-number="2.8.1" class="anchored" data-anchor-id="parada-temprana-early-stopping"><span class="header-section-number">2.8.1</span> Parada temprana (early stopping)</h3>
<p>La parada temprana es una de las t√©cnicas de regularizaci√≥n m√°s sencillas e intuitivas. Consiste en detener el entrenamiento de la red neuronal en una √©poca anterior, de ah√≠ su nombre.</p>
<p>Pero, ¬øc√≥mo y cu√°ndo se detiene? A medida que se entrena la red neuronal durante muchas √©pocas, el error de entrenamiento disminuye.</p>
<p>Si el error de entrenamiento es demasiado bajo y se acerca arbitrariamente a cero, la red se ajustar√° en exceso al conjunto de datos de entrenamiento. Una red neuronal de este tipo es un modelo de alta varianza que funciona mal en datos de prueba que nunca ha visto antes a pesar de su rendimiento casi perfecto en las muestras de entrenamiento.</p>
<p>Por lo tanto, heur√≠sticamente, si podemos evitar que la p√©rdida de entrenamiento sea arbitrariamente baja, es menos probable que el modelo se ajuste en exceso al conjunto de datos de entrenamiento y generalizar√° mejor.</p>
<p>¬øC√≥mo lo hacemos en la pr√°ctica?</p>
<section id="m√©tricas-de-validaci√≥n" class="level4" data-number="2.8.1.1">
<h4 data-number="2.8.1.1" class="anchored" data-anchor-id="m√©tricas-de-validaci√≥n"><span class="header-section-number">2.8.1.1</span> M√©tricas de validaci√≥n</h4>
<p>Un m√©todo sencillo consiste en controlar m√©tricas como el error de validaci√≥n y la precisi√≥n de validaci√≥n a medida que avanza el entrenamiento de la red neuronal, y utilizarlas para decidir cu√°ndo parar.</p>
<p>Si vemos que el error de validaci√≥n no disminuye significativamente o aumenta en un intervalo de √©pocas, digamos p √©pocas, podemos detener el entrenamiento. Tambi√©n podemos reducir la tasa de aprendizaje y entrenar unas cuantas √©pocas m√°s antes de parar. En la imagen siguiente (extra√≠da de https://www.pinecone.io/learn/regularization-in-neural-networks/) podemos ver el cambio en las m√©tricas y el punto de parada temprana.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ia4legos/Deeplearning/main/images/earlystop01.png" width="650" height="300" class="figure-img"></p>
</figure>
</div>
<p>De forma equivalente, se puede pensar en t√©rminos de la precisi√≥n de la red neuronal en los conjuntos de datos de entrenamiento y validaci√≥n. Detenerse antes de tiempo cuando el error de validaci√≥n empieza a aumentar (o deja de disminuir) equivale a detenerse cuando la precisi√≥n de validaci√≥n empieza a disminuir.</p>
<p><img src="https://raw.githubusercontent.com/ia4legos/Deeplearning/main/images/earlystop02.png" data-fig-align="center" width="650" height="300"> #### Monitorizando el cambio en el vector de pesos</p>
<p>Otra forma de saber cu√°ndo parar es controlar el cambio en los pesos de la red. Sean <span class="math inline">\(w^{(t)}\)</span> y <span class="math inline">\(w^{(t-k)}\)</span> los vectores de pesos en las √©pocas <span class="math inline">\(t\)</span> y <span class="math inline">\(t-k\)</span> respectivamente. Podemos calcular la norma l2 para la diferencia de los vectores anteriores y detener el entrenamiento cuando esta sea suficientemente peque√±a, digamos una cantidad <span class="math inline">\(epsilon\)</span>, es decir:</p>
<p><span class="math display">\[||w^{(t)} - w^{(t-k)}|| &lt; \epsilon\]</span></p>
<p>Pero este enfoque de utilizar la norma del vector diferencia no es muy fiable. ¬øPor qu√©? Algunos pesos pueden haber cambiado mucho en las √∫ltimas k √©pocas, mientras que otros pueden haber sufrido cambios insignificantes. Por lo tanto, la norma del vector diferencia resultante puede ser peque√±a a pesar del cambio dr√°stico en ciertos componentes del vector peso.</p>
<p>Un enfoque mejor es calcular el cambio en componentes individuales del vector de pesos. Si el cambio m√°ximo (en todos los componentes) es inferior a <span class="math inline">\(\epsilon\)</span> podemos concluir que los pesos no est√°n cambiando significativamente, por lo que podemos detener el entrenamiento de la red neuronal. Matem√°ticamente:</p>
<p><span class="math display">\[\underset{i}{max} |w_i^{(t)} - w_i^{(t-k)}| &lt; \epsilon\]</span></p>
</section>
</section>
<section id="aumento-de-datos-data-augmentation" class="level3" data-number="2.8.2">
<h3 data-number="2.8.2" class="anchored" data-anchor-id="aumento-de-datos-data-augmentation"><span class="header-section-number">2.8.2</span> Aumento de datos (data augmentation)</h3>
<p>El aumento de datos es una t√©cnica de regularizaci√≥n que ayuda a una red neuronal a generalizar mejor exponi√©ndola a un conjunto m√°s diverso de ejemplos de entrenamiento. Como las redes neuronales profundas requieren un gran conjunto de datos de entrenamiento, el aumento de datos tambi√©n es √∫til cuando no tenemos datos suficientes para entrenar una red neuronal.</p>
<p>Tomemos el ejemplo del aumento de datos de im√°genes. Supongamos que tenemos un conjunto de datos con N ejemplos de entrenamiento en C clases. Podemos aplicar ciertas transformaciones a estas N im√°genes para construir un conjunto de datos mayor.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ia4legos/Deeplearning/main/images/dataaugmentation01.png" width="650" height="200" class="figure-img"></p>
</figure>
</div>
<p>¬øQu√© es una transformaci√≥n v√°lida? Es cualquier operaci√≥n que no altere la etiqueta original de los datos. Por ejemplo, un panda es un panda, est√© mirando a la derecha o a la izquierda, situado cerca del centro de la imagen o en una de las esquinas.</p>
<p>En resumen: podemos aplicar cualquier transformaci√≥n invariante de la etiqueta para realizar el aumento de datos. He aqu√≠ algunos ejemplos:</p>
<ul>
<li>Transformaciones del espacio de color, como el cambio de las intensidades de los p√≠xeles.</li>
<li>Rotaci√≥n y reflejo.</li>
<li>Inyecci√≥n de ruido, distorsi√≥n y desenfoque.</li>
</ul>
<p>Adem√°s de las transformaciones b√°sicas del espacio de color y de la imagen geom√©trica, existen nuevas t√©cnicas de aumento de la imagen. Mixup es una t√©cnica de regularizaci√≥n que utiliza una combinaci√≥n convexa de entradas existentes para aumentar el conjunto de datos.</p>
<p>Supongamos que <span class="math inline">\(x_i\)</span> y <span class="math inline">\(x_j\)</span> son muestras de entrada pertenecientes a las clases <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span>, respectivamente; <span class="math inline">\(y_i\)</span> y <span class="math inline">\(y_j\)</span> son los vectores unidireccionales correspondientes a las etiquetas de clase <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span>, respectivamente. Se puede formar forma una nueva imagen tomando una combinaci√≥n convexa de <span class="math inline">\(x_i\)</span> y <span class="math inline">\(x_j\)</span>:</p>
<span class="math display">\[\begin{eqnarray}
\tilde{x} = &amp;\lambda x_i +(1-\lambda)x_j\\
\tilde{y} = &amp;\lambda y_i +(1-\lambda)y_j\\
\end{eqnarray}\]</span>
<p>con <span class="math inline">\(\lambda \in [0,1]\)</span>.</p>
<p>Otros m√©todos de aumento de datos son Cutout, CutMix y AugMix. Cutout implica la eliminaci√≥n aleatoria de partes de una imagen de entrada durante el entrenamiento. CutMix sustituye las secciones eliminadas por partes de otra imagen. AugMix es una t√©cnica de regularizaci√≥n que hace que una red neuronal sea robusta a los cambios de distribuci√≥n. A diferencia de Mixup, que utiliza im√°genes de dos clases diferentes, AugMix realiza una serie de transformaciones en la misma imagen y, a continuaci√≥n, utiliza una composici√≥n de estas im√°genes transformadas para obtener la imagen resultante.</p>
</section>
<section id="penalizaci√≥n-de-pesos" class="level3" data-number="2.8.3">
<h3 data-number="2.8.3" class="anchored" data-anchor-id="penalizaci√≥n-de-pesos"><span class="header-section-number">2.8.3</span> Penalizaci√≥n de pesos</h3>
<p>En este caso actuamos como en otros muchos algoritmos de aprendiaje autom√°tico donde se a√±aden restricciones o penalizaciones sobre los coeficientes del modelo utilizado. En este caso se trata de introducir penalizaciones sobre los pesos de la red neuronal.</p>
<section id="regularizaci√≥n-l2" class="level4" data-number="2.8.3.1">
<h4 data-number="2.8.3.1" class="anchored" data-anchor-id="regularizaci√≥n-l2"><span class="header-section-number">2.8.3.1</span> Regularizaci√≥n L2</h4>
<p>La idea detr√°s de este tipo de regularizaci√≥n es reducir el valor de los par√°metros para que sean peque√±os. Esta t√©cnica introduce un t√©rmino adicional de penalizaci√≥n en la funci√≥n de coste original, a√±adiendo a su valor la suma de los cuadrados de los par√°metros, es decir, consideramos una nueva funci√≥n de coste que viene dada por:</p>
<p><span class="math display">\[J_2(W) = J(W) + \lambda \sum w_i^2\]</span></p>
<p>La mala noticia es que este nuevo t√©rmino puede ser alto; tanto que la red minimizar√≠a la funci√≥n de coste haciendo los par√°metros muy cercanos a 0, lo que no ser√≠a nada conveniente. Es por ello que multiplicaremos ese sumando por una constante (<span class="math inline">\(\lambda\)</span>) peque√±a, cuyo valor escogeremos de forma arbitraria (0.1, 0.01, ‚Ä¶). La actualizaci√≥n de pesos en el proceso de entrenamiento de la red viene dado entonces (para SGD) por:</p>
<p><span class="math display">\[ W_{t+1} = W_{t} - \eta  \left[\frac{\partial J(W)}{\partial W_t} +2\lambda W_t\right]\]</span></p>
<p>El par√°metro <span class="math inline">\(\lambda\)</span> controla la regularizaci√≥n, de forma que si deseamos m√°s podemos aumentar el valor de <span class="math inline">\(\lambda\)</span>.</p>
</section>
<section id="regularizaci√≥n-l1" class="level4" data-number="2.8.3.2">
<h4 data-number="2.8.3.2" class="anchored" data-anchor-id="regularizaci√≥n-l1"><span class="header-section-number">2.8.3.2</span> Regularizaci√≥n L1</h4>
<p>Existe otra t√©cnica muy parecida a la anterior denominada regularizaci√≥n L1 donde los par√°metros en el sumatorio del t√©rmino de penalizaci√≥n no se elevan al cuadrado, sino que se usa su valor absoluto:</p>
<p><span class="math display">\[J_1(W) = J(W) + \lambda \sum |w_i|\]</span></p>
<p>de forma que la actualizaci√≥n de pesos viene dada por:</p>
<p><span class="math display">\[W_{t+1} = W_{t} - \eta  \left[\frac{\partial J(W)}{\partial W_t} +\lambda sgn(W_t)\right]\]</span></p>
<p>donde <span class="math inline">\(sgn()\)</span> es la funci√≥n signo.</p>
<p>Esta variante empuja el valor de los par√°metros hacia valores m√°s peque√±os, haciendo incluso que la influencia de algunas variables de entrada sea nula en la salida de la red, lo que supone una selecci√≥n de variables autom√°tica. El resultado es una una mejor generalizaci√≥n, pero s√≥lo hasta cierto punto (la elecci√≥n del valor de Œª cobra m√°s importancia en este caso).</p>
</section>
<section id="decaimiento-de-pesos-weight-decay" class="level4" data-number="2.8.3.3">
<h4 data-number="2.8.3.3" class="anchored" data-anchor-id="decaimiento-de-pesos-weight-decay"><span class="header-section-number">2.8.3.3</span> Decaimiento de pesos (weight decay)</h4>
<p>Esta t√©cnica podr√≠amos decir que es id√©ntica a la regularizaci√≥n L2, pero aplicada en otro punto. En lugar de introducir la penalizaci√≥n como un sumando en la funci√≥n de coste, la a√±adimos como un t√©rmino extra en la f√≥rmula de actualizaci√≥n de los pesos:</p>
<p><span class="math display">\[W_{t+1} = W_{t} - \eta  \left[\frac{\partial J(W)}{\partial W_t} +\lambda W_t\right]\]</span></p>
<p>Como vemos esta actualizaci√≥n es pr√°cticamente igual a la actualizaci√≥n de los pesos en la regularizaci√≥n L2, salvo que en este caso no aparece un 2 multiplicando en el t√©rmino a√±adido.</p>
</section>
</section>
<section id="drop-out" class="level3" data-number="2.8.4">
<h3 data-number="2.8.4" class="anchored" data-anchor-id="drop-out"><span class="header-section-number">2.8.4</span> Drop out</h3>
<p><em>Drop out</em> es uno de los tipos de t√©cnicas de regularizaci√≥n m√°s interesantes. Tambi√©n produce muy buenos resultados y, en consecuencia, es la t√©cnica de regularizaci√≥n m√°s utilizada en el campo del aprendizaje profundo.</p>
<p>Para entender c√≥mo funciona el abandono, conviene repasar el concepto de modelos de conjunto.</p>
<p>En el aprendizaje autom√°tico tradicional, los modelos de conjunto ayudan a reducir el sobreajuste y a mejorar el rendimiento del modelo. Para un problema de clasificaci√≥n simple, podemos adoptar uno de los siguientes enfoques:</p>
<ul>
<li>Entrenar varios clasificadores para resolver la misma tarea.</li>
<li>Entrenar diferentes instancias del mismo clasificador para diferentes subconjuntos del conjunto de datos de entrenamiento.</li>
</ul>
<p>Para un modelo de clasificaci√≥n simple, una t√©cnica de conjunto como el <em>bagging</em> implica entrenar el mismo clasificador en diferentes subconjuntos de datos de entrenamiento, muestreados con reemplazo. Supongamos que hay N instancias. En el momento de la prueba, cada clasificador pasa por la muestra de prueba y se utiliza un conjunto de sus predicciones.</p>
<p>En general, el rendimiento de un conjunto es al menos tan bueno como el de los modelos individuales; no puede ser peor que el de los modelos individuales.</p>
<p>Si traslad√°ramos esta idea a las redes neuronales, podr√≠amos intentar hacer lo siguiente (identificando al mismo tiempo las limitaciones de este enfoque):</p>
<ul>
<li>Entrenar varias redes neuronales con diferentes arquitecturas. Entrenar una red neuronal en diferentes subconjuntos de los datos de entrenamiento. Sin embargo, entrenar m√∫ltiples redes neuronales es prohibitivamente caro.</li>
<li>Incluso si entrenamos N redes neuronales diferentes, ejecutar el punto de datos a trav√©s de cada uno de los N modelos -en el momento de la prueba- introduce una sobrecarga computacional sustancial.</li>
</ul>
<p>Para entender el <em>drop out</em>, digamos que la estructura de nuestra red neuronal es parecida a la que se muestra a continuaci√≥n (red densa donde todas las neuronas est√°n interconectadas):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ia4legos/Deeplearning/main/images/dropout01.png" width="300" height="200" class="figure-img"></p>
</figure>
</div>
<p>¬øQu√© hace <em>drop out</em>? En cada iteraci√≥n, selecciona aleatoriamente algunos nodos y los elimina junto con todas sus conexiones entrantes y salientes, como se muestra a continuaci√≥n</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/ia4legos/Deeplearning/main/images/dropout02.png" width="275" height="200" class="figure-img"></p>
</figure>
</div>
<p>As√≠, cada iteraci√≥n tiene un conjunto diferente de nodos, lo que da lugar a un conjunto diferente de resultados. Tambi√©n puede considerarse una t√©cnica de conjunto en el aprendizaje autom√°tico. Los modelos de conjunto suelen funcionar mejor que un modelo √∫nico, ya que capturan m√°s aleatoriedad. Del mismo modo, el abandono tambi√©n funciona mejor que un modelo de red neuronal normal.</p>
<p>Con un abandono de 0,5, hay un 50% de posibilidades de que cada neurona participe en el entrenamiento dentro de cada lote de entrenamiento. El resultado es una arquitectura de red ligeramente diferente para cada lote. Equivale a entrenar redes neuronales diferentes en subconjuntos diferentes de los datos de entrenamiento.</p>
<p>Esta probabilidad de elegir cu√°ntos nodos deben abandonarse es el hiperpar√°metro de la funci√≥n de abandono. Como se ve en la imagen anterior, el <em>drop out</em> puede aplicarse tanto a las capas ocultas como a las capas de entrada.</p>
<p>La matriz de pesos se inicializa una vez al principio del entrenamiento. En general, para el lote k-√©simo, la retropropagaci√≥n se produce s√≥lo a lo largo de los caminos de las neuronas presentes para ese lote. Esto significa que s√≥lo se actualizan los pesos correspondientes a las neuronas que est√°n presentes.</p>
<p>En el momento de la prueba, todas las neuronas est√°n presentes en la red. Entonces, ¬øc√≥mo tenemos en cuenta los abandonos durante el entrenamiento? Ponderamos la salida de cada neurona con la misma probabilidad p, proporcional a la fracci√≥n de tiempo que la neurona estuvo presente durante el entrenamiento.</p>
</section>
<section id="normalizaci√≥n-por-lotes" class="level3" data-number="2.8.5">
<h3 data-number="2.8.5" class="anchored" data-anchor-id="normalizaci√≥n-por-lotes"><span class="header-section-number">2.8.5</span> Normalizaci√≥n por lotes</h3>
<p>La normalizaci√≥n en lotes consiste b√°sicamente en a√±adir un paso extra, habitualmente entre las neuronas y la funci√≥n de activaci√≥n, con la idea de normalizar las activaciones de salida. Lo ideal es que la normalizaci√≥n se hiciera usando la media y la varianza de todo el conjunto de entrenamiento, pero si estamos aplicando el descenso del gradiente estoc√°stico para entrenar la red, se usar√° la media y la varianza de cada mini-lote de entrada.</p>
<p>Nota: cada salida de cada neurona se normalizar√° de forma independiente, lo que quiere decir que en cada iteraci√≥n se calcular√° la media y la varianza de cada salida para el mini-lote en curso.</p>
<p>A continuaci√≥n de la normalizaci√≥n se a√±aden 2 par√°metros: un bias como sumando, y otra constante similar a un bias pero que aparece multiplicando cada activaci√≥n. Esto se hace para que el rango de la entrada escale f√°cilmente hasta el rango de salida, lo que ayudar√° mucho a nuestra red a la hora de ajustar a los datos de entrada, y reducir√° las oscilaciones de la funci√≥n de coste. Como consecuencia de esto podremos aumentar la tasa de aprendizaje (no hay tanto riesgo de acabar en un m√≠nimo local) y la convergencia hacia el m√≠nimo global se producir√° m√°s r√°pidamente.</p>
<p>La normalizaci√≥n por lotes es m√°s una t√©cnica de ayuda al entrenamiento que una estrategia de regularizaci√≥n en s√≠ misma. Esto √∫ltimo se logra realmente aplicando algo adicional conocido como <em>momentum</em>. La idea de este <em>momentum</em> es que cuando introduzcamos un nuevo mini-lote de entrada (N muestras procesadas en paralelo) no se usen una media y una desviaci√≥n muy distintas a las de la iteraci√≥n anterior, para lo que se tendr√° en cuenta el hist√≥rico, y se elegir√° una constante que pondere la importancia de los valores del mini-lote actual frente a los valores del anterior. Gracias a todo esto se conseguir√° reducir el sobreajuste.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./10_IntroDL.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducci√≥n</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./30_RMDDL.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Redes multicapa densas con Keras</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2023, IA4LEGOS. Universidad Miguel Hern√°ndez de Elche</div>   
  </div>
</footer>



</body></html>