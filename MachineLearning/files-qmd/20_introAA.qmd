# Introducción al Aprendizaje Automático (AA) {#sec-introAA}

Vivimos en la era de las máquinas inteligentes. En la última década, los sistemas basados en el aprendizaje automático (AA), machine learning (ML) en su nomenclatura en inglés, han pasado de vencer a los mejores jugadores de ajedrez, a influir en la mayoría de nuestras decisiones diarias, como hacen los sistemas de recomendación que encontramos en servicios de streaming como Netlix y Youtube.

Como habitantes del siglo XXI queremos responder a la pregunta que quizá alguna vez te has hecho, sobre qué es el aprendizaje automático, y también por qué te recomendamos estudiarlo. Trataremos de mostrarte qué tipo de problemas resuelve y cuándo es apropiado, o no, utilizar esta tecnología.

El aprendizaje automático es un nuevo paradigma de programación en el que, en lugar de programar explícitamente los ordenadores para que realicen algunas tareas, lo hacemos para que aprendan de los datos encontrando los patrones subyacentes en ellos. En pocas palabras, el aprendizaje automático es la ciencia, o la tecnología, que da a la máquina la capacidad de aprender sobre los datos.

Está intrínsecamente relacionado con el análisis de datos y la estadística. En general, las técnicas de aprendizaje automático son métodos basados en aprovechar al máximo la información disponible en los datos, que utilizan procedimientos y computación informática, junto con conceptos y técnicas de estadística, probabilidad y optimización. El objetivo del aprendizaje automático, la minería de datos o la inteligencia artificial es facilitarnos la vida, para automatizar tareas y tomar mejores decisiones a partir de la información disponible.

En la programación ordinaria, el trabajo del programador consiste en escribir claramente cada una de las reglas que componen la tarea que intenta realizar para tratar los datos. El aprendizaje automático invierte esta situación. Se utilizan los datos y los resultados (o etiquetas) para generar reglas con las que predecir cómo se relacionan entre sí o qué patrones muestran, y poder aplicar esas reglas a otros datos para predecir sus resultados o patrones.

Tomemos un ejemplo del mundo real. Si queremos crear una aplicación que, a partir de una foto de una persona, pueda determinar si lleva o no una mascarilla, abasteceremos al modelo de aprendizaje automático con un buen número de imágenes de personas con y sin mascarilla, cada una de ellas convenientemente identificada, y el modelo derivará (aprenderá) las reglas o patrones que relacionan las imágenes con el hecho de llevar o no una mascarilla. Posteriormente se podrán utilizar esas reglas para reconocer mascarillas en otras imágenes distintas para las que no tenemos una identificación dada y que no fueron usadas por el modelo.

Abordar un problema de reconocimiento de imágenes con programación basada en reglas a definir sería realmente difícil, considerando que hay distintos tipos de máscaras, colores, e infinidad de escenarios posibles sobre los que poder reconocer si hay y dónde empieza y acaba una mascarilla. Con el aprendizaje automático sólo será necesario un buen número de imágenes de personas con y sin mascarillas, en las que de modo manual hayamos identificado si está o no presente una mascarilla. De ahí estaremos a pocos pasos de conseguir un reconocedor de máscaras facial eficaz.

En la actualidad existen dos lenguajes de programación especialmente preparados para las tareas de aprendizaje automático: R y Python. En estos materiales se ha optado por usar R, aunque en muchos cursos avanzados sobre este tema el lenguaje utilizado es Python por su facilidad para trabajar con banos de datos muy granes o por su facilidad para el acceso mediante web scraping a bases de datos en internet. Si el lector está interesado en la versión de python de estos materiales puede consultar la web http://ia4legos.umh.es.

# Problemas típicos del AA {#sec-problemsAA}

El aprendizaje automático ha transformado muchos sectores, desde la banca, la fabricación, el streaming, los vehículos autónomos, la agricultura, etc. De hecho, la mayoría de los productos y servicios tecnológicos que utilizamos a diario poseen algún tipo de algoritmo de aprendizaje automático en su interior.

Estos son los casos de uso más comunes del aprendizaje automático:

-   **Sistemas de recomendación**. Prácticamente todos los sitios web recopilan información de sus clientes (*cookies*) para identificar perfiles en función de su comportamiento e intereses al navegar, y ofrecerles, en consecuencia, recomendaciones y anuncios personalizados. Lo mismo ocurre con servicios de streaming como YouTube y Netflix, o de compras como Amazon.

-   **Detección de fraudes** en tiempo real, que pueden utilizar los bancos y otras organizaciones financieras.

-   **Predicción del reembolso de préstamos** a partir de los datos históricos de los clientes bancarios, para predecir -con antelación a la concesión- si un sujeto concreto será capaz de devolver el crédito.

-   **Diagnóstico de enfermedades y predicción de la tasa de supervivencia**. El aprendizaje automático está encontrando cada vez más su valor en la medicina. Puede ayudar a los profesionales de la medicina a diagnosticar enfermedades en unos pocos minutos a partir de las pruebas diagnósticas digitalizadas realizadas. Estos profesionales también pueden utilizar el aprendizaje automático para predecir la probabilidad de sufrir una enfermedad o de sobrevivir, cómo evolucionará una enfermedad, etc.

-   **Detección de defectos en la industria**. Algunas empresas de fabricación han automatizado la tarea de inspección y utilizan la visión artificial para revisar los productos en busca de defectos, lo que permite reajustar y mejorar los procesos, y en última instancia acelerar la producción y reducir costes. En [2020 State of AI-Based Machine Vision by Landing AI](https://landing.ai/wp-content/uploads/2020/11/MachineVisionSurvey.pdf) se dan más ejemplos del uso de la inteligencia artificial en la industria.

-   **Predicción de bajas**. Las empresas de servicios pueden utilizar el aprendizaje automático para predecir el riesgo de que un cliente cancele su cuenta o suscripción, y en consecuencia ayudarles a definir estrategias con las que mejoren la experiencia de usuario y consigan retener mejor a sus clientes. De hecho, esto se ha utilizado en universidades para predecir la renuncia de los estudiantes y abastecerlos, antes de que se produzca, de soluciones para retenerlos.

-   **Detección de spam**. Casi todos los proveedores de correo electrónico, como Outlook o Gmail, tienen la capacidad de detectar el spam de todos los correos electrónicos entrantes para proteger a los usuarios de promociones falsas y estafas. Este filtrado está basado en aprendizaje automático.

-   **Vehículos autónomos**. Los vehículos autónomos actuales utilizan sistemas de aprendizaje automático y aprendizaje profundo para navegar por las carreteras. Utilizando la visión por ordenador pueden ser capaces de detectar peatones, vehículos, semáforos, así como reconocer señales y otros objetos circundantes.

-   **Clasificación de textos o documentos**. Con el aprendizaje automático es viable identificar el tema del que se trata en un texto o documento, o determinar, por ejemplo, si el contenido de una página web es inapropiado.

-   **Procesamiento del lenguaje natural (PLN)**. Cada día es más común que nuestro procesador de textos nos sugiera palabras con las que continuar el discurso, o correcciones ortográficas o sintácticas. Estos recursos son resultados de algoritmos de aprendizaje automático para el etiquetado de partes del discurso, el reconocimiento de entidades con nombre, el análisis sintáctico libre de contexto o el análisis sintáctico de dependencias. Se trata de problemas de aprendizaje más ricos, conocidos como "*problemas de predicción estructurada*".

-   **Aplicaciones de procesamiento del habla**, como las de reconocimiento de patrones de lenguaje, recuperación de fragmentos, clasificación de información, traducción automática, resumen de textos, identificación de emociones, o incluso generación de lenguaje natural en la interacción de máquinas con humanos.

-   **Aplicaciones de visión por ordenador**. Incluyen el reconocimiento e identificación de objetos, la detección de rostros, el reconocimiento óptico de caracteres (*OCR, Optical Character Recognition*), la recuperación de imágenes basada en el contenido, o la estimación de la postura.

-   **Aplicaciones de biología computacional**. Incluyen la predicción de la función de las proteínas, la identificación de puntos clave en el metabolismo celular, o el análisis de redes de genes y proteínas.

Hay muchas más aplicaciones del aprendizaje automático y la lista podría resultar infinita. Esperamos que esta introducción te haya servido para percibir, a groso modo, el potencial del aprendizaje automático en el mundo actual.

## Cuándo usar y no usar el AA {#sec-useAA}

El aprendizaje automático es una tecnología increíble y ha demostrado muchos éxitos en la resolución de diversos problemas del mundo real. Sin embargo, como cualquier otra tecnología, el aprendizaje automático no es adecuado para resolver todo tipo de problemas. Por lo tanto, es igualmente importante saber cuándo utilizarlo y cuándo no.

¿Cuándo utilizar el aprendizaje automático? El uso de este tipo de aprendizaje es preferible cuando se abordan:

-   Problemas que son demasiado complejos para ser resueltos con programación ordinaria, como por ejemplo el reconocimiento de rostros o la detección de correos spam. Es excesivamente complejo confeccionar todas las reglas de inclusión/exclusión. Para este tipo de problemas, probablemente sea seguro probar el aprendizaje automático.

-   Problemas que implican el razonamiento visual y la comprensión del lenguaje, como el reconocimiento de imágenes, el reconocimiento del habla, la traducción automática, etc. Como veremos más adelante, los problemas de percepción a gran escala o los problemas visuales y de lenguaje suelen ser manejados por sistemas de aprendizaje profundo.

-   Problemas que cambian rápidamente y cuyas características cambian con el tiempo, y es necesario que el sistema siga funcionando bien. El aprendizaje automático es adecuado para este tipo de problemas porque estos algoritmos pueden re-entrenarse con nuevos datos.

-   Los problemas que son claros y tienen objetivos sencillos, como una pregunta de sí/no (ante un problema de clasificación: enfermo/no enfermo) o la predicción de un único número continuo (como la cantidad de electricidad que se consumirá en un día).

Dicho esto, es poco recomendable utilizar el aprendizaje automático en:

-   Problemas de predicción en los que precisas conocer el efecto concreto que tienen las variables que están relacionadas con una respuesta sobre ella. La mayoría de los modelos de aprendizaje automático se consideran cajas negras y no proporcionan reglas fijas para relacionar distintas variables.

-   Problemas de predicción de fenómenos variables y cambiantes con el tiempo y que no siguen patrones preestablecidos fijos, como por ejemplo el comportamiento de la bolsa, que si bien tiene ciertos patrones identificados, su capacidad de fluctuar e invertir de repente una tendencia dada es un fenómeno frecuente.

-   Problemas que puedan ser resueltos con programación ordinaria o con métodos heurísticos simples.

-   Problemas que requieran de una solución única y estable que no precise ser actualizada. Si no va a ser posible capturar de modo continuado información con la que abastecer, re-entrenar los modelos y actualizar las predicciones realizadas por los modelos de aprendizaje automático, carece de sentido utilizarlos. Es preferible acudir a modelos estadísticos clásicos.

El aprendizaje automático sigue transformando procesos que nunca se imaginaron y, sin lugar a dudas, continuará creciendo y aplicándose en más y más ámbitos y problemas.

# Tipos de Aprendizaje Automático {#sec-tiposAA}

En líneas generales, existen 5 tipos principales de sistemas de aprendizaje automático, que son:

-   Aprendizaje supervisado
-   Aprendizaje no supervisado
-   Aprendizaje semi-supervisado
-   Aprendizaje auto-supervisado
-   Aprendizaje por refuerzo

Vamos a repasar todos estos tipos para conseguir una comprensión alta de lo que realmente significan.

## Aprendizaje supervisado {#sec-Asupervised}

La mayoría de las tareas de aprendizaje automático pertenecen al tipo de aprendizaje supervisado. Un modelo de aprendizaje supervisado se entrena con datos etiquetados, esto es, datos que se han observado junto a algún tipo de resultado a predecir, al que llamamos etiqueta (cuando es de tipo categórico) o respuesta (en sentido general o numérico). En otras palabras, un modelo de aprendizaje supervisado utiliza los datos de entrada para establecer una solución que aproxime la respuesta observada (también proporcionada en la entrada).

El ejemplo de reconocimiento de caras que hemos mencionado anteriormente es un buen ejemplo de aprendizaje supervisado. En general, hay dos tipos principales de problemas de aprendizaje supervisado, a saber:

-   Los problemas de clasificación, en los que la tarea a resolver consiste en clasificar, esto es, en atribuir una categoría determinada a un sujeto/registro, de entre varias categorías posibles. Por ejemplo, en salud diagnosticar a un sujeto como enfermo/no enfermo; en reconocimiento de textos, identificar su idioma, o si un correo entrante es spam o no. En este caso la variable respuesta es de tipo categórico.

-   Problemas de regresión, en los que el objetivo es predecir un valor numérico de alguna característica o variable. Por ejemplo, predecir el precio de un coche usado según sus características (marca, antigüedad, prestaciones, ...). En este caso la variable respuesta es de tipo numérico.

Los algoritmos de aprendizaje supervisado incluyen algoritmos poco profundos como la regresión lineal y logística, los árboles de decisión (conocidos como *regression trees*, *decision trees* o *classification trees*), los bosques aleatorios (*random forests*), los K-vecinos más próximos, (*K-Nearest Neighbors, KNN*) y las máquinas de vectores de apoyo (*Super Vector Machine, SVM*).

Dicho esto, hay otros problemas "avanzados" que pueden resolverse con técnicas de aprendizaje supervisado, como:

-   Subtitulado de imágenes, para predecir el título de una imagen determinada.
-   Detección de objetos, para reconocer un objeto en una imagen y dibujar un contorno delimitador a su alrededor.
-   Segmentación de imágenes, para identificar los píxeles que componen cada objeto en una imagen.

Algunas de estas tareas pueden incluir tanto técnicas de clasificación como de regresión. Así por ejemplo, para la detección de objetos es precisa una tarea de clasificación, con la que reconocer el objeto entre los otros en una imagen, y la regresión, para predecir sus coordenadas y poder definir su contorno delimitador.

## Aprendizaje no supervisado {#sec-Anosupervised}

En este caso disponemos exclusivamente de datos genéricos con los que entrenar el modelo, pero no se ha observado a la par una variable respuesta a predecir, y con la que juzgar la bondad del modelo. El objetivo en este tipo de problemas es establecer o identificar patrones de comportamiento en el conjunto de datos disponible. El resultado de estos modelos suele ser una etiqueta de clasificación, esto es, una clase o categoría, para cada uno de los registros en la muestra, en base a que se haya identificado un patrón de comportamiento común a todos aquellos que comparten la misma etiqueta.

Los algoritmos de aprendizaje no supervisado se utilizan principalmente para:

-   Agrupar datos por patrones, como los algoritmos de K-Medias y de agrupación jerárquica.
-   Reducir la dimensión del banco de datos y mejorar su visualización, como el análisis de componentes principales (PCA), o el t-Distributed Stochastic Neighbor Embedding (t-SNE).

## Aprendizaje semi-supervisado {#sec-Asemisupervised}

El aprendizaje semi-supervisado se sitúa entre el aprendizaje supervisado y el no supervisado. En el aprendizaje semi-supervisado se etiqueta una pequeña parte de los datos de entrenamiento, mientras que el resto de los datos no se etiquetan.

El etiquetado de los datos es uno de los mayores retos del aprendizaje automático, dado que requiere de una toma de datos específica para la etiqueta/respuesta, a veces automatizada, pero en otras ocasiones manual, y que obviamente ocasiona costes y puede conllevar errores. Por lo tanto, disponer de procedimientos analíticos que permitan minimizar el número de datos etiquetados, y a la vez utilizar datos no etiquetados para mejorar el ajuste puede generar interesantes beneficios relacionados con la reducción de tiempos y costes vinculados al etiquetado.

El aprendizaje semi-supervisado es más notable en los problemas que implican trabajar con conjuntos de datos masivos, como las búsquedas de imágenes en Internet, el reconocimiento de imágenes y audio, y la clasificación de páginas web. Como se puede imaginar, nadie en su sano juicio se va a dedicar a etiquetar las millones de imágenes que se suben a diario a plataformas de medios sociales como Instagram, o los miles de páginas web que se crean cada día (252.000 según Sitefy). En estos casos, el aprendizaje semi-supervisado puede generar aproximaciones más eficientes en términos de recursos.

## Aprendizaje auto-supervisado {#sec-Aautosupervised}

El aprendizaje auto-supervisado es uno de los tipos de aprendizaje automático más emocionantes por cuanto al tipo de aplicaciones que tiene relacionadas con la visión por ordenador y la robótica. Mientras que el aprendizaje semi-supervisado utiliza una pequeña porción de datos etiquetados, el aprendizaje auto-supervisado utiliza todos los datos sin etiquetar y no requiere de anotaciones manuales, lo que elimina la necesidad de los humanos en el proceso.

La motivación del aprendizaje auto-supervisado es aprovechar la gran cantidad de datos sin etiquetar, generando con ellos etiquetas según su estructura o características y luego entrenar sobre estos datos etiquetados artificialmente, con técnicas de aprendizaje supervisado.

## Aprendizaje por refuerzo {#sec-Areforced}

El aprendizaje por refuerzo es un tipo especial de aprendizaje automático que se aplica sobre todo en robótica y juegos. En el aprendizaje por refuerzo, un sistema de aprendizaje llamado agente puede percibir el entorno, realizar algunas acciones y ser recompensado o penalizado en función de su rendimiento. El objetivo principal del agente es acumular tantas recompensas como sea posible. El agente aprende así la mejor estrategia (política) buscando obtener la máxima recompensa.

El aprendizaje por refuerzo ha protagonizado algunos de los momentos más históricos de la IA. En 2016, *DeepMind AlphaGo*, un sistema de aprendizaje por refuerzo, ganó jugando a *Go*, un complejo juego de mesa que requiere intuición, a Lee Sedol -uno de los mejores jugadores a nivel mundial- en el *Google DeepMind Challenge Match*.

Muchos de nosotros puede que no saquemos el máximo partido al aprendizaje por refuerzo, normalmente debido a la limitación de los recursos y su ámbito de aplicación, pero es una herramienta poderosa en los ámbitos de la robótica y los juegos.

# Conceptos básicos en un problema de aprendizaje automático {#sec-conceptsAA}

A continuación, utilizamos el problema de la detección de spam como ejemplo práctico para ilustrar algunas definiciones básicas y describir el uso y la evaluación de los algoritmos de aprendizaje automático en la práctica, incluyendo sus diferentes etapas.

La detección de spam consiste en clasificar automáticamente los mensajes de correo electrónico como spam o correo regular.

-   **Muestra**: es el conjunto de datos recopilados para dar respuesta a un problema dado, y que se utilizan para el aprendizaje o la evaluación del modelo con el que conseguir dicha respuesta. Es común en jerga de aprendizaje automático, aludir como "muestras" a las observaciones individualizadas.

En nuestro problema de spam, las muestras son la colección de mensajes de correo electrónico disponibles para el análisis.

-   **Variables predictoras**: es el conjunto de atributos o características disponibles sobre cada uno de los datos o registros de la muestra. A menudo se representan, para cada dato con un vector que contiene información sobre las características observadas y que se utilizan como inputs del modelo de aprendizaje. Estas variables pueden ser de tipo numérico o categórico y recibirán tratos distintos en los modelos de aprendizaje. En el lenguaje habitual en aprendizaje automático, las variables predictoras se denominan **características**.

En el ejemplo de spam, algunas características relevantes pueden ser la longitud del mensaje, el nombre o dominio del remitente, diversas características del encabezado, la presencia de ciertas palabras clave en el cuerpo del mensaje, etc.

-   **Variable respuesta**: Identifica la característica objetivo observada en la muestra, que tratamos de predecir a partir de las variables predictoras recopiladas mediante un modelo de aprendizaje. También es reconocida como *target* en la bibliografía. Si la respuesta es de tipo numérico hablamos de problemas de regresión, y si es de tipo categórico hablamos de un problema de clasificación, y se reconoce también como etiqueta. En los modelos de aprendizaje no-supervisado no disponemos de variable respuesta.

En el problema de correo spam la respuesta es una variable categórica binaria con dos posibles resultados spam y no spam, cuyos valores han sido confirmados a través de una revisión manual de los mensajes disponibles. El objetivo será predecirla en nuevos correos, sin necesidad de hacer un reconocimiento explícito, sino a través de aquellas características con las que se relaciona.

-   **Parámetros**: son los valores desconocidos del modelo de aprendizaje que pretendemos estimar y que aproximan las relaciones entre las variables predictoras, o explican su relación con la variable respuesta.

En el problema del correo spam los parámetros se corresponderían con los pesos que el modelo asigna a cada una de las predictoras a la hora de establecer su influencia en la clasificación de un correo como spam o no.

-   **Hiperparámetros**: Se trata de parámetros libres que se pueden variar para reajustar el algoritmo de aprendizaje en busca de soluciones alternativas más adaptadas a las muestras disponibles. Todos los algoritmos utilizan unos valores por defecto para los hiperparámetros.

-   **Muestra de entrenamiento**: conjunto de datos utilizados para entrenar un algoritmo de aprendizaje, esto es, para estimar parámetros y generar una primera respuesta al problema planteado. El tamaño recomendado para la muestra de entrenamiento varía en función de los distintos escenarios de aprendizaje, si bien suele representar en torno al 70% del total de los datos disponibles o muestra total del modelo.

En nuestro problema de spam, la muestra de entrenamiento consiste en un conjunto de correos electrónicos que proporcionan datos sobre todas las variables predictoras identificadas como relevantes, y también sobre la variable respuesta, esto es, han sido identificados explícitamente como spam o correo regular.

-   **Muestra de validación**: Es el conjunto de datos utilizado tras el entrenamiento del modelo, para refinar las estimaciones de los parámetros del algoritmo de aprendizaje en muestras con variable respuesta. La muestra de validación se utiliza para reajustar el modelo (en términos de hiperparámetros), para hacerlo más eficiente y preciso en la respuesta que da. Su volumen suele representar en torno al 20% de la muestra disponible.

-   **Muestra de test**: Es el conjunto de datos utilizados para evaluar el rendimiento de un algoritmo de aprendizaje, separado de los datos de entrenamiento y validación, y no está disponible en la fase de aprendizaje. Puede representar en torno al 10% del total de datos disponibles.

En el problema del spam, la muestra de test consiste en una serie de muestras de correo electrónico para los que el algoritmo de aprendizaje debe predecir etiquetas basándose en las características. Estas predicciones se comparan con las etiquetas de la muestra de test para medir el rendimiento del algoritmo.

-   **Función de pérdida**: Función que mide la diferencia, o pérdida, entre el valor de la respuesta predicha por el modelo de aprendizaje y el valor de la respuesta observada. Las funciones de pérdida tienen sentido en los modelos de aprendizaje supervisado pero no así en los de aprendizaje no supervisado.

# Flujo de trabajo en un problema de AA {#sec-flujoAA}

Aunque cada problema de aprendizaje automático es único, todos siguen un flujo de trabajo similar. En esta sección, aprenderemos a abordar los problemas de aprendizaje automático de forma sistemática.

En general, el flujo de trabajo típico de un proyecto de aprendizaje automático consiste en:

1.  Definir y formular un problema.
2.  Recoger datos.
3.  Establecer un modelo de partida.
4.  Análisis exploratorio de datos (AED).
5.  Preprocesar los datos.
6.  Seleccionar y entrenar un modelo.
7.  Realizar un análisis de errores y mejora del modelo.
8.  Evaluar el modelo.
9.  Implantar y automatizar el modelo.

A continuación describimos brevemente cada etapa.

## Definir un problema {#sec-problemaAA}

Todo comienza aquí. La definición del problema es el paso inicial crucial en cualquier proyecto de aprendizaje automático. Aquí es donde te aseguras de entender el problema realmente bien. Entender el problema te dará las intuiciones adecuadas sobre los pasos a seguir, algoritmos a utilizar, etc. Pero espera, ¿qué significa entender el problema?

Entender el problema consiste en profundizar en los detalles del problema en cuestión y formular las preguntas adecuadas. En primer lugar, siempre será importante simplificar el problema o seccionarlo en varios problemas más sencillos que nos permitan concretar objetivos claros y abordables. He aquí ejemplos de objetivos sencillos: clasificar productos en diferentes categorías, predecir el precio de un coche usado dadas sus características (como la marca, la edad, etc...), reconocer si una persona lleva una máscara facial, dividir a los clientes en diferentes grupos que comparten comportamientos similares, etc... Formular el objetivo generalmente nos conducirá a formular el problema como un problema de clasificación, de regresión, de agrupación, etc.

En esta fase es fundamental evitar expresiones vagas para la formulación de las preguntas a responder. Cuanto más simple sea la formulación del problema, mejor irán las cosas en el futuro. También es preciso evaluar si el proyecto es abordable, o no, mediante aprendizaje automático.

La definición del problema también conlleva reflexionar sobre los datos que se necesitan para resolverlo. Los modelos de aprendizaje automático se basan en datos, y con datos defectuosos sólo conseguiremos modelos defectuosos. ¿Qué información tenemos o de cuál podemos disponer?, ¿tenemos datos para cada una de las preguntas que hemos formulado? Hacernos estas preguntas y reajustar los objetivos o la recogida de datos conforme a ellas será fundamental para garantizar el éxito. Modelizar y resolver problemas con modelos NO es magia. Sólo podremos identificar patrones y predecir eficientemente si los datos que utilizamos contienen información significativamente relevante para ello.

## Recoger datos {#sec-recogerdatos}

Esta suele ser la siguiente etapa tras la formulación de un problema (no podemos obviar que en ocasiones se recopilan datos y después se plantea qué se puede hacer con ellos). Antes de entrar en detalle sobre la recogida de datos, vamos a repasar el significado de "dato". Según Wikipedia, "los datos son un conjunto de valores de variables cualitativas o cuantitativas sobre una o varias personas u objetos". En nuestro caso, cuando hablemos de datos estaremos hablando de una serie de registros recopilados para un conjunto de variables, y a veces también para alguna variable respuesta (que ya identificamos también como "etiqueta", cuando describimos el aprendizaje supervisado).

Hay 2 tipos principales de datos que son:

-   **Datos estructurados**, que se pueden organizar/registrar en formato tabular u hoja de cálculo. Ejemplos de datos tabulares son los registros de clientes, las ventas de coches, etc.

-   **Datos no estructurados**, como imágenes, textos, sonidos y vídeos. Los datos no estructurados no están organizados como los anteriores.

Hoy en día hay muchas bases de datos en abierto, en plataformas como [*Kaggle*](https://www.kaggle.com/), [Conjuntos de datos de Google](https://cloud.google.com/datasets), [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php) y muchos otros sitios web gubernamentales (fuera de España fundamentalmente). Así que, si estás resolviendo un problema que alguien resolvió antes, o alguno similar, es muy probable que encuentres información en algún lugar de esas plataformas o en otras fuentes públicas. Inicia pues tu análisis, buscando.

Dicho esto, hay veces que tendrás que recoger tus propios datos, especialmente si estás resolviendo un problema específico que nadie ha resuelto antes. En este caso, ten en cuenta el tiempo a dedicar en la recogida de datos y sus costes. Ten también en cuenta que a veces no es necesario tener todos los registros deseados para poder empezar a resolver; adopta la dinámica de Machine Learning desde el inicio para ir aprendiendo si necesitas más datos.

Además, al recopilar los datos, es siempre preferible la calidad a la cantidad. Hay veces que pocos datos buenos pueden generar soluciones superiores a las que generan muchos datos pobres. La cantidad de datos que necesites va a depender del problema que a resolver y de su alcance, pero ten presente siempre como objetivo el conseguir datos de la mejor calidad posible.

## Establecer un modelo de partida {#sec-modelobasal}

Sin un modelo de partida será inviable evaluar los resultados, y mucho menos las mejoras que se consigan. Un modelo de partida es el modelo más sencillo que puede resolver un problema dado, con unos requisitos mínimos. No tiene por qué ser un modelo estrictamente hablando, y podría ser una aplicación de código abierto existente, un análisis estadístico, o incluso meras intuiciones que se obtienen de los datos a partir de un análisis preliminar.

El objetivo principal del modelo base o de partida es servir como punto de referencia para comparar el modelo de aprendizaje y evaluar las mejoras que genera. El objetivo final es superar al modelo base, en términos de reducción de pérdidas que se cuantificarán, como veremos más adelante, con la función de pérdida, y nos permitirán evaluar la bondad de un modelo. En caso de no poder mejorar el modelo de partida, deberemos entender que el modelo de aprendizaje no merece la pena, y bastará con asumir el modelo de partida. En ocasiones no serán posibles las mejoras del modelo base porque la recogida de datos no haya sido la adecuada y haya generado datos de mala calidad, o simplemente no se haya identificado ninguna variable predictora que realmente influya en la respuesta. Reiteramos que la calidad de los modelos de aprendizaje que generemos dependerá, en buena parte, de la calidad de los datos disponibles.

## Análisis exploratorio de datos (AED) {#sec-aedenAA}

Antes de manipular los datos, es muy importante inspeccionarlos. Esto podría obviarse, pero hacerlo y hacerlo bien ayudará a identificar mejor estrategias eficaces para limpiar los datos e incrementar su calidad.

El análisis exploratorio de los datos consiste en revisar los valores para descubrir si hay

-   incoherencias o errores
-   valores faltantes
-   carencias
-   valores corrompidos o con formatos no soportados (por ejemplo una imagen en un fichero .txt. y con 0Kb)
-   desequilibrios de clase
-   duplicados
-   sesgos

Realizar un análisis exploratorio va incluso más allá de estas cuestiones y utiliza representaciones gráficas y numéricas que permitan descubrir cómo son los datos (cómo se distribuyen, qué variabilidad tienen, ...), qué tipo de relación y correlación hay entre las variables disponibles, etc.

Dado que en los modelos de aprendizaje automático se seccionan los datos en tres subconjuntos o muestras, entrenamiento, validación y prueba o testado, hay que asegurar, mediante un análisis exploratorio, que estas tres muestras compartan la misma distribución estadística, pues de no ser así, los resultados obtenidos con los datos de entrenamiento no serán extrapolables al resto y el error se disparará.

## Preprocesar los datos {#sec-preprocesado}

En muchos libros de texto y cursos, el preprocesamiento de datos también se denomina limpieza de datos o preparación de datos.

El preprocesado de datos es quizás el proceso que consume la mayor parte del tiempo en cualquier proyecto de aprendizaje automático. No es inusual que esta parte consuma alrededor del 80% del tiempo total de la modelización, y esto siempre será así porque los datos del mundo real están desordenados y contienen errores.

Preprocesar los datos implica convertir los datos en bruto en un formato que pueda ser aceptado por los algoritmos de aprendizaje automático.

El preprocesamiento de datos es difícil porque hay diferentes tipos de datos y diferentes circunstancias de recogida de datos, y la forma de procesar cada uno generalmente será diferente al resto. Por ejemplo, en datos estructurados, la forma de procesar las características numéricas va a ser diferente a las características categóricas. También en los datos no estructurados, la forma de manipular las imágenes va a ser diferente a la forma de manipular los textos o los sonidos.

Tratamientos específicos para el procesado de datos los iremos encontrando a lo largo del módulo formativo. En términos generales hay una serie de tareas de preprocesado muy comunes en todos los problemas, que son las que comentamos a continuación.

-   **Imputación de valores perdidos**: los valores perdidos o faltantes en un banco de datos pueden rellenarse, eliminarse o dejarse como están. Con la excepción de los modelos basados en árboles, la mayoría de los modelos de aprendizaje automático no aceptan valores perdidos, con lo cual el hecho de que un banco de datos tenga valores faltantes puede ser un problema. Surge así la imputación como un método interesante para evitar los valores perdidos. Hay varias estrategias de imputación como la media o la mediana de los valores restantes, el relleno hacia atrás y hacia delante con valores colindantes cuando los datos tienen un carácter temporal, y las imputaciones iterativas. La técnica de imputación adecuada dependerá del problema y de las características de los datos, y habitualmente con la práctica el profesional adquiere experiencia para utilizar la técnica óptima.

-   **Codificación de características categóricas**: en los modelos de aprendizaje automático las variables de tipo categórico han de contener etiquetas numéricas para identificar las categorías. Esto hace necesario recodificar las variables categóricas, que generalmente no vienen expresadas en términos de números. Las técnicas más comunes de codificación son la codificación por etiquetas y la codificación en caliente. Por ejemplo, la variable de sexo, con categorías Hombre y Mujer, se puede codificar con etiqueta 0 para el Hombre y 1 para la Mujer, o en caliente, en la que se elige una categoría que recibe el 1 (por ejemplo Hombre) y se asigna 0 a todas las restantes.

-   **Escalar las características numéricas**: la mayoría de los modelos de aprendizaje automátio funcionan bien cuando los valores de entrada se escalan a valores pequeños, pues de este modo se entrenan y convergen más rápido a una solución. Hay dos técnicas principales de escalado, que son la normalización y la estandarización. La normalización reescala las características a valores entre 0 y 1, mientras que la estandarización reescala las características para que tengan una media de 0 y una desviación estándar de 1. Si los datos tienen una distribución normal, la estandarización puede ser una buena opción; en otro caso, o simplemente si no se sabe cuál es la distribución, la normalización funcionará bien.

-   **Definir nuevas variables**: la definición de nuevas variables a partir de las existentes, que contengan información relevante, incluso a veces más significativa que la que contienen algunas variables originales, también forma parte del preprocesamiento de datos. Se trata de una tarea creativa y requiere un conocimiento y experiencia adicional del modelador o analista.

## Seleccionar y entrenar un modelo {#sec-entrenamiento}

Seleccionar, crear y entrenar, esto es, ajustar, un modelo de aprendizaje automático es la parte menor, en cuanto a carga de trabajo, en un proyecto de aprendizaje automático. Hay diferentes tipos de modelos, pero en líneas generales, la mayoría de ellos entran en estas categorías: **modelos lineales** como la regresión lineal y logística, **modelos basados en árboles** como los árboles de clasificación, **modelos de conjunto** como los bosques aleatorios y, por último, las **redes neuronales**.

Dependiendo del problema, se puede elegir uno de estos modelos, o probar varios. En general hay que experimentar con diferentes modelos alternativos para conseguir, al final, uno que funcione razonablemente bien para el problema y conjunto de datos con el que se está trabajando.

Para reducir la curva de modelado, esto es, el tiempo que se tarda en conseguir un modelo óptimo, existen una serie de cuestiones a considerar para elegir cómo abordar el aprendizaje automático:

-   **El objetivo y tipo de datos** pueden dar señales importantes sobre qué algoritmo de aprendizaje utilizar. Por ejemplo, si se pretende conseguir un clasificador de imágenes, las redes neuronales (concretamente las redes neuronales convolucionales) son las herramientas preferidas.

-   **El tamaño de la base de datos**. Los modelos lineales tienden a funcionar bien en problemas con pocos datos, mientras que los modelos de conjunto y las redes neuronales pueden funcionar bien cuando se dispone de bases con muchos datos y variables.

-   **El nivel de interpretabilidad deseado**. Si se pretende que las predicciones del modelo sean explicables, un modelo lineal o de árbol puede dar una buena solución, pero no será así si se elige un modelo de redes neuronales.

-   **El tiempo de entrenamiento** y la capacidad computacional para ajustar un modelo. Los modelos complejos, como las redes neuronales y los modelos de conjunto, requieren importantes recursos de memoria y suelen tardar más tiempo en entrenarse. En cambio los modelos lineales suelen entrenarse más rápidamente.

## Análisis de errores {#sec-errores}

El análisis de los errores nos debe guiar en el proceso de ajuste para mejorar los resultados del modelo. Las mejoras pueden provenir de los datos o del modelo.

Una de las mejores maneras de realizar el análisis de errores es trazar la curva de aprendizaje e identificar dónde está fallando el modelo y cuál podría ser la razón, así como las acciones correctas que se pueden tomar para reducir los errores.

Para mejorar el modelo se pueden probar diferentes alternativas para la configuración del modelo a través de los hiperparámetros. También se pueden probar diferentes modelos hasta encontrar uno que funcione mejor.

Pero además, hay que tener en cuenta que no puede haber un buen modelo sin unos buenos datos, por lo que es importante dedicar tiempo a examinar los resultados del modelo con respecto a los datos de entrada, inspeccionar si va mal en general o sólo para un subconjunto de los datos, y en general cuál es el margen de mejora.

A menudo, las mejoras no vendrán de afinar el modelo, sino de dedicar tiempo a aumentar el volumen de la muestra de entrenamiento y la calidad de los datos. Para mejorar los datos también se pueden crear variables artificiales más informativas a partir de las disponibles o aumentar los datos (por ejemplo aplicación de filtros en imágenes), entre otros.

Con todo es importante tener presente que el análisis de errores es un proceso iterativo basado en el ensayo error hasta conseguir un modelo mejor o quizás simplemente aceptable.

## Evaluar el modelo {#sec-evaluar}

En un modelo de aprendizaje automático, los datos se suelen dividir en varios subconjuntos, cada uno de los cuales se utiliza en una etapa de la modelización. El conjunto de datos de entrenamiento se utiliza para entrenar un modelo, el conjunto de validación para evaluar el rendimiento del modelo durante el entrenamiento, con el que sugerir mejoras, y el conjunto de prueba, que se utiliza para evaluar el rendimiento final y la mejora del modelo.

Cuando se ha seleccionado un modelo que funciona, es habitual evaluarlo con la muestra de validación. Si el modelo ofrece buenos resultados sobre estos datos, ajenos a aquellos con los que se ha ajustado (entrenado) el modelo, es el momento de pasar a la fase de test, si se dispone de una muestra de test, o de implementar el modelo de aprendizaje para que continúe aprendiendo conforme se recoja nueva información.

Los sistemas de recomendación en plataformas de comercio online son un buen ejemplo de modelos que continúan aprendiendo conforme se nutren de más datos de los usuarios.

## Automatizar el modelo {#sec-automatizar}

La automatización o implementación del modelo es la última parte del flujo de trabajo en un análisis abordado con aprendizaje automático. Cuando todos los pasos anteriores han ido bien y se ha alcanzado un modelo con buenos resultados en la muestra de validación, el siguiente paso será implementar el modelo para que los usuarios puedan empezar a hacer uso de él, retroalimentando con nuevos datos con los que obtener mejores predicciones y servicios.

# Métricas de evaluación {#sec-metricas}

Las métricas de evaluación se utilizan para medir el rendimiento de los modelos de aprendizaje automático. Al principio de esta introducción al aprendizaje automático comentamos que la mayoría de los problemas se refieren a regresión y clasificación; en función del objetivo y tipo de respuesta, la evaluación del modelo será diferente. Veamos las alternativas.

## Métricas para problemas de regresión {#sec-metricasregresion}

En las tareas de regresión el objetivo es predecir el valor numérico de la variable respuesta (identificada también como *target* en la literatura). Así pues, las métricas que surgen de manera natural para evaluar estos modelos son las basadas en distancias entre las observaciones y las predicciones.

La diferencia entre el valor real de la respuesta ($y$) y el valor predicho ($\hat{y}$) se llama **error de predicción**:

$$Error = y - \hat{y}$$ {#eq-error}

El cuadrado del error de predicción sobre los datos observados $\{y_1,y_2,..., y_n\}$, se llama **error cuadrático medio** (MSE o *Mean Squared Error*, en inglés), y se calcula como:

$$MSE = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}$$ {#eq-MSE}

Otra métrica común es la **raíz cuadrada del error cuadrático medio** (RMSE). El RMSE es la métrica de regresión más utilizada.

$$RMSE = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}}$$ {#eq-RMSE}

Hay veces que se trabaja con conjuntos de datos que contienen valores atípicos. Una métrica adecuada para este tipo de conjuntos de datos es el **error medio absoluto** (MAE, *Mean Absolute Error*) que se calcula como:

$$MAE = \frac{\sum_{i=1}^n |y_i - \hat{y}_i|}{n}$$ {#eq-MAE}

## Métricas para problemas de clasificación {#sec-metricasclasif}

En los problemas de clasificación el objetivo es predecir correctamente la categoría de clasificación de cada observación. Así pues, las métricas naturales para evaluar estos modelos estarán basadas en contabilizar las coincidencias entre la clasificación correcta y la conseguida o predicha con el modelo.

Para definir estos indicadores partimos de identificar las clasificaciones posibles, cuestión que salvamos con una **matriz de confusión**, esto es, una tabla que muestra el número de predicciones correctas e incorrectas realizadas por un clasificador en todas las clases disponibles. En el caso de una respuesta binaria, 0=Negativo y 1=Positivo en relación al hecho de que se dé determinada característica, las clasificaciones/predicciones posibles sobre el total de datos o registros disponibles son las que se muestran en la siguiente matriz:

|                     |            | Valor Predicho |            |
|---------------------|------------|----------------|------------|
|                     |            | *Negativo*     | *Positivo* |
| **Valor observado** | *Negativo* | TN             | FP         |
|                     | *Positivo* | FN             | TP         |

Se identifican entonces, las siguientes cantidades de interés:

-   **TP**, *True Positive* o Verdaderos positivos, que es el número de registros que se clasifican correctamente como positivos.

-   **FP**, *False Positive* o Falsos positivos es el número de registros que siendo negativos (0), se clasifican de modo incorrecto como positivos (1).

-   **TN**, *True Negative* o Verdaderos negativos es el número de registros negativos (0) que con correctamente clasificados como negativos.

-   **FN**, *False Negative* o Falsos negativos es el número de registros positivos (1) que son clasificados incorrectamente (con 0).

En base a estos números se definen las siguientes métricas de evaluación de los modelos de aprendizaje basados en clasificación:

-   La **exactitud** (representado en la literatura por A, de *accuracy*) es la métrica más utilizada y muestra la capacidad del modelo para hacer predicciones correctas, es decir, el ratio de observaciones clasificadas correctamente. Se calcula a partir de la proporción de observaciones clasificadas correctamente (TP + TN sobre el total), esto es,

$$Exactitud (A)= \frac{TP + TN}{TP + TN + FP + FN}.$$ {#eq-accuracy}

La exactitud es un indicador de la bondad del modelo para hacer predicciones correctas, por lo que sólo será útil cuando el conjunto de datos esté repartido de forma equilibrada entre las categorías de respuesta.

Supongamos que hemos construido un clasificador de imágenes que distingue entre caballos y humanos, con 250 imágenes de caballos y otras 250 de humanos. Si el modelo predice correctamente 400 imágenes en total (caballos y humanos), su exactitud es de 0.8, o lo que es lo mismo, del 80% en términos porcentuales.

Cuando tenemos un conjunto de datos sesgado o desequilibrado hacia alguna categoría, necesitamos una perspectiva diferente sobre cómo evaluar el modelo. Por ejemplo, si tenemos 450 imágenes de caballos y 50 imágenes de humanos, hay una probabilidad del 90% (450/500) de que el caballo se prediga correctamente porque el conjunto de datos está dominado por los caballos. Y aunque la predicción sea nefasta para los humanos, la exactitud seguirá siendo buena.

Surgen pues otras métricas que, en estas circunstancias, serán más útiles que la exactitud, como la precisión, el recuerdo y la puntuación F1.

La **precisión** (representado habitualmente como P, de *precision*) representa el porcentaje de observaciones correctamente clasificadas, de entre las que se han clasificado como positivas.

$$Precisión (P)= \frac{TP}{TP + FP}.$$ {#eq-precision}

Por ejemplo, nos podría interesar especialmente saber, de las imágenes que se clasificaron como humanos, cuántas eran realmente de humanos.

Por otro lado, el **recuerdo** (representado por R, de *recall*) muestra el porcentaje de las observaciones que siendo positivas se clasificaron correctamente, y nos resulta útil cuando nos interesa especialmente la clasificación correcta de una determinada clase.

$$Recuerdo (R)= \frac{TP}{TP+FN}.$$ {#eq-recall}

En el ejemplo de caballos y humanos, descubriríamos, con el recuerdo, de las imágenes que eran de humanos, qué porcentaje fueron correctamente clasificadas como humanos.

Dado que existe cierta relación entre la precisión y la recuperación, y a menudo el aumento de una conlleva la disminución de la otra, se propuso otra métrica construida a partir de las dos, para conjugar las ventajas de ambas.

La puntuación F1 (o *F1 Score*) es la media armónica de la precisión y la recuperación, y muestra lo bueno que es el modelo a la hora de clasificar todas las clases sin tener que equilibrar la precisión y la recuperación. Si la precisión o la recuperación son muy bajas, la puntuación F1 también lo será.

$$F1 = \frac{2 \cdot P \cdot R}{P + R}.$$ {#eq-f1}

# Retos en los sistemas de aprendizaje automático {#sec-retos}

Al igual que cualquier otra tecnología, el aprendizaje automático ofrece desafíos relacionados tanto con la mejora de los datos como con la mejora de los modelos de ajuste.

No puede haber buenos modelos con malos datos, pero es un hecho que en el mundo real es raro encontrar "buenos datos" a la primera. Por lo general los datos presentan desorden, errores, incoherencias, y se convierte en todo un arte la labor de procesarlos (depurar, etiquetar, e incluso crear nuevas variables) para conseguir una base de datos apta para iniciar el ajuste de un modelo.

Por otro lado, una vez disponemos de una base de datos razonablemente buena, el ajuste del modelo no es una labor trivial. En principio se inicia la labor de elegir qué modelo o modelos son razonables para los objetivos y el tipo de datos. A continuación hay que ajustarlos y compararlos para tomar una decisión.

Cuando ajustamos un modelo lo haremos sobre una submuestra del banco de datos, la muestra de **entrenamiento**, que si no está bien seleccionada y contiene todas las características de la población que representa, ocasionará sesgos. Otra parte de la muestra se dedica a la **validación**, es decir, a evaluar el error que genera el modelo ajustado en la predicción, y utilizarlo para volver sobre el ajuste y afinarlo en pos de reducirlo. Se puede optar también por utilizar otra parte de la muestra para la evaluación o **testado** del modelo que se ha ajustado. En la bibliografía encontramos diferentes recomendaciones sobre el porcentaje de la muestra que ha de dedicarse a cada parte. Una opción puede ser 70/20/10, pero es el científico de datos el que decide, en función de utilizar el mayor número de datos para entrenar el modelo (y conseguir así una buena representación de las características de la población a ajustar), y también en función de no sobrecargar el aprendizaje con los cálculos del error de validación. La muestra final de testado podría tener una representación mayor si el objetivo es concluir sobre la bondad de un modelo en la predicción de datos que no ha visto antes, esto es, que no ha utilizado para entrenar o validar.

Ya en el ajuste se nos pueden presentar dos tipos de problemas: el infra-ajuste (*underfitting*) y el sobre-ajuste (*overfitting*). Veamos en qué consisten.

El **infra-ajuste** o infra-adaptación del modelo se produce cuando el modelo da malos resultados (en términos de métricas de error) con los datos de entrenamiento; por ejemplo, una clasificación correcta de sólo el 50% de la muestra. Un problema de infra-ajuste ocasiona un problema de sesgo en la predicción. Esto puede ser causado porque el modelo es demasiado simple para los datos de entrenamiento o los datos no contienen las características que se están tratando de predecir. Para reducir este tipo de problema se puede optar por:

-   Ajustar modelos más complejos. Modelos más sencillos, como los modelos lineales, pueden dar resultados más simples que los más complejos, como los bosques aleatorios, las máquinas de vectores de apoyo, o las redes neuronales.

-   Añadir más datos de entrenamiento y seleccionar mejor las mejores características para predecir.

-   Aumentar la complejidad de los modelos (lo que se conoce como "reducir el nivel de regularización del modelo"). Por ejemplo, si se utilizan redes neuronales, incrementar el número de épocas o iteraciones de entrenamiento.

El **sobre-ajuste** o sobre-adaptación se produce cuando un modelo funciona muy bien con los datos de entrenamiento, pero no con datos nuevos que el modelo no utilizó para ajustar. Esto repercute en un inflado de la varianza de la muestra de validación.

El sobre-ajuste se puede deber a que el modelo es demasiado complejo para los datos disponibles o el volumen de la muestra de entrenamiento es demasiado pequeño. Para aliviar este tipo de problema, podemos intentar:

-   Usar modelos más sencillos, o simplificar el modelo actual a través de los hiperparámetros de los que dependen (reducir el número de capas en las redes neuronales, simplificar el kernel en las máquinas de vectores de apoyo, reducir el número de predictores en los modelos lineales, ...).

-   Utilizar más datos de entrenamiento para el ajuste.

-   Aumentar el nivel de regularización del modelo, o lo que es lo mismo, reducir la complejidad del modelo, penalizando a los modelos que generan un error excesivamente pequeño para la muestra de entrenamiento.

Tanto el sobre-ajuste como el infra-ajuste lo podemos detectar evaluando el error en las muestras de entrenamiento y de validación cuando hacemos crecer el tamaño de la muestra de entrenamiento progresivamente, y en consecuencia reducimos la de validación. Lo lógico es que cuando incrementamos la muestra de entrenamiento reduzcamos el error del ajuste en dicha muestra, a cambio de que provoquemos un aumento en el error de validación. Si ambos errores se encuentran (convergen) en un punto, significa que tenemos un problema de infra-ajuste y la curva de aprendizaje se detiene pronto. Si al aumentar el tamaño llega un punto en que ambos se estabilizan pero se mantienen alejados entre sí, tenemos un problema de sobre-ajuste.

Para terminar, hemos de comentar la necesidad de tener una buena formación en aprendizaje automático. Seremos más capaces de ajustar buenos modelos de aprendizaje automático si tenemos un amplio conocimiento de las distintas opciones en cuanto a modelos, y una comprensión avanzada de cómo funciona cada uno de ellos. Podremos proponer entonces alternativas (quizás a través de hiperparámetros del modelo que trabajamos) que mejoren los resultados previos que vamos consiguiendo. Aunque hay técnicas específicas que nos ayudan y simplifican la búsqueda y elección de hiperparámetros (como el *grid search*, *random search*, *Keras tuner*), es fundamental comprender el modelo para tomar buenas decisiones al respecto.

Hasta aquí la introducción sobre los fundamentos del aprendizaje automático. A medida que vayamos desarrollando contenidos, entraremos en detalle en los procedimientos y descubriremos sus aplicaciones.
